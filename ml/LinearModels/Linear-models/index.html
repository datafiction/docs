<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../../img/favicon.ico">
  <title>Linear Regression - Machine Learning</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>
  <link href='https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Linear Regression";
    var mkdocs_page_input_path = "ml/LinearModels/Linear-models.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../../.." class="icon icon-home"> Machine Learning</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="tocbase current">
    
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../../..">Home</a>
  </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">Machine Learning</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  
    
    <li class="navtree toctree-l2 page current">
      <a class="current" href="./">
        Linear Regression
          <span class="toctree-expand"></span>
      </a>
    </li>
    
      

  <li class="toctree-l2 current with-children">
    <a href="#linear-model">
      Linear model
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l2 current">
    <ul class="subnav-l2 current">
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#introduction">Introduction</a>
        </li>
    
      
          

  <li class="toctree-l3">
    <a href="#examples">
      Examples
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l3">
    <ul class="subnav-l3 toc-hidden">
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#1-linear-regression-example-source">1. Linear Regression Example (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#2-sparsity-example-fitting-only-features-1-and-2-source">2. Sparsity Example: Fitting only features 1  and 2 (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#3-ordinary-least-squares-and-ridge-regression-variance-source">3. Ordinary Least Squares and Ridge Regression Variance (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#4-plot-ridge-coefficients-as-a-function-of-the-l2-regularization-source">4. Plot Ridge coefficients as a function of the L2 regularization (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#5-plot-ridge-coefficients-as-a-function-of-the-regularization-source">5. Plot Ridge coefficients as a function of the regularization (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#6-bayesian-ridge-regression-source">6. Bayesian Ridge Regression (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#7-logistic-regression-3-class-classifier-source">7. Logistic Regression 3-class Classifier (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#8-l1-penalty-and-sparsity-in-logistic-regression-source">8. L1 Penalty and Sparsity in Logistic Regression (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#9-path-with-l1-logistic-regression-source">9. Path with L1- Logistic Regression (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#10-logistic-function-source">10. Logistic function (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#11-plot-multinomial-and-one-vs-rest-logistic-regression-source">11. Plot multinomial and One-vs-Rest Logistic Regression (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#12-lasso-path-using-lars-source">12. Lasso path using LARS (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#13-lasso-and-elastic-net-source">13. Lasso and Elastic Net (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#14-lasso-model-selection-cross-validation-aic-bic-source">14. Lasso model selection: Cross-Validation / AIC / BIC (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#15-lasso-and-elastic-net-for-sparse-signals-source">15. Lasso and Elastic Net for Sparse Signals (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#16-joint-feature-selection-with-multi-task-lasso-source">16. Joint feature selection with multi-task Lasso (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#17-lasso-on-dense-and-sparse-data-source">17. Lasso on dense and sparse data (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#18-plot-multi-class-sgd-on-the-iris-dataset-source">18. Plot multi-class SGD on the iris dataset (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#19-sgd-penalties-source">19. SGD: Penalties (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#20-comparing-various-online-solvers-source">20. Comparing various online solvers  (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#21-sgd-convex-loss-functions-source">21. SGD: convex loss functions (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#22-sgd-maximum-margin-separating-hyperplane-source">22. SGD: Maximum margin separating hyperplane (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#23-sgd-weighted-samples-source">23. SGD: Weighted samples  (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#24-polynomial-interpolation-source">24. Polynomial interpolation (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#25-orthogonal-matching-pursuit-source">25. Orthogonal Matching Pursuit (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#26-automatic-relevance-determination-regression-ard-source">26. Automatic Relevance Determination Regression (ARD) (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#27-huberregressor-vs-ridge-on-dataset-with-strong-outliers-source">27. HuberRegressor vs Ridge on dataset with strong outliers (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#28-robust-linear-model-estimation-using-ransac-source">28. Robust linear model estimation using RANSAC (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#29-robust-linear-estimator-fitting-source">29.  Robust linear estimator fitting (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#30-sparse-recovery-feature-selection-for-sparse-linear-models-source">30. Sparse recovery: feature selection for sparse linear models (source)</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#31-theil-sen-regression-source">31.  Theil-Sen Regression (source)</a>
        </li>
    
    </ul>
  </li>

      
    
    </ul>
  </li>


  
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../Classifiers/Logistic/Logistic/">Logistic Regression</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../Classifiers/Tree/Tree/">Decision Tree</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../Classifiers/Ensamble/ensamble/">Ensamble Methods</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../Classifiers/SVM/svm/">Support Vector Machine</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../Clustering/Kmeans/Kmeans/">Kmeans Clustering</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../Clustering/Agglomerative/Agglomerative/">Agglomerative Clustering</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../Clustering/AffinityPropagation/Affinity-Propagation/">Affinity Propagation</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../Clustering/Spectral/Spectral/">Spectral Clustering</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../Clustering/DBSCAN/DBSCAN/">DBSCAN Clustering</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../Clustering/MeanShift/Mean-shift/">Mean Shift Clustering</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../Clustering/Comparison/Comparison/">Cluster Comparison</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">ML Projects</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../mlp/intro/">Introduction</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../mlp/boston_housing/boston_housing/">Boston Housing</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../mlp/Customer_segments/customer_segments/">Customer Clustering</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../mlp/finding_donors/finding_donors/">Finding Donors</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../mlp/vehicle-detection/CARND-Project-5/">Vehicle Detection</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../mlp/perceptron/dlnd-your-first-neural-network/">Perceptron</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">Deep Learning</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../dl/intro/">Introduction</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../dl/Vanila/1.Vanila-LSTM/">Vanila LSTM</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../dl/stacked/2.Stacked-LSTM/">Stacked LSTM</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../dl/bidirectional/5. BiDirectional-LSTM/">Bidirectional LSTM</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../dl/rnn/RNN_project/">Recurrent Neural Network</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">DL Projects</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../dl/NMIST/NMIST/">Digit Classifier</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../dl/CIFRT10/CIFR10/">Image Classifier</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../dl/traffic-sign/Traffic_Sign_Classifier/">Traffic Sign Detection</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../dl/translator/dlnd_language_translation/">Language Translator</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">Rinforcement Learning</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../rl/smartcab/smartcab/">SmartCab</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">GAN Project</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../dl/gan/dlnd_face_generation/">Face Generation</a>
  </li>
        
      </ul>
    </li>
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../../../References/ref.md">References</a>
  </li>
    
  </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../..">Machine Learning</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../..">Docs</a> &raquo;</li>
    
      
        
          <li>Machine Learning &raquo;</li>
        
      
    
    <li>Linear Regression</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="linear-model">Linear model</h1>
<hr />
<h3 id="introduction">Introduction</h3>
<hr />
<h2 id="examples">Examples</h2>
<h3 id="1-linear-regression-example-source">1. Linear Regression Example (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>This example uses the only the first feature of the <code>diabetes</code> dataset, in
order to illustrate a two-dimensional plot of this regression technique. The
straight line can be seen in the plot, showing how linear regression attempts
to draw a straight line that will best minimize the residual sum of squares
between the observed responses in the dataset, and the responses predicted by
the linear approximation.</p>
<p>The coefficients, the residual sum of squares and the variance score are also
calculated.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> datasets, linear_model

<span style="color: #0099FF; font-style: italic"># Load the diabetes dataset</span>
diabetes <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>load_diabetes()


<span style="color: #0099FF; font-style: italic"># Use only one feature</span>
diabetes_X <span style="color: #555555">=</span> diabetes<span style="color: #555555">.</span>data[:, np<span style="color: #555555">.</span>newaxis, <span style="color: #FF6600">2</span>]

<span style="color: #0099FF; font-style: italic"># Split the data into training/testing sets</span>
diabetes_X_train <span style="color: #555555">=</span> diabetes_X[:<span style="color: #555555">-</span><span style="color: #FF6600">20</span>]
diabetes_X_test <span style="color: #555555">=</span> diabetes_X[<span style="color: #555555">-</span><span style="color: #FF6600">20</span>:]

<span style="color: #0099FF; font-style: italic"># Split the targets into training/testing sets</span>
diabetes_y_train <span style="color: #555555">=</span> diabetes<span style="color: #555555">.</span>target[:<span style="color: #555555">-</span><span style="color: #FF6600">20</span>]
diabetes_y_test <span style="color: #555555">=</span> diabetes<span style="color: #555555">.</span>target[<span style="color: #555555">-</span><span style="color: #FF6600">20</span>:]

<span style="color: #0099FF; font-style: italic"># Create linear regression object</span>
regr <span style="color: #555555">=</span> linear_model<span style="color: #555555">.</span>LinearRegression()

<span style="color: #0099FF; font-style: italic"># Train the model using the training sets</span>
regr<span style="color: #555555">.</span>fit(diabetes_X_train, diabetes_y_train)

<span style="color: #0099FF; font-style: italic"># The coefficients</span>
<span style="color: #336666">print</span>(<span style="color: #CC3300">&#39;Coefficients: </span><span style="color: #CC3300; font-weight: bold">\n</span><span style="color: #CC3300">&#39;</span>, regr<span style="color: #555555">.</span>coef_)
<span style="color: #0099FF; font-style: italic"># The mean squared error</span>
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Mean squared error: </span><span style="color: #AA0000">%.2f</span><span style="color: #CC3300">&quot;</span>
      <span style="color: #555555">%</span> np<span style="color: #555555">.</span>mean((regr<span style="color: #555555">.</span>predict(diabetes_X_test) <span style="color: #555555">-</span> diabetes_y_test) <span style="color: #555555">**</span> <span style="color: #FF6600">2</span>))
<span style="color: #0099FF; font-style: italic"># Explained variance score: 1 is perfect prediction</span>
<span style="color: #336666">print</span>(<span style="color: #CC3300">&#39;Variance score: </span><span style="color: #AA0000">%.2f</span><span style="color: #CC3300">&#39;</span> <span style="color: #555555">%</span> regr<span style="color: #555555">.</span>score(diabetes_X_test, diabetes_y_test))

<span style="color: #0099FF; font-style: italic"># Plot outputs</span>
plt<span style="color: #555555">.</span>scatter(diabetes_X_test, diabetes_y_test,  color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;black&#39;</span>)
plt<span style="color: #555555">.</span>plot(diabetes_X_test, regr<span style="color: #555555">.</span>predict(diabetes_X_test), color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;blue&#39;</span>,
         linewidth<span style="color: #555555">=</span><span style="color: #FF6600">3</span>)

plt<span style="color: #555555">.</span>xticks(())
plt<span style="color: #555555">.</span>yticks(())

plt<span style="color: #555555">.</span>show()
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>Coefficients: 
 [ 938.23786125]
Mean squared error: 2548.07
Variance score: 0.47
</pre></div>


<p><img alt="png" src="../output_5_1.png" /></p>
<h3 id="2-sparsity-example-fitting-only-features-1-and-2-source">2. Sparsity Example: Fitting only features 1  and 2 (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Features 1 and 2 of the diabetes-dataset are fitted and
plotted below. It illustrates that although feature 2
has a strong coefficient on the full model, it does not
give us much regarding <code>y</code> when compared to just feature 1</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">mpl_toolkits.mplot3d</span> <span style="color: #006699; font-weight: bold">import</span> Axes3D

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> datasets, linear_model

diabetes <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>load_diabetes()
indices <span style="color: #555555">=</span> (<span style="color: #FF6600">0</span>, <span style="color: #FF6600">1</span>)

X_train <span style="color: #555555">=</span> diabetes<span style="color: #555555">.</span>data[:<span style="color: #555555">-</span><span style="color: #FF6600">20</span>, indices]
X_test <span style="color: #555555">=</span> diabetes<span style="color: #555555">.</span>data[<span style="color: #555555">-</span><span style="color: #FF6600">20</span>:, indices]
y_train <span style="color: #555555">=</span> diabetes<span style="color: #555555">.</span>target[:<span style="color: #555555">-</span><span style="color: #FF6600">20</span>]
y_test <span style="color: #555555">=</span> diabetes<span style="color: #555555">.</span>target[<span style="color: #555555">-</span><span style="color: #FF6600">20</span>:]

ols <span style="color: #555555">=</span> linear_model<span style="color: #555555">.</span>LinearRegression()
ols<span style="color: #555555">.</span>fit(X_train, y_train)


<span style="color: #0099FF; font-style: italic">#####################################</span>
<span style="color: #0099FF; font-style: italic"># Plot the figure</span>
<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">plot_figs</span>(fig_num, elev, azim, X_train, clf):
    fig <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>figure(fig_num, figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">4</span>, <span style="color: #FF6600">3</span>))
    plt<span style="color: #555555">.</span>clf()
    ax <span style="color: #555555">=</span> Axes3D(fig, elev<span style="color: #555555">=</span>elev, azim<span style="color: #555555">=</span>azim)

    ax<span style="color: #555555">.</span>scatter(X_train[:, <span style="color: #FF6600">0</span>], X_train[:, <span style="color: #FF6600">1</span>], y_train, c<span style="color: #555555">=</span><span style="color: #CC3300">&#39;k&#39;</span>, marker<span style="color: #555555">=</span><span style="color: #CC3300">&#39;+&#39;</span>)
    ax<span style="color: #555555">.</span>plot_surface(np<span style="color: #555555">.</span>array([[<span style="color: #555555">-.</span><span style="color: #FF6600">1</span>, <span style="color: #555555">-.</span><span style="color: #FF6600">1</span>], [<span style="color: #555555">.</span><span style="color: #FF6600">15</span>, <span style="color: #555555">.</span><span style="color: #FF6600">15</span>]]),
                    np<span style="color: #555555">.</span>array([[<span style="color: #555555">-.</span><span style="color: #FF6600">1</span>, <span style="color: #555555">.</span><span style="color: #FF6600">15</span>], [<span style="color: #555555">-.</span><span style="color: #FF6600">1</span>, <span style="color: #555555">.</span><span style="color: #FF6600">15</span>]]),
                    clf<span style="color: #555555">.</span>predict(np<span style="color: #555555">.</span>array([[<span style="color: #555555">-.</span><span style="color: #FF6600">1</span>, <span style="color: #555555">-.</span><span style="color: #FF6600">1</span>, <span style="color: #555555">.</span><span style="color: #FF6600">15</span>, <span style="color: #555555">.</span><span style="color: #FF6600">15</span>],
                                          [<span style="color: #555555">-.</span><span style="color: #FF6600">1</span>, <span style="color: #555555">.</span><span style="color: #FF6600">15</span>, <span style="color: #555555">-.</span><span style="color: #FF6600">1</span>, <span style="color: #555555">.</span><span style="color: #FF6600">15</span>]])<span style="color: #555555">.</span>T
                                )<span style="color: #555555">.</span>reshape((<span style="color: #FF6600">2</span>, <span style="color: #FF6600">2</span>)),
                    alpha<span style="color: #555555">=.</span><span style="color: #FF6600">5</span>)
    ax<span style="color: #555555">.</span>set_xlabel(<span style="color: #CC3300">&#39;X_1&#39;</span>)
    ax<span style="color: #555555">.</span>set_ylabel(<span style="color: #CC3300">&#39;X_2&#39;</span>)
    ax<span style="color: #555555">.</span>set_zlabel(<span style="color: #CC3300">&#39;Y&#39;</span>)
    ax<span style="color: #555555">.</span>w_xaxis<span style="color: #555555">.</span>set_ticklabels([])
    ax<span style="color: #555555">.</span>w_yaxis<span style="color: #555555">.</span>set_ticklabels([])
    ax<span style="color: #555555">.</span>w_zaxis<span style="color: #555555">.</span>set_ticklabels([])

<span style="color: #0099FF; font-style: italic">#Generate the three different figures from different views</span>
elev <span style="color: #555555">=</span> <span style="color: #FF6600">43.5</span>
azim <span style="color: #555555">=</span> <span style="color: #555555">-</span><span style="color: #FF6600">110</span>
plot_figs(<span style="color: #FF6600">1</span>, elev, azim, X_train, ols)

elev <span style="color: #555555">=</span> <span style="color: #555555">-.</span><span style="color: #FF6600">5</span>
azim <span style="color: #555555">=</span> <span style="color: #FF6600">0</span>
plot_figs(<span style="color: #FF6600">2</span>, elev, azim, X_train, ols)

elev <span style="color: #555555">=</span> <span style="color: #555555">-.</span><span style="color: #FF6600">5</span>
azim <span style="color: #555555">=</span> <span style="color: #FF6600">90</span>
plot_figs(<span style="color: #FF6600">3</span>, elev, azim, X_train, ols)

plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_8_0.png" /></p>
<p><img alt="png" src="../output_8_1.png" /></p>
<p><img alt="png" src="../output_8_2.png" /></p>
<h3 id="3-ordinary-least-squares-and-ridge-regression-variance-source">3. Ordinary Least Squares and Ridge Regression Variance (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Due to the few points in each dimension and the straight
line that linear regression uses to follow these points
as well as it can, noise on the observations will cause
great variance as shown in the first plot. Every line's slope
can vary quite a bit for each prediction due to the noise
induced in the observations.</p>
<p>Ridge regression is basically minimizing a penalised version
of the least-squared function. The penalising <code>shrinks</code> the
value of the regression coefficients.
Despite the few data points in each dimension, the slope
of the prediction is much more stable and the variance
in the line itself is greatly reduced, in comparison to that
of the standard linear regression</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> linear_model

X_train <span style="color: #555555">=</span> np<span style="color: #555555">.</span>c_[<span style="color: #555555">.</span><span style="color: #FF6600">5</span>, <span style="color: #FF6600">1</span>]<span style="color: #555555">.</span>T
y_train <span style="color: #555555">=</span> [<span style="color: #555555">.</span><span style="color: #FF6600">5</span>, <span style="color: #FF6600">1</span>]
X_test <span style="color: #555555">=</span> np<span style="color: #555555">.</span>c_[<span style="color: #FF6600">0</span>, <span style="color: #FF6600">2</span>]<span style="color: #555555">.</span>T

np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>seed(<span style="color: #FF6600">0</span>)

classifiers <span style="color: #555555">=</span> <span style="color: #336666">dict</span>(ols<span style="color: #555555">=</span>linear_model<span style="color: #555555">.</span>LinearRegression(),
                   ridge<span style="color: #555555">=</span>linear_model<span style="color: #555555">.</span>Ridge(alpha<span style="color: #555555">=.</span><span style="color: #FF6600">1</span>))

fignum <span style="color: #555555">=</span> <span style="color: #FF6600">1</span>
<span style="color: #006699; font-weight: bold">for</span> name, clf <span style="color: #000000; font-weight: bold">in</span> classifiers<span style="color: #555555">.</span>items():
    fig <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>figure(fignum, figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">4</span>, <span style="color: #FF6600">3</span>))
    plt<span style="color: #555555">.</span>clf()
    plt<span style="color: #555555">.</span>title(name)
    ax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>axes([<span style="color: #555555">.</span><span style="color: #FF6600">12</span>, <span style="color: #555555">.</span><span style="color: #FF6600">12</span>, <span style="color: #555555">.</span><span style="color: #FF6600">8</span>, <span style="color: #555555">.</span><span style="color: #FF6600">8</span>])

    <span style="color: #006699; font-weight: bold">for</span> _ <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">range</span>(<span style="color: #FF6600">6</span>):
        this_X <span style="color: #555555">=</span> <span style="color: #555555">.</span><span style="color: #FF6600">1</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>normal(size<span style="color: #555555">=</span>(<span style="color: #FF6600">2</span>, <span style="color: #FF6600">1</span>)) <span style="color: #555555">+</span> X_train
        clf<span style="color: #555555">.</span>fit(this_X, y_train)

        ax<span style="color: #555555">.</span>plot(X_test, clf<span style="color: #555555">.</span>predict(X_test), color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;.5&#39;</span>)
        ax<span style="color: #555555">.</span>scatter(this_X, y_train, s<span style="color: #555555">=</span><span style="color: #FF6600">3</span>, c<span style="color: #555555">=</span><span style="color: #CC3300">&#39;.5&#39;</span>, marker<span style="color: #555555">=</span><span style="color: #CC3300">&#39;o&#39;</span>, zorder<span style="color: #555555">=</span><span style="color: #FF6600">10</span>)

    clf<span style="color: #555555">.</span>fit(X_train, y_train)
    ax<span style="color: #555555">.</span>plot(X_test, clf<span style="color: #555555">.</span>predict(X_test), linewidth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;blue&#39;</span>)
    ax<span style="color: #555555">.</span>scatter(X_train, y_train, s<span style="color: #555555">=</span><span style="color: #FF6600">30</span>, c<span style="color: #555555">=</span><span style="color: #CC3300">&#39;r&#39;</span>, marker<span style="color: #555555">=</span><span style="color: #CC3300">&#39;+&#39;</span>, zorder<span style="color: #555555">=</span><span style="color: #FF6600">10</span>)

    ax<span style="color: #555555">.</span>set_xticks(())
    ax<span style="color: #555555">.</span>set_yticks(())
    ax<span style="color: #555555">.</span>set_ylim((<span style="color: #FF6600">0</span>, <span style="color: #FF6600">1.6</span>))
    ax<span style="color: #555555">.</span>set_xlabel(<span style="color: #CC3300">&#39;X&#39;</span>)
    ax<span style="color: #555555">.</span>set_ylabel(<span style="color: #CC3300">&#39;y&#39;</span>)
    ax<span style="color: #555555">.</span>set_xlim(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">2</span>)
    fignum <span style="color: #555555">+=</span> <span style="color: #FF6600">1</span>

plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_11_0.png" /></p>
<p><img alt="png" src="../output_11_1.png" /></p>
<hr />
<h3 id="4-plot-ridge-coefficients-as-a-function-of-the-l2-regularization-source">4. Plot Ridge coefficients as a function of the L2 regularization (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p><code>Ridge</code> Regression is the estimator used in this example.
Each color in the left plot represents one different dimension of the
coefficient vector, and this is displayed as a function of the
regularization parameter. The right plot shows how exact the solution
is. This example illustrates how a well defined solution is
found by Ridge regression and how regularization affects the
coefficients and their values. The plot on the right shows how
the difference of the coefficients from the estimator changes
as a function of regularization.
In this example the dependent variable Y is set as a function
of the input features:</p>
<p>$y = X*w + c. $</p>
<p>The coefficient vector w is
randomly sampled from a normal distribution, whereas the bias term c is
set to a constant.
As alpha tends toward zero the coefficients found by Ridge
regression stabilize towards the randomly sampled vector w.
For big alpha (strong regularisation) the coefficients
are smaller (eventually converging at 0) leading to a
simpler and biased solution.
These dependencies can be observed on the left plot.
The right plot shows the mean squared error between the
coefficients found by the model and the chosen vector w.
Less regularised models retrieve the exact
coefficients (error is equal to 0), stronger regularised
models increase the error.
Please note that in this example the data is non-noisy, hence
it is possible to extract the exact coefficients.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.datasets</span> <span style="color: #006699; font-weight: bold">import</span> make_regression
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> Ridge
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.metrics</span> <span style="color: #006699; font-weight: bold">import</span> mean_squared_error

clf <span style="color: #555555">=</span> Ridge()

X, y, w <span style="color: #555555">=</span> make_regression(n_samples<span style="color: #555555">=</span><span style="color: #FF6600">10</span>, n_features<span style="color: #555555">=</span><span style="color: #FF6600">10</span>, coef<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>,
                          random_state<span style="color: #555555">=</span><span style="color: #FF6600">1</span>, bias<span style="color: #555555">=</span><span style="color: #FF6600">3.5</span>)

coefs <span style="color: #555555">=</span> []
errors <span style="color: #555555">=</span> []

alphas <span style="color: #555555">=</span> np<span style="color: #555555">.</span>logspace(<span style="color: #555555">-</span><span style="color: #FF6600">6</span>, <span style="color: #FF6600">6</span>, <span style="color: #FF6600">200</span>)

<span style="color: #0099FF; font-style: italic"># Train the model with different regularisation strengths</span>
<span style="color: #006699; font-weight: bold">for</span> a <span style="color: #000000; font-weight: bold">in</span> alphas:
    clf<span style="color: #555555">.</span>set_params(alpha<span style="color: #555555">=</span>a)
    clf<span style="color: #555555">.</span>fit(X, y)
    coefs<span style="color: #555555">.</span>append(clf<span style="color: #555555">.</span>coef_)
    errors<span style="color: #555555">.</span>append(mean_squared_error(clf<span style="color: #555555">.</span>coef_, w))

<span style="color: #0099FF; font-style: italic"># Display results</span>
plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">20</span>, <span style="color: #FF6600">6</span>))

plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">121</span>)
ax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>gca()
ax<span style="color: #555555">.</span>plot(alphas, coefs)
ax<span style="color: #555555">.</span>set_xscale(<span style="color: #CC3300">&#39;log&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;alpha&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;weights&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Ridge coefficients as a function of the regularization&#39;</span>)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)

plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">122</span>)
ax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>gca()
ax<span style="color: #555555">.</span>plot(alphas, errors)
ax<span style="color: #555555">.</span>set_xscale(<span style="color: #CC3300">&#39;log&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;alpha&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;error&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Coefficient error as a function of the regularization&#39;</span>)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)

plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_15_0.png" /></p>
<h3 id="5-plot-ridge-coefficients-as-a-function-of-the-regularization-source">5. Plot Ridge coefficients as a function of the regularization (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Shows the effect of collinearity in the coefficients of an estimator.</p>
<p>Regression is the estimator used in this example.
Each color represents a different feature of the
coefficient vector, and this is displayed as a function of the
regularization parameter.</p>
<p>This example also shows the usefulness of applying Ridge regression
to highly ill-conditioned matrices. For such matrices, a slight
change in the target variable can cause huge variances in the
calculated weights. In such cases, it is useful to set a certain
regularization (alpha) to reduce this variation (noise).</p>
<p>When alpha is very large, the regularization effect dominates the
squared loss function and the coefficients tend to zero.
At the end of the path, as alpha tends toward zero
and the solution tends towards the ordinary least squares, coefficients
exhibit big oscillations. In practise it is necessary to tune alpha
in such a way that a balance is maintained between both.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> linear_model

<span style="color: #0099FF; font-style: italic"># X is the 10x10 Hilbert matrix</span>
X <span style="color: #555555">=</span> <span style="color: #FF6600">1.</span> <span style="color: #555555">/</span> (np<span style="color: #555555">.</span>arange(<span style="color: #FF6600">1</span>, <span style="color: #FF6600">11</span>) <span style="color: #555555">+</span> np<span style="color: #555555">.</span>arange(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">10</span>)[:, np<span style="color: #555555">.</span>newaxis])
y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>ones(<span style="color: #FF6600">10</span>)

<span style="color: #0099FF; font-style: italic">#########################################</span>
<span style="color: #0099FF; font-style: italic"># Compute paths</span>

n_alphas <span style="color: #555555">=</span> <span style="color: #FF6600">200</span>
alphas <span style="color: #555555">=</span> np<span style="color: #555555">.</span>logspace(<span style="color: #555555">-</span><span style="color: #FF6600">10</span>, <span style="color: #555555">-</span><span style="color: #FF6600">2</span>, n_alphas)
clf <span style="color: #555555">=</span> linear_model<span style="color: #555555">.</span>Ridge(fit_intercept<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">False</span>)

coefs <span style="color: #555555">=</span> []
<span style="color: #006699; font-weight: bold">for</span> a <span style="color: #000000; font-weight: bold">in</span> alphas:
    clf<span style="color: #555555">.</span>set_params(alpha<span style="color: #555555">=</span>a)
    clf<span style="color: #555555">.</span>fit(X, y)
    coefs<span style="color: #555555">.</span>append(clf<span style="color: #555555">.</span>coef_)

<span style="color: #0099FF; font-style: italic">########################################</span>
<span style="color: #0099FF; font-style: italic"># Display results</span>

ax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>gca()

ax<span style="color: #555555">.</span>plot(alphas, coefs)
ax<span style="color: #555555">.</span>set_xscale(<span style="color: #CC3300">&#39;log&#39;</span>)
ax<span style="color: #555555">.</span>set_xlim(ax<span style="color: #555555">.</span>get_xlim()[::<span style="color: #555555">-</span><span style="color: #FF6600">1</span>])  <span style="color: #0099FF; font-style: italic"># reverse axis</span>
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;alpha&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;weights&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Ridge coefficients as a function of the regularization&#39;</span>)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_18_0.png" /></p>
<h3 id="6-bayesian-ridge-regression-source">6. Bayesian Ridge Regression (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Computes a Bayesian Ridge Regression on a synthetic dataset.</p>
<p><code>bayesian_ridge_regression</code> for more information on the regressor.</p>
<p>Compared to the OLS (ordinary least squares) estimator, the coefficient
weights are slightly shifted toward zeros, which stabilises them.</p>
<p>As the prior on the weights is a Gaussian prior, the histogram of the
estimated weights is Gaussian.</p>
<p>The estimation of the model is done by iteratively maximizing the
marginal log-likelihood of the observations.</p>
<p>We also plot predictions and uncertainties for Bayesian Ridge Regression
for one dimensional regression using polynomial feature expansion.
Note the uncertainty starts going up on the right side of the plot.
This is because these test samples are outside of the range of the training
samples.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">scipy</span> <span style="color: #006699; font-weight: bold">import</span> stats
<span style="color: #555555">%</span>matplotlib inline
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> BayesianRidge, LinearRegression

<span style="color: #0099FF; font-style: italic">#################################################</span>
<span style="color: #0099FF; font-style: italic"># Generating simulated data with Gaussian weights</span>
np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>seed(<span style="color: #FF6600">0</span>)
n_samples, n_features <span style="color: #555555">=</span> <span style="color: #FF6600">100</span>, <span style="color: #FF6600">100</span>
X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>randn(n_samples, n_features)  <span style="color: #0099FF; font-style: italic"># Create Gaussian data</span>
<span style="color: #0099FF; font-style: italic"># Create weights with a precision lambda_ of 4.</span>
lambda_ <span style="color: #555555">=</span> <span style="color: #FF6600">4.</span>
w <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros(n_features)
<span style="color: #0099FF; font-style: italic"># Only keep 10 weights of interest</span>
relevant_features <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>randint(<span style="color: #FF6600">0</span>, n_features, <span style="color: #FF6600">10</span>)
<span style="color: #006699; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> relevant_features:
    w[i] <span style="color: #555555">=</span> stats<span style="color: #555555">.</span>norm<span style="color: #555555">.</span>rvs(loc<span style="color: #555555">=</span><span style="color: #FF6600">0</span>, scale<span style="color: #555555">=</span><span style="color: #FF6600">1.</span> <span style="color: #555555">/</span> np<span style="color: #555555">.</span>sqrt(lambda_))
<span style="color: #0099FF; font-style: italic"># Create noise with a precision alpha of 50.</span>
alpha_ <span style="color: #555555">=</span> <span style="color: #FF6600">50.</span>
noise <span style="color: #555555">=</span> stats<span style="color: #555555">.</span>norm<span style="color: #555555">.</span>rvs(loc<span style="color: #555555">=</span><span style="color: #FF6600">0</span>, scale<span style="color: #555555">=</span><span style="color: #FF6600">1.</span> <span style="color: #555555">/</span> np<span style="color: #555555">.</span>sqrt(alpha_), size<span style="color: #555555">=</span>n_samples)
<span style="color: #0099FF; font-style: italic"># Create the target</span>
y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>dot(X, w) <span style="color: #555555">+</span> noise

<span style="color: #0099FF; font-style: italic">############################################</span>
<span style="color: #0099FF; font-style: italic"># Fit the Bayesian Ridge Regression and an OLS for comparison</span>
clf <span style="color: #555555">=</span> BayesianRidge(compute_score<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>)
clf<span style="color: #555555">.</span>fit(X, y)

ols <span style="color: #555555">=</span> LinearRegression()
ols<span style="color: #555555">.</span>fit(X, y)

<span style="color: #0099FF; font-style: italic">################################################</span>
<span style="color: #0099FF; font-style: italic"># Plot true weights, estimated weights, histogram of the weights, and</span>
<span style="color: #0099FF; font-style: italic"># predictions with standard deviations</span>
lw <span style="color: #555555">=</span> <span style="color: #FF6600">2</span>
plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">6</span>, <span style="color: #FF6600">5</span>))
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Weights of the model&quot;</span>)
plt<span style="color: #555555">.</span>plot(clf<span style="color: #555555">.</span>coef_, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;lightgreen&#39;</span>, linewidth<span style="color: #555555">=</span>lw,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Bayesian Ridge estimate&quot;</span>)
plt<span style="color: #555555">.</span>plot(w, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;gold&#39;</span>, linewidth<span style="color: #555555">=</span>lw, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Ground truth&quot;</span>)
plt<span style="color: #555555">.</span>plot(ols<span style="color: #555555">.</span>coef_, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;navy&#39;</span>, linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;--&#39;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;OLS estimate&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Features&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;Values of the weights&quot;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&quot;best&quot;</span>, prop<span style="color: #555555">=</span><span style="color: #336666">dict</span>(size<span style="color: #555555">=</span><span style="color: #FF6600">12</span>))

plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">6</span>, <span style="color: #FF6600">5</span>))
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Histogram of the weights&quot;</span>)
plt<span style="color: #555555">.</span>hist(clf<span style="color: #555555">.</span>coef_, bins<span style="color: #555555">=</span>n_features, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;gold&#39;</span>, log<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>)
plt<span style="color: #555555">.</span>scatter(clf<span style="color: #555555">.</span>coef_[relevant_features], <span style="color: #FF6600">5</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>ones(<span style="color: #336666">len</span>(relevant_features)),
            color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;navy&#39;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Relevant features&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;Features&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Values of the weights&quot;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&quot;upper left&quot;</span>)

plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">6</span>, <span style="color: #FF6600">5</span>))
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Marginal log-likelihood&quot;</span>)
plt<span style="color: #555555">.</span>plot(clf<span style="color: #555555">.</span>scores_, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;navy&#39;</span>, linewidth<span style="color: #555555">=</span>lw)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;Score&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Iterations&quot;</span>)


<span style="color: #0099FF; font-style: italic"># Plotting some predictions for polynomial regression</span>
<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">f</span>(x, noise_amount):
    y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>sqrt(x) <span style="color: #555555">*</span> np<span style="color: #555555">.</span>sin(x)
    noise <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>normal(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">1</span>, <span style="color: #336666">len</span>(x))
    <span style="color: #006699; font-weight: bold">return</span> y <span style="color: #555555">+</span> noise_amount <span style="color: #555555">*</span> noise


degree <span style="color: #555555">=</span> <span style="color: #FF6600">10</span>
X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">10</span>, <span style="color: #FF6600">100</span>)
y <span style="color: #555555">=</span> f(X, noise_amount<span style="color: #555555">=</span><span style="color: #FF6600">0.1</span>)
clf_poly <span style="color: #555555">=</span> BayesianRidge()
clf_poly<span style="color: #555555">.</span>fit(np<span style="color: #555555">.</span>vander(X, degree), y)

X_plot <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">11</span>, <span style="color: #FF6600">25</span>)
y_plot <span style="color: #555555">=</span> f(X_plot, noise_amount<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)
y_mean, y_std <span style="color: #555555">=</span> clf_poly<span style="color: #555555">.</span>predict(np<span style="color: #555555">.</span>vander(X_plot, degree), return_std<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>)
plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">6</span>, <span style="color: #FF6600">5</span>))
plt<span style="color: #555555">.</span>errorbar(X_plot, y_mean, y_std, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;navy&#39;</span>,
             label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Polynomial Bayesian Ridge Regression&quot;</span>, linewidth<span style="color: #555555">=</span>lw)
plt<span style="color: #555555">.</span>plot(X_plot, y_plot, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;gold&#39;</span>, linewidth<span style="color: #555555">=</span>lw,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Ground Truth&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;Output y&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Feature X&quot;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&quot;lower left&quot;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_21_0.png" /></p>
<p><img alt="png" src="../output_21_1.png" /></p>
<p><img alt="png" src="../output_21_2.png" /></p>
<p><img alt="png" src="../output_21_3.png" /></p>
<h3 id="7-logistic-regression-3-class-classifier-source">7. Logistic Regression 3-class Classifier (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Show below is a logistic-regression classifiers decision boundaries on the
<code>iris &lt;https://en.wikipedia.org/wiki/Iris_flower_data_set&gt;</code>_ dataset. The
datapoints are colored according to their labels.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> linear_model, datasets

<span style="color: #0099FF; font-style: italic"># import some data to play with</span>
iris <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>load_iris()
X <span style="color: #555555">=</span> iris<span style="color: #555555">.</span>data[:, :<span style="color: #FF6600">2</span>]  <span style="color: #0099FF; font-style: italic"># we only take the first two features.</span>
Y <span style="color: #555555">=</span> iris<span style="color: #555555">.</span>target

h <span style="color: #555555">=</span> <span style="color: #555555">.</span><span style="color: #FF6600">02</span>  <span style="color: #0099FF; font-style: italic"># step size in the mesh</span>

logreg <span style="color: #555555">=</span> linear_model<span style="color: #555555">.</span>LogisticRegression(C<span style="color: #555555">=</span><span style="color: #FF6600">1e5</span>)

<span style="color: #0099FF; font-style: italic"># we create an instance of Neighbours Classifier and fit the data.</span>
logreg<span style="color: #555555">.</span>fit(X, Y)

<span style="color: #0099FF; font-style: italic"># Plot the decision boundary. For that, we will assign a color to each</span>
<span style="color: #0099FF; font-style: italic"># point in the mesh [x_min, x_max]x[y_min, y_max].</span>
x_min, x_max <span style="color: #555555">=</span> X[:, <span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>min() <span style="color: #555555">-</span> <span style="color: #555555">.</span><span style="color: #FF6600">5</span>, X[:, <span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>max() <span style="color: #555555">+</span> <span style="color: #555555">.</span><span style="color: #FF6600">5</span>
y_min, y_max <span style="color: #555555">=</span> X[:, <span style="color: #FF6600">1</span>]<span style="color: #555555">.</span>min() <span style="color: #555555">-</span> <span style="color: #555555">.</span><span style="color: #FF6600">5</span>, X[:, <span style="color: #FF6600">1</span>]<span style="color: #555555">.</span>max() <span style="color: #555555">+</span> <span style="color: #555555">.</span><span style="color: #FF6600">5</span>
xx, yy <span style="color: #555555">=</span> np<span style="color: #555555">.</span>meshgrid(np<span style="color: #555555">.</span>arange(x_min, x_max, h), np<span style="color: #555555">.</span>arange(y_min, y_max, h))
Z <span style="color: #555555">=</span> logreg<span style="color: #555555">.</span>predict(np<span style="color: #555555">.</span>c_[xx<span style="color: #555555">.</span>ravel(), yy<span style="color: #555555">.</span>ravel()])

<span style="color: #0099FF; font-style: italic"># Put the result into a color plot</span>
Z <span style="color: #555555">=</span> Z<span style="color: #555555">.</span>reshape(xx<span style="color: #555555">.</span>shape)
plt<span style="color: #555555">.</span>figure(<span style="color: #FF6600">1</span>, figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">4</span>, <span style="color: #FF6600">3</span>))
plt<span style="color: #555555">.</span>pcolormesh(xx, yy, Z, cmap<span style="color: #555555">=</span>plt<span style="color: #555555">.</span>cm<span style="color: #555555">.</span>Paired)

<span style="color: #0099FF; font-style: italic"># Plot also the training points</span>
plt<span style="color: #555555">.</span>scatter(X[:, <span style="color: #FF6600">0</span>], X[:, <span style="color: #FF6600">1</span>], c<span style="color: #555555">=</span>Y, edgecolors<span style="color: #555555">=</span><span style="color: #CC3300">&#39;k&#39;</span>, cmap<span style="color: #555555">=</span>plt<span style="color: #555555">.</span>cm<span style="color: #555555">.</span>Paired)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;Sepal length&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Sepal width&#39;</span>)

plt<span style="color: #555555">.</span>xlim(xx<span style="color: #555555">.</span>min(), xx<span style="color: #555555">.</span>max())
plt<span style="color: #555555">.</span>ylim(yy<span style="color: #555555">.</span>min(), yy<span style="color: #555555">.</span>max())
plt<span style="color: #555555">.</span>xticks(())
plt<span style="color: #555555">.</span>yticks(())

plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_24_0.png" /></p>
<p><img alt="png" src="../output_24_1.png" /></p>
<p><img alt="png" src="../output_24_2.png" /></p>
<h3 id="8-l1-penalty-and-sparsity-in-logistic-regression-source">8. L1 Penalty and Sparsity in Logistic Regression (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Comparison of the sparsity (percentage of zero coefficients) of solutions when
L1 and L2 penalty are used for different values of C. We can see that large
values of C give more freedom to the model.  Conversely, smaller values of C
constrain the model more. In the L1 penalty case, this leads to sparser
solutions.</p>
<p>We classify 8x8 images of digits into two classes: 0-4 against 5-9.
The visualization shows coefficients of the models for varying C.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> LogisticRegression
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> datasets
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #006699; font-weight: bold">import</span> StandardScaler

digits <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>load_digits()

X, y <span style="color: #555555">=</span> digits<span style="color: #555555">.</span>data, digits<span style="color: #555555">.</span>target
X <span style="color: #555555">=</span> StandardScaler()<span style="color: #555555">.</span>fit_transform(X)

<span style="color: #0099FF; font-style: italic"># classify small against large digits</span>
y <span style="color: #555555">=</span> (y <span style="color: #555555">&gt;</span> <span style="color: #FF6600">4</span>)<span style="color: #555555">.</span>astype(np<span style="color: #555555">.</span>int)


<span style="color: #0099FF; font-style: italic"># Set regularization parameter</span>
<span style="color: #006699; font-weight: bold">for</span> i, C <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">enumerate</span>((<span style="color: #FF6600">100</span>, <span style="color: #FF6600">1</span>, <span style="color: #FF6600">0.01</span>)):
    <span style="color: #0099FF; font-style: italic"># turn down tolerance for short training time</span>
    clf_l1_LR <span style="color: #555555">=</span> LogisticRegression(C<span style="color: #555555">=</span>C, penalty<span style="color: #555555">=</span><span style="color: #CC3300">&#39;l1&#39;</span>, tol<span style="color: #555555">=</span><span style="color: #FF6600">0.01</span>)
    clf_l2_LR <span style="color: #555555">=</span> LogisticRegression(C<span style="color: #555555">=</span>C, penalty<span style="color: #555555">=</span><span style="color: #CC3300">&#39;l2&#39;</span>, tol<span style="color: #555555">=</span><span style="color: #FF6600">0.01</span>)
    clf_l1_LR<span style="color: #555555">.</span>fit(X, y)
    clf_l2_LR<span style="color: #555555">.</span>fit(X, y)

    coef_l1_LR <span style="color: #555555">=</span> clf_l1_LR<span style="color: #555555">.</span>coef_<span style="color: #555555">.</span>ravel()
    coef_l2_LR <span style="color: #555555">=</span> clf_l2_LR<span style="color: #555555">.</span>coef_<span style="color: #555555">.</span>ravel()

    <span style="color: #0099FF; font-style: italic"># coef_l1_LR contains zeros due to the</span>
    <span style="color: #0099FF; font-style: italic"># L1 sparsity inducing norm</span>

    sparsity_l1_LR <span style="color: #555555">=</span> np<span style="color: #555555">.</span>mean(coef_l1_LR <span style="color: #555555">==</span> <span style="color: #FF6600">0</span>) <span style="color: #555555">*</span> <span style="color: #FF6600">100</span>
    sparsity_l2_LR <span style="color: #555555">=</span> np<span style="color: #555555">.</span>mean(coef_l2_LR <span style="color: #555555">==</span> <span style="color: #FF6600">0</span>) <span style="color: #555555">*</span> <span style="color: #FF6600">100</span>

    <span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;C=</span><span style="color: #AA0000">%.2f</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> C)
    <span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Sparsity with L1 penalty: </span><span style="color: #AA0000">%.2f%%</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> sparsity_l1_LR)
    <span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;score with L1 penalty: </span><span style="color: #AA0000">%.4f</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> clf_l1_LR<span style="color: #555555">.</span>score(X, y))
    <span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Sparsity with L2 penalty: </span><span style="color: #AA0000">%.2f%%</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> sparsity_l2_LR)
    <span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;score with L2 penalty: </span><span style="color: #AA0000">%.4f</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> clf_l2_LR<span style="color: #555555">.</span>score(X, y))

    l1_plot <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">3</span>, <span style="color: #FF6600">2</span>, <span style="color: #FF6600">2</span> <span style="color: #555555">*</span> i <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>)
    l2_plot <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">3</span>, <span style="color: #FF6600">2</span>, <span style="color: #FF6600">2</span> <span style="color: #555555">*</span> (i <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>))
    <span style="color: #006699; font-weight: bold">if</span> i <span style="color: #555555">==</span> <span style="color: #FF6600">0</span>:
        l1_plot<span style="color: #555555">.</span>set_title(<span style="color: #CC3300">&quot;L1 penalty&quot;</span>)
        l2_plot<span style="color: #555555">.</span>set_title(<span style="color: #CC3300">&quot;L2 penalty&quot;</span>)

    l1_plot<span style="color: #555555">.</span>imshow(np<span style="color: #555555">.</span>abs(coef_l1_LR<span style="color: #555555">.</span>reshape(<span style="color: #FF6600">8</span>, <span style="color: #FF6600">8</span>)), interpolation<span style="color: #555555">=</span><span style="color: #CC3300">&#39;nearest&#39;</span>,
                   cmap<span style="color: #555555">=</span><span style="color: #CC3300">&#39;binary&#39;</span>, vmax<span style="color: #555555">=</span><span style="color: #FF6600">1</span>, vmin<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)
    l2_plot<span style="color: #555555">.</span>imshow(np<span style="color: #555555">.</span>abs(coef_l2_LR<span style="color: #555555">.</span>reshape(<span style="color: #FF6600">8</span>, <span style="color: #FF6600">8</span>)), interpolation<span style="color: #555555">=</span><span style="color: #CC3300">&#39;nearest&#39;</span>,
                   cmap<span style="color: #555555">=</span><span style="color: #CC3300">&#39;binary&#39;</span>, vmax<span style="color: #555555">=</span><span style="color: #FF6600">1</span>, vmin<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)
    plt<span style="color: #555555">.</span>text(<span style="color: #555555">-</span><span style="color: #FF6600">8</span>, <span style="color: #FF6600">3</span>, <span style="color: #CC3300">&quot;C = </span><span style="color: #AA0000">%.2f</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> C)

    l1_plot<span style="color: #555555">.</span>set_xticks(())
    l1_plot<span style="color: #555555">.</span>set_yticks(())
    l2_plot<span style="color: #555555">.</span>set_xticks(())
    l2_plot<span style="color: #555555">.</span>set_yticks(())

plt<span style="color: #555555">.</span>show()
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>C=100.00
Sparsity with L1 penalty: 4.69%
score with L1 penalty: 0.9093
Sparsity with L2 penalty: 4.69%
score with L2 penalty: 0.9098
C=1.00
Sparsity with L1 penalty: 9.38%
score with L1 penalty: 0.9098
Sparsity with L2 penalty: 4.69%
score with L2 penalty: 0.9093
C=0.01
Sparsity with L1 penalty: 85.94%
score with L1 penalty: 0.8625
Sparsity with L2 penalty: 4.69%
score with L2 penalty: 0.8915
</pre></div>


<p><img alt="png" src="../output_27_1.png" /></p>
<h3 id="9-path-with-l1-logistic-regression-source">9. Path with L1- Logistic Regression (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Computes path on IRIS dataset.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">datetime</span> <span style="color: #006699; font-weight: bold">import</span> datetime
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> linear_model
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> datasets
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.svm</span> <span style="color: #006699; font-weight: bold">import</span> l1_min_c

iris <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>load_iris()
X <span style="color: #555555">=</span> iris<span style="color: #555555">.</span>data
y <span style="color: #555555">=</span> iris<span style="color: #555555">.</span>target

X <span style="color: #555555">=</span> X[y <span style="color: #555555">!=</span> <span style="color: #FF6600">2</span>]
y <span style="color: #555555">=</span> y[y <span style="color: #555555">!=</span> <span style="color: #FF6600">2</span>]

X <span style="color: #555555">-=</span> np<span style="color: #555555">.</span>mean(X, <span style="color: #FF6600">0</span>)

<span style="color: #0099FF; font-style: italic">####################################</span>
<span style="color: #0099FF; font-style: italic"># Demo path functions</span>

cs <span style="color: #555555">=</span> l1_min_c(X, y, loss<span style="color: #555555">=</span><span style="color: #CC3300">&#39;log&#39;</span>) <span style="color: #555555">*</span> np<span style="color: #555555">.</span>logspace(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">3</span>)


<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Computing regularization path ...&quot;</span>)
start <span style="color: #555555">=</span> datetime<span style="color: #555555">.</span>now()
clf <span style="color: #555555">=</span> linear_model<span style="color: #555555">.</span>LogisticRegression(C<span style="color: #555555">=</span><span style="color: #FF6600">1.0</span>, penalty<span style="color: #555555">=</span><span style="color: #CC3300">&#39;l1&#39;</span>, tol<span style="color: #555555">=</span><span style="color: #FF6600">1e-6</span>)
coefs_ <span style="color: #555555">=</span> []
<span style="color: #006699; font-weight: bold">for</span> c <span style="color: #000000; font-weight: bold">in</span> cs:
    clf<span style="color: #555555">.</span>set_params(C<span style="color: #555555">=</span>c)
    clf<span style="color: #555555">.</span>fit(X, y)
    coefs_<span style="color: #555555">.</span>append(clf<span style="color: #555555">.</span>coef_<span style="color: #555555">.</span>ravel()<span style="color: #555555">.</span>copy())
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;This took &quot;</span>, datetime<span style="color: #555555">.</span>now() <span style="color: #555555">-</span> start)

coefs_ <span style="color: #555555">=</span> np<span style="color: #555555">.</span>array(coefs_)
plt<span style="color: #555555">.</span>plot(np<span style="color: #555555">.</span>log10(cs), coefs_)
ymin, ymax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>ylim()
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;log(C)&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Coefficients&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Logistic Regression Path&#39;</span>)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>Computing regularization path ...
This took  0:00:00.036331
</pre></div>


<p><img alt="png" src="../output_30_1.png" /></p>
<h3 id="10-logistic-function-source">10. Logistic function (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Shown in the plot is how the logistic regression would, in this
synthetic dataset, classify values as either 0 or 1,
i.e. class one or two, using the logistic curve.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> linear_model

<span style="color: #0099FF; font-style: italic"># this is our test set, it&#39;s just a straight line with some</span>
<span style="color: #0099FF; font-style: italic"># Gaussian noise</span>
xmin, xmax <span style="color: #555555">=</span> <span style="color: #555555">-</span><span style="color: #FF6600">5</span>, <span style="color: #FF6600">5</span>
n_samples <span style="color: #555555">=</span> <span style="color: #FF6600">100</span>
np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>seed(<span style="color: #FF6600">0</span>)
X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>normal(size<span style="color: #555555">=</span>n_samples)
y <span style="color: #555555">=</span> (X <span style="color: #555555">&gt;</span> <span style="color: #FF6600">0</span>)<span style="color: #555555">.</span>astype(np<span style="color: #555555">.</span>float)
X[X <span style="color: #555555">&gt;</span> <span style="color: #FF6600">0</span>] <span style="color: #555555">*=</span> <span style="color: #FF6600">4</span>
X <span style="color: #555555">+=</span> <span style="color: #555555">.</span><span style="color: #FF6600">3</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>normal(size<span style="color: #555555">=</span>n_samples)

X <span style="color: #555555">=</span> X[:, np<span style="color: #555555">.</span>newaxis]
<span style="color: #0099FF; font-style: italic"># run the classifier</span>
clf <span style="color: #555555">=</span> linear_model<span style="color: #555555">.</span>LogisticRegression(C<span style="color: #555555">=</span><span style="color: #FF6600">1e5</span>)
clf<span style="color: #555555">.</span>fit(X, y)

<span style="color: #0099FF; font-style: italic"># and plot the result</span>
plt<span style="color: #555555">.</span>figure(<span style="color: #FF6600">1</span>, figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">4</span>, <span style="color: #FF6600">3</span>))
plt<span style="color: #555555">.</span>clf()
plt<span style="color: #555555">.</span>scatter(X<span style="color: #555555">.</span>ravel(), y, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;black&#39;</span>, zorder<span style="color: #555555">=</span><span style="color: #FF6600">20</span>)
X_test <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(<span style="color: #555555">-</span><span style="color: #FF6600">5</span>, <span style="color: #FF6600">10</span>, <span style="color: #FF6600">300</span>)


<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">model</span>(x):
    <span style="color: #006699; font-weight: bold">return</span> <span style="color: #FF6600">1</span> <span style="color: #555555">/</span> (<span style="color: #FF6600">1</span> <span style="color: #555555">+</span> np<span style="color: #555555">.</span>exp(<span style="color: #555555">-</span>x))
loss <span style="color: #555555">=</span> model(X_test <span style="color: #555555">*</span> clf<span style="color: #555555">.</span>coef_ <span style="color: #555555">+</span> clf<span style="color: #555555">.</span>intercept_)<span style="color: #555555">.</span>ravel()
plt<span style="color: #555555">.</span>plot(X_test, loss, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;red&#39;</span>, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">3</span>)

ols <span style="color: #555555">=</span> linear_model<span style="color: #555555">.</span>LinearRegression()
ols<span style="color: #555555">.</span>fit(X, y)
plt<span style="color: #555555">.</span>plot(X_test, ols<span style="color: #555555">.</span>coef_ <span style="color: #555555">*</span> X_test <span style="color: #555555">+</span> ols<span style="color: #555555">.</span>intercept_, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)
plt<span style="color: #555555">.</span>axhline(<span style="color: #555555">.</span><span style="color: #FF6600">5</span>, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;.5&#39;</span>)

plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;y&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;X&#39;</span>)
plt<span style="color: #555555">.</span>xticks(<span style="color: #336666">range</span>(<span style="color: #555555">-</span><span style="color: #FF6600">5</span>, <span style="color: #FF6600">10</span>))
plt<span style="color: #555555">.</span>yticks([<span style="color: #FF6600">0</span>, <span style="color: #FF6600">0.5</span>, <span style="color: #FF6600">1</span>])
plt<span style="color: #555555">.</span>ylim(<span style="color: #555555">-.</span><span style="color: #FF6600">25</span>, <span style="color: #FF6600">1.25</span>)
plt<span style="color: #555555">.</span>xlim(<span style="color: #555555">-</span><span style="color: #FF6600">4</span>, <span style="color: #FF6600">10</span>)
plt<span style="color: #555555">.</span>legend((<span style="color: #CC3300">&#39;Logistic Regression Model&#39;</span>, <span style="color: #CC3300">&#39;Linear Regression Model&#39;</span>),
           loc<span style="color: #555555">=</span><span style="color: #CC3300">&quot;lower right&quot;</span>, fontsize<span style="color: #555555">=</span><span style="color: #CC3300">&#39;small&#39;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_33_0.png" /></p>
<h3 id="11-plot-multinomial-and-one-vs-rest-logistic-regression-source">11. Plot multinomial and One-vs-Rest Logistic Regression (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Plot decision surface of multinomial and One-vs-Rest Logistic Regression.
The hyperplanes corresponding to the three One-vs-Rest (OVR) classifiers
are represented by the dashed lines.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.datasets</span> <span style="color: #006699; font-weight: bold">import</span> make_blobs
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> LogisticRegression

<span style="color: #0099FF; font-style: italic"># make 3-class dataset for classification</span>
centers <span style="color: #555555">=</span> [[<span style="color: #555555">-</span><span style="color: #FF6600">5</span>, <span style="color: #FF6600">0</span>], [<span style="color: #FF6600">0</span>, <span style="color: #FF6600">1.5</span>], [<span style="color: #FF6600">5</span>, <span style="color: #555555">-</span><span style="color: #FF6600">1</span>]]
X, y <span style="color: #555555">=</span> make_blobs(n_samples<span style="color: #555555">=</span><span style="color: #FF6600">1000</span>, centers<span style="color: #555555">=</span>centers, random_state<span style="color: #555555">=</span><span style="color: #FF6600">40</span>)
transformation <span style="color: #555555">=</span> [[<span style="color: #FF6600">0.4</span>, <span style="color: #FF6600">0.2</span>], [<span style="color: #555555">-</span><span style="color: #FF6600">0.4</span>, <span style="color: #FF6600">1.2</span>]]
X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>dot(X, transformation)

<span style="color: #006699; font-weight: bold">for</span> multi_class <span style="color: #000000; font-weight: bold">in</span> (<span style="color: #CC3300">&#39;multinomial&#39;</span>, <span style="color: #CC3300">&#39;ovr&#39;</span>):
    clf <span style="color: #555555">=</span> LogisticRegression(solver<span style="color: #555555">=</span><span style="color: #CC3300">&#39;sag&#39;</span>, max_iter<span style="color: #555555">=</span><span style="color: #FF6600">100</span>, random_state<span style="color: #555555">=</span><span style="color: #FF6600">42</span>,
                             multi_class<span style="color: #555555">=</span>multi_class)<span style="color: #555555">.</span>fit(X, y)

    <span style="color: #0099FF; font-style: italic"># print the training scores</span>
    <span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;training score : </span><span style="color: #AA0000">%.3f</span><span style="color: #CC3300"> (</span><span style="color: #AA0000">%s</span><span style="color: #CC3300">)&quot;</span> <span style="color: #555555">%</span> (clf<span style="color: #555555">.</span>score(X, y), multi_class))

    <span style="color: #0099FF; font-style: italic"># create a mesh to plot in</span>
    h <span style="color: #555555">=</span> <span style="color: #555555">.</span><span style="color: #FF6600">02</span>  <span style="color: #0099FF; font-style: italic"># step size in the mesh</span>
    x_min, x_max <span style="color: #555555">=</span> X[:, <span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>min() <span style="color: #555555">-</span> <span style="color: #FF6600">1</span>, X[:, <span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>max() <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>
    y_min, y_max <span style="color: #555555">=</span> X[:, <span style="color: #FF6600">1</span>]<span style="color: #555555">.</span>min() <span style="color: #555555">-</span> <span style="color: #FF6600">1</span>, X[:, <span style="color: #FF6600">1</span>]<span style="color: #555555">.</span>max() <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>
    xx, yy <span style="color: #555555">=</span> np<span style="color: #555555">.</span>meshgrid(np<span style="color: #555555">.</span>arange(x_min, x_max, h),
                         np<span style="color: #555555">.</span>arange(y_min, y_max, h))

    <span style="color: #0099FF; font-style: italic"># Plot the decision boundary. For that, we will assign a color to each</span>
    <span style="color: #0099FF; font-style: italic"># point in the mesh [x_min, x_max]x[y_min, y_max].</span>
    Z <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>predict(np<span style="color: #555555">.</span>c_[xx<span style="color: #555555">.</span>ravel(), yy<span style="color: #555555">.</span>ravel()])
    <span style="color: #0099FF; font-style: italic"># Put the result into a color plot</span>
    Z <span style="color: #555555">=</span> Z<span style="color: #555555">.</span>reshape(xx<span style="color: #555555">.</span>shape)
    plt<span style="color: #555555">.</span>figure()
    plt<span style="color: #555555">.</span>contourf(xx, yy, Z, cmap<span style="color: #555555">=</span>plt<span style="color: #555555">.</span>cm<span style="color: #555555">.</span>Paired)
    plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Decision surface of LogisticRegression (</span><span style="color: #AA0000">%s</span><span style="color: #CC3300">)&quot;</span> <span style="color: #555555">%</span> multi_class)
    plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)

    <span style="color: #0099FF; font-style: italic"># Plot also the training points</span>
    colors <span style="color: #555555">=</span> <span style="color: #CC3300">&quot;bry&quot;</span>
    <span style="color: #006699; font-weight: bold">for</span> i, color <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">zip</span>(clf<span style="color: #555555">.</span>classes_, colors):
        idx <span style="color: #555555">=</span> np<span style="color: #555555">.</span>where(y <span style="color: #555555">==</span> i)
        plt<span style="color: #555555">.</span>scatter(X[idx, <span style="color: #FF6600">0</span>], X[idx, <span style="color: #FF6600">1</span>], c<span style="color: #555555">=</span>color, cmap<span style="color: #555555">=</span>plt<span style="color: #555555">.</span>cm<span style="color: #555555">.</span>Paired)

    <span style="color: #0099FF; font-style: italic"># Plot the three one-against-all classifiers</span>
    xmin, xmax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>xlim()
    ymin, ymax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>ylim()
    coef <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>coef_
    intercept <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>intercept_

    <span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">plot_hyperplane</span>(c, color):
        <span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">line</span>(x0):
            <span style="color: #006699; font-weight: bold">return</span> (<span style="color: #555555">-</span>(x0 <span style="color: #555555">*</span> coef[c, <span style="color: #FF6600">0</span>]) <span style="color: #555555">-</span> intercept[c]) <span style="color: #555555">/</span> coef[c, <span style="color: #FF6600">1</span>]
        plt<span style="color: #555555">.</span>plot([xmin, xmax], [line(xmin), line(xmax)],
                 ls<span style="color: #555555">=</span><span style="color: #CC3300">&quot;--&quot;</span>, color<span style="color: #555555">=</span>color)

    <span style="color: #006699; font-weight: bold">for</span> i, color <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">zip</span>(clf<span style="color: #555555">.</span>classes_, colors):
        plot_hyperplane(i, color)

plt<span style="color: #555555">.</span>show()
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>training score : 0.995 (multinomial)
training score : 0.976 (ovr)
</pre></div>


<p><img alt="png" src="../output_36_1.png" /></p>
<p><img alt="png" src="../output_36_2.png" /></p>
<h3 id="12-lasso-path-using-lars-source">12. Lasso path using LARS (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Computes Lasso Path along the regularization parameter using the LARS
algorithm on the diabetes dataset. Each color represents a different
feature of the coefficient vector, and this is displayed as a function
of the regularization parameter.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> linear_model
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> datasets

diabetes <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>load_diabetes()
X <span style="color: #555555">=</span> diabetes<span style="color: #555555">.</span>data
y <span style="color: #555555">=</span> diabetes<span style="color: #555555">.</span>target

<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Computing regularization path using the LARS ...&quot;</span>)
alphas, _, coefs <span style="color: #555555">=</span> linear_model<span style="color: #555555">.</span>lars_path(X, y, method<span style="color: #555555">=</span><span style="color: #CC3300">&#39;lasso&#39;</span>, verbose<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>)

xx <span style="color: #555555">=</span> np<span style="color: #555555">.</span>sum(np<span style="color: #555555">.</span>abs(coefs<span style="color: #555555">.</span>T), axis<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)
xx <span style="color: #555555">/=</span> xx[<span style="color: #555555">-</span><span style="color: #FF6600">1</span>]

plt<span style="color: #555555">.</span>plot(xx, coefs<span style="color: #555555">.</span>T)
ymin, ymax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>ylim()
plt<span style="color: #555555">.</span>vlines(xx, ymin, ymax, linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;dashed&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;|coef| / max|coef|&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Coefficients&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;LASSO Path&#39;</span>)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>Computing regularization path using the LARS ...
.
</pre></div>


<p><img alt="png" src="../output_39_1.png" /></p>
<h3 id="13-lasso-and-elastic-net-source">13. Lasso and Elastic Net (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Lasso and elastic net (L1 and L2 penalisation) implemented using a
coordinate descent.</p>
<p>The coefficients can be forced to be positive.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">itertools</span> <span style="color: #006699; font-weight: bold">import</span> cycle
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> lasso_path, enet_path
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> datasets

diabetes <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>load_diabetes()
X <span style="color: #555555">=</span> diabetes<span style="color: #555555">.</span>data
y <span style="color: #555555">=</span> diabetes<span style="color: #555555">.</span>target

X <span style="color: #555555">/=</span> X<span style="color: #555555">.</span>std(axis<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)  <span style="color: #0099FF; font-style: italic"># Standardize data (easier to set the l1_ratio parameter)</span>

<span style="color: #0099FF; font-style: italic"># Compute paths</span>

eps <span style="color: #555555">=</span> <span style="color: #FF6600">5e-3</span>  <span style="color: #0099FF; font-style: italic"># the smaller it is the longer is the path</span>

<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Computing regularization path using the lasso...&quot;</span>)
alphas_lasso, coefs_lasso, _ <span style="color: #555555">=</span> lasso_path(X, y, eps, fit_intercept<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">False</span>)

<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Computing regularization path using the positive lasso...&quot;</span>)
alphas_positive_lasso, coefs_positive_lasso, _ <span style="color: #555555">=</span> lasso_path(
    X, y, eps, positive<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>, fit_intercept<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">False</span>)
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Computing regularization path using the elastic net...&quot;</span>)
alphas_enet, coefs_enet, _ <span style="color: #555555">=</span> enet_path(
    X, y, eps<span style="color: #555555">=</span>eps, l1_ratio<span style="color: #555555">=</span><span style="color: #FF6600">0.8</span>, fit_intercept<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">False</span>)

<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Computing regularization path using the positive elastic net...&quot;</span>)
alphas_positive_enet, coefs_positive_enet, _ <span style="color: #555555">=</span> enet_path(
    X, y, eps<span style="color: #555555">=</span>eps, l1_ratio<span style="color: #555555">=</span><span style="color: #FF6600">0.8</span>, positive<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>, fit_intercept<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">False</span>)

<span style="color: #0099FF; font-style: italic"># Display results</span>

plt<span style="color: #555555">.</span>figure(<span style="color: #FF6600">1</span>)
ax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>gca()

colors <span style="color: #555555">=</span> cycle([<span style="color: #CC3300">&#39;b&#39;</span>, <span style="color: #CC3300">&#39;r&#39;</span>, <span style="color: #CC3300">&#39;g&#39;</span>, <span style="color: #CC3300">&#39;c&#39;</span>, <span style="color: #CC3300">&#39;k&#39;</span>])
neg_log_alphas_lasso <span style="color: #555555">=</span> <span style="color: #555555">-</span>np<span style="color: #555555">.</span>log10(alphas_lasso)
neg_log_alphas_enet <span style="color: #555555">=</span> <span style="color: #555555">-</span>np<span style="color: #555555">.</span>log10(alphas_enet)
<span style="color: #006699; font-weight: bold">for</span> coef_l, coef_e, c <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">zip</span>(coefs_lasso, coefs_enet, colors):
    l1 <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>plot(neg_log_alphas_lasso, coef_l, c<span style="color: #555555">=</span>c)
    l2 <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>plot(neg_log_alphas_enet, coef_e, linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;--&#39;</span>, c<span style="color: #555555">=</span>c)

plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;-Log(alpha)&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;coefficients&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Lasso and Elastic-Net Paths&#39;</span>)
plt<span style="color: #555555">.</span>legend((l1[<span style="color: #555555">-</span><span style="color: #FF6600">1</span>], l2[<span style="color: #555555">-</span><span style="color: #FF6600">1</span>]), (<span style="color: #CC3300">&#39;Lasso&#39;</span>, <span style="color: #CC3300">&#39;Elastic-Net&#39;</span>), loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;lower left&#39;</span>)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)


plt<span style="color: #555555">.</span>figure(<span style="color: #FF6600">2</span>)
ax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>gca()
neg_log_alphas_positive_lasso <span style="color: #555555">=</span> <span style="color: #555555">-</span>np<span style="color: #555555">.</span>log10(alphas_positive_lasso)
<span style="color: #006699; font-weight: bold">for</span> coef_l, coef_pl, c <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">zip</span>(coefs_lasso, coefs_positive_lasso, colors):
    l1 <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>plot(neg_log_alphas_lasso, coef_l, c<span style="color: #555555">=</span>c)
    l2 <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>plot(neg_log_alphas_positive_lasso, coef_pl, linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;--&#39;</span>, c<span style="color: #555555">=</span>c)

plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;-Log(alpha)&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;coefficients&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Lasso and positive Lasso&#39;</span>)
plt<span style="color: #555555">.</span>legend((l1[<span style="color: #555555">-</span><span style="color: #FF6600">1</span>], l2[<span style="color: #555555">-</span><span style="color: #FF6600">1</span>]), (<span style="color: #CC3300">&#39;Lasso&#39;</span>, <span style="color: #CC3300">&#39;positive Lasso&#39;</span>), loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;lower left&#39;</span>)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)


plt<span style="color: #555555">.</span>figure(<span style="color: #FF6600">3</span>)
ax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>gca()
neg_log_alphas_positive_enet <span style="color: #555555">=</span> <span style="color: #555555">-</span>np<span style="color: #555555">.</span>log10(alphas_positive_enet)
<span style="color: #006699; font-weight: bold">for</span> (coef_e, coef_pe, c) <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">zip</span>(coefs_enet, coefs_positive_enet, colors):
    l1 <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>plot(neg_log_alphas_enet, coef_e, c<span style="color: #555555">=</span>c)
    l2 <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>plot(neg_log_alphas_positive_enet, coef_pe, linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;--&#39;</span>, c<span style="color: #555555">=</span>c)

plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;-Log(alpha)&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;coefficients&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Elastic-Net and positive Elastic-Net&#39;</span>)
plt<span style="color: #555555">.</span>legend((l1[<span style="color: #555555">-</span><span style="color: #FF6600">1</span>], l2[<span style="color: #555555">-</span><span style="color: #FF6600">1</span>]), (<span style="color: #CC3300">&#39;Elastic-Net&#39;</span>, <span style="color: #CC3300">&#39;positive Elastic-Net&#39;</span>),
           loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;lower left&#39;</span>)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>Computing regularization path using the lasso...
Computing regularization path using the positive lasso...
Computing regularization path using the elastic net...
Computing regularization path using the positive elastic net...
</pre></div>


<p><img alt="png" src="../output_42_1.png" /></p>
<p><img alt="png" src="../output_42_2.png" /></p>
<p><img alt="png" src="../output_42_3.png" /></p>
<h3 id="14-lasso-model-selection-cross-validation-aic-bic-source">14. Lasso model selection: Cross-Validation / AIC / BIC (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Use the Akaike information criterion (AIC), the Bayes Information
criterion (BIC) and cross-validation to select an optimal value
of the regularization parameter alpha of the :ref:<code>lasso</code> estimator.</p>
<p>Results obtained with LassoLarsIC are based on AIC/BIC criteria.</p>
<p>Information-criterion based model selection is very fast, but it
relies on a proper estimation of degrees of freedom, are
derived for large samples (asymptotic results) and assume the model
is correct, i.e. that the data are actually generated by this model.
They also tend to break when the problem is badly conditioned
(more features than samples).</p>
<p>For cross-validation, we use 20-fold with 2 algorithms to compute the
Lasso path: coordinate descent, as implemented by the LassoCV class, and
Lars (least angle regression) as implemented by the LassoLarsCV class.
Both algorithms give roughly the same results. They differ with regards
to their execution speed and sources of numerical errors.</p>
<p>Lars computes a path solution only for each kink in the path. As a
result, it is very efficient when there are only of few kinks, which is
the case if there are few features or samples. Also, it is able to
compute the full path without setting any meta parameter. On the
opposite, coordinate descent compute the path points on a pre-specified
grid (here we use the default). Thus it is more efficient if the number
of grid points is smaller than the number of kinks in the path. Such a
strategy can be interesting if the number of features is really large
and there are enough samples to select a large amount. In terms of
numerical errors, for heavily correlated variables, Lars will accumulate
more errors, while the coordinate descent algorithm will only sample the
path on a grid.</p>
<p>Note how the optimal value of alpha varies for each fold. This
illustrates why nested-cross validation is necessary when trying to
evaluate the performance of a method for which a parameter is chosen by
cross-validation: this choice of parameter may not be optimal for unseen
data.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">time</span>

<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> LassoCV, LassoLarsCV, LassoLarsIC
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> datasets

diabetes <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>load_diabetes()
X <span style="color: #555555">=</span> diabetes<span style="color: #555555">.</span>data
y <span style="color: #555555">=</span> diabetes<span style="color: #555555">.</span>target

rng <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>RandomState(<span style="color: #FF6600">42</span>)
X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>c_[X, rng<span style="color: #555555">.</span>randn(X<span style="color: #555555">.</span>shape[<span style="color: #FF6600">0</span>], <span style="color: #FF6600">14</span>)]  <span style="color: #0099FF; font-style: italic"># add some bad features</span>

<span style="color: #0099FF; font-style: italic"># normalize data as done by Lars to allow for comparison</span>
X <span style="color: #555555">/=</span> np<span style="color: #555555">.</span>sqrt(np<span style="color: #555555">.</span>sum(X <span style="color: #555555">**</span> <span style="color: #FF6600">2</span>, axis<span style="color: #555555">=</span><span style="color: #FF6600">0</span>))

<span style="color: #0099FF; font-style: italic">########################################</span>
<span style="color: #0099FF; font-style: italic"># LassoLarsIC: least angle regression with BIC/AIC criterion</span>

model_bic <span style="color: #555555">=</span> LassoLarsIC(criterion<span style="color: #555555">=</span><span style="color: #CC3300">&#39;bic&#39;</span>)
t1 <span style="color: #555555">=</span> time<span style="color: #555555">.</span>time()
model_bic<span style="color: #555555">.</span>fit(X, y)
t_bic <span style="color: #555555">=</span> time<span style="color: #555555">.</span>time() <span style="color: #555555">-</span> t1
alpha_bic_ <span style="color: #555555">=</span> model_bic<span style="color: #555555">.</span>alpha_

model_aic <span style="color: #555555">=</span> LassoLarsIC(criterion<span style="color: #555555">=</span><span style="color: #CC3300">&#39;aic&#39;</span>)
model_aic<span style="color: #555555">.</span>fit(X, y)
alpha_aic_ <span style="color: #555555">=</span> model_aic<span style="color: #555555">.</span>alpha_


<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">plot_ic_criterion</span>(model, name, color):
    alpha_ <span style="color: #555555">=</span> model<span style="color: #555555">.</span>alpha_
    alphas_ <span style="color: #555555">=</span> model<span style="color: #555555">.</span>alphas_
    criterion_ <span style="color: #555555">=</span> model<span style="color: #555555">.</span>criterion_
    plt<span style="color: #555555">.</span>plot(<span style="color: #555555">-</span>np<span style="color: #555555">.</span>log10(alphas_), criterion_, <span style="color: #CC3300">&#39;--&#39;</span>, color<span style="color: #555555">=</span>color,
             linewidth<span style="color: #555555">=</span><span style="color: #FF6600">3</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;</span><span style="color: #AA0000">%s</span><span style="color: #CC3300"> criterion&#39;</span> <span style="color: #555555">%</span> name)
    plt<span style="color: #555555">.</span>axvline(<span style="color: #555555">-</span>np<span style="color: #555555">.</span>log10(alpha_), color<span style="color: #555555">=</span>color, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">3</span>,
                label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;alpha: </span><span style="color: #AA0000">%s</span><span style="color: #CC3300"> estimate&#39;</span> <span style="color: #555555">%</span> name)
    plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;-log(alpha)&#39;</span>)
    plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;criterion&#39;</span>)

plt<span style="color: #555555">.</span>figure()
plot_ic_criterion(model_aic, <span style="color: #CC3300">&#39;AIC&#39;</span>, <span style="color: #CC3300">&#39;b&#39;</span>)
plot_ic_criterion(model_bic, <span style="color: #CC3300">&#39;BIC&#39;</span>, <span style="color: #CC3300">&#39;r&#39;</span>)
plt<span style="color: #555555">.</span>legend()
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Information-criterion for model selection (training time </span><span style="color: #AA0000">%.3f</span><span style="color: #CC3300">s)&#39;</span>
          <span style="color: #555555">%</span> t_bic)

<span style="color: #0099FF; font-style: italic">##############################################</span>
<span style="color: #0099FF; font-style: italic"># LassoCV: coordinate descent</span>

<span style="color: #0099FF; font-style: italic"># Compute paths</span>
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Computing regularization path using the coordinate descent lasso...&quot;</span>)
t1 <span style="color: #555555">=</span> time<span style="color: #555555">.</span>time()
model <span style="color: #555555">=</span> LassoCV(cv<span style="color: #555555">=</span><span style="color: #FF6600">20</span>)<span style="color: #555555">.</span>fit(X, y)
t_lasso_cv <span style="color: #555555">=</span> time<span style="color: #555555">.</span>time() <span style="color: #555555">-</span> t1

<span style="color: #0099FF; font-style: italic"># Display results</span>
m_log_alphas <span style="color: #555555">=</span> <span style="color: #555555">-</span>np<span style="color: #555555">.</span>log10(model<span style="color: #555555">.</span>alphas_)

plt<span style="color: #555555">.</span>figure()
ymin, ymax <span style="color: #555555">=</span> <span style="color: #FF6600">2300</span>, <span style="color: #FF6600">3800</span>
plt<span style="color: #555555">.</span>plot(m_log_alphas, model<span style="color: #555555">.</span>mse_path_, <span style="color: #CC3300">&#39;:&#39;</span>)
plt<span style="color: #555555">.</span>plot(m_log_alphas, model<span style="color: #555555">.</span>mse_path_<span style="color: #555555">.</span>mean(axis<span style="color: #555555">=-</span><span style="color: #FF6600">1</span>), <span style="color: #CC3300">&#39;k&#39;</span>,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Average across the folds&#39;</span>, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>)
plt<span style="color: #555555">.</span>axvline(<span style="color: #555555">-</span>np<span style="color: #555555">.</span>log10(model<span style="color: #555555">.</span>alpha_), linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;--&#39;</span>, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;k&#39;</span>,
            label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;alpha: CV estimate&#39;</span>)

plt<span style="color: #555555">.</span>legend()

plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;-log(alpha)&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Mean square error&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Mean square error on each fold: coordinate descent &#39;</span>
          <span style="color: #CC3300">&#39;(train time: </span><span style="color: #AA0000">%.2f</span><span style="color: #CC3300">s)&#39;</span> <span style="color: #555555">%</span> t_lasso_cv)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)
plt<span style="color: #555555">.</span>ylim(ymin, ymax)

<span style="color: #0099FF; font-style: italic">######################################</span>
<span style="color: #0099FF; font-style: italic"># LassoLarsCV: least angle regression</span>

<span style="color: #0099FF; font-style: italic"># Compute paths</span>
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Computing regularization path using the Lars lasso...&quot;</span>)
t1 <span style="color: #555555">=</span> time<span style="color: #555555">.</span>time()
model <span style="color: #555555">=</span> LassoLarsCV(cv<span style="color: #555555">=</span><span style="color: #FF6600">20</span>)<span style="color: #555555">.</span>fit(X, y)
t_lasso_lars_cv <span style="color: #555555">=</span> time<span style="color: #555555">.</span>time() <span style="color: #555555">-</span> t1

<span style="color: #0099FF; font-style: italic"># Display results</span>
m_log_alphas <span style="color: #555555">=</span> <span style="color: #555555">-</span>np<span style="color: #555555">.</span>log10(model<span style="color: #555555">.</span>cv_alphas_)

plt<span style="color: #555555">.</span>figure()
plt<span style="color: #555555">.</span>plot(m_log_alphas, model<span style="color: #555555">.</span>mse_path_, <span style="color: #CC3300">&#39;:&#39;</span>)
plt<span style="color: #555555">.</span>plot(m_log_alphas, model<span style="color: #555555">.</span>mse_path_<span style="color: #555555">.</span>mean(axis<span style="color: #555555">=-</span><span style="color: #FF6600">1</span>), <span style="color: #CC3300">&#39;k&#39;</span>,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Average across the folds&#39;</span>, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>)
plt<span style="color: #555555">.</span>axvline(<span style="color: #555555">-</span>np<span style="color: #555555">.</span>log10(model<span style="color: #555555">.</span>alpha_), linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;--&#39;</span>, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;k&#39;</span>,
            label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;alpha CV&#39;</span>)
plt<span style="color: #555555">.</span>legend()

plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;-log(alpha)&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Mean square error&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Mean square error on each fold: Lars (train time: </span><span style="color: #AA0000">%.2f</span><span style="color: #CC3300">s)&#39;</span>
          <span style="color: #555555">%</span> t_lasso_lars_cv)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)
plt<span style="color: #555555">.</span>ylim(ymin, ymax)

plt<span style="color: #555555">.</span>show()
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>/home/bitnami/anaconda3/envs/caseolap/lib/python3.6/site-packages/ipykernel_launcher.py:37: RuntimeWarning: divide by zero encountered in log10


Computing regularization path using the coordinate descent lasso...
Computing regularization path using the Lars lasso...


/home/bitnami/anaconda3/envs/caseolap/lib/python3.6/site-packages/ipykernel_launcher.py:90: RuntimeWarning: divide by zero encountered in log10
</pre></div>


<p><img alt="png" src="../output_45_3.png" /></p>
<p><img alt="png" src="../output_45_4.png" /></p>
<p><img alt="png" src="../output_45_5.png" /></p>
<h3 id="15-lasso-and-elastic-net-for-sparse-signals-source">15. Lasso and Elastic Net for Sparse Signals (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Estimates Lasso and Elastic-Net regression models on a manually generated
sparse signal corrupted with an additive noise. Estimated coefficients are
compared with the ground-truth.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.metrics</span> <span style="color: #006699; font-weight: bold">import</span> r2_score

<span style="color: #0099FF; font-style: italic">#########################################</span>
<span style="color: #0099FF; font-style: italic"># generate some sparse data to play with</span>
np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>seed(<span style="color: #FF6600">42</span>)

n_samples, n_features <span style="color: #555555">=</span> <span style="color: #FF6600">50</span>, <span style="color: #FF6600">200</span>
X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>randn(n_samples, n_features)
coef <span style="color: #555555">=</span> <span style="color: #FF6600">3</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>randn(n_features)
inds <span style="color: #555555">=</span> np<span style="color: #555555">.</span>arange(n_features)
np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>shuffle(inds)
coef[inds[<span style="color: #FF6600">10</span>:]] <span style="color: #555555">=</span> <span style="color: #FF6600">0</span>  <span style="color: #0099FF; font-style: italic"># sparsify coef</span>
y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>dot(X, coef)

<span style="color: #0099FF; font-style: italic"># add noise</span>
y <span style="color: #555555">+=</span> <span style="color: #FF6600">0.01</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>normal((n_samples,))

<span style="color: #0099FF; font-style: italic"># Split data in train set and test set</span>
n_samples <span style="color: #555555">=</span> X<span style="color: #555555">.</span>shape[<span style="color: #FF6600">0</span>]
X_train, y_train <span style="color: #555555">=</span> X[:n_samples <span style="color: #555555">/</span> <span style="color: #FF6600">2</span>], y[:n_samples <span style="color: #555555">/</span> <span style="color: #FF6600">2</span>]
X_test, y_test <span style="color: #555555">=</span> X[n_samples <span style="color: #555555">/</span> <span style="color: #FF6600">2</span>:], y[n_samples <span style="color: #555555">/</span> <span style="color: #FF6600">2</span>:]

<span style="color: #0099FF; font-style: italic">###############################################</span>
<span style="color: #0099FF; font-style: italic"># Lasso</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> Lasso

alpha <span style="color: #555555">=</span> <span style="color: #FF6600">0.1</span>
lasso <span style="color: #555555">=</span> Lasso(alpha<span style="color: #555555">=</span>alpha)

y_pred_lasso <span style="color: #555555">=</span> lasso<span style="color: #555555">.</span>fit(X_train, y_train)<span style="color: #555555">.</span>predict(X_test)
r2_score_lasso <span style="color: #555555">=</span> r2_score(y_test, y_pred_lasso)
<span style="color: #336666">print</span>(lasso)
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;r^2 on test data : </span><span style="color: #AA0000">%f</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> r2_score_lasso)

<span style="color: #0099FF; font-style: italic">############################################</span>
<span style="color: #0099FF; font-style: italic"># ElasticNet</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> ElasticNet

enet <span style="color: #555555">=</span> ElasticNet(alpha<span style="color: #555555">=</span>alpha, l1_ratio<span style="color: #555555">=</span><span style="color: #FF6600">0.7</span>)

y_pred_enet <span style="color: #555555">=</span> enet<span style="color: #555555">.</span>fit(X_train, y_train)<span style="color: #555555">.</span>predict(X_test)
r2_score_enet <span style="color: #555555">=</span> r2_score(y_test, y_pred_enet)
<span style="color: #336666">print</span>(enet)
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;r^2 on test data : </span><span style="color: #AA0000">%f</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> r2_score_enet)

plt<span style="color: #555555">.</span>plot(enet<span style="color: #555555">.</span>coef_, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;lightgreen&#39;</span>, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Elastic net coefficients&#39;</span>)
plt<span style="color: #555555">.</span>plot(lasso<span style="color: #555555">.</span>coef_, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;gold&#39;</span>, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Lasso coefficients&#39;</span>)
plt<span style="color: #555555">.</span>plot(coef, <span style="color: #CC3300">&#39;--&#39;</span>, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;navy&#39;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;original coefficients&#39;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;best&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Lasso R^2: </span><span style="color: #AA0000">%f</span><span style="color: #CC3300">, Elastic Net R^2: </span><span style="color: #AA0000">%f</span><span style="color: #CC3300">&quot;</span>
          <span style="color: #555555">%</span> (r2_score_lasso, r2_score_enet))
plt<span style="color: #555555">.</span>show()
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>/Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:23: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future
/Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future


Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute=False, random_state=None,
   selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)
r^2 on test data : 0.384710
ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.7,
      max_iter=1000, normalize=False, positive=False, precompute=False,
      random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)
r^2 on test data : 0.240176
</pre></div>


<p><img alt="png" src="../output_48_2.png" /></p>
<p><img alt="png" src="../output_48_3.png" /></p>
<p><img alt="png" src="../output_48_4.png" /></p>
<h3 id="16-joint-feature-selection-with-multi-task-lasso-source">16. Joint feature selection with multi-task Lasso (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>The multi-task lasso allows to fit multiple regression problems
jointly enforcing the selected features to be the same across
tasks. This example simulates sequential measurements, each task
is a time instant, and the relevant features vary in amplitude
over time while being the same. The multi-task lasso imposes that
features that are selected at one time point are select for all time
point. This makes feature selection by the Lasso more stable.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> MultiTaskLasso, Lasso

rng <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>RandomState(<span style="color: #FF6600">42</span>)

<span style="color: #0099FF; font-style: italic"># Generate some 2D coefficients with sine waves with random frequency and phase</span>
n_samples, n_features, n_tasks <span style="color: #555555">=</span> <span style="color: #FF6600">100</span>, <span style="color: #FF6600">30</span>, <span style="color: #FF6600">40</span>
n_relevant_features <span style="color: #555555">=</span> <span style="color: #FF6600">5</span>
coef <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros((n_tasks, n_features))
times <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">2</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>pi, n_tasks)
<span style="color: #006699; font-weight: bold">for</span> k <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">range</span>(n_relevant_features):
    coef[:, k] <span style="color: #555555">=</span> np<span style="color: #555555">.</span>sin((<span style="color: #FF6600">1.</span> <span style="color: #555555">+</span> rng<span style="color: #555555">.</span>randn(<span style="color: #FF6600">1</span>)) <span style="color: #555555">*</span> times <span style="color: #555555">+</span> <span style="color: #FF6600">3</span> <span style="color: #555555">*</span> rng<span style="color: #555555">.</span>randn(<span style="color: #FF6600">1</span>))

X <span style="color: #555555">=</span> rng<span style="color: #555555">.</span>randn(n_samples, n_features)
Y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>dot(X, coef<span style="color: #555555">.</span>T) <span style="color: #555555">+</span> rng<span style="color: #555555">.</span>randn(n_samples, n_tasks)

coef_lasso_ <span style="color: #555555">=</span> np<span style="color: #555555">.</span>array([Lasso(alpha<span style="color: #555555">=</span><span style="color: #FF6600">0.5</span>)<span style="color: #555555">.</span>fit(X, y)<span style="color: #555555">.</span>coef_ <span style="color: #006699; font-weight: bold">for</span> y <span style="color: #000000; font-weight: bold">in</span> Y<span style="color: #555555">.</span>T])
coef_multi_task_lasso_ <span style="color: #555555">=</span> MultiTaskLasso(alpha<span style="color: #555555">=</span><span style="color: #FF6600">1.</span>)<span style="color: #555555">.</span>fit(X, Y)<span style="color: #555555">.</span>coef_

<span style="color: #0099FF; font-style: italic">#############################################</span>
<span style="color: #0099FF; font-style: italic"># Plot support and time series</span>
fig <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">8</span>, <span style="color: #FF6600">5</span>))
plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">1</span>, <span style="color: #FF6600">2</span>, <span style="color: #FF6600">1</span>)
plt<span style="color: #555555">.</span>spy(coef_lasso_)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;Feature&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Time (or Task)&#39;</span>)
plt<span style="color: #555555">.</span>text(<span style="color: #FF6600">10</span>, <span style="color: #FF6600">5</span>, <span style="color: #CC3300">&#39;Lasso&#39;</span>)
plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">1</span>, <span style="color: #FF6600">2</span>, <span style="color: #FF6600">2</span>)
plt<span style="color: #555555">.</span>spy(coef_multi_task_lasso_)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;Feature&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Time (or Task)&#39;</span>)
plt<span style="color: #555555">.</span>text(<span style="color: #FF6600">10</span>, <span style="color: #FF6600">5</span>, <span style="color: #CC3300">&#39;MultiTaskLasso&#39;</span>)
fig<span style="color: #555555">.</span>suptitle(<span style="color: #CC3300">&#39;Coefficient non-zero location&#39;</span>)

feature_to_plot <span style="color: #555555">=</span> <span style="color: #FF6600">0</span>
plt<span style="color: #555555">.</span>figure()
lw <span style="color: #555555">=</span> <span style="color: #FF6600">2</span>
plt<span style="color: #555555">.</span>plot(coef[:, feature_to_plot], color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;seagreen&#39;</span>, linewidth<span style="color: #555555">=</span>lw,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Ground truth&#39;</span>)
plt<span style="color: #555555">.</span>plot(coef_lasso_[:, feature_to_plot], color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;cornflowerblue&#39;</span>, linewidth<span style="color: #555555">=</span>lw,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Lasso&#39;</span>)
plt<span style="color: #555555">.</span>plot(coef_multi_task_lasso_[:, feature_to_plot], color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;gold&#39;</span>, linewidth<span style="color: #555555">=</span>lw,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;MultiTaskLasso&#39;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;upper center&#39;</span>)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)
plt<span style="color: #555555">.</span>ylim([<span style="color: #555555">-</span><span style="color: #FF6600">1.1</span>, <span style="color: #FF6600">1.1</span>])
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_51_0.png" /></p>
<p><img alt="png" src="../output_51_1.png" /></p>
<h3 id="17-lasso-on-dense-and-sparse-data-source">17. Lasso on dense and sparse data (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>We show that linear_model.Lasso provides the same results for dense and sparse
data and that in the case of sparse data the speed is improved.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">time</span> <span style="color: #006699; font-weight: bold">import</span> time
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">scipy</span> <span style="color: #006699; font-weight: bold">import</span> sparse
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">scipy</span> <span style="color: #006699; font-weight: bold">import</span> linalg

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.datasets.samples_generator</span> <span style="color: #006699; font-weight: bold">import</span> make_regression
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> Lasso


<span style="color: #0099FF; font-style: italic">#########################################</span>
<span style="color: #0099FF; font-style: italic"># The two Lasso implementations on Dense data</span>
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;--- Dense matrices&quot;</span>)

X, y <span style="color: #555555">=</span> make_regression(n_samples<span style="color: #555555">=</span><span style="color: #FF6600">200</span>, n_features<span style="color: #555555">=</span><span style="color: #FF6600">5000</span>, random_state<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)
X_sp <span style="color: #555555">=</span> sparse<span style="color: #555555">.</span>coo_matrix(X)

alpha <span style="color: #555555">=</span> <span style="color: #FF6600">1</span>
sparse_lasso <span style="color: #555555">=</span> Lasso(alpha<span style="color: #555555">=</span>alpha, fit_intercept<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">False</span>, max_iter<span style="color: #555555">=</span><span style="color: #FF6600">1000</span>)
dense_lasso <span style="color: #555555">=</span> Lasso(alpha<span style="color: #555555">=</span>alpha, fit_intercept<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">False</span>, max_iter<span style="color: #555555">=</span><span style="color: #FF6600">1000</span>)

t0 <span style="color: #555555">=</span> time()
sparse_lasso<span style="color: #555555">.</span>fit(X_sp, y)
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Sparse Lasso done in </span><span style="color: #AA0000">%f</span><span style="color: #CC3300">s&quot;</span> <span style="color: #555555">%</span> (time() <span style="color: #555555">-</span> t0))

t0 <span style="color: #555555">=</span> time()
dense_lasso<span style="color: #555555">.</span>fit(X, y)
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Dense Lasso done in </span><span style="color: #AA0000">%f</span><span style="color: #CC3300">s&quot;</span> <span style="color: #555555">%</span> (time() <span style="color: #555555">-</span> t0))

<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Distance between coefficients : </span><span style="color: #AA0000">%s</span><span style="color: #CC3300">&quot;</span>
      <span style="color: #555555">%</span> linalg<span style="color: #555555">.</span>norm(sparse_lasso<span style="color: #555555">.</span>coef_ <span style="color: #555555">-</span> dense_lasso<span style="color: #555555">.</span>coef_))

<span style="color: #0099FF; font-style: italic">############################################</span>
<span style="color: #0099FF; font-style: italic"># The two Lasso implementations on Sparse data</span>
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;--- Sparse matrices&quot;</span>)

Xs <span style="color: #555555">=</span> X<span style="color: #555555">.</span>copy()
Xs[Xs <span style="color: #555555">&lt;</span> <span style="color: #FF6600">2.5</span>] <span style="color: #555555">=</span> <span style="color: #FF6600">0.0</span>
Xs <span style="color: #555555">=</span> sparse<span style="color: #555555">.</span>coo_matrix(Xs)
Xs <span style="color: #555555">=</span> Xs<span style="color: #555555">.</span>tocsc()

<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Matrix density : </span><span style="color: #AA0000">%s</span><span style="color: #CC3300"> </span><span style="color: #AA0000">%%</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> (Xs<span style="color: #555555">.</span>nnz <span style="color: #555555">/</span> <span style="color: #336666">float</span>(X<span style="color: #555555">.</span>size) <span style="color: #555555">*</span> <span style="color: #FF6600">100</span>))

alpha <span style="color: #555555">=</span> <span style="color: #FF6600">0.1</span>
sparse_lasso <span style="color: #555555">=</span> Lasso(alpha<span style="color: #555555">=</span>alpha, fit_intercept<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">False</span>, max_iter<span style="color: #555555">=</span><span style="color: #FF6600">10000</span>)
dense_lasso <span style="color: #555555">=</span> Lasso(alpha<span style="color: #555555">=</span>alpha, fit_intercept<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">False</span>, max_iter<span style="color: #555555">=</span><span style="color: #FF6600">10000</span>)

t0 <span style="color: #555555">=</span> time()
sparse_lasso<span style="color: #555555">.</span>fit(Xs, y)
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Sparse Lasso done in </span><span style="color: #AA0000">%f</span><span style="color: #CC3300">s&quot;</span> <span style="color: #555555">%</span> (time() <span style="color: #555555">-</span> t0))

t0 <span style="color: #555555">=</span> time()
dense_lasso<span style="color: #555555">.</span>fit(Xs<span style="color: #555555">.</span>toarray(), y)
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Dense Lasso done in </span><span style="color: #AA0000">%f</span><span style="color: #CC3300">s&quot;</span> <span style="color: #555555">%</span> (time() <span style="color: #555555">-</span> t0))

<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Distance between coefficients : </span><span style="color: #AA0000">%s</span><span style="color: #CC3300">&quot;</span>
      <span style="color: #555555">%</span> linalg<span style="color: #555555">.</span>norm(sparse_lasso<span style="color: #555555">.</span>coef_ <span style="color: #555555">-</span> dense_lasso<span style="color: #555555">.</span>coef_))
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>--- Dense matrices
Sparse Lasso done in 0.385830s
Dense Lasso done in 0.069551s
Distance between coefficients : 8.407255028117243e-14
--- Sparse matrices
Matrix density : 0.6263000000000001 %
Sparse Lasso done in 0.237209s
Dense Lasso done in 1.616462s
Distance between coefficients : 1.0424172088134681e-11
</pre></div>


<h3 id="18-plot-multi-class-sgd-on-the-iris-dataset-source">18. Plot multi-class SGD on the iris dataset (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Plot decision surface of multi-class SGD on iris dataset.
The hyperplanes corresponding to the three one-versus-all (OVA) classifiers
are represented by the dashed lines.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> datasets
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> SGDClassifier

<span style="color: #0099FF; font-style: italic"># import some data to play with</span>
iris <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>load_iris()
X <span style="color: #555555">=</span> iris<span style="color: #555555">.</span>data[:, :<span style="color: #FF6600">2</span>]  <span style="color: #0099FF; font-style: italic"># we only take the first two features. We could</span>
                      <span style="color: #0099FF; font-style: italic"># avoid this ugly slicing by using a two-dim dataset</span>
y <span style="color: #555555">=</span> iris<span style="color: #555555">.</span>target
colors <span style="color: #555555">=</span> <span style="color: #CC3300">&quot;bry&quot;</span>

<span style="color: #0099FF; font-style: italic"># shuffle</span>
idx <span style="color: #555555">=</span> np<span style="color: #555555">.</span>arange(X<span style="color: #555555">.</span>shape[<span style="color: #FF6600">0</span>])
np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>seed(<span style="color: #FF6600">13</span>)
np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>shuffle(idx)
X <span style="color: #555555">=</span> X[idx]
y <span style="color: #555555">=</span> y[idx]

<span style="color: #0099FF; font-style: italic"># standardize</span>
mean <span style="color: #555555">=</span> X<span style="color: #555555">.</span>mean(axis<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)
std <span style="color: #555555">=</span> X<span style="color: #555555">.</span>std(axis<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)
X <span style="color: #555555">=</span> (X <span style="color: #555555">-</span> mean) <span style="color: #555555">/</span> std

h <span style="color: #555555">=</span> <span style="color: #555555">.</span><span style="color: #FF6600">02</span>  <span style="color: #0099FF; font-style: italic"># step size in the mesh</span>

clf <span style="color: #555555">=</span> SGDClassifier(alpha<span style="color: #555555">=</span><span style="color: #FF6600">0.001</span>, n_iter<span style="color: #555555">=</span><span style="color: #FF6600">100</span>)<span style="color: #555555">.</span>fit(X, y)

<span style="color: #0099FF; font-style: italic"># create a mesh to plot in</span>
x_min, x_max <span style="color: #555555">=</span> X[:, <span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>min() <span style="color: #555555">-</span> <span style="color: #FF6600">1</span>, X[:, <span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>max() <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>
y_min, y_max <span style="color: #555555">=</span> X[:, <span style="color: #FF6600">1</span>]<span style="color: #555555">.</span>min() <span style="color: #555555">-</span> <span style="color: #FF6600">1</span>, X[:, <span style="color: #FF6600">1</span>]<span style="color: #555555">.</span>max() <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>
xx, yy <span style="color: #555555">=</span> np<span style="color: #555555">.</span>meshgrid(np<span style="color: #555555">.</span>arange(x_min, x_max, h),
                     np<span style="color: #555555">.</span>arange(y_min, y_max, h))

<span style="color: #0099FF; font-style: italic"># Plot the decision boundary. For that, we will assign a color to each</span>
<span style="color: #0099FF; font-style: italic"># point in the mesh [x_min, x_max]x[y_min, y_max].</span>
Z <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>predict(np<span style="color: #555555">.</span>c_[xx<span style="color: #555555">.</span>ravel(), yy<span style="color: #555555">.</span>ravel()])
<span style="color: #0099FF; font-style: italic"># Put the result into a color plot</span>
Z <span style="color: #555555">=</span> Z<span style="color: #555555">.</span>reshape(xx<span style="color: #555555">.</span>shape)
cs <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>contourf(xx, yy, Z, cmap<span style="color: #555555">=</span>plt<span style="color: #555555">.</span>cm<span style="color: #555555">.</span>Paired)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)

<span style="color: #0099FF; font-style: italic"># Plot also the training points</span>
<span style="color: #006699; font-weight: bold">for</span> i, color <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">zip</span>(clf<span style="color: #555555">.</span>classes_, colors):
    idx <span style="color: #555555">=</span> np<span style="color: #555555">.</span>where(y <span style="color: #555555">==</span> i)
    plt<span style="color: #555555">.</span>scatter(X[idx, <span style="color: #FF6600">0</span>], X[idx, <span style="color: #FF6600">1</span>], c<span style="color: #555555">=</span>color, label<span style="color: #555555">=</span>iris<span style="color: #555555">.</span>target_names[i],
                cmap<span style="color: #555555">=</span>plt<span style="color: #555555">.</span>cm<span style="color: #555555">.</span>Paired)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Decision surface of multi-class SGD&quot;</span>)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)

<span style="color: #0099FF; font-style: italic"># Plot the three one-against-all classifiers</span>
xmin, xmax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>xlim()
ymin, ymax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>ylim()
coef <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>coef_
intercept <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>intercept_


<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">plot_hyperplane</span>(c, color):
    <span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">line</span>(x0):
        <span style="color: #006699; font-weight: bold">return</span> (<span style="color: #555555">-</span>(x0 <span style="color: #555555">*</span> coef[c, <span style="color: #FF6600">0</span>]) <span style="color: #555555">-</span> intercept[c]) <span style="color: #555555">/</span> coef[c, <span style="color: #FF6600">1</span>]

    plt<span style="color: #555555">.</span>plot([xmin, xmax], [line(xmin), line(xmax)],
             ls<span style="color: #555555">=</span><span style="color: #CC3300">&quot;--&quot;</span>, color<span style="color: #555555">=</span>color)

<span style="color: #006699; font-weight: bold">for</span> i, color <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">zip</span>(clf<span style="color: #555555">.</span>classes_, colors):
    plot_hyperplane(i, color)
plt<span style="color: #555555">.</span>legend()
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_57_0.png" /></p>
<h3 id="19-sgd-penalties-source">19. SGD: Penalties (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Plot the contours of the three penalties.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>


<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">l1</span>(xs):
    <span style="color: #006699; font-weight: bold">return</span> np<span style="color: #555555">.</span>array([np<span style="color: #555555">.</span>sqrt((<span style="color: #FF6600">1</span> <span style="color: #555555">-</span> np<span style="color: #555555">.</span>sqrt(x <span style="color: #555555">**</span> <span style="color: #FF6600">2.0</span>)) <span style="color: #555555">**</span> <span style="color: #FF6600">2.0</span>) <span style="color: #006699; font-weight: bold">for</span> x <span style="color: #000000; font-weight: bold">in</span> xs])


<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">l2</span>(xs):
    <span style="color: #006699; font-weight: bold">return</span> np<span style="color: #555555">.</span>array([np<span style="color: #555555">.</span>sqrt(<span style="color: #FF6600">1.0</span> <span style="color: #555555">-</span> x <span style="color: #555555">**</span> <span style="color: #FF6600">2.0</span>) <span style="color: #006699; font-weight: bold">for</span> x <span style="color: #000000; font-weight: bold">in</span> xs])


<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">el</span>(xs, z):
    <span style="color: #006699; font-weight: bold">return</span> np<span style="color: #555555">.</span>array([(<span style="color: #FF6600">2</span> <span style="color: #555555">-</span> <span style="color: #FF6600">2</span> <span style="color: #555555">*</span> x <span style="color: #555555">-</span> <span style="color: #FF6600">2</span> <span style="color: #555555">*</span> z <span style="color: #555555">+</span> <span style="color: #FF6600">4</span> <span style="color: #555555">*</span> x <span style="color: #555555">*</span> z <span style="color: #555555">-</span>
                      (<span style="color: #FF6600">4</span> <span style="color: #555555">*</span> z <span style="color: #555555">**</span> <span style="color: #FF6600">2</span>
                       <span style="color: #555555">-</span> <span style="color: #FF6600">8</span> <span style="color: #555555">*</span> x <span style="color: #555555">*</span> z <span style="color: #555555">**</span> <span style="color: #FF6600">2</span>
                       <span style="color: #555555">+</span> <span style="color: #FF6600">8</span> <span style="color: #555555">*</span> x <span style="color: #555555">**</span> <span style="color: #FF6600">2</span> <span style="color: #555555">*</span> z <span style="color: #555555">**</span> <span style="color: #FF6600">2</span>
                       <span style="color: #555555">-</span> <span style="color: #FF6600">16</span> <span style="color: #555555">*</span> x <span style="color: #555555">**</span> <span style="color: #FF6600">2</span> <span style="color: #555555">*</span> z <span style="color: #555555">**</span> <span style="color: #FF6600">3</span>
                       <span style="color: #555555">+</span> <span style="color: #FF6600">8</span> <span style="color: #555555">*</span> x <span style="color: #555555">*</span> z <span style="color: #555555">**</span> <span style="color: #FF6600">3</span> <span style="color: #555555">+</span> <span style="color: #FF6600">4</span> <span style="color: #555555">*</span> x <span style="color: #555555">**</span> <span style="color: #FF6600">2</span> <span style="color: #555555">*</span> z <span style="color: #555555">**</span> <span style="color: #FF6600">4</span>) <span style="color: #555555">**</span> (<span style="color: #FF6600">1.</span> <span style="color: #555555">/</span> <span style="color: #FF6600">2</span>)
                      <span style="color: #555555">-</span> <span style="color: #FF6600">2</span> <span style="color: #555555">*</span> x <span style="color: #555555">*</span> z <span style="color: #555555">**</span> <span style="color: #FF6600">2</span>) <span style="color: #555555">/</span> (<span style="color: #FF6600">2</span> <span style="color: #555555">-</span> <span style="color: #FF6600">4</span> <span style="color: #555555">*</span> z) <span style="color: #006699; font-weight: bold">for</span> x <span style="color: #000000; font-weight: bold">in</span> xs])


<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">cross</span>(ext):
    plt<span style="color: #555555">.</span>plot([<span style="color: #555555">-</span>ext, ext], [<span style="color: #FF6600">0</span>, <span style="color: #FF6600">0</span>], <span style="color: #CC3300">&quot;k-&quot;</span>)
    plt<span style="color: #555555">.</span>plot([<span style="color: #FF6600">0</span>, <span style="color: #FF6600">0</span>], [<span style="color: #555555">-</span>ext, ext], <span style="color: #CC3300">&quot;k-&quot;</span>)

xs <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">1</span>, <span style="color: #FF6600">100</span>)

alpha <span style="color: #555555">=</span> <span style="color: #FF6600">0.501</span>  <span style="color: #0099FF; font-style: italic"># 0.5 division throuh zero</span>

cross(<span style="color: #FF6600">1.2</span>)

l1_color <span style="color: #555555">=</span> <span style="color: #CC3300">&quot;navy&quot;</span>
l2_color <span style="color: #555555">=</span> <span style="color: #CC3300">&quot;c&quot;</span>
elastic_net_color <span style="color: #555555">=</span> <span style="color: #CC3300">&quot;darkorange&quot;</span>
lw <span style="color: #555555">=</span> <span style="color: #FF6600">2</span>

plt<span style="color: #555555">.</span>plot(xs, l1(xs), color<span style="color: #555555">=</span>l1_color, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;L1&quot;</span>, lw<span style="color: #555555">=</span>lw)
plt<span style="color: #555555">.</span>plot(xs, <span style="color: #555555">-</span><span style="color: #FF6600">1.0</span> <span style="color: #555555">*</span> l1(xs), color<span style="color: #555555">=</span>l1_color, lw<span style="color: #555555">=</span>lw)
plt<span style="color: #555555">.</span>plot(<span style="color: #555555">-</span><span style="color: #FF6600">1</span> <span style="color: #555555">*</span> xs, l1(xs), color<span style="color: #555555">=</span>l1_color, lw<span style="color: #555555">=</span>lw)
plt<span style="color: #555555">.</span>plot(<span style="color: #555555">-</span><span style="color: #FF6600">1</span> <span style="color: #555555">*</span> xs, <span style="color: #555555">-</span><span style="color: #FF6600">1.0</span> <span style="color: #555555">*</span> l1(xs), color<span style="color: #555555">=</span>l1_color, lw<span style="color: #555555">=</span>lw)

plt<span style="color: #555555">.</span>plot(xs, l2(xs), color<span style="color: #555555">=</span>l2_color, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;L2&quot;</span>, lw<span style="color: #555555">=</span>lw)
plt<span style="color: #555555">.</span>plot(xs, <span style="color: #555555">-</span><span style="color: #FF6600">1.0</span> <span style="color: #555555">*</span> l2(xs), color<span style="color: #555555">=</span>l2_color, lw<span style="color: #555555">=</span>lw)
plt<span style="color: #555555">.</span>plot(<span style="color: #555555">-</span><span style="color: #FF6600">1</span> <span style="color: #555555">*</span> xs, l2(xs), color<span style="color: #555555">=</span>l2_color, lw<span style="color: #555555">=</span>lw)
plt<span style="color: #555555">.</span>plot(<span style="color: #555555">-</span><span style="color: #FF6600">1</span> <span style="color: #555555">*</span> xs, <span style="color: #555555">-</span><span style="color: #FF6600">1.0</span> <span style="color: #555555">*</span> l2(xs), color<span style="color: #555555">=</span>l2_color, lw<span style="color: #555555">=</span>lw)

plt<span style="color: #555555">.</span>plot(xs, el(xs, alpha), color<span style="color: #555555">=</span>elastic_net_color, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Elastic Net&quot;</span>, lw<span style="color: #555555">=</span>lw)
plt<span style="color: #555555">.</span>plot(xs, <span style="color: #555555">-</span><span style="color: #FF6600">1.0</span> <span style="color: #555555">*</span> el(xs, alpha), color<span style="color: #555555">=</span>elastic_net_color, lw<span style="color: #555555">=</span>lw)
plt<span style="color: #555555">.</span>plot(<span style="color: #555555">-</span><span style="color: #FF6600">1</span> <span style="color: #555555">*</span> xs, el(xs, alpha), color<span style="color: #555555">=</span>elastic_net_color, lw<span style="color: #555555">=</span>lw)
plt<span style="color: #555555">.</span>plot(<span style="color: #555555">-</span><span style="color: #FF6600">1</span> <span style="color: #555555">*</span> xs, <span style="color: #555555">-</span><span style="color: #FF6600">1.0</span> <span style="color: #555555">*</span> el(xs, alpha), color<span style="color: #555555">=</span>elastic_net_color, lw<span style="color: #555555">=</span>lw)

plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">r&quot;$w_0$&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">r&quot;$w_1$&quot;</span>)
plt<span style="color: #555555">.</span>legend()

plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&quot;equal&quot;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_60_0.png" /></p>
<h3 id="20-comparing-various-online-solvers-source">20. Comparing various online solvers  (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>An example showing how different online solvers perform
on the hand-written digits dataset.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> datasets

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.model_selection</span> <span style="color: #006699; font-weight: bold">import</span> train_test_split
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> SGDClassifier, Perceptron
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> PassiveAggressiveClassifier
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> LogisticRegression

heldout <span style="color: #555555">=</span> [<span style="color: #FF6600">0.95</span>, <span style="color: #FF6600">0.90</span>, <span style="color: #FF6600">0.75</span>, <span style="color: #FF6600">0.50</span>, <span style="color: #FF6600">0.01</span>]
rounds <span style="color: #555555">=</span> <span style="color: #FF6600">20</span>
digits <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>load_digits()
X, y <span style="color: #555555">=</span> digits<span style="color: #555555">.</span>data, digits<span style="color: #555555">.</span>target

classifiers <span style="color: #555555">=</span> [
    (<span style="color: #CC3300">&quot;SGD&quot;</span>, SGDClassifier()),
    (<span style="color: #CC3300">&quot;ASGD&quot;</span>, SGDClassifier(average<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>)),
    (<span style="color: #CC3300">&quot;Perceptron&quot;</span>, Perceptron()),
    (<span style="color: #CC3300">&quot;Passive-Aggressive I&quot;</span>, PassiveAggressiveClassifier(loss<span style="color: #555555">=</span><span style="color: #CC3300">&#39;hinge&#39;</span>,
                                                         C<span style="color: #555555">=</span><span style="color: #FF6600">1.0</span>)),
    (<span style="color: #CC3300">&quot;Passive-Aggressive II&quot;</span>, PassiveAggressiveClassifier(loss<span style="color: #555555">=</span><span style="color: #CC3300">&#39;squared_hinge&#39;</span>,
                                                          C<span style="color: #555555">=</span><span style="color: #FF6600">1.0</span>)),
    (<span style="color: #CC3300">&quot;SAG&quot;</span>, LogisticRegression(solver<span style="color: #555555">=</span><span style="color: #CC3300">&#39;sag&#39;</span>, tol<span style="color: #555555">=</span><span style="color: #FF6600">1e-1</span>, C<span style="color: #555555">=</span><span style="color: #FF6600">1.e4</span> <span style="color: #555555">/</span> X<span style="color: #555555">.</span>shape[<span style="color: #FF6600">0</span>]))
]

xx <span style="color: #555555">=</span> <span style="color: #FF6600">1.</span> <span style="color: #555555">-</span> np<span style="color: #555555">.</span>array(heldout)

<span style="color: #006699; font-weight: bold">for</span> name, clf <span style="color: #000000; font-weight: bold">in</span> classifiers:
    <span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;training </span><span style="color: #AA0000">%s</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> name)
    rng <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>RandomState(<span style="color: #FF6600">42</span>)
    yy <span style="color: #555555">=</span> []
    <span style="color: #006699; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> heldout:
        yy_ <span style="color: #555555">=</span> []
        <span style="color: #006699; font-weight: bold">for</span> r <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">range</span>(rounds):
            X_train, X_test, y_train, y_test <span style="color: #555555">=</span> \
                train_test_split(X, y, test_size<span style="color: #555555">=</span>i, random_state<span style="color: #555555">=</span>rng)
            clf<span style="color: #555555">.</span>fit(X_train, y_train)
            y_pred <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>predict(X_test)
            yy_<span style="color: #555555">.</span>append(<span style="color: #FF6600">1</span> <span style="color: #555555">-</span> np<span style="color: #555555">.</span>mean(y_pred <span style="color: #555555">==</span> y_test))
        yy<span style="color: #555555">.</span>append(np<span style="color: #555555">.</span>mean(yy_))
    plt<span style="color: #555555">.</span>plot(xx, yy, label<span style="color: #555555">=</span>name)

plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&quot;upper right&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Proportion train&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;Test Error Rate&quot;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>training SGD
training ASGD
training Perceptron
training Passive-Aggressive I
training Passive-Aggressive II
training SAG
</pre></div>


<p><img alt="png" src="../output_63_1.png" /></p>
<h3 id="21-sgd-convex-loss-functions-source">21. SGD: convex loss functions (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>A plot that compares the various convex loss functions supported by
:class:<code>sklearn.linear_model.SGDClassifier</code> .</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>


<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">modified_huber_loss</span>(y_true, y_pred):
    z <span style="color: #555555">=</span> y_pred <span style="color: #555555">*</span> y_true
    loss <span style="color: #555555">=</span> <span style="color: #555555">-</span><span style="color: #FF6600">4</span> <span style="color: #555555">*</span> z
    loss[z <span style="color: #555555">&gt;=</span> <span style="color: #555555">-</span><span style="color: #FF6600">1</span>] <span style="color: #555555">=</span> (<span style="color: #FF6600">1</span> <span style="color: #555555">-</span> z[z <span style="color: #555555">&gt;=</span> <span style="color: #555555">-</span><span style="color: #FF6600">1</span>]) <span style="color: #555555">**</span> <span style="color: #FF6600">2</span>
    loss[z <span style="color: #555555">&gt;=</span> <span style="color: #FF6600">1.</span>] <span style="color: #555555">=</span> <span style="color: #FF6600">0</span>
    <span style="color: #006699; font-weight: bold">return</span> loss


xmin, xmax <span style="color: #555555">=</span> <span style="color: #555555">-</span><span style="color: #FF6600">4</span>, <span style="color: #FF6600">4</span>
xx <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(xmin, xmax, <span style="color: #FF6600">100</span>)
lw <span style="color: #555555">=</span> <span style="color: #FF6600">2</span>
plt<span style="color: #555555">.</span>plot([xmin, <span style="color: #FF6600">0</span>, <span style="color: #FF6600">0</span>, xmax], [<span style="color: #FF6600">1</span>, <span style="color: #FF6600">1</span>, <span style="color: #FF6600">0</span>, <span style="color: #FF6600">0</span>], color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;gold&#39;</span>, lw<span style="color: #555555">=</span>lw,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Zero-one loss&quot;</span>)
plt<span style="color: #555555">.</span>plot(xx, np<span style="color: #555555">.</span>where(xx <span style="color: #555555">&lt;</span> <span style="color: #FF6600">1</span>, <span style="color: #FF6600">1</span> <span style="color: #555555">-</span> xx, <span style="color: #FF6600">0</span>), color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;teal&#39;</span>, lw<span style="color: #555555">=</span>lw,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Hinge loss&quot;</span>)
plt<span style="color: #555555">.</span>plot(xx, <span style="color: #555555">-</span>np<span style="color: #555555">.</span>minimum(xx, <span style="color: #FF6600">0</span>), color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;yellowgreen&#39;</span>, lw<span style="color: #555555">=</span>lw,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Perceptron loss&quot;</span>)
plt<span style="color: #555555">.</span>plot(xx, np<span style="color: #555555">.</span>log2(<span style="color: #FF6600">1</span> <span style="color: #555555">+</span> np<span style="color: #555555">.</span>exp(<span style="color: #555555">-</span>xx)), color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;cornflowerblue&#39;</span>, lw<span style="color: #555555">=</span>lw,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Log loss&quot;</span>)
plt<span style="color: #555555">.</span>plot(xx, np<span style="color: #555555">.</span>where(xx <span style="color: #555555">&lt;</span> <span style="color: #FF6600">1</span>, <span style="color: #FF6600">1</span> <span style="color: #555555">-</span> xx, <span style="color: #FF6600">0</span>) <span style="color: #555555">**</span> <span style="color: #FF6600">2</span>, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;orange&#39;</span>, lw<span style="color: #555555">=</span>lw,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Squared hinge loss&quot;</span>)
plt<span style="color: #555555">.</span>plot(xx, modified_huber_loss(xx, <span style="color: #FF6600">1</span>), color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;darkorchid&#39;</span>, lw<span style="color: #555555">=</span>lw,
         linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;--&#39;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Modified Huber loss&quot;</span>)
plt<span style="color: #555555">.</span>ylim((<span style="color: #FF6600">0</span>, <span style="color: #FF6600">8</span>))
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&quot;upper right&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">r&quot;Decision function $f(x)$&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;$L(y, f(x))$&quot;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_66_0.png" /></p>
<h3 id="22-sgd-maximum-margin-separating-hyperplane-source">22. SGD: Maximum margin separating hyperplane (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Plot the maximum margin separating hyperplane within a two-class
separable dataset using a linear Support Vector Machines classifier
trained using SGD.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> SGDClassifier
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.datasets.samples_generator</span> <span style="color: #006699; font-weight: bold">import</span> make_blobs

<span style="color: #0099FF; font-style: italic"># we create 50 separable points</span>
X, Y <span style="color: #555555">=</span> make_blobs(n_samples<span style="color: #555555">=</span><span style="color: #FF6600">50</span>, centers<span style="color: #555555">=</span><span style="color: #FF6600">2</span>, random_state<span style="color: #555555">=</span><span style="color: #FF6600">0</span>, cluster_std<span style="color: #555555">=</span><span style="color: #FF6600">0.60</span>)

<span style="color: #0099FF; font-style: italic"># fit the model</span>
clf <span style="color: #555555">=</span> SGDClassifier(loss<span style="color: #555555">=</span><span style="color: #CC3300">&quot;hinge&quot;</span>, alpha<span style="color: #555555">=</span><span style="color: #FF6600">0.01</span>, n_iter<span style="color: #555555">=</span><span style="color: #FF6600">200</span>, fit_intercept<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>)
clf<span style="color: #555555">.</span>fit(X, Y)

<span style="color: #0099FF; font-style: italic"># plot the line, the points, and the nearest vectors to the plane</span>
xx <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(<span style="color: #555555">-</span><span style="color: #FF6600">1</span>, <span style="color: #FF6600">5</span>, <span style="color: #FF6600">10</span>)
yy <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(<span style="color: #555555">-</span><span style="color: #FF6600">1</span>, <span style="color: #FF6600">5</span>, <span style="color: #FF6600">10</span>)

X1, X2 <span style="color: #555555">=</span> np<span style="color: #555555">.</span>meshgrid(xx, yy)
Z <span style="color: #555555">=</span> np<span style="color: #555555">.</span>empty(X1<span style="color: #555555">.</span>shape)
<span style="color: #006699; font-weight: bold">for</span> (i, j), val <span style="color: #000000; font-weight: bold">in</span> np<span style="color: #555555">.</span>ndenumerate(X1):
    x1 <span style="color: #555555">=</span> val
    x2 <span style="color: #555555">=</span> X2[i, j]
    p <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>decision_function([[x1, x2]])
    Z[i, j] <span style="color: #555555">=</span> p[<span style="color: #FF6600">0</span>]
levels <span style="color: #555555">=</span> [<span style="color: #555555">-</span><span style="color: #FF6600">1.0</span>, <span style="color: #FF6600">0.0</span>, <span style="color: #FF6600">1.0</span>]
linestyles <span style="color: #555555">=</span> [<span style="color: #CC3300">&#39;dashed&#39;</span>, <span style="color: #CC3300">&#39;solid&#39;</span>, <span style="color: #CC3300">&#39;dashed&#39;</span>]
colors <span style="color: #555555">=</span> <span style="color: #CC3300">&#39;k&#39;</span>
plt<span style="color: #555555">.</span>contour(X1, X2, Z, levels, colors<span style="color: #555555">=</span>colors, linestyles<span style="color: #555555">=</span>linestyles)
plt<span style="color: #555555">.</span>scatter(X[:, <span style="color: #FF6600">0</span>], X[:, <span style="color: #FF6600">1</span>], c<span style="color: #555555">=</span>Y, cmap<span style="color: #555555">=</span>plt<span style="color: #555555">.</span>cm<span style="color: #555555">.</span>Paired)

plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_69_0.png" /></p>
<h3 id="23-sgd-weighted-samples-source">23. SGD: Weighted samples  (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Plot decision function of a weighted dataset, where the size of points
is proportional to its weight.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> linear_model

<span style="color: #0099FF; font-style: italic"># we create 20 points</span>
np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>seed(<span style="color: #FF6600">0</span>)
X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>r_[np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>randn(<span style="color: #FF6600">10</span>, <span style="color: #FF6600">2</span>) <span style="color: #555555">+</span> [<span style="color: #FF6600">1</span>, <span style="color: #FF6600">1</span>], np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>randn(<span style="color: #FF6600">10</span>, <span style="color: #FF6600">2</span>)]
y <span style="color: #555555">=</span> [<span style="color: #FF6600">1</span>] <span style="color: #555555">*</span> <span style="color: #FF6600">10</span> <span style="color: #555555">+</span> [<span style="color: #555555">-</span><span style="color: #FF6600">1</span>] <span style="color: #555555">*</span> <span style="color: #FF6600">10</span>
sample_weight <span style="color: #555555">=</span> <span style="color: #FF6600">100</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>abs(np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>randn(<span style="color: #FF6600">20</span>))
<span style="color: #0099FF; font-style: italic"># and assign a bigger weight to the last 10 samples</span>
sample_weight[:<span style="color: #FF6600">10</span>] <span style="color: #555555">*=</span> <span style="color: #FF6600">10</span>

<span style="color: #0099FF; font-style: italic"># plot the weighted data points</span>
xx, yy <span style="color: #555555">=</span> np<span style="color: #555555">.</span>meshgrid(np<span style="color: #555555">.</span>linspace(<span style="color: #555555">-</span><span style="color: #FF6600">4</span>, <span style="color: #FF6600">5</span>, <span style="color: #FF6600">500</span>), np<span style="color: #555555">.</span>linspace(<span style="color: #555555">-</span><span style="color: #FF6600">4</span>, <span style="color: #FF6600">5</span>, <span style="color: #FF6600">500</span>))
plt<span style="color: #555555">.</span>figure()
plt<span style="color: #555555">.</span>scatter(X[:, <span style="color: #FF6600">0</span>], X[:, <span style="color: #FF6600">1</span>], c<span style="color: #555555">=</span>y, s<span style="color: #555555">=</span>sample_weight, alpha<span style="color: #555555">=</span><span style="color: #FF6600">0.9</span>,
            cmap<span style="color: #555555">=</span>plt<span style="color: #555555">.</span>cm<span style="color: #555555">.</span>bone)

<span style="color: #0099FF; font-style: italic">## fit the unweighted model</span>
clf <span style="color: #555555">=</span> linear_model<span style="color: #555555">.</span>SGDClassifier(alpha<span style="color: #555555">=</span><span style="color: #FF6600">0.01</span>, n_iter<span style="color: #555555">=</span><span style="color: #FF6600">100</span>)
clf<span style="color: #555555">.</span>fit(X, y)
Z <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>decision_function(np<span style="color: #555555">.</span>c_[xx<span style="color: #555555">.</span>ravel(), yy<span style="color: #555555">.</span>ravel()])
Z <span style="color: #555555">=</span> Z<span style="color: #555555">.</span>reshape(xx<span style="color: #555555">.</span>shape)
no_weights <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>contour(xx, yy, Z, levels<span style="color: #555555">=</span>[<span style="color: #FF6600">0</span>], linestyles<span style="color: #555555">=</span>[<span style="color: #CC3300">&#39;solid&#39;</span>])

<span style="color: #0099FF; font-style: italic">## fit the weighted model</span>
clf <span style="color: #555555">=</span> linear_model<span style="color: #555555">.</span>SGDClassifier(alpha<span style="color: #555555">=</span><span style="color: #FF6600">0.01</span>, n_iter<span style="color: #555555">=</span><span style="color: #FF6600">100</span>)
clf<span style="color: #555555">.</span>fit(X, y, sample_weight<span style="color: #555555">=</span>sample_weight)
Z <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>decision_function(np<span style="color: #555555">.</span>c_[xx<span style="color: #555555">.</span>ravel(), yy<span style="color: #555555">.</span>ravel()])
Z <span style="color: #555555">=</span> Z<span style="color: #555555">.</span>reshape(xx<span style="color: #555555">.</span>shape)
samples_weights <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>contour(xx, yy, Z, levels<span style="color: #555555">=</span>[<span style="color: #FF6600">0</span>], linestyles<span style="color: #555555">=</span>[<span style="color: #CC3300">&#39;dashed&#39;</span>])

plt<span style="color: #555555">.</span>legend([no_weights<span style="color: #555555">.</span>collections[<span style="color: #FF6600">0</span>], samples_weights<span style="color: #555555">.</span>collections[<span style="color: #FF6600">0</span>]],
           [<span style="color: #CC3300">&quot;no weights&quot;</span>, <span style="color: #CC3300">&quot;with weights&quot;</span>], loc<span style="color: #555555">=</span><span style="color: #CC3300">&quot;lower left&quot;</span>)

plt<span style="color: #555555">.</span>xticks(())
plt<span style="color: #555555">.</span>yticks(())
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_72_0.png" /></p>
<h3 id="24-polynomial-interpolation-source">24. Polynomial interpolation (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>This example demonstrates how to approximate a function with a polynomial of
degree n_degree by using ridge regression. Concretely, from n_samples 1d
points, it suffices to build the Vandermonde matrix, which is n_samples x
n_degree+1 and has the following form:</p>
<p>[[1, x_1, x_1 <strong> 2, x_1 </strong> 3, ...],
 [1, x_2, x_2 <strong> 2, x_2 </strong> 3, ...],
 ...]</p>
<p>Intuitively, this matrix can be interpreted as a matrix of pseudo features (the
points raised to some power). The matrix is akin to (but different from) the
matrix induced by a polynomial kernel.</p>
<p>This example shows that you can do non-linear regression with a linear model,
using a pipeline to add non-linear features. Kernel methods extend this idea
and can induce very high (even infinite) dimensional feature spaces.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> Ridge
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #006699; font-weight: bold">import</span> PolynomialFeatures
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.pipeline</span> <span style="color: #006699; font-weight: bold">import</span> make_pipeline


<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">f</span>(x):
    <span style="color: #CC3300; font-style: italic">&quot;&quot;&quot; function to approximate by polynomial interpolation&quot;&quot;&quot;</span>
    <span style="color: #006699; font-weight: bold">return</span> x <span style="color: #555555">*</span> np<span style="color: #555555">.</span>sin(x)


<span style="color: #0099FF; font-style: italic"># generate points used to plot</span>
x_plot <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">10</span>, <span style="color: #FF6600">100</span>)

<span style="color: #0099FF; font-style: italic"># generate points and keep a subset of them</span>
x <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">10</span>, <span style="color: #FF6600">100</span>)
rng <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>RandomState(<span style="color: #FF6600">0</span>)
rng<span style="color: #555555">.</span>shuffle(x)
x <span style="color: #555555">=</span> np<span style="color: #555555">.</span>sort(x[:<span style="color: #FF6600">20</span>])
y <span style="color: #555555">=</span> f(x)

<span style="color: #0099FF; font-style: italic"># create matrix versions of these arrays</span>
X <span style="color: #555555">=</span> x[:, np<span style="color: #555555">.</span>newaxis]
X_plot <span style="color: #555555">=</span> x_plot[:, np<span style="color: #555555">.</span>newaxis]

colors <span style="color: #555555">=</span> [<span style="color: #CC3300">&#39;teal&#39;</span>, <span style="color: #CC3300">&#39;yellowgreen&#39;</span>, <span style="color: #CC3300">&#39;gold&#39;</span>]
lw <span style="color: #555555">=</span> <span style="color: #FF6600">2</span>
plt<span style="color: #555555">.</span>plot(x_plot, f(x_plot), color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;cornflowerblue&#39;</span>, linewidth<span style="color: #555555">=</span>lw,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;ground truth&quot;</span>)
plt<span style="color: #555555">.</span>scatter(x, y, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;navy&#39;</span>, s<span style="color: #555555">=</span><span style="color: #FF6600">30</span>, marker<span style="color: #555555">=</span><span style="color: #CC3300">&#39;o&#39;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;training points&quot;</span>)

<span style="color: #006699; font-weight: bold">for</span> count, degree <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">enumerate</span>([<span style="color: #FF6600">3</span>, <span style="color: #FF6600">4</span>, <span style="color: #FF6600">5</span>]):
    model <span style="color: #555555">=</span> make_pipeline(PolynomialFeatures(degree), Ridge())
    model<span style="color: #555555">.</span>fit(X, y)
    y_plot <span style="color: #555555">=</span> model<span style="color: #555555">.</span>predict(X_plot)
    plt<span style="color: #555555">.</span>plot(x_plot, y_plot, color<span style="color: #555555">=</span>colors[count], linewidth<span style="color: #555555">=</span>lw,
             label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;degree </span><span style="color: #AA0000">%d</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> degree)

plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;lower left&#39;</span>)

plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_75_0.png" /></p>
<h3 id="25-orthogonal-matching-pursuit-source">25. Orthogonal Matching Pursuit (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Using orthogonal matching pursuit for recovering a sparse signal from a noisy
measurement encoded with a dictionary</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> OrthogonalMatchingPursuit
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> OrthogonalMatchingPursuitCV
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.datasets</span> <span style="color: #006699; font-weight: bold">import</span> make_sparse_coded_signal

n_components, n_features <span style="color: #555555">=</span> <span style="color: #FF6600">512</span>, <span style="color: #FF6600">100</span>
n_nonzero_coefs <span style="color: #555555">=</span> <span style="color: #FF6600">17</span>

<span style="color: #0099FF; font-style: italic"># generate the data</span>
<span style="color: #0099FF; font-style: italic">###################</span>

<span style="color: #0099FF; font-style: italic"># y = Xw</span>
<span style="color: #0099FF; font-style: italic"># |x|_0 = n_nonzero_coefs</span>

y, X, w <span style="color: #555555">=</span> make_sparse_coded_signal(n_samples<span style="color: #555555">=</span><span style="color: #FF6600">1</span>,
                                   n_components<span style="color: #555555">=</span>n_components,
                                   n_features<span style="color: #555555">=</span>n_features,
                                   n_nonzero_coefs<span style="color: #555555">=</span>n_nonzero_coefs,
                                   random_state<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)

idx, <span style="color: #555555">=</span> w<span style="color: #555555">.</span>nonzero()

<span style="color: #0099FF; font-style: italic"># distort the clean signal</span>
<span style="color: #0099FF; font-style: italic">##########################</span>
y_noisy <span style="color: #555555">=</span> y <span style="color: #555555">+</span> <span style="color: #FF6600">0.05</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>randn(<span style="color: #336666">len</span>(y))

<span style="color: #0099FF; font-style: italic"># plot the sparse signal</span>
<span style="color: #0099FF; font-style: italic">########################</span>
plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">7</span>, <span style="color: #FF6600">7</span>))
plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">4</span>, <span style="color: #FF6600">1</span>, <span style="color: #FF6600">1</span>)
plt<span style="color: #555555">.</span>xlim(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">512</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Sparse signal&quot;</span>)
plt<span style="color: #555555">.</span>stem(idx, w[idx])

<span style="color: #0099FF; font-style: italic"># plot the noise-free reconstruction</span>
<span style="color: #0099FF; font-style: italic">####################################</span>

omp <span style="color: #555555">=</span> OrthogonalMatchingPursuit(n_nonzero_coefs<span style="color: #555555">=</span>n_nonzero_coefs)
omp<span style="color: #555555">.</span>fit(X, y)
coef <span style="color: #555555">=</span> omp<span style="color: #555555">.</span>coef_
idx_r, <span style="color: #555555">=</span> coef<span style="color: #555555">.</span>nonzero()
plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">4</span>, <span style="color: #FF6600">1</span>, <span style="color: #FF6600">2</span>)
plt<span style="color: #555555">.</span>xlim(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">512</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Recovered signal from noise-free measurements&quot;</span>)
plt<span style="color: #555555">.</span>stem(idx_r, coef[idx_r])

<span style="color: #0099FF; font-style: italic"># plot the noisy reconstruction</span>
<span style="color: #0099FF; font-style: italic">###############################</span>
omp<span style="color: #555555">.</span>fit(X, y_noisy)
coef <span style="color: #555555">=</span> omp<span style="color: #555555">.</span>coef_
idx_r, <span style="color: #555555">=</span> coef<span style="color: #555555">.</span>nonzero()
plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">4</span>, <span style="color: #FF6600">1</span>, <span style="color: #FF6600">3</span>)
plt<span style="color: #555555">.</span>xlim(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">512</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Recovered signal from noisy measurements&quot;</span>)
plt<span style="color: #555555">.</span>stem(idx_r, coef[idx_r])

<span style="color: #0099FF; font-style: italic"># plot the noisy reconstruction with number of non-zeros set by CV</span>
<span style="color: #0099FF; font-style: italic">##################################################################</span>
omp_cv <span style="color: #555555">=</span> OrthogonalMatchingPursuitCV()
omp_cv<span style="color: #555555">.</span>fit(X, y_noisy)
coef <span style="color: #555555">=</span> omp_cv<span style="color: #555555">.</span>coef_
idx_r, <span style="color: #555555">=</span> coef<span style="color: #555555">.</span>nonzero()
plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">4</span>, <span style="color: #FF6600">1</span>, <span style="color: #FF6600">4</span>)
plt<span style="color: #555555">.</span>xlim(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">512</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Recovered signal from noisy measurements with CV&quot;</span>)
plt<span style="color: #555555">.</span>stem(idx_r, coef[idx_r])

plt<span style="color: #555555">.</span>subplots_adjust(<span style="color: #FF6600">0.06</span>, <span style="color: #FF6600">0.04</span>, <span style="color: #FF6600">0.94</span>, <span style="color: #FF6600">0.90</span>, <span style="color: #FF6600">0.20</span>, <span style="color: #FF6600">0.38</span>)
plt<span style="color: #555555">.</span>suptitle(<span style="color: #CC3300">&#39;Sparse signal recovery with Orthogonal Matching Pursuit&#39;</span>,
             fontsize<span style="color: #555555">=</span><span style="color: #FF6600">16</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_78_0.png" /></p>
<h3 id="26-automatic-relevance-determination-regression-ard-source">26. Automatic Relevance Determination Regression (ARD) (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Fit regression model with Bayesian Ridge Regression.</p>
<p>See :ref:<code>bayesian_ridge_regression</code> for more information on the regressor.</p>
<p>Compared to the OLS (ordinary least squares) estimator, the coefficient
weights are slightly shifted toward zeros, which stabilises them.</p>
<p>The histogram of the estimated weights is very peaked, as a sparsity-inducing
prior is implied on the weights.</p>
<p>The estimation of the model is done by iteratively maximizing the
marginal log-likelihood of the observations.</p>
<p>We also plot predictions and uncertainties for ARD
for one dimensional regression using polynomial feature expansion.
Note the uncertainty starts going up on the right side of the plot.
This is because these test samples are outside of the range of the training
samples.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">scipy</span> <span style="color: #006699; font-weight: bold">import</span> stats

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> ARDRegression, LinearRegression

<span style="color: #0099FF; font-style: italic">###############################################################################</span>
<span style="color: #0099FF; font-style: italic"># Generating simulated data with Gaussian weights</span>

<span style="color: #0099FF; font-style: italic"># Parameters of the example</span>
np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>seed(<span style="color: #FF6600">0</span>)
n_samples, n_features <span style="color: #555555">=</span> <span style="color: #FF6600">100</span>, <span style="color: #FF6600">100</span>
<span style="color: #0099FF; font-style: italic"># Create Gaussian data</span>
X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>randn(n_samples, n_features)
<span style="color: #0099FF; font-style: italic"># Create weights with a precision lambda_ of 4.</span>
lambda_ <span style="color: #555555">=</span> <span style="color: #FF6600">4.</span>
w <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros(n_features)
<span style="color: #0099FF; font-style: italic"># Only keep 10 weights of interest</span>
relevant_features <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>randint(<span style="color: #FF6600">0</span>, n_features, <span style="color: #FF6600">10</span>)
<span style="color: #006699; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> relevant_features:
    w[i] <span style="color: #555555">=</span> stats<span style="color: #555555">.</span>norm<span style="color: #555555">.</span>rvs(loc<span style="color: #555555">=</span><span style="color: #FF6600">0</span>, scale<span style="color: #555555">=</span><span style="color: #FF6600">1.</span> <span style="color: #555555">/</span> np<span style="color: #555555">.</span>sqrt(lambda_))
<span style="color: #0099FF; font-style: italic"># Create noise with a precision alpha of 50.</span>
alpha_ <span style="color: #555555">=</span> <span style="color: #FF6600">50.</span>
noise <span style="color: #555555">=</span> stats<span style="color: #555555">.</span>norm<span style="color: #555555">.</span>rvs(loc<span style="color: #555555">=</span><span style="color: #FF6600">0</span>, scale<span style="color: #555555">=</span><span style="color: #FF6600">1.</span> <span style="color: #555555">/</span> np<span style="color: #555555">.</span>sqrt(alpha_), size<span style="color: #555555">=</span>n_samples)
<span style="color: #0099FF; font-style: italic"># Create the target</span>
y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>dot(X, w) <span style="color: #555555">+</span> noise

<span style="color: #0099FF; font-style: italic">#####################################</span>
<span style="color: #0099FF; font-style: italic"># Fit the ARD Regression</span>
clf <span style="color: #555555">=</span> ARDRegression(compute_score<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>)
clf<span style="color: #555555">.</span>fit(X, y)

ols <span style="color: #555555">=</span> LinearRegression()
ols<span style="color: #555555">.</span>fit(X, y)

<span style="color: #0099FF; font-style: italic">#####################################</span>
<span style="color: #0099FF; font-style: italic"># Plot the true weights, the estimated weights, the histogram of the</span>
<span style="color: #0099FF; font-style: italic"># weights, and predictions with standard deviations</span>
plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">6</span>, <span style="color: #FF6600">5</span>))
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Weights of the model&quot;</span>)
plt<span style="color: #555555">.</span>plot(clf<span style="color: #555555">.</span>coef_, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;darkblue&#39;</span>, linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;-&#39;</span>, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;ARD estimate&quot;</span>)
plt<span style="color: #555555">.</span>plot(ols<span style="color: #555555">.</span>coef_, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;yellowgreen&#39;</span>, linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;:&#39;</span>, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;OLS estimate&quot;</span>)
plt<span style="color: #555555">.</span>plot(w, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;orange&#39;</span>, linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;-&#39;</span>, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Ground truth&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Features&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;Values of the weights&quot;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)

plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">6</span>, <span style="color: #FF6600">5</span>))
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Histogram of the weights&quot;</span>)
plt<span style="color: #555555">.</span>hist(clf<span style="color: #555555">.</span>coef_, bins<span style="color: #555555">=</span>n_features, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;navy&#39;</span>, log<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>)
plt<span style="color: #555555">.</span>scatter(clf<span style="color: #555555">.</span>coef_[relevant_features], <span style="color: #FF6600">5</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>ones(<span style="color: #336666">len</span>(relevant_features)),
            color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;gold&#39;</span>, marker<span style="color: #555555">=</span><span style="color: #CC3300">&#39;o&#39;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Relevant features&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;Features&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Values of the weights&quot;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)

plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">6</span>, <span style="color: #FF6600">5</span>))
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Marginal log-likelihood&quot;</span>)
plt<span style="color: #555555">.</span>plot(clf<span style="color: #555555">.</span>scores_, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;navy&#39;</span>, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;Score&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Iterations&quot;</span>)


<span style="color: #0099FF; font-style: italic"># Plotting some predictions for polynomial regression</span>
<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">f</span>(x, noise_amount):
    y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>sqrt(x) <span style="color: #555555">*</span> np<span style="color: #555555">.</span>sin(x)
    noise <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>normal(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">1</span>, <span style="color: #336666">len</span>(x))
    <span style="color: #006699; font-weight: bold">return</span> y <span style="color: #555555">+</span> noise_amount <span style="color: #555555">*</span> noise


degree <span style="color: #555555">=</span> <span style="color: #FF6600">10</span>
X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">10</span>, <span style="color: #FF6600">100</span>)
y <span style="color: #555555">=</span> f(X, noise_amount<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)
clf_poly <span style="color: #555555">=</span> ARDRegression(threshold_lambda<span style="color: #555555">=</span><span style="color: #FF6600">1e5</span>)
clf_poly<span style="color: #555555">.</span>fit(np<span style="color: #555555">.</span>vander(X, degree), y)

X_plot <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">11</span>, <span style="color: #FF6600">25</span>)
y_plot <span style="color: #555555">=</span> f(X_plot, noise_amount<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)
y_mean, y_std <span style="color: #555555">=</span> clf_poly<span style="color: #555555">.</span>predict(np<span style="color: #555555">.</span>vander(X_plot, degree), return_std<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>)
plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">6</span>, <span style="color: #FF6600">5</span>))
plt<span style="color: #555555">.</span>errorbar(X_plot, y_mean, y_std, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;navy&#39;</span>,
             label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Polynomial ARD&quot;</span>, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>)
plt<span style="color: #555555">.</span>plot(X_plot, y_plot, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;gold&#39;</span>, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Ground Truth&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;Output y&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Feature X&quot;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&quot;lower left&quot;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_81_0.png" /></p>
<p><img alt="png" src="../output_81_1.png" /></p>
<p><img alt="png" src="../output_81_2.png" /></p>
<p><img alt="png" src="../output_81_3.png" /></p>
<h3 id="27-huberregressor-vs-ridge-on-dataset-with-strong-outliers-source">27. HuberRegressor vs Ridge on dataset with strong outliers (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Fit Ridge and HuberRegressor on a dataset with outliers.</p>
<p>The example shows that the predictions in ridge are strongly influenced
by the outliers present in the dataset. The Huber regressor is less
influenced by the outliers since the model uses the linear loss for these.
As the parameter epsilon is increased for the Huber regressor, the decision
function approaches that of the ridge.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.datasets</span> <span style="color: #006699; font-weight: bold">import</span> make_regression
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> HuberRegressor, Ridge

<span style="color: #0099FF; font-style: italic"># Generate toy data.</span>
rng <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>RandomState(<span style="color: #FF6600">0</span>)
X, y <span style="color: #555555">=</span> make_regression(n_samples<span style="color: #555555">=</span><span style="color: #FF6600">20</span>, n_features<span style="color: #555555">=</span><span style="color: #FF6600">1</span>, random_state<span style="color: #555555">=</span><span style="color: #FF6600">0</span>, noise<span style="color: #555555">=</span><span style="color: #FF6600">4.0</span>,
                       bias<span style="color: #555555">=</span><span style="color: #FF6600">100.0</span>)

<span style="color: #0099FF; font-style: italic"># Add four strong outliers to the dataset.</span>
X_outliers <span style="color: #555555">=</span> rng<span style="color: #555555">.</span>normal(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">0.5</span>, size<span style="color: #555555">=</span>(<span style="color: #FF6600">4</span>, <span style="color: #FF6600">1</span>))
y_outliers <span style="color: #555555">=</span> rng<span style="color: #555555">.</span>normal(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">2.0</span>, size<span style="color: #555555">=</span><span style="color: #FF6600">4</span>)
X_outliers[:<span style="color: #FF6600">2</span>, :] <span style="color: #555555">+=</span> X<span style="color: #555555">.</span>max() <span style="color: #555555">+</span> X<span style="color: #555555">.</span>mean() <span style="color: #555555">/</span> <span style="color: #FF6600">4.</span>
X_outliers[<span style="color: #FF6600">2</span>:, :] <span style="color: #555555">+=</span> X<span style="color: #555555">.</span>min() <span style="color: #555555">-</span> X<span style="color: #555555">.</span>mean() <span style="color: #555555">/</span> <span style="color: #FF6600">4.</span>
y_outliers[:<span style="color: #FF6600">2</span>] <span style="color: #555555">+=</span> y<span style="color: #555555">.</span>min() <span style="color: #555555">-</span> y<span style="color: #555555">.</span>mean() <span style="color: #555555">/</span> <span style="color: #FF6600">4.</span>
y_outliers[<span style="color: #FF6600">2</span>:] <span style="color: #555555">+=</span> y<span style="color: #555555">.</span>max() <span style="color: #555555">+</span> y<span style="color: #555555">.</span>mean() <span style="color: #555555">/</span> <span style="color: #FF6600">4.</span>
X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>vstack((X, X_outliers))
y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>concatenate((y, y_outliers))
plt<span style="color: #555555">.</span>plot(X, y, <span style="color: #CC3300">&#39;b.&#39;</span>)

<span style="color: #0099FF; font-style: italic"># Fit the huber regressor over a series of epsilon values.</span>
colors <span style="color: #555555">=</span> [<span style="color: #CC3300">&#39;r-&#39;</span>, <span style="color: #CC3300">&#39;b-&#39;</span>, <span style="color: #CC3300">&#39;y-&#39;</span>, <span style="color: #CC3300">&#39;m-&#39;</span>]

x <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(X<span style="color: #555555">.</span>min(), X<span style="color: #555555">.</span>max(), <span style="color: #FF6600">7</span>)
epsilon_values <span style="color: #555555">=</span> [<span style="color: #FF6600">1.35</span>, <span style="color: #FF6600">1.5</span>, <span style="color: #FF6600">1.75</span>, <span style="color: #FF6600">1.9</span>]
<span style="color: #006699; font-weight: bold">for</span> k, epsilon <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">enumerate</span>(epsilon_values):
    huber <span style="color: #555555">=</span> HuberRegressor(fit_intercept<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>, alpha<span style="color: #555555">=</span><span style="color: #FF6600">0.0</span>, max_iter<span style="color: #555555">=</span><span style="color: #FF6600">100</span>,
                           epsilon<span style="color: #555555">=</span>epsilon)
    huber<span style="color: #555555">.</span>fit(X, y)
    coef_ <span style="color: #555555">=</span> huber<span style="color: #555555">.</span>coef_ <span style="color: #555555">*</span> x <span style="color: #555555">+</span> huber<span style="color: #555555">.</span>intercept_
    plt<span style="color: #555555">.</span>plot(x, coef_, colors[k], label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;huber loss, </span><span style="color: #AA0000">%s</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> epsilon)

<span style="color: #0099FF; font-style: italic"># Fit a ridge regressor to compare it to huber regressor.</span>
ridge <span style="color: #555555">=</span> Ridge(fit_intercept<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>, alpha<span style="color: #555555">=</span><span style="color: #FF6600">0.0</span>, random_state<span style="color: #555555">=</span><span style="color: #FF6600">0</span>, normalize<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>)
ridge<span style="color: #555555">.</span>fit(X, y)
coef_ridge <span style="color: #555555">=</span> ridge<span style="color: #555555">.</span>coef_
coef_ <span style="color: #555555">=</span> ridge<span style="color: #555555">.</span>coef_ <span style="color: #555555">*</span> x <span style="color: #555555">+</span> ridge<span style="color: #555555">.</span>intercept_
plt<span style="color: #555555">.</span>plot(x, coef_, <span style="color: #CC3300">&#39;g-&#39;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;ridge regression&quot;</span>)

plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Comparison of HuberRegressor vs Ridge&quot;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;X&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;y&quot;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_84_0.png" /></p>
<p><img alt="png" src="../output_84_1.png" /></p>
<p><img alt="png" src="../output_84_2.png" /></p>
<h3 id="28-robust-linear-model-estimation-using-ransac-source">28. Robust linear model estimation using RANSAC (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">matplotlib</span> <span style="color: #006699; font-weight: bold">import</span> pyplot <span style="color: #006699; font-weight: bold">as</span> plt

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> linear_model, datasets


n_samples <span style="color: #555555">=</span> <span style="color: #FF6600">1000</span>
n_outliers <span style="color: #555555">=</span> <span style="color: #FF6600">50</span>


X, y, coef <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>make_regression(n_samples<span style="color: #555555">=</span>n_samples, n_features<span style="color: #555555">=</span><span style="color: #FF6600">1</span>,
                                      n_informative<span style="color: #555555">=</span><span style="color: #FF6600">1</span>, noise<span style="color: #555555">=</span><span style="color: #FF6600">10</span>,
                                      coef<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>, random_state<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)

<span style="color: #0099FF; font-style: italic"># Add outlier data</span>
np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>seed(<span style="color: #FF6600">0</span>)
X[:n_outliers] <span style="color: #555555">=</span> <span style="color: #FF6600">3</span> <span style="color: #555555">+</span> <span style="color: #FF6600">0.5</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>normal(size<span style="color: #555555">=</span>(n_outliers, <span style="color: #FF6600">1</span>))
y[:n_outliers] <span style="color: #555555">=</span> <span style="color: #555555">-</span><span style="color: #FF6600">3</span> <span style="color: #555555">+</span> <span style="color: #FF6600">10</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>normal(size<span style="color: #555555">=</span>n_outliers)

<span style="color: #0099FF; font-style: italic"># Fit line using all data</span>
model <span style="color: #555555">=</span> linear_model<span style="color: #555555">.</span>LinearRegression()
model<span style="color: #555555">.</span>fit(X, y)

<span style="color: #0099FF; font-style: italic"># Robustly fit linear model with RANSAC algorithm</span>
model_ransac <span style="color: #555555">=</span> linear_model<span style="color: #555555">.</span>RANSACRegressor(linear_model<span style="color: #555555">.</span>LinearRegression())
model_ransac<span style="color: #555555">.</span>fit(X, y)
inlier_mask <span style="color: #555555">=</span> model_ransac<span style="color: #555555">.</span>inlier_mask_
outlier_mask <span style="color: #555555">=</span> np<span style="color: #555555">.</span>logical_not(inlier_mask)

<span style="color: #0099FF; font-style: italic"># Predict data of estimated models</span>
line_X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>arange(<span style="color: #555555">-</span><span style="color: #FF6600">5</span>, <span style="color: #FF6600">5</span>)
line_y <span style="color: #555555">=</span> model<span style="color: #555555">.</span>predict(line_X[:, np<span style="color: #555555">.</span>newaxis])
line_y_ransac <span style="color: #555555">=</span> model_ransac<span style="color: #555555">.</span>predict(line_X[:, np<span style="color: #555555">.</span>newaxis])

<span style="color: #0099FF; font-style: italic"># Compare estimated coefficients</span>
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Estimated coefficients (true, normal, RANSAC):&quot;</span>)
<span style="color: #336666">print</span>(coef, model<span style="color: #555555">.</span>coef_, model_ransac<span style="color: #555555">.</span>estimator_<span style="color: #555555">.</span>coef_)

lw <span style="color: #555555">=</span> <span style="color: #FF6600">2</span>
plt<span style="color: #555555">.</span>scatter(X[inlier_mask], y[inlier_mask], color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;yellowgreen&#39;</span>, marker<span style="color: #555555">=</span><span style="color: #CC3300">&#39;.&#39;</span>,
            label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Inliers&#39;</span>)
plt<span style="color: #555555">.</span>scatter(X[outlier_mask], y[outlier_mask], color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;gold&#39;</span>, marker<span style="color: #555555">=</span><span style="color: #CC3300">&#39;.&#39;</span>,
            label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Outliers&#39;</span>)
plt<span style="color: #555555">.</span>plot(line_X, line_y, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;navy&#39;</span>, linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;-&#39;</span>, linewidth<span style="color: #555555">=</span>lw,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Linear regressor&#39;</span>)
plt<span style="color: #555555">.</span>plot(line_X, line_y_ransac, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;cornflowerblue&#39;</span>, linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;-&#39;</span>,
         linewidth<span style="color: #555555">=</span>lw, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;RANSAC regressor&#39;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;lower right&#39;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>Estimated coefficients (true, normal, RANSAC):
82.1903908407869 [ 54.17236387] [ 82.08533159]
</pre></div>


<p><img alt="png" src="../output_86_1.png" /></p>
<h3 id="29-robust-linear-estimator-fitting-source">29.  Robust linear estimator fitting (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Here a sine function is fit with a polynomial of order 3, for values
close to zero.</p>
<p>Robust fitting is demoed in different situations:</p>
<ul>
<li>
<p>No measurement errors, only modelling errors (fitting a sine with a
  polynomial)</p>
</li>
<li>
<p>Measurement errors in X</p>
</li>
<li>
<p>Measurement errors in y</p>
</li>
</ul>
<p>The median absolute deviation to non corrupt new data is used to judge
the quality of the prediction.</p>
<p>What we can see that:</p>
<ul>
<li>
<p>RANSAC is good for strong outliers in the y direction</p>
</li>
<li>
<p>TheilSen is good for small outliers, both in direction X and y, but has
  a break point above which it performs worse than OLS.</p>
</li>
<li>
<p>The scores of HuberRegressor may not be compared directly to both TheilSen
  and RANSAC because it does not attempt to completely filter the outliers
  but lessen their effect.</p>
</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">matplotlib</span> <span style="color: #006699; font-weight: bold">import</span> pyplot <span style="color: #006699; font-weight: bold">as</span> plt
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> (
    LinearRegression, TheilSenRegressor, RANSACRegressor, HuberRegressor)
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.metrics</span> <span style="color: #006699; font-weight: bold">import</span> mean_squared_error
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #006699; font-weight: bold">import</span> PolynomialFeatures
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.pipeline</span> <span style="color: #006699; font-weight: bold">import</span> make_pipeline

np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>seed(<span style="color: #FF6600">42</span>)

X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>normal(size<span style="color: #555555">=</span><span style="color: #FF6600">400</span>)
y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>sin(X)
<span style="color: #0099FF; font-style: italic"># Make sure that it X is 2D</span>
X <span style="color: #555555">=</span> X[:, np<span style="color: #555555">.</span>newaxis]

X_test <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>normal(size<span style="color: #555555">=</span><span style="color: #FF6600">200</span>)
y_test <span style="color: #555555">=</span> np<span style="color: #555555">.</span>sin(X_test)
X_test <span style="color: #555555">=</span> X_test[:, np<span style="color: #555555">.</span>newaxis]

y_errors <span style="color: #555555">=</span> y<span style="color: #555555">.</span>copy()
y_errors[::<span style="color: #FF6600">3</span>] <span style="color: #555555">=</span> <span style="color: #FF6600">3</span>

X_errors <span style="color: #555555">=</span> X<span style="color: #555555">.</span>copy()
X_errors[::<span style="color: #FF6600">3</span>] <span style="color: #555555">=</span> <span style="color: #FF6600">3</span>

y_errors_large <span style="color: #555555">=</span> y<span style="color: #555555">.</span>copy()
y_errors_large[::<span style="color: #FF6600">3</span>] <span style="color: #555555">=</span> <span style="color: #FF6600">10</span>

X_errors_large <span style="color: #555555">=</span> X<span style="color: #555555">.</span>copy()
X_errors_large[::<span style="color: #FF6600">3</span>] <span style="color: #555555">=</span> <span style="color: #FF6600">10</span>

estimators <span style="color: #555555">=</span> [(<span style="color: #CC3300">&#39;OLS&#39;</span>, LinearRegression()),
              (<span style="color: #CC3300">&#39;Theil-Sen&#39;</span>, TheilSenRegressor(random_state<span style="color: #555555">=</span><span style="color: #FF6600">42</span>)),
              (<span style="color: #CC3300">&#39;RANSAC&#39;</span>, RANSACRegressor(random_state<span style="color: #555555">=</span><span style="color: #FF6600">42</span>)),
              (<span style="color: #CC3300">&#39;HuberRegressor&#39;</span>, HuberRegressor())]
colors <span style="color: #555555">=</span> {<span style="color: #CC3300">&#39;OLS&#39;</span>: <span style="color: #CC3300">&#39;turquoise&#39;</span>, <span style="color: #CC3300">&#39;Theil-Sen&#39;</span>: <span style="color: #CC3300">&#39;gold&#39;</span>, <span style="color: #CC3300">&#39;RANSAC&#39;</span>: <span style="color: #CC3300">&#39;lightgreen&#39;</span>, <span style="color: #CC3300">&#39;HuberRegressor&#39;</span>: <span style="color: #CC3300">&#39;black&#39;</span>}
linestyle <span style="color: #555555">=</span> {<span style="color: #CC3300">&#39;OLS&#39;</span>: <span style="color: #CC3300">&#39;-&#39;</span>, <span style="color: #CC3300">&#39;Theil-Sen&#39;</span>: <span style="color: #CC3300">&#39;-.&#39;</span>, <span style="color: #CC3300">&#39;RANSAC&#39;</span>: <span style="color: #CC3300">&#39;--&#39;</span>, <span style="color: #CC3300">&#39;HuberRegressor&#39;</span>: <span style="color: #CC3300">&#39;--&#39;</span>}
lw <span style="color: #555555">=</span> <span style="color: #FF6600">3</span>

x_plot <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(X<span style="color: #555555">.</span>min(), X<span style="color: #555555">.</span>max())
<span style="color: #006699; font-weight: bold">for</span> title, this_X, this_y <span style="color: #000000; font-weight: bold">in</span> [
        (<span style="color: #CC3300">&#39;Modeling Errors Only&#39;</span>, X, y),
        (<span style="color: #CC3300">&#39;Corrupt X, Small Deviants&#39;</span>, X_errors, y),
        (<span style="color: #CC3300">&#39;Corrupt y, Small Deviants&#39;</span>, X, y_errors),
        (<span style="color: #CC3300">&#39;Corrupt X, Large Deviants&#39;</span>, X_errors_large, y),
        (<span style="color: #CC3300">&#39;Corrupt y, Large Deviants&#39;</span>, X, y_errors_large)]:
    plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">5</span>, <span style="color: #FF6600">4</span>))
    plt<span style="color: #555555">.</span>plot(this_X[:, <span style="color: #FF6600">0</span>], this_y, <span style="color: #CC3300">&#39;b+&#39;</span>)

    <span style="color: #006699; font-weight: bold">for</span> name, estimator <span style="color: #000000; font-weight: bold">in</span> estimators:
        model <span style="color: #555555">=</span> make_pipeline(PolynomialFeatures(<span style="color: #FF6600">3</span>), estimator)
        model<span style="color: #555555">.</span>fit(this_X, this_y)
        mse <span style="color: #555555">=</span> mean_squared_error(model<span style="color: #555555">.</span>predict(X_test), y_test)
        y_plot <span style="color: #555555">=</span> model<span style="color: #555555">.</span>predict(x_plot[:, np<span style="color: #555555">.</span>newaxis])
        plt<span style="color: #555555">.</span>plot(x_plot, y_plot, color<span style="color: #555555">=</span>colors[name], linestyle<span style="color: #555555">=</span>linestyle[name],
                 linewidth<span style="color: #555555">=</span>lw, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;</span><span style="color: #AA0000">%s</span><span style="color: #CC3300">: error = </span><span style="color: #AA0000">%.3f</span><span style="color: #CC3300">&#39;</span> <span style="color: #555555">%</span> (name, mse))

    legend_title <span style="color: #555555">=</span> <span style="color: #CC3300">&#39;Error of Mean</span><span style="color: #CC3300; font-weight: bold">\n</span><span style="color: #CC3300">Absolute Deviation</span><span style="color: #CC3300; font-weight: bold">\n</span><span style="color: #CC3300">to Non-corrupt Data&#39;</span>
    legend <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;upper right&#39;</span>, frameon<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">False</span>, title<span style="color: #555555">=</span>legend_title,
                        prop<span style="color: #555555">=</span><span style="color: #336666">dict</span>(size<span style="color: #555555">=</span><span style="color: #CC3300">&#39;x-small&#39;</span>))
    plt<span style="color: #555555">.</span>xlim(<span style="color: #555555">-</span><span style="color: #FF6600">4</span>, <span style="color: #FF6600">10.2</span>)
    plt<span style="color: #555555">.</span>ylim(<span style="color: #555555">-</span><span style="color: #FF6600">2</span>, <span style="color: #FF6600">10.2</span>)
    plt<span style="color: #555555">.</span>title(title)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_89_0.png" /></p>
<p><img alt="png" src="../output_89_1.png" /></p>
<p><img alt="png" src="../output_89_2.png" /></p>
<p><img alt="png" src="../output_89_3.png" /></p>
<p><img alt="png" src="../output_89_4.png" /></p>
<h3 id="30-sparse-recovery-feature-selection-for-sparse-linear-models-source">30. Sparse recovery: feature selection for sparse linear models (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Given a small number of observations, we want to recover which features
of X are relevant to explain y. For this :ref:<code>sparse linear models
&lt;l1_feature_selection&gt;</code> can outperform standard statistical tests if the
true model is sparse, i.e. if a small fraction of the features are
relevant.</p>
<p>As detailed in :ref:<code>the compressive sensing notes
&lt;compressive_sensing&gt;</code>, the ability of L1-based approach to identify the
relevant variables depends on the sparsity of the ground truth, the
number of samples, the number of features, the conditioning of the
design matrix on the signal subspace, the amount of noise, and the
absolute value of the smallest non-zero coefficient [Wainwright2006]
(http://statistics.berkeley.edu/sites/default/files/tech-reports/709.pdf).</p>
<p>Here we keep all parameters constant and vary the conditioning of the
design matrix. For a well-conditioned design matrix (small mutual
incoherence) we are exactly in compressive sensing conditions (i.i.d
Gaussian sensing matrix), and L1-recovery with the Lasso performs very
well. For an ill-conditioned matrix (high mutual incoherence),
regressors are very correlated, and the Lasso randomly selects one.
However, randomized-Lasso can recover the ground truth well.</p>
<p>In each situation, we first vary the alpha parameter setting the sparsity
of the estimated model and look at the stability scores of the randomized
Lasso. This analysis, knowing the ground truth, shows an optimal regime
in which relevant features stand out from the irrelevant ones. If alpha
is chosen too small, non-relevant variables enter the model. On the
opposite, if alpha is selected too large, the Lasso is equivalent to
stepwise regression, and thus brings no advantage over a univariate
F-test.</p>
<p>In a second time, we set alpha and compare the performance of different
feature selection methods, using the area under curve (AUC) of the
precision-recall.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">warnings</span>

<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">scipy</span> <span style="color: #006699; font-weight: bold">import</span> linalg

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> (RandomizedLasso, lasso_stability_path,
                                  LassoLarsCV)
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.feature_selection</span> <span style="color: #006699; font-weight: bold">import</span> f_regression
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #006699; font-weight: bold">import</span> StandardScaler
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.metrics</span> <span style="color: #006699; font-weight: bold">import</span> auc, precision_recall_curve
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> ExtraTreesRegressor
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.utils.extmath</span> <span style="color: #006699; font-weight: bold">import</span> pinvh
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.exceptions</span> <span style="color: #006699; font-weight: bold">import</span> ConvergenceWarning


<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">mutual_incoherence</span>(X_relevant, X_irelevant):
    <span style="color: #CC3300; font-style: italic">&quot;&quot;&quot;Mutual incoherence, as defined by formula (26a) of [Wainwright2006].</span>
<span style="color: #CC3300; font-style: italic">    &quot;&quot;&quot;</span>
    projector <span style="color: #555555">=</span> np<span style="color: #555555">.</span>dot(np<span style="color: #555555">.</span>dot(X_irelevant<span style="color: #555555">.</span>T, X_relevant),
                       pinvh(np<span style="color: #555555">.</span>dot(X_relevant<span style="color: #555555">.</span>T, X_relevant)))
    <span style="color: #006699; font-weight: bold">return</span> np<span style="color: #555555">.</span>max(np<span style="color: #555555">.</span>abs(projector)<span style="color: #555555">.</span>sum(axis<span style="color: #555555">=</span><span style="color: #FF6600">1</span>))


<span style="color: #006699; font-weight: bold">for</span> conditioning <span style="color: #000000; font-weight: bold">in</span> (<span style="color: #FF6600">1</span>, <span style="color: #FF6600">1e-4</span>):
    <span style="color: #0099FF; font-style: italic">##################################</span>
    <span style="color: #0099FF; font-style: italic"># Simulate regression data with a correlated design</span>
    n_features <span style="color: #555555">=</span> <span style="color: #FF6600">501</span>
    n_relevant_features <span style="color: #555555">=</span> <span style="color: #FF6600">3</span>
    noise_level <span style="color: #555555">=</span> <span style="color: #555555">.</span><span style="color: #FF6600">2</span>
    coef_min <span style="color: #555555">=</span> <span style="color: #555555">.</span><span style="color: #FF6600">2</span>
    <span style="color: #0099FF; font-style: italic"># The Donoho-Tanner phase transition is around n_samples=25: below we</span>
    <span style="color: #0099FF; font-style: italic"># will completely fail to recover in the well-conditioned case</span>
    n_samples <span style="color: #555555">=</span> <span style="color: #FF6600">25</span>
    block_size <span style="color: #555555">=</span> n_relevant_features

    rng <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>RandomState(<span style="color: #FF6600">42</span>)

    <span style="color: #0099FF; font-style: italic"># The coefficients of our model</span>
    coef <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros(n_features)
    coef[:n_relevant_features] <span style="color: #555555">=</span> coef_min <span style="color: #555555">+</span> rng<span style="color: #555555">.</span>rand(n_relevant_features)

    <span style="color: #0099FF; font-style: italic"># The correlation of our design: variables correlated by blocs of 3</span>
    corr <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros((n_features, n_features))
    <span style="color: #006699; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">range</span>(<span style="color: #FF6600">0</span>, n_features, block_size):
        corr[i:i <span style="color: #555555">+</span> block_size, i:i <span style="color: #555555">+</span> block_size] <span style="color: #555555">=</span> <span style="color: #FF6600">1</span> <span style="color: #555555">-</span> conditioning
    corr<span style="color: #555555">.</span>flat[::n_features <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>] <span style="color: #555555">=</span> <span style="color: #FF6600">1</span>
    corr <span style="color: #555555">=</span> linalg<span style="color: #555555">.</span>cholesky(corr)

    <span style="color: #0099FF; font-style: italic"># Our design</span>
    X <span style="color: #555555">=</span> rng<span style="color: #555555">.</span>normal(size<span style="color: #555555">=</span>(n_samples, n_features))
    X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>dot(X, corr)
    <span style="color: #0099FF; font-style: italic"># Keep [Wainwright2006] (26c) constant</span>
    X[:n_relevant_features] <span style="color: #555555">/=</span> np<span style="color: #555555">.</span>abs(
        linalg<span style="color: #555555">.</span>svdvals(X[:n_relevant_features]))<span style="color: #555555">.</span>max()
    X <span style="color: #555555">=</span> StandardScaler()<span style="color: #555555">.</span>fit_transform(X<span style="color: #555555">.</span>copy())

    <span style="color: #0099FF; font-style: italic"># The output variable</span>
    y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>dot(X, coef)
    y <span style="color: #555555">/=</span> np<span style="color: #555555">.</span>std(y)
    <span style="color: #0099FF; font-style: italic"># We scale the added noise as a function of the average correlation</span>
    <span style="color: #0099FF; font-style: italic"># between the design and the output variable</span>
    y <span style="color: #555555">+=</span> noise_level <span style="color: #555555">*</span> rng<span style="color: #555555">.</span>normal(size<span style="color: #555555">=</span>n_samples)
    mi <span style="color: #555555">=</span> mutual_incoherence(X[:, :n_relevant_features],
                            X[:, n_relevant_features:])

    <span style="color: #0099FF; font-style: italic">#########################################</span>
    <span style="color: #0099FF; font-style: italic"># Plot stability selection path, using a high eps for early stopping</span>
    <span style="color: #0099FF; font-style: italic"># of the path, to save computation time</span>
    alpha_grid, scores_path <span style="color: #555555">=</span> lasso_stability_path(X, y, random_state<span style="color: #555555">=</span><span style="color: #FF6600">42</span>,
                                                   eps<span style="color: #555555">=</span><span style="color: #FF6600">0.05</span>)

    plt<span style="color: #555555">.</span>figure()
    <span style="color: #0099FF; font-style: italic"># We plot the path as a function of alpha/alpha_max to the power 1/3: the</span>
    <span style="color: #0099FF; font-style: italic"># power 1/3 scales the path less brutally than the log, and enables to</span>
    <span style="color: #0099FF; font-style: italic"># see the progression along the path</span>
    hg <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>plot(alpha_grid[<span style="color: #FF6600">1</span>:] <span style="color: #555555">**</span> <span style="color: #555555">.</span><span style="color: #FF6600">333</span>, scores_path[coef <span style="color: #555555">!=</span> <span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>T[<span style="color: #FF6600">1</span>:], <span style="color: #CC3300">&#39;r&#39;</span>)
    hb <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>plot(alpha_grid[<span style="color: #FF6600">1</span>:] <span style="color: #555555">**</span> <span style="color: #555555">.</span><span style="color: #FF6600">333</span>, scores_path[coef <span style="color: #555555">==</span> <span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>T[<span style="color: #FF6600">1</span>:], <span style="color: #CC3300">&#39;k&#39;</span>)
    ymin, ymax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>ylim()
    plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">r&#39;$(\alpha / \alpha_</span><span style="color: #AA0000">{max}</span><span style="color: #CC3300">)^{1/3}$&#39;</span>)
    plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Stability score: proportion of times selected&#39;</span>)
    plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Stability Scores Path - Mutual incoherence: </span><span style="color: #AA0000">%.1f</span><span style="color: #CC3300">&#39;</span> <span style="color: #555555">%</span> mi)
    plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)
    plt<span style="color: #555555">.</span>legend((hg[<span style="color: #FF6600">0</span>], hb[<span style="color: #FF6600">0</span>]), (<span style="color: #CC3300">&#39;relevant features&#39;</span>, <span style="color: #CC3300">&#39;irrelevant features&#39;</span>),
               loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;best&#39;</span>)

    <span style="color: #0099FF; font-style: italic">#######################################</span>
    <span style="color: #0099FF; font-style: italic"># Plot the estimated stability scores for a given alpha</span>

    <span style="color: #0099FF; font-style: italic"># Use 6-fold cross-validation rather than the default 3-fold: it leads to</span>
    <span style="color: #0099FF; font-style: italic"># a better choice of alpha:</span>
    <span style="color: #0099FF; font-style: italic"># Stop the user warnings outputs- they are not necessary for the example</span>
    <span style="color: #0099FF; font-style: italic"># as it is specifically set up to be challenging.</span>
    <span style="color: #006699; font-weight: bold">with</span> warnings<span style="color: #555555">.</span>catch_warnings():
        warnings<span style="color: #555555">.</span>simplefilter(<span style="color: #CC3300">&#39;ignore&#39;</span>, <span style="color: #CC0000; font-weight: bold">UserWarning</span>)
        warnings<span style="color: #555555">.</span>simplefilter(<span style="color: #CC3300">&#39;ignore&#39;</span>, ConvergenceWarning)
        lars_cv <span style="color: #555555">=</span> LassoLarsCV(cv<span style="color: #555555">=</span><span style="color: #FF6600">6</span>)<span style="color: #555555">.</span>fit(X, y)

    <span style="color: #0099FF; font-style: italic"># Run the RandomizedLasso: we use a paths going down to .1*alpha_max</span>
    <span style="color: #0099FF; font-style: italic"># to avoid exploring the regime in which very noisy variables enter</span>
    <span style="color: #0099FF; font-style: italic"># the model</span>
    alphas <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(lars_cv<span style="color: #555555">.</span>alphas_[<span style="color: #FF6600">0</span>], <span style="color: #555555">.</span><span style="color: #FF6600">1</span> <span style="color: #555555">*</span> lars_cv<span style="color: #555555">.</span>alphas_[<span style="color: #FF6600">0</span>], <span style="color: #FF6600">6</span>)
    clf <span style="color: #555555">=</span> RandomizedLasso(alpha<span style="color: #555555">=</span>alphas, random_state<span style="color: #555555">=</span><span style="color: #FF6600">42</span>)<span style="color: #555555">.</span>fit(X, y)
    trees <span style="color: #555555">=</span> ExtraTreesRegressor(<span style="color: #FF6600">100</span>)<span style="color: #555555">.</span>fit(X, y)
    <span style="color: #0099FF; font-style: italic"># Compare with F-score</span>
    F, _ <span style="color: #555555">=</span> f_regression(X, y)

    plt<span style="color: #555555">.</span>figure()
    <span style="color: #006699; font-weight: bold">for</span> name, score <span style="color: #000000; font-weight: bold">in</span> [(<span style="color: #CC3300">&#39;F-test&#39;</span>, F),
                        (<span style="color: #CC3300">&#39;Stability selection&#39;</span>, clf<span style="color: #555555">.</span>scores_),
                        (<span style="color: #CC3300">&#39;Lasso coefs&#39;</span>, np<span style="color: #555555">.</span>abs(lars_cv<span style="color: #555555">.</span>coef_)),
                        (<span style="color: #CC3300">&#39;Trees&#39;</span>, trees<span style="color: #555555">.</span>feature_importances_),
                        ]:
        precision, recall, thresholds <span style="color: #555555">=</span> precision_recall_curve(coef <span style="color: #555555">!=</span> <span style="color: #FF6600">0</span>,
                                                               score)
        plt<span style="color: #555555">.</span>semilogy(np<span style="color: #555555">.</span>maximum(score <span style="color: #555555">/</span> np<span style="color: #555555">.</span>max(score), <span style="color: #FF6600">1e-4</span>),
                     label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;</span><span style="color: #AA0000">%s</span><span style="color: #CC3300">. AUC: </span><span style="color: #AA0000">%.3f</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> (name, auc(recall, precision)))

    plt<span style="color: #555555">.</span>plot(np<span style="color: #555555">.</span>where(coef <span style="color: #555555">!=</span> <span style="color: #FF6600">0</span>)[<span style="color: #FF6600">0</span>], [<span style="color: #FF6600">2e-4</span>] <span style="color: #555555">*</span> n_relevant_features, <span style="color: #CC3300">&#39;mo&#39;</span>,
             label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Ground truth&quot;</span>)
    plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;Features&quot;</span>)
    plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;Score&quot;</span>)
    <span style="color: #0099FF; font-style: italic"># Plot only the 100 first coefficients</span>
    plt<span style="color: #555555">.</span>xlim(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">100</span>)
    plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;best&#39;</span>)
    plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Feature selection scores - Mutual incoherence: </span><span style="color: #AA0000">%.1f</span><span style="color: #CC3300">&#39;</span>
              <span style="color: #555555">%</span> mi)

plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_92_0.png" /></p>
<p><img alt="png" src="../output_92_1.png" /></p>
<p><img alt="png" src="../output_92_2.png" /></p>
<p><img alt="png" src="../output_92_3.png" /></p>
<h3 id="31-theil-sen-regression-source">31.  Theil-Sen Regression (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/linear_model">source</a>)</h3>
<p>Computes a Theil-Sen Regression on a synthetic dataset.</p>
<p>See :ref:<code>theil_sen_regression</code> for more information on the regressor.</p>
<p>Compared to the OLS (ordinary least squares) estimator, the Theil-Sen
estimator is robust against outliers. It has a breakdown point of about 29.3%
in case of a simple linear regression which means that it can tolerate
arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional
case.</p>
<p>The estimation of the model is done by calculating the slopes and intercepts
of a subpopulation of all possible combinations of p subsample points. If an
intercept is fitted, p must be greater than or equal to n_features + 1. The
final slope and intercept is then defined as the spatial median of these
slopes and intercepts.</p>
<p>In certain cases Theil-Sen performs better than :ref:<code>RANSAC
&lt;ransac_regression&gt;</code> which is also a robust method. This is illustrated in the
second example below where outliers with respect to the x-axis perturb RANSAC.
Tuning the <code>residual_threshold</code> parameter of RANSAC remedies this but in
general a priori knowledge about the data and the nature of the outliers is
needed.
Due to the computational complexity of Theil-Sen it is recommended to use it
only for small problems in terms of number of samples and features. For larger
problems the <code>max_subpopulation</code> parameter restricts the magnitude of all
possible combinations of p subsample points to a randomly chosen subset and
therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger
problems with the drawback of losing some of its mathematical properties since
it then works on a random subset.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">time</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> LinearRegression, TheilSenRegressor
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> RANSACRegressor

<span style="color: #336666">print</span>(<span style="color: #003333">__doc__</span>)

estimators <span style="color: #555555">=</span> [(<span style="color: #CC3300">&#39;OLS&#39;</span>, LinearRegression()),
              (<span style="color: #CC3300">&#39;Theil-Sen&#39;</span>, TheilSenRegressor(random_state<span style="color: #555555">=</span><span style="color: #FF6600">42</span>)),
              (<span style="color: #CC3300">&#39;RANSAC&#39;</span>, RANSACRegressor(random_state<span style="color: #555555">=</span><span style="color: #FF6600">42</span>)), ]
colors <span style="color: #555555">=</span> {<span style="color: #CC3300">&#39;OLS&#39;</span>: <span style="color: #CC3300">&#39;turquoise&#39;</span>, <span style="color: #CC3300">&#39;Theil-Sen&#39;</span>: <span style="color: #CC3300">&#39;gold&#39;</span>, <span style="color: #CC3300">&#39;RANSAC&#39;</span>: <span style="color: #CC3300">&#39;lightgreen&#39;</span>}
lw <span style="color: #555555">=</span> <span style="color: #FF6600">2</span>

<span style="color: #0099FF; font-style: italic">###########################################</span>
<span style="color: #0099FF; font-style: italic"># Outliers only in the y direction</span>

np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>seed(<span style="color: #FF6600">0</span>)
n_samples <span style="color: #555555">=</span> <span style="color: #FF6600">200</span>
<span style="color: #0099FF; font-style: italic"># Linear model y = 3*x + N(2, 0.1**2)</span>
x <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>randn(n_samples)
w <span style="color: #555555">=</span> <span style="color: #FF6600">3.</span>
c <span style="color: #555555">=</span> <span style="color: #FF6600">2.</span>
noise <span style="color: #555555">=</span> <span style="color: #FF6600">0.1</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>randn(n_samples)
y <span style="color: #555555">=</span> w <span style="color: #555555">*</span> x <span style="color: #555555">+</span> c <span style="color: #555555">+</span> noise
<span style="color: #0099FF; font-style: italic"># 10% outliers</span>
y[<span style="color: #555555">-</span><span style="color: #FF6600">20</span>:] <span style="color: #555555">+=</span> <span style="color: #555555">-</span><span style="color: #FF6600">20</span> <span style="color: #555555">*</span> x[<span style="color: #555555">-</span><span style="color: #FF6600">20</span>:]
X <span style="color: #555555">=</span> x[:, np<span style="color: #555555">.</span>newaxis]

plt<span style="color: #555555">.</span>scatter(x, y, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;indigo&#39;</span>, marker<span style="color: #555555">=</span><span style="color: #CC3300">&#39;x&#39;</span>, s<span style="color: #555555">=</span><span style="color: #FF6600">40</span>)
line_x <span style="color: #555555">=</span> np<span style="color: #555555">.</span>array([<span style="color: #555555">-</span><span style="color: #FF6600">3</span>, <span style="color: #FF6600">3</span>])
<span style="color: #006699; font-weight: bold">for</span> name, estimator <span style="color: #000000; font-weight: bold">in</span> estimators:
    t0 <span style="color: #555555">=</span> time<span style="color: #555555">.</span>time()
    estimator<span style="color: #555555">.</span>fit(X, y)
    elapsed_time <span style="color: #555555">=</span> time<span style="color: #555555">.</span>time() <span style="color: #555555">-</span> t0
    y_pred <span style="color: #555555">=</span> estimator<span style="color: #555555">.</span>predict(line_x<span style="color: #555555">.</span>reshape(<span style="color: #FF6600">2</span>, <span style="color: #FF6600">1</span>))
    plt<span style="color: #555555">.</span>plot(line_x, y_pred, color<span style="color: #555555">=</span>colors[name], linewidth<span style="color: #555555">=</span>lw,
             label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;</span><span style="color: #AA0000">%s</span><span style="color: #CC3300"> (fit time: </span><span style="color: #AA0000">%.2f</span><span style="color: #CC3300">s)&#39;</span> <span style="color: #555555">%</span> (name, elapsed_time))

plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;upper left&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Corrupt y&quot;</span>)

<span style="color: #0099FF; font-style: italic">##########################################</span>
<span style="color: #0099FF; font-style: italic"># Outliers in the X direction</span>

np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>seed(<span style="color: #FF6600">0</span>)
<span style="color: #0099FF; font-style: italic"># Linear model y = 3*x + N(2, 0.1**2)</span>
x <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>randn(n_samples)
noise <span style="color: #555555">=</span> <span style="color: #FF6600">0.1</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>randn(n_samples)
y <span style="color: #555555">=</span> <span style="color: #FF6600">3</span> <span style="color: #555555">*</span> x <span style="color: #555555">+</span> <span style="color: #FF6600">2</span> <span style="color: #555555">+</span> noise
<span style="color: #0099FF; font-style: italic"># 10% outliers</span>
x[<span style="color: #555555">-</span><span style="color: #FF6600">20</span>:] <span style="color: #555555">=</span> <span style="color: #FF6600">9.9</span>
y[<span style="color: #555555">-</span><span style="color: #FF6600">20</span>:] <span style="color: #555555">+=</span> <span style="color: #FF6600">22</span>
X <span style="color: #555555">=</span> x[:, np<span style="color: #555555">.</span>newaxis]

plt<span style="color: #555555">.</span>figure()
plt<span style="color: #555555">.</span>scatter(x, y, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;indigo&#39;</span>, marker<span style="color: #555555">=</span><span style="color: #CC3300">&#39;x&#39;</span>, s<span style="color: #555555">=</span><span style="color: #FF6600">40</span>)

line_x <span style="color: #555555">=</span> np<span style="color: #555555">.</span>array([<span style="color: #555555">-</span><span style="color: #FF6600">3</span>, <span style="color: #FF6600">10</span>])
<span style="color: #006699; font-weight: bold">for</span> name, estimator <span style="color: #000000; font-weight: bold">in</span> estimators:
    t0 <span style="color: #555555">=</span> time<span style="color: #555555">.</span>time()
    estimator<span style="color: #555555">.</span>fit(X, y)
    elapsed_time <span style="color: #555555">=</span> time<span style="color: #555555">.</span>time() <span style="color: #555555">-</span> t0
    y_pred <span style="color: #555555">=</span> estimator<span style="color: #555555">.</span>predict(line_x<span style="color: #555555">.</span>reshape(<span style="color: #FF6600">2</span>, <span style="color: #FF6600">1</span>))
    plt<span style="color: #555555">.</span>plot(line_x, y_pred, color<span style="color: #555555">=</span>colors[name], linewidth<span style="color: #555555">=</span>lw,
             label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;</span><span style="color: #AA0000">%s</span><span style="color: #CC3300"> (fit time: </span><span style="color: #AA0000">%.2f</span><span style="color: #CC3300">s)&#39;</span> <span style="color: #555555">%</span> (name, elapsed_time))

plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;upper left&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Corrupt x&quot;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>Automatically created module for IPython interactive environment
</pre></div>


<p><img alt="png" src="../output_95_1.png" /></p>
<p><img alt="png" src="../output_95_2.png" /></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../Classifiers/Logistic/Logistic/" class="btn btn-neutral float-right" title="Logistic Regression">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../.." class="btn btn-neutral" title="Home"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../.." style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../Classifiers/Logistic/Logistic/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../../..';</script>
    <script src="../../../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../../../search/main.js" defer></script>

</body>
</html>
