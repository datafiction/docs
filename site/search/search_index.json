{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Fiction Welcome to Machine Learning, Deep Learning and Reinforcement Learning Projects! This online document helps the absolute beginners to persue the future direction in coding and machine learning.","title":"Home"},{"location":"#data-fiction","text":"Welcome to Machine Learning, Deep Learning and Reinforcement Learning Projects! This online document helps the absolute beginners to persue the future direction in coding and machine learning.","title":"Data Fiction"},{"location":"UnderConstruction/","text":"Why Codes? Codes are apparently the language for Human-Machine interface. Coding is the most fundamental skill required for growing with modern days technology. Sicen our computers are built up based on logic and algorithm, our coding languages are also higher lavel of logical steps made undersatndable for human. By typing a line of code, we are speaking the laguage of the machine. Fundamentally, each programming language has it's inner working principle with underlying datastructure and functions. Computer while running a piece of code maintains the data in the memory ( temporary at RAM or permanent at Hard Disk). Whil code is live in the machine, it has time and space complexicities with the underlying datastructure and algotihm excuting the task.","title":"Why Codes?"},{"location":"UnderConstruction/#why-codes","text":"Codes are apparently the language for Human-Machine interface. Coding is the most fundamental skill required for growing with modern days technology. Sicen our computers are built up based on logic and algorithm, our coding languages are also higher lavel of logical steps made undersatndable for human. By typing a line of code, we are speaking the laguage of the machine. Fundamentally, each programming language has it's inner working principle with underlying datastructure and functions. Computer while running a piece of code maintains the data in the memory ( temporary at RAM or permanent at Hard Disk). Whil code is live in the machine, it has time and space complexicities with the underlying datastructure and algotihm excuting the task.","title":"Why Codes?"},{"location":"WhyCodes/","text":"Why Codes? Codes are apparently the language for Human-Machine interface. Coding is the most fundamental skill required for growing with modern days technology. Sicen our computers are built up based on logic and algorithm, our coding languages are also higher lavel of logical steps made undersatndable for human. By typing a line of code, we are speaking the laguage of the machine. Fundamentally, each programming language has it's inner working principle with underlying datastructure and functions. Computer while running a piece of code maintains the data in the memory ( temporary at RAM or permanent at Hard Disk). Whil code is live in the machine, it has time and space complexicities with the underlying datastructure and algotihm excuting the task.","title":"Why Codes?"},{"location":"WhyCodes/#why-codes","text":"Codes are apparently the language for Human-Machine interface. Coding is the most fundamental skill required for growing with modern days technology. Sicen our computers are built up based on logic and algorithm, our coding languages are also higher lavel of logical steps made undersatndable for human. By typing a line of code, we are speaking the laguage of the machine. Fundamentally, each programming language has it's inner working principle with underlying datastructure and functions. Computer while running a piece of code maintains the data in the memory ( temporary at RAM or permanent at Hard Disk). Whil code is live in the machine, it has time and space complexicities with the underlying datastructure and algotihm excuting the task.","title":"Why Codes?"},{"location":"dl/CIFRT10/CIFR10/","text":"Keras- Convolutional Neural Network 3. Image classifier: CIFAR10 3.1 Simple CNN # Simple CNN model for the CIFAR-10 Dataset import numpy from keras.datasets import cifar10 from keras.models import Sequential from keras.layers import Dense from keras.layers import Dropout from keras.layers import Flatten from keras.constraints import maxnorm from keras.optimizers import SGD from keras.layers.convolutional import Conv2D from keras.layers.convolutional import MaxPooling2D from keras.utils import np_utils from keras import backend as K Using TensorFlow backend. K . set_image_dim_ordering( 'th' ) # fix random seed for reproducibility seed = 7 numpy . random . seed(seed) # load data (X_train, y_train), (X_test, y_test) = cifar10 . load_data() # normalize inputs from 0-255 to 0.0-1.0 X_train = X_train . astype( 'float32' ) X_test = X_test . astype( 'float32' ) X_train = X_train / 255.0 X_test = X_test / 255.0 # one hot encode outputs y_train = np_utils . to_categorical(y_train) y_test = np_utils . to_categorical(y_test) num_classes = y_test . shape[ 1 ] Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 15s 0us/step # Create the model model = Sequential() model . add(Conv2D( 32 , ( 3 , 3 ), input_shape = ( 3 , 32 , 32 ), padding = 'same' , activation = 'relu' , kernel_constraint = maxnorm( 3 ))) model . add(Dropout( 0.2 )) model . add(Conv2D( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , kernel_constraint = maxnorm( 3 ))) model . add(MaxPooling2D(pool_size = ( 2 , 2 ))) model . add(Flatten()) model . add(Dense( 512 , activation = 'relu' , kernel_constraint = maxnorm( 3 ))) model . add(Dropout( 0.5 )) model . add(Dense(num_classes, activation = 'softmax' )) # Compile model epochs = 25 lrate = 0.01 decay = lrate / epochs sgd = SGD(lr = lrate, momentum = 0.9 , decay = decay, nesterov = False ) model . compile(loss = 'categorical_crossentropy' , optimizer = sgd, metrics = [ 'accuracy' ]) print (model . summary()) # Fit the model model . fit(X_train, y_train, validation_data = (X_test, y_test), epochs = epochs, batch_size = 32 ) # Final evaluation of the model scores = model . evaluate(X_test, y_test, verbose = 0 ) print ( \"Accuracy: %.2f%% \" % (scores[ 1 ] * 100 )) WARNING:tensorflow:From /Users/admin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. WARNING:tensorflow:From /Users/admin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating: Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ dropout_1 (Dropout) (None, 32, 32, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 32, 32, 32) 9248 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 32, 16, 16) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 8192) 0 _________________________________________________________________ dense_1 (Dense) (None, 512) 4194816 _________________________________________________________________ dropout_2 (Dropout) (None, 512) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 5130 ================================================================= Total params: 4,210,090 Trainable params: 4,210,090 Non-trainable params: 0 _________________________________________________________________ None WARNING:tensorflow:From /Users/admin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.cast instead. Train on 50000 samples, validate on 10000 samples Epoch 1/25 50000/50000 [==============================] - 164s 3ms/step - loss: 1.6913 - acc: 0.3889 - val_loss: 1.3429 - val_acc: 0.5150 Epoch 2/25 50000/50000 [==============================] - 166s 3ms/step - loss: 1.3011 - acc: 0.5330 - val_loss: 1.1536 - val_acc: 0.5947 Epoch 3/25 50000/50000 [==============================] - 173s 3ms/step - loss: 1.1191 - acc: 0.6014 - val_loss: 1.0297 - val_acc: 0.6303 Epoch 4/25 50000/50000 [==============================] - 175s 3ms/step - loss: 0.9851 - acc: 0.6510 - val_loss: 0.9698 - val_acc: 0.6588 Epoch 5/25 48928/50000 [============================&gt;.] - ETA: 3s - loss: 0.8792 - acc: 0.6877 3.3 Plotting # Plot ad hoc CIFAR10 instances from keras.datasets import cifar10 from matplotlib import pyplot from scipy.misc import toimage # load data (X_train, y_train), (X_test, y_test) = cifar10 . load_data() # create a grid of 3x3 images for i in range ( 0 , 9 ): pyplot . subplot( 330 + 1 + i) pyplot . imshow(toimage(X_train[i])) # show the plot pyplot . show()","title":"CIFT10"},{"location":"dl/CIFRT10/CIFR10/#keras-convolutional-neural-network","text":"","title":"Keras- Convolutional Neural Network"},{"location":"dl/CIFRT10/CIFR10/#3-image-classifier-cifar10","text":"","title":"3. Image classifier: CIFAR10"},{"location":"dl/CIFRT10/CIFR10/#31-simple-cnn","text":"# Simple CNN model for the CIFAR-10 Dataset import numpy from keras.datasets import cifar10 from keras.models import Sequential from keras.layers import Dense from keras.layers import Dropout from keras.layers import Flatten from keras.constraints import maxnorm from keras.optimizers import SGD from keras.layers.convolutional import Conv2D from keras.layers.convolutional import MaxPooling2D from keras.utils import np_utils from keras import backend as K Using TensorFlow backend. K . set_image_dim_ordering( 'th' ) # fix random seed for reproducibility seed = 7 numpy . random . seed(seed) # load data (X_train, y_train), (X_test, y_test) = cifar10 . load_data() # normalize inputs from 0-255 to 0.0-1.0 X_train = X_train . astype( 'float32' ) X_test = X_test . astype( 'float32' ) X_train = X_train / 255.0 X_test = X_test / 255.0 # one hot encode outputs y_train = np_utils . to_categorical(y_train) y_test = np_utils . to_categorical(y_test) num_classes = y_test . shape[ 1 ] Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 15s 0us/step # Create the model model = Sequential() model . add(Conv2D( 32 , ( 3 , 3 ), input_shape = ( 3 , 32 , 32 ), padding = 'same' , activation = 'relu' , kernel_constraint = maxnorm( 3 ))) model . add(Dropout( 0.2 )) model . add(Conv2D( 32 , ( 3 , 3 ), activation = 'relu' , padding = 'same' , kernel_constraint = maxnorm( 3 ))) model . add(MaxPooling2D(pool_size = ( 2 , 2 ))) model . add(Flatten()) model . add(Dense( 512 , activation = 'relu' , kernel_constraint = maxnorm( 3 ))) model . add(Dropout( 0.5 )) model . add(Dense(num_classes, activation = 'softmax' )) # Compile model epochs = 25 lrate = 0.01 decay = lrate / epochs sgd = SGD(lr = lrate, momentum = 0.9 , decay = decay, nesterov = False ) model . compile(loss = 'categorical_crossentropy' , optimizer = sgd, metrics = [ 'accuracy' ]) print (model . summary()) # Fit the model model . fit(X_train, y_train, validation_data = (X_test, y_test), epochs = epochs, batch_size = 32 ) # Final evaluation of the model scores = model . evaluate(X_test, y_test, verbose = 0 ) print ( \"Accuracy: %.2f%% \" % (scores[ 1 ] * 100 )) WARNING:tensorflow:From /Users/admin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. WARNING:tensorflow:From /Users/admin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating: Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ dropout_1 (Dropout) (None, 32, 32, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 32, 32, 32) 9248 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 32, 16, 16) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 8192) 0 _________________________________________________________________ dense_1 (Dense) (None, 512) 4194816 _________________________________________________________________ dropout_2 (Dropout) (None, 512) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 5130 ================================================================= Total params: 4,210,090 Trainable params: 4,210,090 Non-trainable params: 0 _________________________________________________________________ None WARNING:tensorflow:From /Users/admin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.cast instead. Train on 50000 samples, validate on 10000 samples Epoch 1/25 50000/50000 [==============================] - 164s 3ms/step - loss: 1.6913 - acc: 0.3889 - val_loss: 1.3429 - val_acc: 0.5150 Epoch 2/25 50000/50000 [==============================] - 166s 3ms/step - loss: 1.3011 - acc: 0.5330 - val_loss: 1.1536 - val_acc: 0.5947 Epoch 3/25 50000/50000 [==============================] - 173s 3ms/step - loss: 1.1191 - acc: 0.6014 - val_loss: 1.0297 - val_acc: 0.6303 Epoch 4/25 50000/50000 [==============================] - 175s 3ms/step - loss: 0.9851 - acc: 0.6510 - val_loss: 0.9698 - val_acc: 0.6588 Epoch 5/25 48928/50000 [============================&gt;.] - ETA: 3s - loss: 0.8792 - acc: 0.6877","title":"3.1 Simple CNN"},{"location":"dl/CIFRT10/CIFR10/#33-plotting","text":"# Plot ad hoc CIFAR10 instances from keras.datasets import cifar10 from matplotlib import pyplot from scipy.misc import toimage # load data (X_train, y_train), (X_test, y_test) = cifar10 . load_data() # create a grid of 3x3 images for i in range ( 0 , 9 ): pyplot . subplot( 330 + 1 + i) pyplot . imshow(toimage(X_train[i])) # show the plot pyplot . show()","title":"3.3 Plotting"},{"location":"dl/NMIST/NMIST/","text":"Keras- Convolutional Neural Network Digit Recognization: NMIST 1. Baseline MLP for MNIST dataset # Baseline MLP for MNIST dataset import numpy from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense from keras.layers import Dropout from keras.utils import np_utils Using TensorFlow backend. Prepare Data Set # fix random seed for reproducibility seed = 7 numpy . random . seed(seed) # load data (X_train, y_train), (X_test, y_test) = mnist . load_data() How many data files are there? X_train . shape,X_test . shape ((60000, 28, 28), (10000, 28, 28)) y_train . shape,y_test . shape ((60000,), (10000,)) Visualize the data import matplotlib.pyplot as plt % matplotlib inline plt . figure(figsize = [ 10 , 8 ]) for i in range ( 1 , 10 ): plt . subplot( 3 , 3 ,i) plt . imshow(X_train[i]) plt . show() Convert Image matrix to Vector # flatten 28*28 images to a 784 vector for each image num_pixels = X_train . shape[ 1 ] * X_train . shape[ 2 ] X_train = X_train . reshape(X_train . shape[ 0 ], num_pixels) . astype( 'float32' ) X_test = X_test . reshape(X_test . shape[ 0 ], num_pixels) . astype( 'float32' ) Normalization # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 Prepare Labels # one hot encode outputs y_train = np_utils . to_categorical(y_train) y_test = np_utils . to_categorical(y_test) num_classes = y_test . shape[ 1 ] Model # define baseline model def baseline_model (): # create model model = Sequential() model . add(Dense(num_pixels, input_dim = num_pixels, kernel_initializer = 'normal' , activation = 'relu' )) model . add(Dense(num_classes, kernel_initializer = 'normal' , activation = 'softmax' )) # Compile model model . compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return model Build Model model = baseline_model() model . summary() # Fit the model model . fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 10 , batch_size = 200 , verbose = 2 ) WARNING:tensorflow:From /Users/admin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 784) 615440 _________________________________________________________________ dense_2 (Dense) (None, 10) 7850 ================================================================= Total params: 623,290 Trainable params: 623,290 Non-trainable params: 0 _________________________________________________________________ WARNING:tensorflow:From /Users/admin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.cast instead. Train on 60000 samples, validate on 10000 samples Epoch 1/10 - 4s - loss: 0.2801 - acc: 0.9209 - val_loss: 0.1417 - val_acc: 0.9572 Epoch 2/10 - 3s - loss: 0.1118 - acc: 0.9676 - val_loss: 0.0921 - val_acc: 0.9705 Epoch 3/10 - 3s - loss: 0.0720 - acc: 0.9794 - val_loss: 0.0785 - val_acc: 0.9773 Epoch 4/10 - 3s - loss: 0.0505 - acc: 0.9857 - val_loss: 0.0739 - val_acc: 0.9772 Epoch 5/10 - 3s - loss: 0.0372 - acc: 0.9892 - val_loss: 0.0665 - val_acc: 0.9792 Epoch 6/10 - 3s - loss: 0.0267 - acc: 0.9929 - val_loss: 0.0620 - val_acc: 0.9808 Epoch 7/10 - 3s - loss: 0.0208 - acc: 0.9947 - val_loss: 0.0618 - val_acc: 0.9808 Epoch 8/10 - 3s - loss: 0.0142 - acc: 0.9967 - val_loss: 0.0630 - val_acc: 0.9807 Epoch 9/10 - 3s - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0589 - val_acc: 0.9807 Epoch 10/10 - 3s - loss: 0.0082 - acc: 0.9984 - val_loss: 0.0581 - val_acc: 0.9818 &lt;keras.callbacks.History at 0x1298a5eb8&gt; Evaluation # Final evaluation of the model scores = model . evaluate(X_test, y_test, verbose = 0 ) print ( \"Baseline Error: %.2f%% \" % ( 100 - scores[ 1 ] * 100 )) Baseline Error: 1.82% 2. Simple CNN for the MNIST Dataset # Simple CNN for the MNIST Dataset from keras.layers import Flatten from keras.layers.convolutional import Conv2D from keras.layers.convolutional import MaxPooling2D # fix dimension ordering issue from keras import backend as K K . set_image_dim_ordering( 'th' ) Get Data # fix random seed for reproducibility seed = 7 numpy . random . seed(seed) # load data (X_train, y_train), (X_test, y_test) = mnist . load_data() # reshape to be [samples][channels][width][height] X_train = X_train . reshape(X_train . shape[ 0 ], 1 , 28 , 28 ) . astype( 'float32' ) X_test = X_test . reshape(X_test . shape[ 0 ], 1 , 28 , 28 ) . astype( 'float32' ) # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils . to_categorical(y_train) y_test = np_utils . to_categorical(y_test) num_classes = y_test . shape[ 1 ] Model # define a simple CNN model def baseline_model (): # create model model = Sequential() model . add(Conv2D( 32 , ( 5 , 5 ), input_shape = ( 1 , 28 , 28 ), activation = 'relu' )) model . add(MaxPooling2D(pool_size = ( 2 , 2 ))) model . add(Dropout( 0.2 )) model . add(Flatten()) model . add(Dense( 128 , activation = 'relu' )) model . add(Dense(num_classes, activation = 'softmax' )) # Compile model model . compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return model Build # build the model model = baseline_model() model . summary() WARNING:tensorflow:From /Users/admin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating: Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 24, 24) 832 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 32, 12, 12) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 32, 12, 12) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 4608) 0 _________________________________________________________________ dense_3 (Dense) (None, 128) 589952 _________________________________________________________________ dense_4 (Dense) (None, 10) 1290 ================================================================= Total params: 592,074 Trainable params: 592,074 Non-trainable params: 0 _________________________________________________________________ Fit Model # Fit the model model . fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 4 , batch_size = 200 ) Train on 60000 samples, validate on 10000 samples Epoch 1/4 60000/60000 [==============================] - 26s 436us/step - loss: 0.2234 - acc: 0.9364 - val_loss: 0.0783 - val_acc: 0.9751 Epoch 2/4 60000/60000 [==============================] - 26s 428us/step - loss: 0.0712 - acc: 0.9787 - val_loss: 0.0456 - val_acc: 0.9851 Epoch 3/4 60000/60000 [==============================] - 25s 419us/step - loss: 0.0510 - acc: 0.9844 - val_loss: 0.0439 - val_acc: 0.9860 Epoch 4/4 60000/60000 [==============================] - 25s 416us/step - loss: 0.0393 - acc: 0.9880 - val_loss: 0.0402 - val_acc: 0.9878 &lt;keras.callbacks.History at 0xb416675c0&gt; Evaluate Model # Final evaluation of the model scores = model . evaluate(X_test, y_test, verbose = 0 ) print ( \"CNN Error: %.2f%% \" % ( 100 - scores[ 1 ] * 100 )) CNN Error: 1.22% 3. Larger CNN for the MNIST Dataset Prepare Data K . set_image_dim_ordering( 'th' ) # fix random seed for reproducibility seed = 7 numpy . random . seed(seed) # load data (X_train, y_train), (X_test, y_test) = mnist . load_data() # reshape to be [samples][pixels][width][height] X_train = X_train . reshape(X_train . shape[ 0 ], 1 , 28 , 28 ) . astype( 'float32' ) X_test = X_test . reshape(X_test . shape[ 0 ], 1 , 28 , 28 ) . astype( 'float32' ) # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils . to_categorical(y_train) y_test = np_utils . to_categorical(y_test) num_classes = y_test . shape[ 1 ] Model # define the larger model def larger_model (): # create model model = Sequential() model . add(Conv2D( 30 , ( 5 , 5 ), input_shape = ( 1 , 28 , 28 ), activation = 'relu' )) model . add(MaxPooling2D(pool_size = ( 2 , 2 ))) model . add(Conv2D( 15 , ( 3 , 3 ), activation = 'relu' )) model . add(MaxPooling2D(pool_size = ( 2 , 2 ))) model . add(Dropout( 0.2 )) model . add(Flatten()) model . add(Dense( 128 , activation = 'relu' )) model . add(Dense( 50 , activation = 'relu' )) model . add(Dense(num_classes, activation = 'softmax' )) # Compile model model . compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return model Build Model # build the model model = larger_model() Fit Model # Fit the model model . fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 10 , batch_size = 200 ) Train on 60000 samples, validate on 10000 samples Epoch 1/10 60000/60000 [==============================] - 31s 510us/step - loss: 0.3871 - acc: 0.8817 - val_loss: 0.0904 - val_acc: 0.9714 Epoch 2/10 60000/60000 [==============================] - 30s 494us/step - loss: 0.0998 - acc: 0.9699 - val_loss: 0.0540 - val_acc: 0.9829 Epoch 3/10 60000/60000 [==============================] - 30s 504us/step - loss: 0.0738 - acc: 0.9772 - val_loss: 0.0417 - val_acc: 0.9864 Epoch 4/10 60000/60000 [==============================] - 29s 492us/step - loss: 0.0602 - acc: 0.9816 - val_loss: 0.0375 - val_acc: 0.9870 Epoch 5/10 60000/60000 [==============================] - 29s 487us/step - loss: 0.0517 - acc: 0.9835 - val_loss: 0.0359 - val_acc: 0.9886 Epoch 6/10 60000/60000 [==============================] - 29s 476us/step - loss: 0.0446 - acc: 0.9855 - val_loss: 0.0307 - val_acc: 0.9902 Epoch 7/10 60000/60000 [==============================] - 40s 665us/step - loss: 0.0393 - acc: 0.9880 - val_loss: 0.0313 - val_acc: 0.9900 Epoch 8/10 60000/60000 [==============================] - 33s 551us/step - loss: 0.0357 - acc: 0.9887 - val_loss: 0.0259 - val_acc: 0.9911 Epoch 9/10 60000/60000 [==============================] - 30s 508us/step - loss: 0.0322 - acc: 0.9899 - val_loss: 0.0242 - val_acc: 0.9925 Epoch 10/10 60000/60000 [==============================] - 31s 512us/step - loss: 0.0307 - acc: 0.9898 - val_loss: 0.0271 - val_acc: 0.9906 &lt;keras.callbacks.History at 0xb2e19f320&gt; Evalauation # Final evaluation of the model scores = model . evaluate(X_test, y_test, verbose = 0 ) print ( \"Large CNN Error: %.2f%% \" % ( 100 - scores[ 1 ] * 100 )) Large CNN Error: 0.94% Plot # Plot ad hoc mnist instances from keras.datasets import mnist import matplotlib.pyplot as plt # load (downloaded if needed) the MNIST dataset (X_train, y_train), (X_test, y_test) = mnist . load_data() # plot 4 images as gray scale plt . subplot( 221 ) plt . imshow(X_train[ 0 ], cmap = plt . get_cmap( 'gray' )) plt . subplot( 222 ) plt . imshow(X_train[ 1 ], cmap = plt . get_cmap( 'gray' )) plt . subplot( 223 ) plt . imshow(X_train[ 2 ], cmap = plt . get_cmap( 'gray' )) plt . subplot( 224 ) plt . imshow(X_train[ 3 ], cmap = plt . get_cmap( 'gray' )) # show the plot plt . show()","title":"NMIST"},{"location":"dl/NMIST/NMIST/#keras-convolutional-neural-network","text":"","title":"Keras- Convolutional Neural Network"},{"location":"dl/NMIST/NMIST/#digit-recognization-nmist","text":"","title":"Digit Recognization: NMIST"},{"location":"dl/NMIST/NMIST/#1-baseline-mlp-for-mnist-dataset","text":"# Baseline MLP for MNIST dataset import numpy from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense from keras.layers import Dropout from keras.utils import np_utils Using TensorFlow backend.","title":"1. Baseline MLP for MNIST dataset"},{"location":"dl/NMIST/NMIST/#prepare-data-set","text":"# fix random seed for reproducibility seed = 7 numpy . random . seed(seed) # load data (X_train, y_train), (X_test, y_test) = mnist . load_data() How many data files are there? X_train . shape,X_test . shape ((60000, 28, 28), (10000, 28, 28)) y_train . shape,y_test . shape ((60000,), (10000,))","title":"Prepare Data Set"},{"location":"dl/NMIST/NMIST/#visualize-the-data","text":"import matplotlib.pyplot as plt % matplotlib inline plt . figure(figsize = [ 10 , 8 ]) for i in range ( 1 , 10 ): plt . subplot( 3 , 3 ,i) plt . imshow(X_train[i]) plt . show()","title":"Visualize the data"},{"location":"dl/NMIST/NMIST/#convert-image-matrix-to-vector","text":"# flatten 28*28 images to a 784 vector for each image num_pixels = X_train . shape[ 1 ] * X_train . shape[ 2 ] X_train = X_train . reshape(X_train . shape[ 0 ], num_pixels) . astype( 'float32' ) X_test = X_test . reshape(X_test . shape[ 0 ], num_pixels) . astype( 'float32' )","title":"Convert Image matrix to Vector"},{"location":"dl/NMIST/NMIST/#normalization","text":"# normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255","title":"Normalization"},{"location":"dl/NMIST/NMIST/#prepare-labels","text":"# one hot encode outputs y_train = np_utils . to_categorical(y_train) y_test = np_utils . to_categorical(y_test) num_classes = y_test . shape[ 1 ]","title":"Prepare Labels"},{"location":"dl/NMIST/NMIST/#model","text":"# define baseline model def baseline_model (): # create model model = Sequential() model . add(Dense(num_pixels, input_dim = num_pixels, kernel_initializer = 'normal' , activation = 'relu' )) model . add(Dense(num_classes, kernel_initializer = 'normal' , activation = 'softmax' )) # Compile model model . compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return model","title":"Model"},{"location":"dl/NMIST/NMIST/#build-model","text":"model = baseline_model() model . summary() # Fit the model model . fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 10 , batch_size = 200 , verbose = 2 ) WARNING:tensorflow:From /Users/admin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 784) 615440 _________________________________________________________________ dense_2 (Dense) (None, 10) 7850 ================================================================= Total params: 623,290 Trainable params: 623,290 Non-trainable params: 0 _________________________________________________________________ WARNING:tensorflow:From /Users/admin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.cast instead. Train on 60000 samples, validate on 10000 samples Epoch 1/10 - 4s - loss: 0.2801 - acc: 0.9209 - val_loss: 0.1417 - val_acc: 0.9572 Epoch 2/10 - 3s - loss: 0.1118 - acc: 0.9676 - val_loss: 0.0921 - val_acc: 0.9705 Epoch 3/10 - 3s - loss: 0.0720 - acc: 0.9794 - val_loss: 0.0785 - val_acc: 0.9773 Epoch 4/10 - 3s - loss: 0.0505 - acc: 0.9857 - val_loss: 0.0739 - val_acc: 0.9772 Epoch 5/10 - 3s - loss: 0.0372 - acc: 0.9892 - val_loss: 0.0665 - val_acc: 0.9792 Epoch 6/10 - 3s - loss: 0.0267 - acc: 0.9929 - val_loss: 0.0620 - val_acc: 0.9808 Epoch 7/10 - 3s - loss: 0.0208 - acc: 0.9947 - val_loss: 0.0618 - val_acc: 0.9808 Epoch 8/10 - 3s - loss: 0.0142 - acc: 0.9967 - val_loss: 0.0630 - val_acc: 0.9807 Epoch 9/10 - 3s - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0589 - val_acc: 0.9807 Epoch 10/10 - 3s - loss: 0.0082 - acc: 0.9984 - val_loss: 0.0581 - val_acc: 0.9818 &lt;keras.callbacks.History at 0x1298a5eb8&gt;","title":"Build Model"},{"location":"dl/NMIST/NMIST/#evaluation","text":"# Final evaluation of the model scores = model . evaluate(X_test, y_test, verbose = 0 ) print ( \"Baseline Error: %.2f%% \" % ( 100 - scores[ 1 ] * 100 )) Baseline Error: 1.82%","title":"Evaluation"},{"location":"dl/NMIST/NMIST/#2-simple-cnn-for-the-mnist-dataset","text":"# Simple CNN for the MNIST Dataset from keras.layers import Flatten from keras.layers.convolutional import Conv2D from keras.layers.convolutional import MaxPooling2D # fix dimension ordering issue from keras import backend as K K . set_image_dim_ordering( 'th' )","title":"2.  Simple CNN for the MNIST Dataset"},{"location":"dl/NMIST/NMIST/#get-data","text":"# fix random seed for reproducibility seed = 7 numpy . random . seed(seed) # load data (X_train, y_train), (X_test, y_test) = mnist . load_data() # reshape to be [samples][channels][width][height] X_train = X_train . reshape(X_train . shape[ 0 ], 1 , 28 , 28 ) . astype( 'float32' ) X_test = X_test . reshape(X_test . shape[ 0 ], 1 , 28 , 28 ) . astype( 'float32' ) # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils . to_categorical(y_train) y_test = np_utils . to_categorical(y_test) num_classes = y_test . shape[ 1 ]","title":"Get Data"},{"location":"dl/NMIST/NMIST/#model_1","text":"# define a simple CNN model def baseline_model (): # create model model = Sequential() model . add(Conv2D( 32 , ( 5 , 5 ), input_shape = ( 1 , 28 , 28 ), activation = 'relu' )) model . add(MaxPooling2D(pool_size = ( 2 , 2 ))) model . add(Dropout( 0.2 )) model . add(Flatten()) model . add(Dense( 128 , activation = 'relu' )) model . add(Dense(num_classes, activation = 'softmax' )) # Compile model model . compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return model","title":"Model"},{"location":"dl/NMIST/NMIST/#build","text":"# build the model model = baseline_model() model . summary() WARNING:tensorflow:From /Users/admin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating: Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 24, 24) 832 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 32, 12, 12) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 32, 12, 12) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 4608) 0 _________________________________________________________________ dense_3 (Dense) (None, 128) 589952 _________________________________________________________________ dense_4 (Dense) (None, 10) 1290 ================================================================= Total params: 592,074 Trainable params: 592,074 Non-trainable params: 0 _________________________________________________________________","title":"Build"},{"location":"dl/NMIST/NMIST/#fit-model","text":"# Fit the model model . fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 4 , batch_size = 200 ) Train on 60000 samples, validate on 10000 samples Epoch 1/4 60000/60000 [==============================] - 26s 436us/step - loss: 0.2234 - acc: 0.9364 - val_loss: 0.0783 - val_acc: 0.9751 Epoch 2/4 60000/60000 [==============================] - 26s 428us/step - loss: 0.0712 - acc: 0.9787 - val_loss: 0.0456 - val_acc: 0.9851 Epoch 3/4 60000/60000 [==============================] - 25s 419us/step - loss: 0.0510 - acc: 0.9844 - val_loss: 0.0439 - val_acc: 0.9860 Epoch 4/4 60000/60000 [==============================] - 25s 416us/step - loss: 0.0393 - acc: 0.9880 - val_loss: 0.0402 - val_acc: 0.9878 &lt;keras.callbacks.History at 0xb416675c0&gt;","title":"Fit Model"},{"location":"dl/NMIST/NMIST/#evaluate-model","text":"# Final evaluation of the model scores = model . evaluate(X_test, y_test, verbose = 0 ) print ( \"CNN Error: %.2f%% \" % ( 100 - scores[ 1 ] * 100 )) CNN Error: 1.22%","title":"Evaluate Model"},{"location":"dl/NMIST/NMIST/#3-larger-cnn-for-the-mnist-dataset","text":"","title":"3. Larger CNN for the MNIST Dataset"},{"location":"dl/NMIST/NMIST/#prepare-data","text":"K . set_image_dim_ordering( 'th' ) # fix random seed for reproducibility seed = 7 numpy . random . seed(seed) # load data (X_train, y_train), (X_test, y_test) = mnist . load_data() # reshape to be [samples][pixels][width][height] X_train = X_train . reshape(X_train . shape[ 0 ], 1 , 28 , 28 ) . astype( 'float32' ) X_test = X_test . reshape(X_test . shape[ 0 ], 1 , 28 , 28 ) . astype( 'float32' ) # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils . to_categorical(y_train) y_test = np_utils . to_categorical(y_test) num_classes = y_test . shape[ 1 ]","title":"Prepare Data"},{"location":"dl/NMIST/NMIST/#model_2","text":"# define the larger model def larger_model (): # create model model = Sequential() model . add(Conv2D( 30 , ( 5 , 5 ), input_shape = ( 1 , 28 , 28 ), activation = 'relu' )) model . add(MaxPooling2D(pool_size = ( 2 , 2 ))) model . add(Conv2D( 15 , ( 3 , 3 ), activation = 'relu' )) model . add(MaxPooling2D(pool_size = ( 2 , 2 ))) model . add(Dropout( 0.2 )) model . add(Flatten()) model . add(Dense( 128 , activation = 'relu' )) model . add(Dense( 50 , activation = 'relu' )) model . add(Dense(num_classes, activation = 'softmax' )) # Compile model model . compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return model","title":"Model"},{"location":"dl/NMIST/NMIST/#build-model_1","text":"# build the model model = larger_model()","title":"Build Model"},{"location":"dl/NMIST/NMIST/#fit-model_1","text":"# Fit the model model . fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 10 , batch_size = 200 ) Train on 60000 samples, validate on 10000 samples Epoch 1/10 60000/60000 [==============================] - 31s 510us/step - loss: 0.3871 - acc: 0.8817 - val_loss: 0.0904 - val_acc: 0.9714 Epoch 2/10 60000/60000 [==============================] - 30s 494us/step - loss: 0.0998 - acc: 0.9699 - val_loss: 0.0540 - val_acc: 0.9829 Epoch 3/10 60000/60000 [==============================] - 30s 504us/step - loss: 0.0738 - acc: 0.9772 - val_loss: 0.0417 - val_acc: 0.9864 Epoch 4/10 60000/60000 [==============================] - 29s 492us/step - loss: 0.0602 - acc: 0.9816 - val_loss: 0.0375 - val_acc: 0.9870 Epoch 5/10 60000/60000 [==============================] - 29s 487us/step - loss: 0.0517 - acc: 0.9835 - val_loss: 0.0359 - val_acc: 0.9886 Epoch 6/10 60000/60000 [==============================] - 29s 476us/step - loss: 0.0446 - acc: 0.9855 - val_loss: 0.0307 - val_acc: 0.9902 Epoch 7/10 60000/60000 [==============================] - 40s 665us/step - loss: 0.0393 - acc: 0.9880 - val_loss: 0.0313 - val_acc: 0.9900 Epoch 8/10 60000/60000 [==============================] - 33s 551us/step - loss: 0.0357 - acc: 0.9887 - val_loss: 0.0259 - val_acc: 0.9911 Epoch 9/10 60000/60000 [==============================] - 30s 508us/step - loss: 0.0322 - acc: 0.9899 - val_loss: 0.0242 - val_acc: 0.9925 Epoch 10/10 60000/60000 [==============================] - 31s 512us/step - loss: 0.0307 - acc: 0.9898 - val_loss: 0.0271 - val_acc: 0.9906 &lt;keras.callbacks.History at 0xb2e19f320&gt;","title":"Fit Model"},{"location":"dl/NMIST/NMIST/#evalauation","text":"# Final evaluation of the model scores = model . evaluate(X_test, y_test, verbose = 0 ) print ( \"Large CNN Error: %.2f%% \" % ( 100 - scores[ 1 ] * 100 )) Large CNN Error: 0.94%","title":"Evalauation"},{"location":"dl/NMIST/NMIST/#plot","text":"# Plot ad hoc mnist instances from keras.datasets import mnist import matplotlib.pyplot as plt # load (downloaded if needed) the MNIST dataset (X_train, y_train), (X_test, y_test) = mnist . load_data() # plot 4 images as gray scale plt . subplot( 221 ) plt . imshow(X_train[ 0 ], cmap = plt . get_cmap( 'gray' )) plt . subplot( 222 ) plt . imshow(X_train[ 1 ], cmap = plt . get_cmap( 'gray' )) plt . subplot( 223 ) plt . imshow(X_train[ 2 ], cmap = plt . get_cmap( 'gray' )) plt . subplot( 224 ) plt . imshow(X_train[ 3 ], cmap = plt . get_cmap( 'gray' )) # show the plot plt . show()","title":"Plot"},{"location":"dl/Vanila/1.Vanila-LSTM/","text":"1. Vanila LSTM from random import randint from numpy import array from numpy import argmax Sequence Generator def generate_sequence (length, n_features): return [randint( 0 , n_features - 1 ) for _ in range (length)] Generate random sequence sequence = generate_sequence( 25 , 100 ) print (sequence) [81, 54, 44, 82, 86, 43, 46, 20, 85, 71, 55, 58, 26, 78, 66, 70, 38, 55, 91, 25, 51, 32, 89, 9, 83] One Hot Encoder One hot encode sequence def one_hot_encode (sequence, n_features): encoding = list () for value in sequence: vector = [ 0 for _ in range (n_features)] vector[value] = 1 encoding . append(vector) return array(encoding) encoded = one_hot_encode(sequence, 100 ) print (encoded) [[0 0 0 ..., 0 0 0] [0 0 0 ..., 0 0 0] [0 0 0 ..., 0 0 0] ..., [0 0 0 ..., 0 0 0] [0 0 0 ..., 0 0 0] [0 0 0 ..., 0 0 0]] One Hot Decoder Decode a one hot encoded string def one_hot_decode (encoded_seq): return [argmax(vector) for vector in encoded_seq] One hot decode decoded = one_hot_decode(encoded) print (decoded) [81, 54, 44, 82, 86, 43, 46, 20, 85, 71, 55, 58, 26, 78, 66, 70, 38, 55, 91, 25, 51, 32, 89, 9, 83] Generate Sequence Examples length = 5 n_features = 10 #generate sequence sequence = generate_sequence(length, n_features) # one hot encode encoded = one_hot_encode(sequence, n_features) # reshape sequence to be 3D X = encoded . reshape(( 1 , length, n_features)) print ( 'Sequence is: {}, \\n encoded is :{}, \\n X has shape: {}, \\n X is: {}' \\ . format(sequence,encoded,X . shape,X)) Sequence is: [2, 4, 2, 5, 7], encoded is :[[0 0 1 0 0 0 0 0 0 0] [0 0 0 0 1 0 0 0 0 0] [0 0 1 0 0 0 0 0 0 0] [0 0 0 0 0 1 0 0 0 0] [0 0 0 0 0 0 0 1 0 0]], X has shape: (1, 5, 10), X is: [[[0 0 1 0 0 0 0 0 0 0] [0 0 0 0 1 0 0 0 0 0] [0 0 1 0 0 0 0 0 0 0] [0 0 0 0 0 1 0 0 0 0] [0 0 0 0 0 0 0 1 0 0]]] y = encoded[ 2 ] . reshape( 1 , n_features) y array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]) Generate one example for an lstm def generate_example (length, n_features, out_index): # generate sequence sequence = generate_sequence(length, n_features) # one hot encode encoded = one_hot_encode(sequence, n_features) # reshape sequence to be 3D X = encoded . reshape(( 1 , length, n_features)) # select output y = encoded[out_index] . reshape( 1 , n_features) return X, y Vanila LSTM from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense Define model length = 50 n_features = 100 out_index = 2 model = Sequential() model . add(LSTM( 25 , input_shape = (length, n_features))) model . add(Dense(n_features, activation = 'softmax' )) model . compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'acc' ]) print (model . summary()) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm_3 (LSTM) (None, 25) 12600 _________________________________________________________________ dense_3 (Dense) (None, 100) 2600 ================================================================= Total params: 15,200 Trainable params: 15,200 Non-trainable params: 0 _________________________________________________________________ None Fit model for i in range ( 10 ): X, y = generate_example(length, n_features, out_index) model . fit(X, y, epochs = 1 , verbose = 2 ) Epoch 1/1 1s - loss: 4.6111 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.5636 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.6073 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.5968 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.5582 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.5816 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.6481 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.5947 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.6196 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.5835 - acc: 0.0000e+00 Evaluate model correct = 0 for i in range ( 100 ): X, y = generate_example(length, n_features, out_index) yhat = model . predict(X) if one_hot_decode(yhat) == one_hot_decode(y): correct += 1 print ( 'Accuracy: %f ' % ((correct / 100 ) * 100.0 )) Accuracy: 0.000000 Prediction on new data X, y = generate_example(length, n_features, out_index) yhat = model . predict(X) print ( 'Sequence: %s ' % [one_hot_decode(x) for x in X]) print ( 'Expected: %s ' % one_hot_decode(y)) print ( 'Predicted: %s ' % one_hot_decode(yhat)) Sequence: [[60, 56, 22, 5, 67, 76, 43, 14, 35, 72, 11, 48, 30, 44, 12, 55, 41, 79, 17, 70, 20, 39, 70, 1, 48, 94, 12, 56, 46, 4, 92, 77, 50, 12, 91, 50, 78, 59, 47, 83, 75, 19, 15, 57, 78, 34, 88, 75, 3, 19]] Expected: [22] Predicted: [30]","title":"Vanila LSTM"},{"location":"dl/Vanila/1.Vanila-LSTM/#1-vanila-lstm","text":"from random import randint from numpy import array from numpy import argmax","title":"1. Vanila LSTM"},{"location":"dl/Vanila/1.Vanila-LSTM/#sequence-generator","text":"def generate_sequence (length, n_features): return [randint( 0 , n_features - 1 ) for _ in range (length)] Generate random sequence sequence = generate_sequence( 25 , 100 ) print (sequence) [81, 54, 44, 82, 86, 43, 46, 20, 85, 71, 55, 58, 26, 78, 66, 70, 38, 55, 91, 25, 51, 32, 89, 9, 83]","title":"Sequence Generator"},{"location":"dl/Vanila/1.Vanila-LSTM/#one-hot-encoder","text":"One hot encode sequence def one_hot_encode (sequence, n_features): encoding = list () for value in sequence: vector = [ 0 for _ in range (n_features)] vector[value] = 1 encoding . append(vector) return array(encoding) encoded = one_hot_encode(sequence, 100 ) print (encoded) [[0 0 0 ..., 0 0 0] [0 0 0 ..., 0 0 0] [0 0 0 ..., 0 0 0] ..., [0 0 0 ..., 0 0 0] [0 0 0 ..., 0 0 0] [0 0 0 ..., 0 0 0]]","title":"One Hot Encoder"},{"location":"dl/Vanila/1.Vanila-LSTM/#one-hot-decoder","text":"Decode a one hot encoded string def one_hot_decode (encoded_seq): return [argmax(vector) for vector in encoded_seq] One hot decode decoded = one_hot_decode(encoded) print (decoded) [81, 54, 44, 82, 86, 43, 46, 20, 85, 71, 55, 58, 26, 78, 66, 70, 38, 55, 91, 25, 51, 32, 89, 9, 83]","title":"One Hot Decoder"},{"location":"dl/Vanila/1.Vanila-LSTM/#generate-sequence-examples","text":"length = 5 n_features = 10 #generate sequence sequence = generate_sequence(length, n_features) # one hot encode encoded = one_hot_encode(sequence, n_features) # reshape sequence to be 3D X = encoded . reshape(( 1 , length, n_features)) print ( 'Sequence is: {}, \\n encoded is :{}, \\n X has shape: {}, \\n X is: {}' \\ . format(sequence,encoded,X . shape,X)) Sequence is: [2, 4, 2, 5, 7], encoded is :[[0 0 1 0 0 0 0 0 0 0] [0 0 0 0 1 0 0 0 0 0] [0 0 1 0 0 0 0 0 0 0] [0 0 0 0 0 1 0 0 0 0] [0 0 0 0 0 0 0 1 0 0]], X has shape: (1, 5, 10), X is: [[[0 0 1 0 0 0 0 0 0 0] [0 0 0 0 1 0 0 0 0 0] [0 0 1 0 0 0 0 0 0 0] [0 0 0 0 0 1 0 0 0 0] [0 0 0 0 0 0 0 1 0 0]]] y = encoded[ 2 ] . reshape( 1 , n_features) y array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]) Generate one example for an lstm def generate_example (length, n_features, out_index): # generate sequence sequence = generate_sequence(length, n_features) # one hot encode encoded = one_hot_encode(sequence, n_features) # reshape sequence to be 3D X = encoded . reshape(( 1 , length, n_features)) # select output y = encoded[out_index] . reshape( 1 , n_features) return X, y","title":"Generate Sequence Examples"},{"location":"dl/Vanila/1.Vanila-LSTM/#vanila-lstm","text":"from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense Define model length = 50 n_features = 100 out_index = 2 model = Sequential() model . add(LSTM( 25 , input_shape = (length, n_features))) model . add(Dense(n_features, activation = 'softmax' )) model . compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'acc' ]) print (model . summary()) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm_3 (LSTM) (None, 25) 12600 _________________________________________________________________ dense_3 (Dense) (None, 100) 2600 ================================================================= Total params: 15,200 Trainable params: 15,200 Non-trainable params: 0 _________________________________________________________________ None Fit model for i in range ( 10 ): X, y = generate_example(length, n_features, out_index) model . fit(X, y, epochs = 1 , verbose = 2 ) Epoch 1/1 1s - loss: 4.6111 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.5636 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.6073 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.5968 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.5582 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.5816 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.6481 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.5947 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.6196 - acc: 0.0000e+00 Epoch 1/1 0s - loss: 4.5835 - acc: 0.0000e+00 Evaluate model correct = 0 for i in range ( 100 ): X, y = generate_example(length, n_features, out_index) yhat = model . predict(X) if one_hot_decode(yhat) == one_hot_decode(y): correct += 1 print ( 'Accuracy: %f ' % ((correct / 100 ) * 100.0 )) Accuracy: 0.000000 Prediction on new data X, y = generate_example(length, n_features, out_index) yhat = model . predict(X) print ( 'Sequence: %s ' % [one_hot_decode(x) for x in X]) print ( 'Expected: %s ' % one_hot_decode(y)) print ( 'Predicted: %s ' % one_hot_decode(yhat)) Sequence: [[60, 56, 22, 5, 67, 76, 43, 14, 35, 72, 11, 48, 30, 44, 12, 55, 41, 79, 17, 70, 20, 39, 70, 1, 48, 94, 12, 56, 46, 4, 92, 77, 50, 12, 91, 50, 78, 59, 47, 83, 75, 19, 15, 57, 78, 34, 88, 75, 3, 19]] Expected: [22] Predicted: [30]","title":"Vanila LSTM"},{"location":"dl/bidirectional/5. BiDirectional-LSTM/","text":"Bydirectional-LSTM from random import random from numpy import array from numpy import cumsum from numpy import array_equal # create a cumulative sum sequence def get_sequence (n_timesteps): # create a sequence of random numbers in [0,1] X = array([random() for _ in range (n_timesteps)]) # calculate cut-off value to change class values limit = n_timesteps / 4.0 # determine the class outcome for each item in cumulative sequence y = array([ 0 if x < limit else 1 for x in cumsum(X)]) return X, y # create multiple samples of cumulative sum sequences def get_sequences (n_sequences, n_timesteps): seqX, seqY = list (), list () # create and store sequences for _ in range (n_sequences): X, y = get_sequence(n_timesteps) seqX . append(X) seqY . append(y) # reshape input and output for lstm seqX = array(seqX) . reshape(n_sequences, n_timesteps, 1 ) seqY = array(seqY) . reshape(n_sequences, n_timesteps, 1 ) return seqX, seqY model from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense from keras.layers import TimeDistributed from keras.layers import Bidirectional # define problem n_timesteps = 10 # define LSTM model = Sequential() model . add(Bidirectional(LSTM( 50 , return_sequences = True ), input_shape = (n_timesteps, 1 ))) model . add(TimeDistributed(Dense( 1 , activation = 'sigmoid' ))) model . compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'acc' ]) print (model . summary()) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= bidirectional_1 (Bidirection (None, 10, 100) 20800 _________________________________________________________________ time_distributed_1 (TimeDist (None, 10, 1) 101 ================================================================= Total params: 20,901 Trainable params: 20,901 Non-trainable params: 0 _________________________________________________________________ None # train LSTM X, y = get_sequences( 50000 , n_timesteps) model . fit(X, y, epochs = 1 , batch_size = 10 ) Epoch 1/1 50000/50000 [==============================] - 120s - loss: 0.0505 - acc: 0.9820 &lt;keras.callbacks.History at 0x11e90d550&gt; # evaluate LSTM X, y = get_sequences( 100 , n_timesteps) loss, acc = model . evaluate(X, y, verbose = 0 ) print ( 'Loss: %f , Accuracy: %f ' % (loss, acc * 100 )) Loss: 0.029906, Accuracy: 98.800002 # make predictions for _ in range ( 10 ): X, y = get_sequences( 1 , n_timesteps) yhat = model . predict_classes(X, verbose = 0 ) exp, pred = y . reshape(n_timesteps), yhat . reshape(n_timesteps) print ( 'y= %s , yhat= %s , correct= %s ' % (exp, pred, array_equal(exp,pred))) y=[0 0 1 1 1 1 1 1 1 1], yhat=[0 0 1 1 1 1 1 1 1 1], correct=True y=[0 0 0 1 1 1 1 1 1 1], yhat=[0 0 0 1 1 1 1 1 1 1], correct=True y=[0 0 0 1 1 1 1 1 1 1], yhat=[0 0 0 1 1 1 1 1 1 1], correct=True y=[0 0 0 0 0 1 1 1 1 1], yhat=[0 0 0 0 1 1 1 1 1 1], correct=False y=[0 0 0 0 0 0 1 1 1 1], yhat=[0 0 0 0 0 1 1 1 1 1], correct=False y=[0 0 0 0 0 0 1 1 1 1], yhat=[0 0 0 0 0 0 1 1 1 1], correct=True y=[0 0 0 0 1 1 1 1 1 1], yhat=[0 0 0 0 1 1 1 1 1 1], correct=True y=[0 0 0 0 0 1 1 1 1 1], yhat=[0 0 0 0 0 1 1 1 1 1], correct=True y=[0 0 0 1 1 1 1 1 1 1], yhat=[0 0 0 1 1 1 1 1 1 1], correct=True y=[0 0 0 1 1 1 1 1 1 1], yhat=[0 0 0 1 1 1 1 1 1 1], correct=True","title":"Bidirectional LSTM"},{"location":"dl/bidirectional/5. BiDirectional-LSTM/#bydirectional-lstm","text":"from random import random from numpy import array from numpy import cumsum from numpy import array_equal # create a cumulative sum sequence def get_sequence (n_timesteps): # create a sequence of random numbers in [0,1] X = array([random() for _ in range (n_timesteps)]) # calculate cut-off value to change class values limit = n_timesteps / 4.0 # determine the class outcome for each item in cumulative sequence y = array([ 0 if x < limit else 1 for x in cumsum(X)]) return X, y # create multiple samples of cumulative sum sequences def get_sequences (n_sequences, n_timesteps): seqX, seqY = list (), list () # create and store sequences for _ in range (n_sequences): X, y = get_sequence(n_timesteps) seqX . append(X) seqY . append(y) # reshape input and output for lstm seqX = array(seqX) . reshape(n_sequences, n_timesteps, 1 ) seqY = array(seqY) . reshape(n_sequences, n_timesteps, 1 ) return seqX, seqY","title":"Bydirectional-LSTM"},{"location":"dl/bidirectional/5. BiDirectional-LSTM/#model","text":"from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense from keras.layers import TimeDistributed from keras.layers import Bidirectional # define problem n_timesteps = 10 # define LSTM model = Sequential() model . add(Bidirectional(LSTM( 50 , return_sequences = True ), input_shape = (n_timesteps, 1 ))) model . add(TimeDistributed(Dense( 1 , activation = 'sigmoid' ))) model . compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'acc' ]) print (model . summary()) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= bidirectional_1 (Bidirection (None, 10, 100) 20800 _________________________________________________________________ time_distributed_1 (TimeDist (None, 10, 1) 101 ================================================================= Total params: 20,901 Trainable params: 20,901 Non-trainable params: 0 _________________________________________________________________ None # train LSTM X, y = get_sequences( 50000 , n_timesteps) model . fit(X, y, epochs = 1 , batch_size = 10 ) Epoch 1/1 50000/50000 [==============================] - 120s - loss: 0.0505 - acc: 0.9820 &lt;keras.callbacks.History at 0x11e90d550&gt; # evaluate LSTM X, y = get_sequences( 100 , n_timesteps) loss, acc = model . evaluate(X, y, verbose = 0 ) print ( 'Loss: %f , Accuracy: %f ' % (loss, acc * 100 )) Loss: 0.029906, Accuracy: 98.800002 # make predictions for _ in range ( 10 ): X, y = get_sequences( 1 , n_timesteps) yhat = model . predict_classes(X, verbose = 0 ) exp, pred = y . reshape(n_timesteps), yhat . reshape(n_timesteps) print ( 'y= %s , yhat= %s , correct= %s ' % (exp, pred, array_equal(exp,pred))) y=[0 0 1 1 1 1 1 1 1 1], yhat=[0 0 1 1 1 1 1 1 1 1], correct=True y=[0 0 0 1 1 1 1 1 1 1], yhat=[0 0 0 1 1 1 1 1 1 1], correct=True y=[0 0 0 1 1 1 1 1 1 1], yhat=[0 0 0 1 1 1 1 1 1 1], correct=True y=[0 0 0 0 0 1 1 1 1 1], yhat=[0 0 0 0 1 1 1 1 1 1], correct=False y=[0 0 0 0 0 0 1 1 1 1], yhat=[0 0 0 0 0 1 1 1 1 1], correct=False y=[0 0 0 0 0 0 1 1 1 1], yhat=[0 0 0 0 0 0 1 1 1 1], correct=True y=[0 0 0 0 1 1 1 1 1 1], yhat=[0 0 0 0 1 1 1 1 1 1], correct=True y=[0 0 0 0 0 1 1 1 1 1], yhat=[0 0 0 0 0 1 1 1 1 1], correct=True y=[0 0 0 1 1 1 1 1 1 1], yhat=[0 0 0 1 1 1 1 1 1 1], correct=True y=[0 0 0 1 1 1 1 1 1 1], yhat=[0 0 0 1 1 1 1 1 1 1], correct=True","title":"model"},{"location":"dl/gan/dlnd_face_generation/","text":"Face Generation In this project, you'll use generative adversarial networks to generate new images of faces. Get the Data You'll be using two datasets in this project: - MNIST - CelebA Since the celebA dataset is complex and you're doing GANs in a project for the first time, we want you to test your neural network on MNIST before CelebA. Running the GANs on MNIST will allow you to see how well your model trains sooner. If you're using FloydHub , set data_dir to \"/input\" and use the FloydHub data ID \"R5KrjnANiKVhLWAkpXhNBe\". data_dir = './data' # FloydHub - Use with data ID \"R5KrjnANiKVhLWAkpXhNBe\" #data_dir = '/input' \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" import helper helper . download_extract( 'mnist' , data_dir) helper . download_extract( 'celeba' , data_dir) Found mnist Data Found celeba Data Explore the Data MNIST As you're aware, the MNIST dataset contains images of handwritten digits. You can view the first number of examples by changing show_n_images . show_n_images = 25 \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" % matplotlib inline import os from glob import glob from matplotlib import pyplot mnist_images = helper . get_batch(glob(os . path . join(data_dir, 'mnist/*.jpg' ))[:show_n_images], 28 , 28 , 'L' ) pyplot . imshow(helper . images_square_grid(mnist_images, 'L' ), cmap = 'gray' ) &lt;matplotlib.image.AxesImage at 0x7f5ca6429390&gt; CelebA The CelebFaces Attributes Dataset (CelebA) dataset contains over 200,000 celebrity images with annotations. Since you're going to be generating faces, you won't need the annotations. You can view the first number of examples by changing show_n_images . show_n_images = 25 \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" mnist_images = helper . get_batch(glob(os . path . join(data_dir, 'img_align_celeba/*.jpg' ))[:show_n_images], 28 , 28 , 'RGB' ) pyplot . imshow(helper . images_square_grid(mnist_images, 'RGB' )) &lt;matplotlib.image.AxesImage at 0x7f5ca634ff60&gt; Preprocess the Data Since the project's main focus is on building the GANs, we'll preprocess the data for you. The values of the MNIST and CelebA dataset will be in the range of -0.5 to 0.5 of 28x28 dimensional images. The CelebA images will be cropped to remove parts of the image that don't include a face, then resized down to 28x28. The MNIST images are black and white images with a single [color channel](https://en.wikipedia.org/wiki/Channel_(digital_image%29) while the CelebA images have [3 color channels (RGB color channel)](https://en.wikipedia.org/wiki/Channel_(digital_image%29#RGB_Images). Build the Neural Network You'll build the components necessary to build a GANs by implementing the following functions below: - model_inputs - discriminator - generator - model_loss - model_opt - train Check the Version of TensorFlow and Access to GPU This will check to make sure you have the correct version of TensorFlow and access to a GPU \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" from distutils.version import LooseVersion import warnings import tensorflow as tf # Check TensorFlow Version assert LooseVersion(tf . __version__) >= LooseVersion( '1.0' ), 'Please use TensorFlow version 1.0 or newer. You are using {}' . format(tf . __version__) print ( 'TensorFlow Version: {}' . format(tf . __version__)) # Check for a GPU if not tf . test . gpu_device_name(): warnings . warn( 'No GPU found. Please use a GPU to train your neural network.' ) else : print ( 'Default GPU Device: {}' . format(tf . test . gpu_device_name())) TensorFlow Version: 1.0.0 Default GPU Device: /gpu:0 Input Implement the model_inputs function to create TF Placeholders for the Neural Network. It should create the following placeholders: - Real input images placeholder with rank 4 using image_width , image_height , and image_channels . - Z input placeholder with rank 2 using z_dim . - Learning rate placeholder with rank 0. Return the placeholders in the following the tuple (tensor of real input images, tensor of z data) import problem_unittests as tests def model_inputs (image_width, image_height, image_channels, z_dim): \"\"\" Create the model inputs :param image_width: The input image width :param image_height: The input image height :param image_channels: The number of image channels :param z_dim: The dimension of Z :return: Tuple of (tensor of real input images, tensor of z data, learning rate) \"\"\" # TODO: Implement Function #1. Real input images placeholder with rank 4 using image_width, image_height, and image_channels. input_real = tf . placeholder(tf . float32, [ None , image_width,\\ image_height,\\ image_channels],\\ name = \"input_real\" ) #2. Z input placeholder with rank 2 using z_dim. input_z = tf . placeholder(tf . float32, [ None , z_dim],\\ name = \"input_z\" ) #3. Learning rate placeholder with rank 0. learning_rate = tf . placeholder(tf . float32, None ,\\ name = \"learning_rate\" ) return input_real, input_z, learning_rate \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_model_inputs(model_inputs) Tests Passed Discriminator Implement discriminator to create a discriminator neural network that discriminates on images . This function should be able to reuse the variabes in the neural network. Use tf.variable_scope with a scope name of \"discriminator\" to allow the variables to be reused. The function should return a tuple of (tensor output of the generator, tensor logits of the generator). def discriminator (images, reuse = False ): \"\"\" Create the discriminator network :param image: Tensor of input image(s) :param reuse: Boolean if the weights should be reused :return: Tuple of (tensor output of the discriminator, tensor logits of the discriminator) \"\"\" # TODO: Implement Function alpha = 0.1 sdev = 0.02 with tf . variable_scope( 'discriminator' , reuse = reuse): # input layer with image size(28*28*3) x1 = tf . layers . conv2d(images, 32 , 5 , 2 ,\\ padding = 'same' ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) lrelu1 = tf . maximum(alpha * x1, x1) # Layer 1 out: 14x14x32 # Layer 2: 14x14x32 x2 = tf . layers . conv2d(lrelu1, 64 , 5 , 2 ,\\ padding = 'same' ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) bn2 = tf . layers . batch_normalization(x2, training = True ) lrelu2 = tf . maximum(alpha * bn2, bn2) # Layer 2 out: 7x7x64 x3 = tf . layers . conv2d(lrelu2, 128 , 5 , 2 ,\\ padding = 'same' ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) bn3 = tf . layers . batch_normalization(x3, training = True ) lrelu3 = tf . maximum(alpha * bn3, bn3) # Layer 3 out: 4x4x128 x4 = tf . layers . conv2d(lrelu3, 256 , 5 , 2 ,\\ padding = 'same' ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) bn4 = tf . layers . batch_normalization(x4, training = True ) lrelu4 = tf . maximum(alpha * bn4, bn4) # Layer 4 out: 2x2x256 flattened = tf . reshape(lrelu4, ( - 1 , 2 * 2 * 256 )) logits = tf . layers . dense(flattened, 1 ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) output = tf . sigmoid(logits) return output, logits \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_discriminator(discriminator, tf) Tests Passed Generator Implement generator to generate an image using z . This function should be able to reuse the variabes in the neural network. Use tf.variable_scope with a scope name of \"generator\" to allow the variables to be reused. The function should return the generated 28 x 28 x out_channel_dim images. def generator (z, out_channel_dim, is_train = True ): \"\"\" Create the generator network :param z: Input z :param out_channel_dim: The number of channels in the output image :param is_train: Boolean if generator is being used for training :return: The tensor output of the generator \"\"\" # TODO: Implement Function alpha = 0.2 sdev = 0.02 # variable scope for generator with tf . variable_scope( 'generator' , reuse = not is_train): #fake with fully connected # Layer 1 in: 7x7x256 x1 = tf . layers . dense(z, 4 * 4 * 512 ) x1 = tf . reshape(x1, ( - 1 , 4 , 4 , 512 )) lrelu1 = tf . maximum(alpha * x1, x1) # Layer 1 out: 4x4x512 x2 = tf . layers . conv2d_transpose(lrelu1, 128 , 4 , 1 ,\\ padding = 'valid' ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) bn2 = tf . layers . batch_normalization(x2, training = is_train) lrelu2 = tf . maximum(alpha * bn2, bn2) # Layer 2 out: 8x8x128 x3 = tf . layers . conv2d_transpose(lrelu2, 64 , 5 , 2 ,\\ padding = 'same' , \\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) bn3 = tf . layers . batch_normalization(x3, training = is_train) lrelu3 = tf . maximum(alpha * bn3, bn3) # Layer 3 out: 16x16x64 x4 = tf . layers . conv2d_transpose(lrelu3, 32 , 5 , 2 ,\\ padding = 'same' ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) bn4 = tf . layers . batch_normalization(x4, training = is_train) lrelu4 = tf . maximum(alpha * bn4, bn4) # Layer 4 out: 32x32x32 logits = tf . layers . conv2d_transpose(lrelu4, out_channel_dim, 3 , 1 ,\\ padding = 'same' ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) output = tf . tanh(logits) return output \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_generator(generator, tf) Tests Passed Loss Implement model_loss to build the GANs for training and calculate the loss. The function should return a tuple of (discriminator loss, generator loss). Use the following functions you implemented: - discriminator(images, reuse=False) - generator(z, out_channel_dim, is_train=True) def model_loss (input_real, input_z, out_channel_dim): \"\"\" Get the loss for the discriminator and generator :param input_real: Images from the real dataset :param input_z: Z input :param out_channel_dim: The number of channels in the output image :return: A tuple of (discriminator loss, generator loss) \"\"\" # TODO: Implement Function g_model = generator(input_z, out_channel_dim) d_model_real, d_logits_real = discriminator(input_real) d_model_fake, d_logits_fake = discriminator(g_model, reuse = True ) d_loss_real = tf . reduce_mean( tf . nn . sigmoid_cross_entropy_with_logits(logits = d_logits_real,\\ labels = tf . ones_like(d_model_real))) d_loss_fake = tf . reduce_mean( tf . nn . sigmoid_cross_entropy_with_logits(logits = d_logits_fake,\\ labels = tf . zeros_like(d_model_fake))) g_loss = tf . reduce_mean( tf . nn . sigmoid_cross_entropy_with_logits(logits = d_logits_fake,\\ labels = tf . ones_like(d_model_fake))) d_loss = d_loss_real + d_loss_fake return d_loss, g_loss \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_model_loss(model_loss) Tests Passed Optimization Implement model_opt to create the optimization operations for the GANs. Use tf.trainable_variables to get all the trainable variables. Filter the variables with names that are in the discriminator and generator scope names. The function should return a tuple of (discriminator training operation, generator training operation). def model_opt (d_loss, g_loss, learning_rate, beta1): \"\"\" Get optimization operations :param d_loss: Discriminator loss Tensor :param g_loss: Generator loss Tensor :param learning_rate: Learning Rate Placeholder :param beta1: The exponential decay rate for the 1st moment in the optimizer :return: A tuple of (discriminator training operation, generator training operation) \"\"\" # TODO: Implement Function t_vars = tf . trainable_variables() d_vars = [var for var in t_vars if var . name . startswith( 'discriminator' )] g_vars = [var for var in t_vars if var . name . startswith( 'generator' )] # Optimize d_train_opt = tf . train . AdamOptimizer(learning_rate,\\ beta1 = beta1) . minimize(d_loss, var_list = d_vars) with tf . control_dependencies(tf . get_collection(tf . GraphKeys . UPDATE_OPS)): g_train_opt = tf . train . AdamOptimizer(learning_rate,\\ beta1 = beta1) . minimize(g_loss, var_list = g_vars) return d_train_opt, g_train_opt \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_model_opt(model_opt, tf) Tests Passed Neural Network Training Show Output Use this function to show the current output of the generator during training. It will help you determine how well the GANs is training. \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" import numpy as np def show_generator_output (sess, n_images, input_z, out_channel_dim, image_mode): \"\"\" Show example output for the generator :param sess: TensorFlow session :param n_images: Number of Images to display :param input_z: Input Z Tensor :param out_channel_dim: The number of channels in the output image :param image_mode: The mode to use for images (\"RGB\" or \"L\") \"\"\" cmap = None if image_mode == 'RGB' else 'gray' z_dim = input_z . get_shape() . as_list()[ - 1 ] example_z = np . random . uniform( - 1 , 1 , size = [n_images, z_dim]) samples = sess . run( generator(input_z, out_channel_dim, False ), feed_dict = {input_z: example_z}) images_grid = helper . images_square_grid(samples, image_mode) pyplot . imshow(images_grid, cmap = cmap) pyplot . show() Train Implement train to build and train the GANs. Use the following functions you implemented: - model_inputs(image_width, image_height, image_channels, z_dim) - model_loss(input_real, input_z, out_channel_dim) - model_opt(d_loss, g_loss, learning_rate, beta1) Use the show_generator_output to show generator output while you train. Running show_generator_output for every batch will drastically increase training time and increase the size of the notebook. It's recommended to print the generator output every 100 batches. def train (epoch_count, batch_size, z_dim, learning_rate, beta1, get_batches, data_shape, data_image_mode): \"\"\" Train the GAN :param epoch_count: Number of epochs :param batch_size: Batch Size :param z_dim: Z dimension :param learning_rate: Learning Rate :param beta1: The exponential decay rate for the 1st moment in the optimizer :param get_batches: Function to get batches :param data_shape: Shape of the data :param data_image_mode: The image mode to use for images (\"RGB\" or \"L\") \"\"\" # TODO: Build Model n_samples, width, height, channels = data_shape input_real, input_z, learn_rate = model_inputs(width, height, channels, z_dim) d_loss, g_loss = model_loss(input_real, input_z, channels) d_opt, g_opt = model_opt(d_loss, g_loss, learn_rate, beta1) steps = 0 show_every = 50 print_every = 25 with tf . Session() as sess: sess . run(tf . global_variables_initializer()) for epoch_i in range (epoch_count): for batch_images in get_batches(batch_size): batch_images *= 2 # TODO: Train Model steps += 1 # Sample random noise for G batch_z = np . random . uniform( - 1 , 1 , size = (batch_size, z_dim)) # Run optimizers _ = sess . run(d_opt, feed_dict = {input_real: batch_images,\\ input_z: batch_z,\\ learn_rate: learning_rate}) _ = sess . run(g_opt, feed_dict = {input_real: batch_images,\\ input_z: batch_z,\\ learn_rate: learning_rate}) # show_generator_output to show generator output if steps % show_every == 0 : n_images = 16 show_generator_output(sess, n_images, input_z, channels, data_image_mode) if steps % print_every == 0 : train_loss_d = d_loss . eval({input_z: batch_z, input_real: batch_images}) train_loss_g = g_loss . eval({input_z: batch_z}) print ( \"Epoch {}/{}...\" . format(epoch_i, epoch_count), \"Discriminator Loss: {:.4f}...\" . format(train_loss_d), \"Generator Loss: {:.4f}\" . format(train_loss_g)) MNIST Test your GANs architecture on MNIST. After 2 epochs, the GANs should be able to generate images that look like handwritten digits. Make sure the loss of the generator is lower than the loss of the discriminator or close to 0. batch_size = 64 z_dim = 100 learning_rate = 0.0005 beta1 = 0.5 \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" epochs = 2 mnist_dataset = helper . Dataset( 'mnist' , glob(os . path . join(data_dir, 'mnist/*.jpg' ))) with tf . Graph() . as_default(): train(epochs, batch_size, z_dim, learning_rate, beta1, mnist_dataset . get_batches, mnist_dataset . shape, mnist_dataset . image_mode) Epoch 0/2... Discriminator Loss: 0.1132... Generator Loss: 3.8164 Epoch 0/2... Discriminator Loss: 0.4083... Generator Loss: 1.6668 Epoch 0/2... Discriminator Loss: 0.2134... Generator Loss: 2.3080 Epoch 0/2... Discriminator Loss: 0.8935... Generator Loss: 0.7820 Epoch 0/2... Discriminator Loss: 0.9974... Generator Loss: 0.9753 Epoch 0/2... Discriminator Loss: 1.4598... Generator Loss: 0.4637 Epoch 0/2... Discriminator Loss: 1.0837... Generator Loss: 0.9862 Epoch 0/2... Discriminator Loss: 1.0771... Generator Loss: 1.2634 Epoch 0/2... Discriminator Loss: 1.5585... Generator Loss: 0.3205 Epoch 0/2... Discriminator Loss: 1.0825... Generator Loss: 0.6745 Epoch 0/2... Discriminator Loss: 0.9129... Generator Loss: 1.0191 Epoch 0/2... Discriminator Loss: 0.9434... Generator Loss: 0.8767 Epoch 0/2... Discriminator Loss: 0.9463... Generator Loss: 0.9866 Epoch 0/2... Discriminator Loss: 1.1447... Generator Loss: 0.6476 Epoch 0/2... Discriminator Loss: 1.3758... Generator Loss: 0.4319 Epoch 0/2... Discriminator Loss: 0.9714... Generator Loss: 0.6623 Epoch 0/2... Discriminator Loss: 1.5291... Generator Loss: 0.3816 Epoch 0/2... Discriminator Loss: 1.3693... Generator Loss: 2.0586 Epoch 0/2... Discriminator Loss: 1.7561... Generator Loss: 0.2967 Epoch 0/2... Discriminator Loss: 1.5004... Generator Loss: 0.3576 Epoch 0/2... Discriminator Loss: 0.7120... Generator Loss: 1.2979 Epoch 0/2... Discriminator Loss: 0.8413... Generator Loss: 1.0782 Epoch 0/2... Discriminator Loss: 1.1858... Generator Loss: 0.5920 Epoch 0/2... Discriminator Loss: 1.0060... Generator Loss: 0.6120 Epoch 0/2... Discriminator Loss: 1.2757... Generator Loss: 0.4474 Epoch 0/2... Discriminator Loss: 1.5774... Generator Loss: 0.3005 Epoch 0/2... Discriminator Loss: 1.2721... Generator Loss: 0.4242 Epoch 0/2... Discriminator Loss: 1.0972... Generator Loss: 0.5191 Epoch 0/2... Discriminator Loss: 0.8444... Generator Loss: 1.2999 Epoch 0/2... Discriminator Loss: 0.8549... Generator Loss: 0.9284 Epoch 0/2... Discriminator Loss: 0.8498... Generator Loss: 0.9417 Epoch 0/2... Discriminator Loss: 0.6521... Generator Loss: 1.2973 Epoch 0/2... Discriminator Loss: 0.9847... Generator Loss: 0.6543 Epoch 0/2... Discriminator Loss: 1.2542... Generator Loss: 0.4832 Epoch 0/2... Discriminator Loss: 0.9244... Generator Loss: 1.4905 Epoch 0/2... Discriminator Loss: 1.0157... Generator Loss: 0.5582 Epoch 0/2... Discriminator Loss: 0.5705... Generator Loss: 1.7848 Epoch 1/2... Discriminator Loss: 1.0804... Generator Loss: 0.7368 Epoch 1/2... Discriminator Loss: 1.3714... Generator Loss: 2.7104 Epoch 1/2... Discriminator Loss: 0.7393... Generator Loss: 1.0792 Epoch 1/2... Discriminator Loss: 0.8901... Generator Loss: 0.9193 Epoch 1/2... Discriminator Loss: 1.3041... Generator Loss: 0.4482 Epoch 1/2... Discriminator Loss: 1.0246... Generator Loss: 1.6754 Epoch 1/2... Discriminator Loss: 1.4366... Generator Loss: 0.4005 Epoch 1/2... Discriminator Loss: 0.7496... Generator Loss: 0.9064 Epoch 1/2... Discriminator Loss: 1.0066... Generator Loss: 0.6571 Epoch 1/2... Discriminator Loss: 0.6344... Generator Loss: 1.0048 Epoch 1/2... Discriminator Loss: 0.9111... Generator Loss: 1.4843 Epoch 1/2... Discriminator Loss: 0.8954... Generator Loss: 1.0413 Epoch 1/2... Discriminator Loss: 2.1049... Generator Loss: 0.1755 Epoch 1/2... Discriminator Loss: 1.0708... Generator Loss: 0.5250 Epoch 1/2... Discriminator Loss: 1.4498... Generator Loss: 0.3438 Epoch 1/2... Discriminator Loss: 0.8951... Generator Loss: 0.7148 Epoch 1/2... Discriminator Loss: 0.9677... Generator Loss: 0.6380 Epoch 1/2... Discriminator Loss: 2.0528... Generator Loss: 0.1594 Epoch 1/2... Discriminator Loss: 0.5354... Generator Loss: 1.1890 Epoch 1/2... Discriminator Loss: 1.5721... Generator Loss: 0.5104 Epoch 1/2... Discriminator Loss: 0.7788... Generator Loss: 0.9999 Epoch 1/2... Discriminator Loss: 0.8880... Generator Loss: 0.6436 Epoch 1/2... Discriminator Loss: 0.6986... Generator Loss: 1.4282 Epoch 1/2... Discriminator Loss: 1.2309... Generator Loss: 0.4484 Epoch 1/2... Discriminator Loss: 1.0756... Generator Loss: 0.6155 Epoch 1/2... Discriminator Loss: 3.1573... Generator Loss: 3.8849 Epoch 1/2... Discriminator Loss: 0.8739... Generator Loss: 0.6847 Epoch 1/2... Discriminator Loss: 1.7689... Generator Loss: 0.2244 Epoch 1/2... Discriminator Loss: 0.8876... Generator Loss: 0.7815 Epoch 1/2... Discriminator Loss: 1.1661... Generator Loss: 0.4628 Epoch 1/2... Discriminator Loss: 1.5133... Generator Loss: 0.3179 Epoch 1/2... Discriminator Loss: 0.6079... Generator Loss: 1.3311 Epoch 1/2... Discriminator Loss: 0.7159... Generator Loss: 1.5858 Epoch 1/2... Discriminator Loss: 0.7321... Generator Loss: 0.8564 Epoch 1/2... Discriminator Loss: 1.2747... Generator Loss: 0.4073 Epoch 1/2... Discriminator Loss: 1.3412... Generator Loss: 0.3678 Epoch 1/2... Discriminator Loss: 0.8914... Generator Loss: 0.7424 CelebA Run your GANs on CelebA. It will take around 20 minutes on the average GPU to run one epoch. You can run the whole epoch or stop when it starts to generate realistic faces. batch_size = 64 z_dim = 100 learning_rate = 0.0005 beta1 = 0.5 \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" epochs = 1 celeba_dataset = helper . Dataset( 'celeba' , glob(os . path . join(data_dir, 'img_align_celeba/*.jpg' ))) with tf . Graph() . as_default(): train(epochs, batch_size, z_dim, learning_rate, beta1, celeba_dataset . get_batches, celeba_dataset . shape, celeba_dataset . image_mode) Epoch 0/1... Discriminator Loss: 0.6341... Generator Loss: 1.5425 Epoch 0/1... Discriminator Loss: 1.0661... Generator Loss: 1.4051 Epoch 0/1... Discriminator Loss: 1.4774... Generator Loss: 0.4814 Epoch 0/1... Discriminator Loss: 0.5296... Generator Loss: 2.0486 Epoch 0/1... Discriminator Loss: 0.4144... Generator Loss: 1.7049 Epoch 0/1... Discriminator Loss: 0.8953... Generator Loss: 1.3714 Epoch 0/1... Discriminator Loss: 1.2676... Generator Loss: 2.1938 Epoch 0/1... Discriminator Loss: 0.6703... Generator Loss: 1.9268 Epoch 0/1... Discriminator Loss: 1.3622... Generator Loss: 0.9100 Epoch 0/1... Discriminator Loss: 0.8473... Generator Loss: 1.2377 Epoch 0/1... Discriminator Loss: 0.7923... Generator Loss: 1.3153 Epoch 0/1... Discriminator Loss: 2.3745... Generator Loss: 3.9981 Epoch 0/1... Discriminator Loss: 1.3498... Generator Loss: 0.9091 Epoch 0/1... Discriminator Loss: 0.9757... Generator Loss: 1.3260 Epoch 0/1... Discriminator Loss: 0.9970... Generator Loss: 0.8662 Epoch 0/1... Discriminator Loss: 1.2872... Generator Loss: 0.7643 Epoch 0/1... Discriminator Loss: 0.9423... Generator Loss: 1.0006 Epoch 0/1... Discriminator Loss: 0.6227... Generator Loss: 1.2088 Epoch 0/1... Discriminator Loss: 0.5900... Generator Loss: 1.5671 Epoch 0/1... Discriminator Loss: 1.0243... Generator Loss: 0.9068 Epoch 0/1... Discriminator Loss: 1.5452... Generator Loss: 0.3467 Epoch 0/1... Discriminator Loss: 0.9187... Generator Loss: 1.3025 Epoch 0/1... Discriminator Loss: 0.9896... Generator Loss: 0.9055 Epoch 0/1... Discriminator Loss: 0.9112... Generator Loss: 1.2679 Epoch 0/1... Discriminator Loss: 1.0865... Generator Loss: 1.3766 Epoch 0/1... Discriminator Loss: 1.0086... Generator Loss: 0.7582 Epoch 0/1... Discriminator Loss: 0.8523... Generator Loss: 1.3466 Epoch 0/1... Discriminator Loss: 0.7004... Generator Loss: 1.6204 Epoch 0/1... Discriminator Loss: 1.1147... Generator Loss: 1.0215 Epoch 0/1... Discriminator Loss: 0.9817... Generator Loss: 0.9707 Epoch 0/1... Discriminator Loss: 0.8524... Generator Loss: 0.7910 Epoch 0/1... Discriminator Loss: 1.3500... Generator Loss: 0.6876 Epoch 0/1... Discriminator Loss: 1.1861... Generator Loss: 0.7478 Epoch 0/1... Discriminator Loss: 1.4163... Generator Loss: 0.4300 Epoch 0/1... Discriminator Loss: 1.3214... Generator Loss: 0.6985 Epoch 0/1... Discriminator Loss: 1.6581... Generator Loss: 0.3179 Epoch 0/1... Discriminator Loss: 0.8624... Generator Loss: 2.0154 Epoch 0/1... Discriminator Loss: 0.7325... Generator Loss: 1.5246 Epoch 0/1... Discriminator Loss: 1.3200... Generator Loss: 0.4783 Epoch 0/1... Discriminator Loss: 0.9317... Generator Loss: 0.8402 Epoch 0/1... Discriminator Loss: 1.2700... Generator Loss: 1.6259 Epoch 0/1... Discriminator Loss: 1.6959... Generator Loss: 1.6403 Epoch 0/1... Discriminator Loss: 0.9565... Generator Loss: 1.8874 Epoch 0/1... Discriminator Loss: 0.7762... Generator Loss: 1.6332 Epoch 0/1... Discriminator Loss: 0.9902... Generator Loss: 0.6471 Epoch 0/1... Discriminator Loss: 1.0775... Generator Loss: 0.6818 Epoch 0/1... Discriminator Loss: 1.0928... Generator Loss: 0.9076 Epoch 0/1... Discriminator Loss: 1.2636... Generator Loss: 0.8888 Epoch 0/1... Discriminator Loss: 1.9158... Generator Loss: 0.1960 Epoch 0/1... Discriminator Loss: 1.0883... Generator Loss: 0.6902 Epoch 0/1... Discriminator Loss: 1.0772... Generator Loss: 0.6896 Epoch 0/1... Discriminator Loss: 1.1797... Generator Loss: 0.5554 Epoch 0/1... Discriminator Loss: 1.1049... Generator Loss: 0.9699 Epoch 0/1... Discriminator Loss: 1.2788... Generator Loss: 2.0408 Epoch 0/1... Discriminator Loss: 0.7399... Generator Loss: 1.1408 Epoch 0/1... Discriminator Loss: 1.3329... Generator Loss: 0.4466 Epoch 0/1... Discriminator Loss: 0.9657... Generator Loss: 0.9572 Epoch 0/1... Discriminator Loss: 1.0361... Generator Loss: 0.7271 Epoch 0/1... Discriminator Loss: 1.0897... Generator Loss: 0.8504 Epoch 0/1... Discriminator Loss: 1.2010... Generator Loss: 1.1790 Epoch 0/1... Discriminator Loss: 1.6883... Generator Loss: 1.1168 Epoch 0/1... Discriminator Loss: 1.0092... Generator Loss: 0.6636 Epoch 0/1... Discriminator Loss: 1.3417... Generator Loss: 0.5133 Epoch 0/1... Discriminator Loss: 0.9791... Generator Loss: 0.9111 Epoch 0/1... Discriminator Loss: 0.8803... Generator Loss: 1.1090 Epoch 0/1... Discriminator Loss: 0.8513... Generator Loss: 1.2366 Epoch 0/1... Discriminator Loss: 0.7197... Generator Loss: 1.5789 Epoch 0/1... Discriminator Loss: 0.9334... Generator Loss: 0.7907 Epoch 0/1... Discriminator Loss: 1.1299... Generator Loss: 0.5871 Epoch 0/1... Discriminator Loss: 0.9805... Generator Loss: 1.1100 Epoch 0/1... Discriminator Loss: 1.1180... Generator Loss: 2.0649 Epoch 0/1... Discriminator Loss: 0.9413... Generator Loss: 0.6843 Epoch 0/1... Discriminator Loss: 1.1723... Generator Loss: 1.0662 Epoch 0/1... Discriminator Loss: 1.3740... Generator Loss: 0.4719 Epoch 0/1... Discriminator Loss: 1.2226... Generator Loss: 0.7428 Epoch 0/1... Discriminator Loss: 1.0916... Generator Loss: 1.5043 Epoch 0/1... Discriminator Loss: 1.2517... Generator Loss: 2.0049 Epoch 0/1... Discriminator Loss: 1.1599... Generator Loss: 0.7082 Epoch 0/1... Discriminator Loss: 1.0402... Generator Loss: 0.8053 Epoch 0/1... Discriminator Loss: 1.0036... Generator Loss: 0.9750 Epoch 0/1... Discriminator Loss: 0.9274... Generator Loss: 0.9230 Epoch 0/1... Discriminator Loss: 1.1304... Generator Loss: 1.1508 Epoch 0/1... Discriminator Loss: 0.7811... Generator Loss: 1.4115 Epoch 0/1... Discriminator Loss: 1.0645... Generator Loss: 0.8931 Epoch 0/1... Discriminator Loss: 1.4299... Generator Loss: 2.5213 Epoch 0/1... Discriminator Loss: 1.0405... Generator Loss: 0.6912 Epoch 0/1... Discriminator Loss: 1.1746... Generator Loss: 0.6077 Epoch 0/1... Discriminator Loss: 0.9569... Generator Loss: 2.3023 Epoch 0/1... Discriminator Loss: 0.9257... Generator Loss: 0.9512 Epoch 0/1... Discriminator Loss: 0.9541... Generator Loss: 0.7513 Epoch 0/1... Discriminator Loss: 0.9034... Generator Loss: 0.9518 Epoch 0/1... Discriminator Loss: 1.1156... Generator Loss: 0.8511 Epoch 0/1... Discriminator Loss: 0.9349... Generator Loss: 1.1430 Epoch 0/1... Discriminator Loss: 1.0691... Generator Loss: 0.6279 Epoch 0/1... Discriminator Loss: 1.1409... Generator Loss: 0.9605 Epoch 0/1... Discriminator Loss: 1.4045... Generator Loss: 0.3858 Epoch 0/1... Discriminator Loss: 1.0732... Generator Loss: 0.7521 Epoch 0/1... Discriminator Loss: 1.0664... Generator Loss: 0.9785 Epoch 0/1... Discriminator Loss: 1.1484... Generator Loss: 1.4160 Epoch 0/1... Discriminator Loss: 1.2619... Generator Loss: 0.5863 Epoch 0/1... Discriminator Loss: 0.9650... Generator Loss: 0.9150 Epoch 0/1... Discriminator Loss: 0.9211... Generator Loss: 1.4288 Epoch 0/1... Discriminator Loss: 0.9854... Generator Loss: 0.7512 Epoch 0/1... Discriminator Loss: 0.8539... Generator Loss: 0.9985 Epoch 0/1... Discriminator Loss: 0.9036... Generator Loss: 1.0608 Epoch 0/1... Discriminator Loss: 1.0430... Generator Loss: 0.7103 Epoch 0/1... Discriminator Loss: 0.8404... Generator Loss: 0.9762 Epoch 0/1... Discriminator Loss: 1.2528... Generator Loss: 0.5786 Epoch 0/1... Discriminator Loss: 1.2818... Generator Loss: 0.4568 Epoch 0/1... Discriminator Loss: 1.0006... Generator Loss: 0.8900 Epoch 0/1... Discriminator Loss: 1.3815... Generator Loss: 0.3886 Epoch 0/1... Discriminator Loss: 1.2679... Generator Loss: 0.5873 Epoch 0/1... Discriminator Loss: 0.9585... Generator Loss: 0.9575 Epoch 0/1... Discriminator Loss: 1.2240... Generator Loss: 0.7966 Epoch 0/1... Discriminator Loss: 0.8579... Generator Loss: 1.4551 Epoch 0/1... Discriminator Loss: 1.3028... Generator Loss: 0.5394 Epoch 0/1... Discriminator Loss: 1.1112... Generator Loss: 0.7762 Epoch 0/1... Discriminator Loss: 1.1230... Generator Loss: 0.9227 Epoch 0/1... Discriminator Loss: 1.2362... Generator Loss: 0.5178 Epoch 0/1... Discriminator Loss: 1.0913... Generator Loss: 0.7929 Epoch 0/1... Discriminator Loss: 0.8596... Generator Loss: 0.9705 Epoch 0/1... Discriminator Loss: 1.1329... Generator Loss: 0.6999 Epoch 0/1... Discriminator Loss: 1.0311... Generator Loss: 0.8163 Epoch 0/1... Discriminator Loss: 0.8289... Generator Loss: 1.1358 Epoch 0/1... Discriminator Loss: 1.2079... Generator Loss: 0.5730 Epoch 0/1... Discriminator Loss: 0.9879... Generator Loss: 0.7466 Submitting This Project When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_face_generation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission.","title":"Face Generation"},{"location":"dl/gan/dlnd_face_generation/#face-generation","text":"In this project, you'll use generative adversarial networks to generate new images of faces.","title":"Face Generation"},{"location":"dl/gan/dlnd_face_generation/#get-the-data","text":"You'll be using two datasets in this project: - MNIST - CelebA Since the celebA dataset is complex and you're doing GANs in a project for the first time, we want you to test your neural network on MNIST before CelebA. Running the GANs on MNIST will allow you to see how well your model trains sooner. If you're using FloydHub , set data_dir to \"/input\" and use the FloydHub data ID \"R5KrjnANiKVhLWAkpXhNBe\". data_dir = './data' # FloydHub - Use with data ID \"R5KrjnANiKVhLWAkpXhNBe\" #data_dir = '/input' \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" import helper helper . download_extract( 'mnist' , data_dir) helper . download_extract( 'celeba' , data_dir) Found mnist Data Found celeba Data","title":"Get the Data"},{"location":"dl/gan/dlnd_face_generation/#explore-the-data","text":"","title":"Explore the Data"},{"location":"dl/gan/dlnd_face_generation/#mnist","text":"As you're aware, the MNIST dataset contains images of handwritten digits. You can view the first number of examples by changing show_n_images . show_n_images = 25 \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" % matplotlib inline import os from glob import glob from matplotlib import pyplot mnist_images = helper . get_batch(glob(os . path . join(data_dir, 'mnist/*.jpg' ))[:show_n_images], 28 , 28 , 'L' ) pyplot . imshow(helper . images_square_grid(mnist_images, 'L' ), cmap = 'gray' ) &lt;matplotlib.image.AxesImage at 0x7f5ca6429390&gt;","title":"MNIST"},{"location":"dl/gan/dlnd_face_generation/#celeba","text":"The CelebFaces Attributes Dataset (CelebA) dataset contains over 200,000 celebrity images with annotations. Since you're going to be generating faces, you won't need the annotations. You can view the first number of examples by changing show_n_images . show_n_images = 25 \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" mnist_images = helper . get_batch(glob(os . path . join(data_dir, 'img_align_celeba/*.jpg' ))[:show_n_images], 28 , 28 , 'RGB' ) pyplot . imshow(helper . images_square_grid(mnist_images, 'RGB' )) &lt;matplotlib.image.AxesImage at 0x7f5ca634ff60&gt;","title":"CelebA"},{"location":"dl/gan/dlnd_face_generation/#preprocess-the-data","text":"Since the project's main focus is on building the GANs, we'll preprocess the data for you. The values of the MNIST and CelebA dataset will be in the range of -0.5 to 0.5 of 28x28 dimensional images. The CelebA images will be cropped to remove parts of the image that don't include a face, then resized down to 28x28. The MNIST images are black and white images with a single [color channel](https://en.wikipedia.org/wiki/Channel_(digital_image%29) while the CelebA images have [3 color channels (RGB color channel)](https://en.wikipedia.org/wiki/Channel_(digital_image%29#RGB_Images).","title":"Preprocess the Data"},{"location":"dl/gan/dlnd_face_generation/#build-the-neural-network","text":"You'll build the components necessary to build a GANs by implementing the following functions below: - model_inputs - discriminator - generator - model_loss - model_opt - train","title":"Build the Neural Network"},{"location":"dl/gan/dlnd_face_generation/#check-the-version-of-tensorflow-and-access-to-gpu","text":"This will check to make sure you have the correct version of TensorFlow and access to a GPU \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" from distutils.version import LooseVersion import warnings import tensorflow as tf # Check TensorFlow Version assert LooseVersion(tf . __version__) >= LooseVersion( '1.0' ), 'Please use TensorFlow version 1.0 or newer. You are using {}' . format(tf . __version__) print ( 'TensorFlow Version: {}' . format(tf . __version__)) # Check for a GPU if not tf . test . gpu_device_name(): warnings . warn( 'No GPU found. Please use a GPU to train your neural network.' ) else : print ( 'Default GPU Device: {}' . format(tf . test . gpu_device_name())) TensorFlow Version: 1.0.0 Default GPU Device: /gpu:0","title":"Check the Version of TensorFlow and Access to GPU"},{"location":"dl/gan/dlnd_face_generation/#input","text":"Implement the model_inputs function to create TF Placeholders for the Neural Network. It should create the following placeholders: - Real input images placeholder with rank 4 using image_width , image_height , and image_channels . - Z input placeholder with rank 2 using z_dim . - Learning rate placeholder with rank 0. Return the placeholders in the following the tuple (tensor of real input images, tensor of z data) import problem_unittests as tests def model_inputs (image_width, image_height, image_channels, z_dim): \"\"\" Create the model inputs :param image_width: The input image width :param image_height: The input image height :param image_channels: The number of image channels :param z_dim: The dimension of Z :return: Tuple of (tensor of real input images, tensor of z data, learning rate) \"\"\" # TODO: Implement Function #1. Real input images placeholder with rank 4 using image_width, image_height, and image_channels. input_real = tf . placeholder(tf . float32, [ None , image_width,\\ image_height,\\ image_channels],\\ name = \"input_real\" ) #2. Z input placeholder with rank 2 using z_dim. input_z = tf . placeholder(tf . float32, [ None , z_dim],\\ name = \"input_z\" ) #3. Learning rate placeholder with rank 0. learning_rate = tf . placeholder(tf . float32, None ,\\ name = \"learning_rate\" ) return input_real, input_z, learning_rate \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_model_inputs(model_inputs) Tests Passed","title":"Input"},{"location":"dl/gan/dlnd_face_generation/#discriminator","text":"Implement discriminator to create a discriminator neural network that discriminates on images . This function should be able to reuse the variabes in the neural network. Use tf.variable_scope with a scope name of \"discriminator\" to allow the variables to be reused. The function should return a tuple of (tensor output of the generator, tensor logits of the generator). def discriminator (images, reuse = False ): \"\"\" Create the discriminator network :param image: Tensor of input image(s) :param reuse: Boolean if the weights should be reused :return: Tuple of (tensor output of the discriminator, tensor logits of the discriminator) \"\"\" # TODO: Implement Function alpha = 0.1 sdev = 0.02 with tf . variable_scope( 'discriminator' , reuse = reuse): # input layer with image size(28*28*3) x1 = tf . layers . conv2d(images, 32 , 5 , 2 ,\\ padding = 'same' ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) lrelu1 = tf . maximum(alpha * x1, x1) # Layer 1 out: 14x14x32 # Layer 2: 14x14x32 x2 = tf . layers . conv2d(lrelu1, 64 , 5 , 2 ,\\ padding = 'same' ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) bn2 = tf . layers . batch_normalization(x2, training = True ) lrelu2 = tf . maximum(alpha * bn2, bn2) # Layer 2 out: 7x7x64 x3 = tf . layers . conv2d(lrelu2, 128 , 5 , 2 ,\\ padding = 'same' ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) bn3 = tf . layers . batch_normalization(x3, training = True ) lrelu3 = tf . maximum(alpha * bn3, bn3) # Layer 3 out: 4x4x128 x4 = tf . layers . conv2d(lrelu3, 256 , 5 , 2 ,\\ padding = 'same' ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) bn4 = tf . layers . batch_normalization(x4, training = True ) lrelu4 = tf . maximum(alpha * bn4, bn4) # Layer 4 out: 2x2x256 flattened = tf . reshape(lrelu4, ( - 1 , 2 * 2 * 256 )) logits = tf . layers . dense(flattened, 1 ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) output = tf . sigmoid(logits) return output, logits \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_discriminator(discriminator, tf) Tests Passed","title":"Discriminator"},{"location":"dl/gan/dlnd_face_generation/#generator","text":"Implement generator to generate an image using z . This function should be able to reuse the variabes in the neural network. Use tf.variable_scope with a scope name of \"generator\" to allow the variables to be reused. The function should return the generated 28 x 28 x out_channel_dim images. def generator (z, out_channel_dim, is_train = True ): \"\"\" Create the generator network :param z: Input z :param out_channel_dim: The number of channels in the output image :param is_train: Boolean if generator is being used for training :return: The tensor output of the generator \"\"\" # TODO: Implement Function alpha = 0.2 sdev = 0.02 # variable scope for generator with tf . variable_scope( 'generator' , reuse = not is_train): #fake with fully connected # Layer 1 in: 7x7x256 x1 = tf . layers . dense(z, 4 * 4 * 512 ) x1 = tf . reshape(x1, ( - 1 , 4 , 4 , 512 )) lrelu1 = tf . maximum(alpha * x1, x1) # Layer 1 out: 4x4x512 x2 = tf . layers . conv2d_transpose(lrelu1, 128 , 4 , 1 ,\\ padding = 'valid' ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) bn2 = tf . layers . batch_normalization(x2, training = is_train) lrelu2 = tf . maximum(alpha * bn2, bn2) # Layer 2 out: 8x8x128 x3 = tf . layers . conv2d_transpose(lrelu2, 64 , 5 , 2 ,\\ padding = 'same' , \\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) bn3 = tf . layers . batch_normalization(x3, training = is_train) lrelu3 = tf . maximum(alpha * bn3, bn3) # Layer 3 out: 16x16x64 x4 = tf . layers . conv2d_transpose(lrelu3, 32 , 5 , 2 ,\\ padding = 'same' ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) bn4 = tf . layers . batch_normalization(x4, training = is_train) lrelu4 = tf . maximum(alpha * bn4, bn4) # Layer 4 out: 32x32x32 logits = tf . layers . conv2d_transpose(lrelu4, out_channel_dim, 3 , 1 ,\\ padding = 'same' ,\\ kernel_initializer = tf . random_normal_initializer(stddev = sdev)) output = tf . tanh(logits) return output \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_generator(generator, tf) Tests Passed","title":"Generator"},{"location":"dl/gan/dlnd_face_generation/#loss","text":"Implement model_loss to build the GANs for training and calculate the loss. The function should return a tuple of (discriminator loss, generator loss). Use the following functions you implemented: - discriminator(images, reuse=False) - generator(z, out_channel_dim, is_train=True) def model_loss (input_real, input_z, out_channel_dim): \"\"\" Get the loss for the discriminator and generator :param input_real: Images from the real dataset :param input_z: Z input :param out_channel_dim: The number of channels in the output image :return: A tuple of (discriminator loss, generator loss) \"\"\" # TODO: Implement Function g_model = generator(input_z, out_channel_dim) d_model_real, d_logits_real = discriminator(input_real) d_model_fake, d_logits_fake = discriminator(g_model, reuse = True ) d_loss_real = tf . reduce_mean( tf . nn . sigmoid_cross_entropy_with_logits(logits = d_logits_real,\\ labels = tf . ones_like(d_model_real))) d_loss_fake = tf . reduce_mean( tf . nn . sigmoid_cross_entropy_with_logits(logits = d_logits_fake,\\ labels = tf . zeros_like(d_model_fake))) g_loss = tf . reduce_mean( tf . nn . sigmoid_cross_entropy_with_logits(logits = d_logits_fake,\\ labels = tf . ones_like(d_model_fake))) d_loss = d_loss_real + d_loss_fake return d_loss, g_loss \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_model_loss(model_loss) Tests Passed","title":"Loss"},{"location":"dl/gan/dlnd_face_generation/#optimization","text":"Implement model_opt to create the optimization operations for the GANs. Use tf.trainable_variables to get all the trainable variables. Filter the variables with names that are in the discriminator and generator scope names. The function should return a tuple of (discriminator training operation, generator training operation). def model_opt (d_loss, g_loss, learning_rate, beta1): \"\"\" Get optimization operations :param d_loss: Discriminator loss Tensor :param g_loss: Generator loss Tensor :param learning_rate: Learning Rate Placeholder :param beta1: The exponential decay rate for the 1st moment in the optimizer :return: A tuple of (discriminator training operation, generator training operation) \"\"\" # TODO: Implement Function t_vars = tf . trainable_variables() d_vars = [var for var in t_vars if var . name . startswith( 'discriminator' )] g_vars = [var for var in t_vars if var . name . startswith( 'generator' )] # Optimize d_train_opt = tf . train . AdamOptimizer(learning_rate,\\ beta1 = beta1) . minimize(d_loss, var_list = d_vars) with tf . control_dependencies(tf . get_collection(tf . GraphKeys . UPDATE_OPS)): g_train_opt = tf . train . AdamOptimizer(learning_rate,\\ beta1 = beta1) . minimize(g_loss, var_list = g_vars) return d_train_opt, g_train_opt \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_model_opt(model_opt, tf) Tests Passed","title":"Optimization"},{"location":"dl/gan/dlnd_face_generation/#neural-network-training","text":"","title":"Neural Network Training"},{"location":"dl/gan/dlnd_face_generation/#show-output","text":"Use this function to show the current output of the generator during training. It will help you determine how well the GANs is training. \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" import numpy as np def show_generator_output (sess, n_images, input_z, out_channel_dim, image_mode): \"\"\" Show example output for the generator :param sess: TensorFlow session :param n_images: Number of Images to display :param input_z: Input Z Tensor :param out_channel_dim: The number of channels in the output image :param image_mode: The mode to use for images (\"RGB\" or \"L\") \"\"\" cmap = None if image_mode == 'RGB' else 'gray' z_dim = input_z . get_shape() . as_list()[ - 1 ] example_z = np . random . uniform( - 1 , 1 , size = [n_images, z_dim]) samples = sess . run( generator(input_z, out_channel_dim, False ), feed_dict = {input_z: example_z}) images_grid = helper . images_square_grid(samples, image_mode) pyplot . imshow(images_grid, cmap = cmap) pyplot . show()","title":"Show Output"},{"location":"dl/gan/dlnd_face_generation/#train","text":"Implement train to build and train the GANs. Use the following functions you implemented: - model_inputs(image_width, image_height, image_channels, z_dim) - model_loss(input_real, input_z, out_channel_dim) - model_opt(d_loss, g_loss, learning_rate, beta1) Use the show_generator_output to show generator output while you train. Running show_generator_output for every batch will drastically increase training time and increase the size of the notebook. It's recommended to print the generator output every 100 batches. def train (epoch_count, batch_size, z_dim, learning_rate, beta1, get_batches, data_shape, data_image_mode): \"\"\" Train the GAN :param epoch_count: Number of epochs :param batch_size: Batch Size :param z_dim: Z dimension :param learning_rate: Learning Rate :param beta1: The exponential decay rate for the 1st moment in the optimizer :param get_batches: Function to get batches :param data_shape: Shape of the data :param data_image_mode: The image mode to use for images (\"RGB\" or \"L\") \"\"\" # TODO: Build Model n_samples, width, height, channels = data_shape input_real, input_z, learn_rate = model_inputs(width, height, channels, z_dim) d_loss, g_loss = model_loss(input_real, input_z, channels) d_opt, g_opt = model_opt(d_loss, g_loss, learn_rate, beta1) steps = 0 show_every = 50 print_every = 25 with tf . Session() as sess: sess . run(tf . global_variables_initializer()) for epoch_i in range (epoch_count): for batch_images in get_batches(batch_size): batch_images *= 2 # TODO: Train Model steps += 1 # Sample random noise for G batch_z = np . random . uniform( - 1 , 1 , size = (batch_size, z_dim)) # Run optimizers _ = sess . run(d_opt, feed_dict = {input_real: batch_images,\\ input_z: batch_z,\\ learn_rate: learning_rate}) _ = sess . run(g_opt, feed_dict = {input_real: batch_images,\\ input_z: batch_z,\\ learn_rate: learning_rate}) # show_generator_output to show generator output if steps % show_every == 0 : n_images = 16 show_generator_output(sess, n_images, input_z, channels, data_image_mode) if steps % print_every == 0 : train_loss_d = d_loss . eval({input_z: batch_z, input_real: batch_images}) train_loss_g = g_loss . eval({input_z: batch_z}) print ( \"Epoch {}/{}...\" . format(epoch_i, epoch_count), \"Discriminator Loss: {:.4f}...\" . format(train_loss_d), \"Generator Loss: {:.4f}\" . format(train_loss_g))","title":"Train"},{"location":"dl/gan/dlnd_face_generation/#mnist_1","text":"Test your GANs architecture on MNIST. After 2 epochs, the GANs should be able to generate images that look like handwritten digits. Make sure the loss of the generator is lower than the loss of the discriminator or close to 0. batch_size = 64 z_dim = 100 learning_rate = 0.0005 beta1 = 0.5 \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" epochs = 2 mnist_dataset = helper . Dataset( 'mnist' , glob(os . path . join(data_dir, 'mnist/*.jpg' ))) with tf . Graph() . as_default(): train(epochs, batch_size, z_dim, learning_rate, beta1, mnist_dataset . get_batches, mnist_dataset . shape, mnist_dataset . image_mode) Epoch 0/2... Discriminator Loss: 0.1132... Generator Loss: 3.8164 Epoch 0/2... Discriminator Loss: 0.4083... Generator Loss: 1.6668 Epoch 0/2... Discriminator Loss: 0.2134... Generator Loss: 2.3080 Epoch 0/2... Discriminator Loss: 0.8935... Generator Loss: 0.7820 Epoch 0/2... Discriminator Loss: 0.9974... Generator Loss: 0.9753 Epoch 0/2... Discriminator Loss: 1.4598... Generator Loss: 0.4637 Epoch 0/2... Discriminator Loss: 1.0837... Generator Loss: 0.9862 Epoch 0/2... Discriminator Loss: 1.0771... Generator Loss: 1.2634 Epoch 0/2... Discriminator Loss: 1.5585... Generator Loss: 0.3205 Epoch 0/2... Discriminator Loss: 1.0825... Generator Loss: 0.6745 Epoch 0/2... Discriminator Loss: 0.9129... Generator Loss: 1.0191 Epoch 0/2... Discriminator Loss: 0.9434... Generator Loss: 0.8767 Epoch 0/2... Discriminator Loss: 0.9463... Generator Loss: 0.9866 Epoch 0/2... Discriminator Loss: 1.1447... Generator Loss: 0.6476 Epoch 0/2... Discriminator Loss: 1.3758... Generator Loss: 0.4319 Epoch 0/2... Discriminator Loss: 0.9714... Generator Loss: 0.6623 Epoch 0/2... Discriminator Loss: 1.5291... Generator Loss: 0.3816 Epoch 0/2... Discriminator Loss: 1.3693... Generator Loss: 2.0586 Epoch 0/2... Discriminator Loss: 1.7561... Generator Loss: 0.2967 Epoch 0/2... Discriminator Loss: 1.5004... Generator Loss: 0.3576 Epoch 0/2... Discriminator Loss: 0.7120... Generator Loss: 1.2979 Epoch 0/2... Discriminator Loss: 0.8413... Generator Loss: 1.0782 Epoch 0/2... Discriminator Loss: 1.1858... Generator Loss: 0.5920 Epoch 0/2... Discriminator Loss: 1.0060... Generator Loss: 0.6120 Epoch 0/2... Discriminator Loss: 1.2757... Generator Loss: 0.4474 Epoch 0/2... Discriminator Loss: 1.5774... Generator Loss: 0.3005 Epoch 0/2... Discriminator Loss: 1.2721... Generator Loss: 0.4242 Epoch 0/2... Discriminator Loss: 1.0972... Generator Loss: 0.5191 Epoch 0/2... Discriminator Loss: 0.8444... Generator Loss: 1.2999 Epoch 0/2... Discriminator Loss: 0.8549... Generator Loss: 0.9284 Epoch 0/2... Discriminator Loss: 0.8498... Generator Loss: 0.9417 Epoch 0/2... Discriminator Loss: 0.6521... Generator Loss: 1.2973 Epoch 0/2... Discriminator Loss: 0.9847... Generator Loss: 0.6543 Epoch 0/2... Discriminator Loss: 1.2542... Generator Loss: 0.4832 Epoch 0/2... Discriminator Loss: 0.9244... Generator Loss: 1.4905 Epoch 0/2... Discriminator Loss: 1.0157... Generator Loss: 0.5582 Epoch 0/2... Discriminator Loss: 0.5705... Generator Loss: 1.7848 Epoch 1/2... Discriminator Loss: 1.0804... Generator Loss: 0.7368 Epoch 1/2... Discriminator Loss: 1.3714... Generator Loss: 2.7104 Epoch 1/2... Discriminator Loss: 0.7393... Generator Loss: 1.0792 Epoch 1/2... Discriminator Loss: 0.8901... Generator Loss: 0.9193 Epoch 1/2... Discriminator Loss: 1.3041... Generator Loss: 0.4482 Epoch 1/2... Discriminator Loss: 1.0246... Generator Loss: 1.6754 Epoch 1/2... Discriminator Loss: 1.4366... Generator Loss: 0.4005 Epoch 1/2... Discriminator Loss: 0.7496... Generator Loss: 0.9064 Epoch 1/2... Discriminator Loss: 1.0066... Generator Loss: 0.6571 Epoch 1/2... Discriminator Loss: 0.6344... Generator Loss: 1.0048 Epoch 1/2... Discriminator Loss: 0.9111... Generator Loss: 1.4843 Epoch 1/2... Discriminator Loss: 0.8954... Generator Loss: 1.0413 Epoch 1/2... Discriminator Loss: 2.1049... Generator Loss: 0.1755 Epoch 1/2... Discriminator Loss: 1.0708... Generator Loss: 0.5250 Epoch 1/2... Discriminator Loss: 1.4498... Generator Loss: 0.3438 Epoch 1/2... Discriminator Loss: 0.8951... Generator Loss: 0.7148 Epoch 1/2... Discriminator Loss: 0.9677... Generator Loss: 0.6380 Epoch 1/2... Discriminator Loss: 2.0528... Generator Loss: 0.1594 Epoch 1/2... Discriminator Loss: 0.5354... Generator Loss: 1.1890 Epoch 1/2... Discriminator Loss: 1.5721... Generator Loss: 0.5104 Epoch 1/2... Discriminator Loss: 0.7788... Generator Loss: 0.9999 Epoch 1/2... Discriminator Loss: 0.8880... Generator Loss: 0.6436 Epoch 1/2... Discriminator Loss: 0.6986... Generator Loss: 1.4282 Epoch 1/2... Discriminator Loss: 1.2309... Generator Loss: 0.4484 Epoch 1/2... Discriminator Loss: 1.0756... Generator Loss: 0.6155 Epoch 1/2... Discriminator Loss: 3.1573... Generator Loss: 3.8849 Epoch 1/2... Discriminator Loss: 0.8739... Generator Loss: 0.6847 Epoch 1/2... Discriminator Loss: 1.7689... Generator Loss: 0.2244 Epoch 1/2... Discriminator Loss: 0.8876... Generator Loss: 0.7815 Epoch 1/2... Discriminator Loss: 1.1661... Generator Loss: 0.4628 Epoch 1/2... Discriminator Loss: 1.5133... Generator Loss: 0.3179 Epoch 1/2... Discriminator Loss: 0.6079... Generator Loss: 1.3311 Epoch 1/2... Discriminator Loss: 0.7159... Generator Loss: 1.5858 Epoch 1/2... Discriminator Loss: 0.7321... Generator Loss: 0.8564 Epoch 1/2... Discriminator Loss: 1.2747... Generator Loss: 0.4073 Epoch 1/2... Discriminator Loss: 1.3412... Generator Loss: 0.3678 Epoch 1/2... Discriminator Loss: 0.8914... Generator Loss: 0.7424","title":"MNIST"},{"location":"dl/gan/dlnd_face_generation/#celeba_1","text":"Run your GANs on CelebA. It will take around 20 minutes on the average GPU to run one epoch. You can run the whole epoch or stop when it starts to generate realistic faces. batch_size = 64 z_dim = 100 learning_rate = 0.0005 beta1 = 0.5 \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" epochs = 1 celeba_dataset = helper . Dataset( 'celeba' , glob(os . path . join(data_dir, 'img_align_celeba/*.jpg' ))) with tf . Graph() . as_default(): train(epochs, batch_size, z_dim, learning_rate, beta1, celeba_dataset . get_batches, celeba_dataset . shape, celeba_dataset . image_mode) Epoch 0/1... Discriminator Loss: 0.6341... Generator Loss: 1.5425 Epoch 0/1... Discriminator Loss: 1.0661... Generator Loss: 1.4051 Epoch 0/1... Discriminator Loss: 1.4774... Generator Loss: 0.4814 Epoch 0/1... Discriminator Loss: 0.5296... Generator Loss: 2.0486 Epoch 0/1... Discriminator Loss: 0.4144... Generator Loss: 1.7049 Epoch 0/1... Discriminator Loss: 0.8953... Generator Loss: 1.3714 Epoch 0/1... Discriminator Loss: 1.2676... Generator Loss: 2.1938 Epoch 0/1... Discriminator Loss: 0.6703... Generator Loss: 1.9268 Epoch 0/1... Discriminator Loss: 1.3622... Generator Loss: 0.9100 Epoch 0/1... Discriminator Loss: 0.8473... Generator Loss: 1.2377 Epoch 0/1... Discriminator Loss: 0.7923... Generator Loss: 1.3153 Epoch 0/1... Discriminator Loss: 2.3745... Generator Loss: 3.9981 Epoch 0/1... Discriminator Loss: 1.3498... Generator Loss: 0.9091 Epoch 0/1... Discriminator Loss: 0.9757... Generator Loss: 1.3260 Epoch 0/1... Discriminator Loss: 0.9970... Generator Loss: 0.8662 Epoch 0/1... Discriminator Loss: 1.2872... Generator Loss: 0.7643 Epoch 0/1... Discriminator Loss: 0.9423... Generator Loss: 1.0006 Epoch 0/1... Discriminator Loss: 0.6227... Generator Loss: 1.2088 Epoch 0/1... Discriminator Loss: 0.5900... Generator Loss: 1.5671 Epoch 0/1... Discriminator Loss: 1.0243... Generator Loss: 0.9068 Epoch 0/1... Discriminator Loss: 1.5452... Generator Loss: 0.3467 Epoch 0/1... Discriminator Loss: 0.9187... Generator Loss: 1.3025 Epoch 0/1... Discriminator Loss: 0.9896... Generator Loss: 0.9055 Epoch 0/1... Discriminator Loss: 0.9112... Generator Loss: 1.2679 Epoch 0/1... Discriminator Loss: 1.0865... Generator Loss: 1.3766 Epoch 0/1... Discriminator Loss: 1.0086... Generator Loss: 0.7582 Epoch 0/1... Discriminator Loss: 0.8523... Generator Loss: 1.3466 Epoch 0/1... Discriminator Loss: 0.7004... Generator Loss: 1.6204 Epoch 0/1... Discriminator Loss: 1.1147... Generator Loss: 1.0215 Epoch 0/1... Discriminator Loss: 0.9817... Generator Loss: 0.9707 Epoch 0/1... Discriminator Loss: 0.8524... Generator Loss: 0.7910 Epoch 0/1... Discriminator Loss: 1.3500... Generator Loss: 0.6876 Epoch 0/1... Discriminator Loss: 1.1861... Generator Loss: 0.7478 Epoch 0/1... Discriminator Loss: 1.4163... Generator Loss: 0.4300 Epoch 0/1... Discriminator Loss: 1.3214... Generator Loss: 0.6985 Epoch 0/1... Discriminator Loss: 1.6581... Generator Loss: 0.3179 Epoch 0/1... Discriminator Loss: 0.8624... Generator Loss: 2.0154 Epoch 0/1... Discriminator Loss: 0.7325... Generator Loss: 1.5246 Epoch 0/1... Discriminator Loss: 1.3200... Generator Loss: 0.4783 Epoch 0/1... Discriminator Loss: 0.9317... Generator Loss: 0.8402 Epoch 0/1... Discriminator Loss: 1.2700... Generator Loss: 1.6259 Epoch 0/1... Discriminator Loss: 1.6959... Generator Loss: 1.6403 Epoch 0/1... Discriminator Loss: 0.9565... Generator Loss: 1.8874 Epoch 0/1... Discriminator Loss: 0.7762... Generator Loss: 1.6332 Epoch 0/1... Discriminator Loss: 0.9902... Generator Loss: 0.6471 Epoch 0/1... Discriminator Loss: 1.0775... Generator Loss: 0.6818 Epoch 0/1... Discriminator Loss: 1.0928... Generator Loss: 0.9076 Epoch 0/1... Discriminator Loss: 1.2636... Generator Loss: 0.8888 Epoch 0/1... Discriminator Loss: 1.9158... Generator Loss: 0.1960 Epoch 0/1... Discriminator Loss: 1.0883... Generator Loss: 0.6902 Epoch 0/1... Discriminator Loss: 1.0772... Generator Loss: 0.6896 Epoch 0/1... Discriminator Loss: 1.1797... Generator Loss: 0.5554 Epoch 0/1... Discriminator Loss: 1.1049... Generator Loss: 0.9699 Epoch 0/1... Discriminator Loss: 1.2788... Generator Loss: 2.0408 Epoch 0/1... Discriminator Loss: 0.7399... Generator Loss: 1.1408 Epoch 0/1... Discriminator Loss: 1.3329... Generator Loss: 0.4466 Epoch 0/1... Discriminator Loss: 0.9657... Generator Loss: 0.9572 Epoch 0/1... Discriminator Loss: 1.0361... Generator Loss: 0.7271 Epoch 0/1... Discriminator Loss: 1.0897... Generator Loss: 0.8504 Epoch 0/1... Discriminator Loss: 1.2010... Generator Loss: 1.1790 Epoch 0/1... Discriminator Loss: 1.6883... Generator Loss: 1.1168 Epoch 0/1... Discriminator Loss: 1.0092... Generator Loss: 0.6636 Epoch 0/1... Discriminator Loss: 1.3417... Generator Loss: 0.5133 Epoch 0/1... Discriminator Loss: 0.9791... Generator Loss: 0.9111 Epoch 0/1... Discriminator Loss: 0.8803... Generator Loss: 1.1090 Epoch 0/1... Discriminator Loss: 0.8513... Generator Loss: 1.2366 Epoch 0/1... Discriminator Loss: 0.7197... Generator Loss: 1.5789 Epoch 0/1... Discriminator Loss: 0.9334... Generator Loss: 0.7907 Epoch 0/1... Discriminator Loss: 1.1299... Generator Loss: 0.5871 Epoch 0/1... Discriminator Loss: 0.9805... Generator Loss: 1.1100 Epoch 0/1... Discriminator Loss: 1.1180... Generator Loss: 2.0649 Epoch 0/1... Discriminator Loss: 0.9413... Generator Loss: 0.6843 Epoch 0/1... Discriminator Loss: 1.1723... Generator Loss: 1.0662 Epoch 0/1... Discriminator Loss: 1.3740... Generator Loss: 0.4719 Epoch 0/1... Discriminator Loss: 1.2226... Generator Loss: 0.7428 Epoch 0/1... Discriminator Loss: 1.0916... Generator Loss: 1.5043 Epoch 0/1... Discriminator Loss: 1.2517... Generator Loss: 2.0049 Epoch 0/1... Discriminator Loss: 1.1599... Generator Loss: 0.7082 Epoch 0/1... Discriminator Loss: 1.0402... Generator Loss: 0.8053 Epoch 0/1... Discriminator Loss: 1.0036... Generator Loss: 0.9750 Epoch 0/1... Discriminator Loss: 0.9274... Generator Loss: 0.9230 Epoch 0/1... Discriminator Loss: 1.1304... Generator Loss: 1.1508 Epoch 0/1... Discriminator Loss: 0.7811... Generator Loss: 1.4115 Epoch 0/1... Discriminator Loss: 1.0645... Generator Loss: 0.8931 Epoch 0/1... Discriminator Loss: 1.4299... Generator Loss: 2.5213 Epoch 0/1... Discriminator Loss: 1.0405... Generator Loss: 0.6912 Epoch 0/1... Discriminator Loss: 1.1746... Generator Loss: 0.6077 Epoch 0/1... Discriminator Loss: 0.9569... Generator Loss: 2.3023 Epoch 0/1... Discriminator Loss: 0.9257... Generator Loss: 0.9512 Epoch 0/1... Discriminator Loss: 0.9541... Generator Loss: 0.7513 Epoch 0/1... Discriminator Loss: 0.9034... Generator Loss: 0.9518 Epoch 0/1... Discriminator Loss: 1.1156... Generator Loss: 0.8511 Epoch 0/1... Discriminator Loss: 0.9349... Generator Loss: 1.1430 Epoch 0/1... Discriminator Loss: 1.0691... Generator Loss: 0.6279 Epoch 0/1... Discriminator Loss: 1.1409... Generator Loss: 0.9605 Epoch 0/1... Discriminator Loss: 1.4045... Generator Loss: 0.3858 Epoch 0/1... Discriminator Loss: 1.0732... Generator Loss: 0.7521 Epoch 0/1... Discriminator Loss: 1.0664... Generator Loss: 0.9785 Epoch 0/1... Discriminator Loss: 1.1484... Generator Loss: 1.4160 Epoch 0/1... Discriminator Loss: 1.2619... Generator Loss: 0.5863 Epoch 0/1... Discriminator Loss: 0.9650... Generator Loss: 0.9150 Epoch 0/1... Discriminator Loss: 0.9211... Generator Loss: 1.4288 Epoch 0/1... Discriminator Loss: 0.9854... Generator Loss: 0.7512 Epoch 0/1... Discriminator Loss: 0.8539... Generator Loss: 0.9985 Epoch 0/1... Discriminator Loss: 0.9036... Generator Loss: 1.0608 Epoch 0/1... Discriminator Loss: 1.0430... Generator Loss: 0.7103 Epoch 0/1... Discriminator Loss: 0.8404... Generator Loss: 0.9762 Epoch 0/1... Discriminator Loss: 1.2528... Generator Loss: 0.5786 Epoch 0/1... Discriminator Loss: 1.2818... Generator Loss: 0.4568 Epoch 0/1... Discriminator Loss: 1.0006... Generator Loss: 0.8900 Epoch 0/1... Discriminator Loss: 1.3815... Generator Loss: 0.3886 Epoch 0/1... Discriminator Loss: 1.2679... Generator Loss: 0.5873 Epoch 0/1... Discriminator Loss: 0.9585... Generator Loss: 0.9575 Epoch 0/1... Discriminator Loss: 1.2240... Generator Loss: 0.7966 Epoch 0/1... Discriminator Loss: 0.8579... Generator Loss: 1.4551 Epoch 0/1... Discriminator Loss: 1.3028... Generator Loss: 0.5394 Epoch 0/1... Discriminator Loss: 1.1112... Generator Loss: 0.7762 Epoch 0/1... Discriminator Loss: 1.1230... Generator Loss: 0.9227 Epoch 0/1... Discriminator Loss: 1.2362... Generator Loss: 0.5178 Epoch 0/1... Discriminator Loss: 1.0913... Generator Loss: 0.7929 Epoch 0/1... Discriminator Loss: 0.8596... Generator Loss: 0.9705 Epoch 0/1... Discriminator Loss: 1.1329... Generator Loss: 0.6999 Epoch 0/1... Discriminator Loss: 1.0311... Generator Loss: 0.8163 Epoch 0/1... Discriminator Loss: 0.8289... Generator Loss: 1.1358 Epoch 0/1... Discriminator Loss: 1.2079... Generator Loss: 0.5730 Epoch 0/1... Discriminator Loss: 0.9879... Generator Loss: 0.7466","title":"CelebA"},{"location":"dl/gan/dlnd_face_generation/#submitting-this-project","text":"When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_face_generation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission.","title":"Submitting This Project"},{"location":"dl/rnn/RNN_project/","text":"Artificial Intelligence Nanodegree Recurrent Neural Network Projects Welcome to the Recurrent Neural Network Project in the Artificial Intelligence Nanodegree! In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with 'Implementation' in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a 'Question X' header. Carefully read each question and provide thorough answers in the following text boxes that begin with 'Answer:' . Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide. Note: Code and Markdown cells can be executed using the Shift + Enter keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode. Implementation TODOs in this notebook This notebook contains two problems, cut into a variety of TODOs. Make sure to complete each section containing a TODO marker throughout the notebook. For convenience we provide links to each of these sections below. TODO #1: Implement a function to window time series TODO #2: Create a simple RNN model using keras to perform regression TODO #3: Finish cleaning a large text corpus TODO #4: Implement a function to window a large text corpus TODO #5: Create a simple RNN model using keras to perform multiclass classification TODO #6: Generate text using a fully trained RNN model and a variety of input sequences Problem 1: Perform time series prediction In this project you will perform time series prediction using a Recurrent Neural Network regressor. In particular you will re-create the figure shown in the notes - where the stock price of Apple was forecasted (or predicted) 7 days in advance. In completing this exercise you will learn how to construct RNNs using Keras, which will also aid in completing the second project in this notebook. The particular network architecture we will employ for our RNN is known as Long Term Short Memory (LSTM) , which helps significantly avoid technical problems with optimization of RNNs. 1.1 Getting started First we must load in our time series - a history of around 140 days of Apple's stock price. Then we need to perform a number of pre-processing steps to prepare it for use with an RNN model. First off, it is good practice to normalize time series - by normalizing its range. This helps us avoid serious numerical issues associated how common activation functions (like tanh) transform very large (positive or negative) numbers, as well as helping us to avoid related issues when computing derivatives. Here we normalize the series to lie in the range [0,1] using this scikit function , but it is also commonplace to normalize by a series standard deviation. import seaborn as sns import pandas as pd ### Load in necessary libraries for data input and normalization % matplotlib inline import numpy as np import matplotlib.pyplot as plt % load_ext autoreload % autoreload 2 from my_answers import * % load_ext autoreload % autoreload 2 from my_answers import * ### load in and normalize the dataset dataset = np . loadtxt( 'datasets/normalized_apple_prices.csv' ) The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Lets take a quick look at the (normalized) time series we'll be performing predictions on. # lets take a look at our time series #pd.DataFrame(dataset).plot() plt . plot(dataset) plt . xlabel( 'time period' ) plt . ylabel( 'normalized series value' ) Text(0,0.5,u'normalized series value') 1.2 Cutting our time series into sequences Remember, our time series is a sequence of numbers that we can represent in general mathematically as $$s_{0},s_{1},s_{2},...,s_{P}$$ where $s_{p}$ is the numerical value of the time series at time period $p$ and where $P$ is the total length of the series. In order to apply our RNN we treat the time series prediction problem as a regression problem, and so need to use a sliding window to construct a set of associated input/output pairs to regress on. This process is animated in the gif below. For example - using a window of size T = 5 (as illustrated in the gif above) we produce a set of input/output pairs like the one shown in the table below $$\\begin{array}{c|c} \\text{Input} & \\text{Output}\\ \\hline \\color{CornflowerBlue} {\\langle s_{1},s_{2},s_{3},s_{4},s_{5}\\rangle} & \\color{Goldenrod}{ s_{6}} \\ \\ \\color{CornflowerBlue} {\\langle s_{2},s_{3},s_{4},s_{5},s_{6} \\rangle } & \\color{Goldenrod} {s_{7} } \\ \\color{CornflowerBlue} {\\vdots} & \\color{Goldenrod} {\\vdots}\\ \\color{CornflowerBlue} { \\langle s_{P-5},s_{P-4},s_{P-3},s_{P-2},s_{P-1} \\rangle } & \\color{Goldenrod} {s_{P}} \\end{array}$$ Notice here that each input is a sequence (or vector) of length 5 (and in general has length equal to the window size T) while each corresponding output is a scalar value. Notice also how given a time series of length P and window size T = 5 as shown above, we created P - 5 input/output pairs. More generally, for a window size T we create P - T such pairs. Now its time for you to window the input time series as described above! TODO: Implement the function called window_transform_series in my_answers.py so that it runs a sliding window along the input series and creates associated input/output pairs. Note that this function should input a) the series and b) the window length, and return the input/output subsequences. Make sure to format returned input/output as generally shown in table above (where window_size = 5), and make sure your returned input is a numpy array. You can test your function on the list of odd numbers given below odd_nums = np . array([ 1 , 3 , 5 , 7 , 9 , 11 , 13 ]) Here is a hard-coded solution for odd_nums. You can compare its results with what you get from your window_transform_series implementation. # run a window of size 2 over the odd number sequence and display the results window_size = 2 X = [] X . append(odd_nums[ 0 : 2 ]) X . append(odd_nums[ 1 : 3 ]) X . append(odd_nums[ 2 : 4 ]) X . append(odd_nums[ 3 : 5 ]) X . append(odd_nums[ 4 : 6 ]) y = odd_nums[ 2 :] X = np . asarray(X) y = np . asarray(y) y = np . reshape(y, ( len (y), 1 )) #optional assert ( type (X) . __name__ == 'ndarray' ) assert ( type (y) . __name__ == 'ndarray' ) assert (X . shape == ( 5 , 2 )) assert (y . shape in [( 5 , 1 ), ( 5 ,)]) # print out input/output pairs --> here input = X, corresponding output = y print ( '--- the input X will look like ----' ) print (X) print ( '--- the associated output y will look like ----' ) print (y) --- the input X will look like ---- [[ 1 3] [ 3 5] [ 5 7] [ 7 9] [ 9 11]] --- the associated output y will look like ---- [[ 5] [ 7] [ 9] [11] [13]] Again - you can check that your completed window_transform_series function works correctly by trying it on the odd_nums sequence - you should get the above output. ### TODO: implement the function window_transform_series in the file my_answers.py from my_answers import window_transform_series With this function in place apply it to the series in the Python cell below. We use a window_size = 7 for these experiments. # window the data using your windowing function window_size = 7 X,y = window_transform_series(series = dataset, window_size = window_size) X . shape,y . shape ((131, 7), (131, 1)) 1.3 Splitting into training and testing sets In order to perform proper testing on our dataset we will lop off the last 1/3 of it for validation (or testing). This is that once we train our model we have something to test it on (like any regression problem!). This splitting into training/testing sets is done in the cell below. Note how here we are not splitting the dataset randomly as one typically would do when validating a regression model. This is because our input/output pairs are related temporally . We don't want to validate our model by training on a random subset of the series and then testing on another random subset, as this simulates the scenario that we receive new points within the timeframe of our training set . We want to train on one solid chunk of the series (in our case, the first full 2/3 of it), and validate on a later chunk (the last 1/3) as this simulates how we would predict future values of a time series. # split our dataset into training / testing sets train_test_split = int (np . ceil( 2 * len (y) / float ( 3 ))) # set the split point print (train_test_split) # partition the training set X_train = X[:train_test_split,:] y_train = y[:train_test_split] # keep the last chunk for testing X_test = X[train_test_split:,:] y_test = y[train_test_split:] # NOTE: to use keras's RNN LSTM module our input must be reshaped to [samples, window size, stepsize] X_train = np . asarray(np . reshape(X_train, (X_train . shape[ 0 ], window_size, 1 ))) X_test = np . asarray(np . reshape(X_test, (X_test . shape[ 0 ], window_size, 1 ))) 88 1.4 Build and run an RNN regression model Having created input/output pairs out of our time series and cut this into training/testing sets, we can now begin setting up our RNN. We use Keras to quickly build a two hidden layer RNN of the following specifications layer 1 uses an LSTM module with 5 hidden units (note here the input_shape = (window_size,1)) layer 2 uses a fully connected module with one unit the 'mean_squared_error' loss should be used (remember: we are performing regression here) This can be constructed using just a few lines - see e.g., the general Keras documentation and the LSTM documentation in particular for examples of how to quickly use Keras to build neural network models. Make sure you are initializing your optimizer given the keras-recommended approach for RNNs (given in the cell below). (remember to copy your completed function into the script my_answers.py function titled build_part1_RNN before submitting your project) ### TODO: create required RNN model # import keras network libraries from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM import keras # given - fix random seed - so we can all reproduce the same results on our default time series np . random . seed( 0 ) # TODO: implement build_part1_RNN in my_answers.py from my_answers import build_part1_RNN model = build_part1_RNN(window_size) # build model using keras documentation recommended optimizer initialization optimizer = keras . optimizers . RMSprop(lr = 0.001 , rho = 0.9 , epsilon = 1e-08 , decay = 0.0 ) # compile the model model . compile(loss = 'mean_squared_error' , optimizer = optimizer) With your model built you can now fit the model by activating the cell below! Note: the number of epochs (np_epochs) and batch_size are preset (so we can all produce the same results). You can choose to toggle the verbose parameter - which gives you regular updates on the progress of the algorithm - on and off by setting it to 1 or 0 respectively. # run your model! model . fit(X_train, y_train, epochs = 1000 , batch_size = 50 , verbose = 1 ) Epoch 1/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 2/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 3/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 4/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 5/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 6/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 7/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 8/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 9/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 10/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 11/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 12/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 13/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 14/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 15/1000 88/88 [==============================] - 0s - loss: 0.0166 Epoch 16/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 17/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 18/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 19/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 20/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 21/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 22/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 23/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 24/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 25/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 26/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 27/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 28/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 29/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 30/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 31/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 32/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 33/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 34/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 35/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 36/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 37/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 38/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 39/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 40/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 41/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 42/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 43/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 44/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 45/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 46/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 47/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 48/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 49/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 50/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 51/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 52/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 53/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 54/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 55/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 56/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 57/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 58/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 59/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 60/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 61/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 62/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 63/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 64/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 65/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 66/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 67/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 68/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 69/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 70/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 71/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 72/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 73/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 74/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 75/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 76/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 77/1000 88/88 [==============================] - 0s - loss: 0.0166 Epoch 78/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 79/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 80/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 81/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 82/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 83/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 84/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 85/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 86/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 87/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 88/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 89/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 90/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 91/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 92/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 93/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 94/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 95/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 96/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 97/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 98/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 99/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 100/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 101/1000 88/88 [==============================] - ETA: 0s - loss: 0.016 - 0s - loss: 0.0163 Epoch 102/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 103/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 104/1000 88/88 [==============================] - ETA: 0s - loss: 0.016 - 0s - loss: 0.0161 Epoch 105/1000 88/88 [==============================] - 0s - loss: 0.0169 Epoch 106/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 107/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 108/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 109/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 110/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 111/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 112/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 113/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 114/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 115/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 116/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 117/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 118/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 119/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 120/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 121/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 122/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 123/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 124/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 125/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 126/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 127/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 128/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 129/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 130/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 131/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 132/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 133/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 134/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 135/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 136/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 137/1000 88/88 [==============================] - ETA: 0s - loss: 0.017 - 0s - loss: 0.0161 Epoch 138/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 139/1000 88/88 [==============================] - 0s - loss: 0.0166 Epoch 140/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 141/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 142/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 143/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 144/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 145/1000 88/88 [==============================] - 0s - loss: 0.0167 Epoch 146/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 147/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 148/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 149/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 150/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 151/1000 88/88 [==============================] - ETA: 0s - loss: 0.017 - 0s - loss: 0.0161 Epoch 152/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 153/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 154/1000 88/88 [==============================] - ETA: 0s - loss: 0.020 - 0s - loss: 0.0161 Epoch 155/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 156/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 157/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 158/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 159/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 160/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 161/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 162/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 163/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 164/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 165/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 166/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 167/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 168/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 169/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 170/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 171/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 172/1000 88/88 [==============================] - ETA: 0s - loss: 0.015 - 0s - loss: 0.0161 Epoch 173/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 174/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 175/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 176/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 177/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 178/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 179/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 180/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 181/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 182/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 183/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 184/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 185/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 186/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 187/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 188/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 189/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 190/1000 88/88 [==============================] - ETA: 0s - loss: 0.017 - 0s - loss: 0.0161 Epoch 191/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 192/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 193/1000 88/88 [==============================] - 0s - loss: 0.0169 Epoch 194/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 195/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 196/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 197/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 198/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 199/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 200/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 201/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 202/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 203/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 204/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 205/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 206/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 207/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 208/1000 88/88 [==============================] - ETA: 0s - loss: 0.014 - 0s - loss: 0.0162 Epoch 209/1000 88/88 [==============================] - ETA: 0s - loss: 0.019 - 0s - loss: 0.0160 Epoch 210/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 211/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 212/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 213/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 214/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 215/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 216/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 217/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 218/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 219/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 220/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 221/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 222/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 223/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 224/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 225/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 226/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 227/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 228/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 229/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 230/1000 88/88 [==============================] - ETA: 0s - loss: 0.013 - 0s - loss: 0.0161 Epoch 231/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 232/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 233/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 234/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 235/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 236/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 237/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 238/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 239/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 240/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 241/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 242/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 243/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 244/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 245/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 246/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 247/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 248/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 249/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 250/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 251/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 252/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 253/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 254/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 255/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 256/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 257/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 258/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 259/1000 88/88 [==============================] - ETA: 0s - loss: 0.013 - 0s - loss: 0.0160 Epoch 260/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 261/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 262/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 263/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 264/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 265/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 266/1000 88/88 [==============================] - 0s - loss: 0.0167 Epoch 267/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 268/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 269/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 270/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 271/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 272/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 273/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 274/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 275/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 276/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 277/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 278/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 279/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 280/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 281/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 282/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 283/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 284/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 285/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 286/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 287/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 288/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 289/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 290/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 291/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 292/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 293/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 294/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 295/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 296/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 297/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 298/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 299/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 300/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 301/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 302/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 303/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 304/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 305/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 306/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 307/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 308/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 309/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 310/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 311/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 312/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 313/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 314/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 315/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 316/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 317/1000 88/88 [==============================] - ETA: 0s - loss: 0.012 - 0s - loss: 0.0161 Epoch 318/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 319/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 320/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 321/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 322/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 323/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 324/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 325/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 326/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 327/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 328/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 329/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 330/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 331/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 332/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 333/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 334/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 335/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 336/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 337/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 338/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 339/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 340/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 341/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 342/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 343/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 344/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 345/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 346/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 347/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 348/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 349/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 350/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 351/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 352/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 353/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 354/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 355/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 356/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 357/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 358/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 359/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 360/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 361/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 362/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 363/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 364/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 365/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 366/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 367/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 368/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 369/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 370/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 371/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 372/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 373/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 374/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 375/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 376/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 377/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 378/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 379/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 380/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 381/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 382/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 383/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 384/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 385/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 386/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 387/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 388/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 389/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 390/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 391/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 392/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 393/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 394/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 395/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 396/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 397/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 398/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 399/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 400/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 401/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 402/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 403/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 404/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 405/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 406/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 407/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 408/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 409/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 410/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 411/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 412/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 413/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 414/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 415/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 416/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 417/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 418/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 419/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 420/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 421/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 422/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 423/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 424/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 425/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 426/1000 88/88 [==============================] - ETA: 0s - loss: 0.017 - 0s - loss: 0.0159 Epoch 427/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 428/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 429/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 430/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 431/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 432/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 433/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 434/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 435/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 436/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 437/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 438/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 439/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 440/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 441/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 442/1000 88/88 [==============================] - ETA: 0s - loss: 0.013 - 0s - loss: 0.0160 Epoch 443/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 444/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 445/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 446/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 447/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 448/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 449/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 450/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 451/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 452/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 453/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 454/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 455/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 456/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 457/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 458/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 459/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 460/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 461/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 462/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 463/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 464/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 465/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 466/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 467/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 468/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 469/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 470/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 471/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 472/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 473/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 474/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 475/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 476/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 477/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 478/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 479/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 480/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 481/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 482/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 483/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 484/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 485/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 486/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 487/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 488/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 489/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 490/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 491/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 492/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 493/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 494/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 495/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 496/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 497/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 498/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 499/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 500/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 501/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 502/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 503/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 504/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 505/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 506/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 507/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 508/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 509/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 510/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 511/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 512/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 513/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 514/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 515/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 516/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 517/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 518/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 519/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 520/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 521/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 522/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 523/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 524/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 525/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 526/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 527/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 528/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 529/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 530/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 531/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 532/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 533/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 534/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 535/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 536/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 537/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 538/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 539/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 540/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 541/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 542/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 543/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 544/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 545/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 546/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 547/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 548/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 549/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 550/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 551/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 552/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 553/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 554/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 555/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 556/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 557/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 558/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 559/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 560/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 561/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 562/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 563/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 564/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 565/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 566/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 567/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 568/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 569/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 570/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 571/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 572/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 573/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 574/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 575/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 576/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 577/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 578/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 579/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 580/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 581/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 582/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 583/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 584/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 585/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 586/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 587/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 588/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 589/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 590/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 591/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 592/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 593/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 594/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 595/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 596/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 597/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 598/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 599/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 600/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 601/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 602/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 603/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 604/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 605/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 606/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 607/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 608/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 609/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 610/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 611/1000 88/88 [==============================] - ETA: 0s - loss: 0.016 - 0s - loss: 0.0156 Epoch 612/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 613/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 614/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 615/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 616/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 617/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 618/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 619/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 620/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 621/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 622/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 623/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 624/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 625/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 626/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 627/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 628/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 629/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 630/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 631/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 632/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 633/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 634/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 635/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 636/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 637/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 638/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 639/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 640/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 641/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 642/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 643/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 644/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 645/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 646/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 647/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 648/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 649/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 650/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 651/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 652/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 653/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 654/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 655/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 656/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 657/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 658/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 659/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 660/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 661/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 662/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 663/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 664/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 665/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 666/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 667/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 668/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 669/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 670/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 671/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 672/1000 88/88 [==============================] - ETA: 0s - loss: 0.014 - 0s - loss: 0.0156 Epoch 673/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 674/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 675/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 676/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 677/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 678/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 679/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 680/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 681/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 682/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 683/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 684/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 685/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 686/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 687/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 688/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 689/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 690/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 691/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 692/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 693/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 694/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 695/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 696/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 697/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 698/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 699/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 700/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 701/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 702/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 703/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 704/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 705/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 706/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 707/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 708/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 709/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 710/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 711/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 712/1000 88/88 [==============================] - ETA: 0s - loss: 0.016 - 0s - loss: 0.0161 Epoch 713/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 714/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 715/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 716/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 717/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 718/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 719/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 720/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 721/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 722/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 723/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 724/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 725/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 726/1000 88/88 [==============================] - ETA: 0s - loss: 0.013 - 0s - loss: 0.0155 Epoch 727/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 728/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 729/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 730/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 731/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 732/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 733/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 734/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 735/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 736/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 737/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 738/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 739/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 740/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 741/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 742/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 743/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 744/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 745/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 746/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 747/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 748/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 749/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 750/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 751/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 752/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 753/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 754/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 755/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 756/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 757/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 758/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 759/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 760/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 761/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 762/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 763/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 764/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 765/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 766/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 767/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 768/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 769/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 770/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 771/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 772/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 773/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 774/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 775/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 776/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 777/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 778/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 779/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 780/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 781/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 782/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 783/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 784/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 785/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 786/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 787/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 788/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 789/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 790/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 791/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 792/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 793/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 794/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 795/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 796/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 797/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 798/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 799/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 800/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 801/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 802/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 803/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 804/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 805/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 806/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 807/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 808/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 809/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 810/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 811/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 812/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 813/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 814/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 815/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 816/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 817/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 818/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 819/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 820/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 821/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 822/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 823/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 824/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 825/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 826/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 827/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 828/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 829/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 830/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 831/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 832/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 833/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 834/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 835/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 836/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 837/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 838/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 839/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 840/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 841/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 842/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 843/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 844/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 845/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 846/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 847/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 848/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 849/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 850/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 851/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 852/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 853/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 854/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 855/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 856/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 857/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 858/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 859/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 860/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 861/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 862/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 863/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 864/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 865/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 866/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 867/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 868/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 869/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 870/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 871/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 872/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 873/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 874/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 875/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 876/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 877/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 878/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 879/1000 88/88 [==============================] - ETA: 0s - loss: 0.013 - 0s - loss: 0.0153 Epoch 880/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 881/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 882/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 883/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 884/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 885/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 886/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 887/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 888/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 889/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 890/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 891/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 892/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 893/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 894/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 895/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 896/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 897/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 898/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 899/1000 88/88 [==============================] - ETA: 0s - loss: 0.013 - 0s - loss: 0.0153 Epoch 900/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 901/1000 88/88 [==============================] - ETA: 0s - loss: 0.015 - 0s - loss: 0.0153 Epoch 902/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 903/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 904/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 905/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 906/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 907/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 908/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 909/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 910/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 911/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 912/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 913/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 914/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 915/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 916/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 917/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 918/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 919/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 920/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 921/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 922/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 923/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 924/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 925/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 926/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 927/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 928/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 929/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 930/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 931/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 932/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 933/1000 88/88 [==============================] - ETA: 0s - loss: 0.012 - 0s - loss: 0.0156 Epoch 934/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 935/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 936/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 937/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 938/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 939/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 940/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 941/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 942/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 943/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 944/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 945/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 946/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 947/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 948/1000 88/88 [==============================] - ETA: 0s - loss: 0.016 - 0s - loss: 0.0153 Epoch 949/1000 88/88 [==============================] - ETA: 0s - loss: 0.018 - 0s - loss: 0.0154 Epoch 950/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 951/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 952/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 953/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 954/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 955/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 956/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 957/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 958/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 959/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 960/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 961/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 962/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 963/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 964/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 965/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 966/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 967/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 968/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 969/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 970/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 971/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 972/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 973/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 974/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 975/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 976/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 977/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 978/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 979/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 980/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 981/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 982/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 983/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 984/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 985/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 986/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 987/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 988/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 989/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 990/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 991/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 992/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 993/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 994/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 995/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 996/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 997/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 998/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 999/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 1000/1000 88/88 [==============================] - 0s - loss: 0.0154 &lt;keras.callbacks.History at 0x7f7bdc5c6090&gt; 1.5 Checking model performance With your model fit we can now make predictions on both our training and testing sets. # generate predictions for training train_predict = model . predict(X_train) test_predict = model . predict(X_test) In the next cell we compute training and testing errors using our trained model - you should be able to achieve at least training_error < 0.02 and testing_error < 0.02 with your fully trained model. If either or both of your accuracies are larger than 0.02 re-train your model - increasing the number of epochs you take (a maximum of around 1,000 should do the job) and/or adjusting your batch_size. # print out training and testing errors training_error = model . evaluate(X_train, y_train, verbose = 0 ) print ( 'training error = ' + str (training_error)) testing_error = model . evaluate(X_test, y_test, verbose = 0 ) print ( 'testing error = ' + str (testing_error)) training error = 0.0151829496026 testing error = 0.0146555935444 Activating the next cell plots the original data, as well as both predictions on the training and testing sets. ### Plot everything - the original series as well as predictions on training and testing sets import matplotlib.pyplot as plt % matplotlib inline # plot original series plt . plot(dataset,color = 'k' ) # plot training set prediction split_pt = train_test_split + window_size plt . plot(np . arange(window_size,split_pt, 1 ),train_predict,color = 'b' ) # plot testing set prediction plt . plot(np . arange(split_pt,split_pt + len (test_predict), 1 ),test_predict,color = 'r' ) # pretty up graph plt . xlabel( 'day' ) plt . ylabel( '(normalized) price of Apple stock' ) plt . legend([ 'original series' , 'training fit' , 'testing fit' ],loc = 'center left' , bbox_to_anchor = ( 1 , 0.5 )) plt . show() Note: you can try out any time series for this exercise! If you would like to try another see e.g., this site containing thousands of time series and pick another one! Problem 2: Create a sequence generator 2.1 Getting started In this project you will implement a popular Recurrent Neural Network (RNN) architecture to create an English language sequence generator capable of building semi-coherent English sentences from scratch by building them up character-by-character. This will require a substantial amount amount of parameter tuning on a large training corpus (at least 100,000 characters long). In particular for this project we will be using a complete version of Sir Arthur Conan Doyle's classic book The Adventures of Sherlock Holmes. How can we train a machine learning model to generate text automatically, character-by-character? By showing the model many training examples so it can learn a pattern between input and output. With this type of text generation each input is a string of valid characters like this one dogs are grea while the corresponding output is the next character in the sentence - which here is 't' (since the complete sentence is 'dogs are great'). We need to show a model many such examples in order for it to make reasonable predictions. Fun note: For those interested in how text generation is being used check out some of the following fun resources: Generate wacky sentences with this academic RNN text generator Various twitter bots that tweet automatically generated text like this one . the NanoGenMo annual contest to automatically produce a 50,000+ novel automatically Robot Shakespeare a text generator that automatically produces Shakespear-esk sentences 2.2 Preprocessing a text dataset Our first task is to get a large text corpus for use in training, and on it we perform a several light pre-processing tasks. The default corpus we will use is the classic book Sherlock Holmes, but you can use a variety of others as well - so long as they are fairly large (around 100,000 characters or more). # read in the text, transforming everything to lower case text = open ( 'datasets/holmes.txt' ) . read() . lower() print ( 'our original text has ' + str ( len (text)) + ' characters' ) our original text has 594933 characters Next, lets examine a bit of the raw text. Because we are interested in creating sentences of English words automatically by building up each word character-by-character, we only want to train on valid English words. In other words - we need to remove all of the other characters that are not part of English words. ### print out the first 1000 characters of the raw text to get a sense of what we need to throw out text[: 2000 ] \"\\xef\\xbb\\xbfproject gutenberg's the adventures of sherlock holmes, by arthur conan doyle\\r\\n\\r\\nthis ebook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever. you may copy it, give it away or\\r\\nre-use it under the terms of the project gutenberg license included\\r\\nwith this ebook or online at www.gutenberg.net\\r\\n\\r\\n\\r\\ntitle: the adventures of sherlock holmes\\r\\n\\r\\nauthor: arthur conan doyle\\r\\n\\r\\nposting date: april 18, 2011 [ebook #1661]\\r\\nfirst posted: november 29, 2002\\r\\n\\r\\nlanguage: english\\r\\n\\r\\n\\r\\n*** start of this project gutenberg ebook the adventures of sherlock holmes ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nproduced by an anonymous project gutenberg volunteer and jose menendez\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nthe adventures of sherlock holmes\\r\\n\\r\\nby\\r\\n\\r\\nsir arthur conan doyle\\r\\n\\r\\n\\r\\n\\r\\n i. a scandal in bohemia\\r\\n ii. the red-headed league\\r\\n iii. a case of identity\\r\\n iv. the boscombe valley mystery\\r\\n v. the five orange pips\\r\\n vi. the man with the twisted lip\\r\\n vii. the adventure of the blue carbuncle\\r\\nviii. the adventure of the speckled band\\r\\n ix. the adventure of the engineer's thumb\\r\\n x. the adventure of the noble bachelor\\r\\n xi. the adventure of the beryl coronet\\r\\n xii. the adventure of the copper beeches\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nadventure i. a scandal in bohemia\\r\\n\\r\\ni.\\r\\n\\r\\nto sherlock holmes she is always the woman. i have seldom heard\\r\\nhim mention her under any other name. in his eyes she eclipses\\r\\nand predominates the whole of her sex. it was not that he felt\\r\\nany emotion akin to love for irene adler. all emotions, and that\\r\\none particularly, were abhorrent to his cold, precise but\\r\\nadmirably balanced mind. he was, i take it, the most perfect\\r\\nreasoning and observing machine that the world has seen, but as a\\r\\nlover he would have placed himself in a false position. he never\\r\\nspoke of the softer passions, save with a gibe and a sneer. they\\r\\nwere admirable things for the observer--excellent for drawing the\\r\\nveil from men's motives and actions. but for the trained reasoner\\r\\nto admit such intrusions int\" Wow - there's a lot of junk here (i.e., weird uncommon character combinations - as this first character chunk contains the title and author page, as well as table of contents)! To keep things simple, we want to train our RNN on a large chunk of more typical English sentences - we don't want it to start thinking non-english words or strange characters are valid! - so lets clean up the data a bit. First, since the dataset is so large and the first few hundred characters contain a lot of junk, lets cut it out. Lets also find-and-replace those newline tags with empty spaces. ### find and replace '\\n' and '\\r' symbols - replacing them text = text[ 1302 :] text = text . replace( ' \\n ' , ' ' ) # replacing '\\n' with '' simply removes the sequence text = text . replace( ' \\r ' , ' ' ) Lets see how the first 1000 characters of our text looks now! ### print out the first 1000 characters of the raw text to get a sense of what we need to throw out text[: 1000 ] \" i have seldom heard him mention her under any other name. in his eyes she eclipses and predominates the whole of her sex. it was not that he felt any emotion akin to love for irene adler. all emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. he was, i take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position. he never spoke of the softer passions, save with a gibe and a sneer. they were admirable things for the observer--excellent for drawing the veil from men's motives and actions. but for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results. grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as hi\" TODO: finish cleaning the text Lets make sure we haven't left any other atypical characters (commas, periods, etc., are ok) lurking around in the depths of the text. You can do this by enumerating all the text's unique characters, examining them, and then replacing any unwanted characters with empty spaces! Once we find all of the text's unique characters, we can remove all of the atypical ones in the next cell. Note: don't remove the punctuation marks given in my_answers.py. ### TODO: implement cleaned_text in my_answers.py from my_answers import cleaned_text text = cleaned_text(text) # shorten any extra dead space created above text = text . replace( ' ' , ' ' ) With your chosen characters removed print out the first few hundred lines again just to double check that everything looks good. ### print out the first 2000 characters of the raw text to get a sense of what we need to throw out text[: 2000 ] ' i have seldom heard him mention her under any other name. in his eyes she eclipses and predominates the whole of her sex. it was not that he felt any emotion akin to love for irene adler. all emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. he was, i take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position. he never spoke of the softer passions, save with a gibe and a sneer. they were admirable things for the observerexcellent for drawing the veil from mens motives and actions. but for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results. grit in a sensitive instrument, or a crack in one of his own highpower lenses, would not be more disturbing than a strong emotion in a nature such as his. and yet there was but one woman to him, and that woman was the late irene adler, of dubious and questionable memory. i had seen little of holmes lately. my marriage had drifted us away from each other. my own complete happiness, and the homecentred interests which rise up around the man who first finds himself master of his own establishment, were sufficient to absorb all my attention, while holmes, who loathed every form of society with his whole bohemian soul, remained in our lodgings in baker street, buried among his old books, and alternating from week to week between cocaine and ambition, the drowsiness of the drug, and the fierce energy of his own keen nature. he was still, as ever, deeply attracted by the study of crime, and occupied his immense faculties and extraordinary powers of observation in following out those clues, and clearing up those mysteries which had been abandoned as hopeless by the official police. from time to time i heard some vague account of his doings: of his summons to od' Now that we have thrown out a good number of non-English characters/character sequences lets print out some statistics about the dataset - including number of total characters and number of unique characters. # count the number of unique characters in the text chars = sorted ( list ( set (text))) # print some of the text, as well as statistics print ( \"this corpus has \" + str ( len (text)) + \" total number of characters\" ) print ( \"this corpus has \" + str ( len (chars)) + \" unique characters\" ) this corpus has 571875 total number of characters this corpus has 33 unique characters 2.3 Cutting data into input/output pairs Now that we have our text all cleaned up, how can we use it to train a model to generate sentences automatically? First we need to train a machine learning model - and in order to do that we need a set of input/output pairs for a model to train on. How can we create a set of input/output pairs from our text to train on? Remember in part 1 of this notebook how we used a sliding window to extract input/output pairs from a time series? We do the same thing here! We slide a window of length $T$ along our giant text corpus - everything in the window becomes one input while the character following becomes its corresponding output. This process of extracting input/output pairs is illustrated in the gif below on a small example text using a window size of T = 5. Notice one aspect of the sliding window in this gif that does not mirror the analogous gif for time series shown in part 1 of the notebook - we do not need to slide the window along one character at a time but can move by a fixed step size $M$ greater than 1 (in the gif indeed $M = 1$). This is done with large input texts (like ours which has over 500,000 characters!) when sliding the window along one character at a time we would create far too many input/output pairs to be able to reasonably compute with. More formally lets denote our text corpus - which is one long string of characters - as follows $$s_{0},s_{1},s_{2},...,s_{P}$$ where $P$ is the length of the text (again for our text $P \\approx 500,000!$). Sliding a window of size T = 5 with a step length of M = 1 (these are the parameters shown in the gif above) over this sequence produces the following list of input/output pairs $$\\begin{array}{c|c} \\text{Input} & \\text{Output}\\ \\hline \\color{CornflowerBlue} {\\langle s_{1},s_{2},s_{3},s_{4},s_{5}\\rangle} & \\color{Goldenrod}{ s_{6}} \\ \\ \\color{CornflowerBlue} {\\langle s_{2},s_{3},s_{4},s_{5},s_{6} \\rangle } & \\color{Goldenrod} {s_{7} } \\ \\color{CornflowerBlue} {\\vdots} & \\color{Goldenrod} {\\vdots}\\ \\color{CornflowerBlue} { \\langle s_{P-5},s_{P-4},s_{P-3},s_{P-2},s_{P-1} \\rangle } & \\color{Goldenrod} {s_{P}} \\end{array}$$ Notice here that each input is a sequence (or vector) of 5 characters (and in general has length equal to the window size T) while each corresponding output is a single character. We created around P total number of input/output pairs (for general step size M we create around ceil(P/M) pairs). Now its time for you to window the input time series as described above! TODO: Create a function that runs a sliding window along the input text and creates associated input/output pairs. A skeleton function has been provided for you. Note that this function should input a) the text b) the window size and c) the step size, and return the input/output sequences. Note: the return items should be lists - not numpy arrays. (remember to copy your completed function into the script my_answers.py function titled window_transform_text before submitting your project) ### TODO: implement window_transform_series in my_answers.py from my_answers import window_transform_series With our function complete we can now use it to produce input/output pairs! We employ the function in the next cell, where the window_size = 50 and step_size = 5. # run your text window-ing function window_size = 100 step_size = 5 inputs, outputs = window_transform_text(text,window_size,step_size) Lets print out a few input/output pairs to verify that we have made the right sort of stuff! # print out a few of the input/output pairs to verify that we've made the right kind of stuff to learn from print ( 'input = ' + inputs[ 2 ]) print ( 'output = ' + outputs[ 2 ]) print ( '--------------' ) print ( 'input = ' + inputs[ 100 ]) print ( 'output = ' + outputs[ 100 ]) input = ldom heard him mention her under any other name. in his eyes she eclipses and predominates the whole output = -------------- input = h a gibe and a sneer. they were admirable things for the observerexcellent for drawing the veil from output = Looks good! 2.4 Wait, what kind of problem is text generation again? In part 1 of this notebook we used the same pre-processing technique - the sliding window - to produce a set of training input/output pairs to tackle the problem of time series prediction by treating the problem as one of regression . So what sort of problem do we have here now, with text generation? Well, the time series prediction was a regression problem because the output (one value of the time series) was a continuous value. Here - for character-by-character text generation - each output is a single character . This isn't a continuous value - but a distinct class - therefore character-by-character text generation is a classification problem . How many classes are there in the data? Well, the number of classes is equal to the number of unique characters we have to predict! How many of those were there in our dataset again? Lets print out the value again. # print out the number of unique characters in the dataset chars = sorted ( list ( set (text))) print ( \"this corpus has \" + str ( len (chars)) + \" unique characters\" ) print ( 'and these characters are ' ) print (chars) this corpus has 33 unique characters and these characters are [' ', '!', ',', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] Rockin' - so we have a multiclass classification problem on our hands! 2.5 One-hot encoding characters The last issue we have to deal with is representing our text data as numerical data so that we can use it as an input to a neural network. One of the conceptually simplest ways of doing this is via a 'one-hot encoding' scheme. Here's how it works. We transform each character in our inputs/outputs into a vector with length equal to the number of unique characters in our text. This vector is all zeros except one location where we place a 1 - and this location is unique to each character type. e.g., we transform 'a', 'b', and 'c' as follows $$a\\longleftarrow\\left[\\begin{array}{c} 1\\ 0\\ 0\\ \\vdots\\ 0\\ 0 \\end{array}\\right]\\,\\,\\,\\,\\,\\,\\,b\\longleftarrow\\left[\\begin{array}{c} 0\\ 1\\ 0\\ \\vdots\\ 0\\ 0 \\end{array}\\right]\\,\\,\\,\\,\\,c\\longleftarrow\\left[\\begin{array}{c} 0\\ 0\\ 1\\ \\vdots\\ 0\\ 0 \\end{array}\\right]\\cdots$$ where each vector has 32 entries (or in general: number of entries = number of unique characters in text). The first practical step towards doing this one-hot encoding is to form a dictionary mapping each unique character to a unique integer, and one dictionary to do the reverse mapping. We can then use these dictionaries to quickly make our one-hot encodings, as well as re-translate (from integers to characters) the results of our trained RNN classification model. # this dictionary is a function mapping each unique character to a unique integer chars_to_indices = dict ((c, i) for i, c in enumerate (chars)) # map each unique character to unique integer # this dictionary is a function mapping each unique integer back to a unique character indices_to_chars = dict ((i, c) for i, c in enumerate (chars)) # map each unique integer back to unique character Now we can transform our input/output pairs - consisting of characters - to equivalent input/output pairs made up of one-hot encoded vectors. In the next cell we provide a function for doing just this: it takes in the raw character input/outputs and returns their numerical versions. In particular the numerical input is given as $\\bf{X}$, and numerical output is given as the $\\bf{y}$ # transform character-based input/output into equivalent numerical versions def encode_io_pairs (text,window_size,step_size): # number of unique chars chars = sorted ( list ( set (text))) num_chars = len (chars) # cut up text into character input/output pairs inputs, outputs = window_transform_text(text,window_size,step_size) # create empty vessels for one-hot encoded input/output X = np . zeros(( len (inputs), window_size, num_chars), dtype = np . bool) y = np . zeros(( len (inputs), num_chars), dtype = np . bool) # loop over inputs/outputs and transform and store in X/y for i, sentence in enumerate (inputs): for t, char in enumerate (sentence): X[i, t, chars_to_indices[char]] = 1 y[i, chars_to_indices[outputs[i]]] = 1 return X,y Now run the one-hot encoding function by activating the cell below and transform our input/output pairs! # use your function window_size = 100 step_size = 5 X,y = encode_io_pairs(text,window_size,step_size) 2.6 Setting up our RNN With our dataset loaded and the input/output pairs extracted / transformed we can now begin setting up our RNN for training. Again we will use Keras to quickly build a single hidden layer RNN - where our hidden layer consists of LSTM modules. Time to get to work: build a 3 layer RNN model of the following specification layer 1 should be an LSTM module with 200 hidden units --> note this should have input_shape = (window_size,len(chars)) where len(chars) = number of unique characters in your cleaned text layer 2 should be a linear module, fully connected, with len(chars) hidden units --> where len(chars) = number of unique characters in your cleaned text layer 3 should be a softmax activation ( since we are solving a multiclass classification ) Use the categorical_crossentropy loss This network can be constructed using just a few lines - as with the RNN network you made in part 1 of this notebook. See e.g., the general Keras documentation and the LSTM documentation in particular for examples of how to quickly use Keras to build neural network models. ### necessary functions from the keras library from keras.models import Sequential from keras.layers import Dense, Activation, LSTM from keras.optimizers import RMSprop from keras.utils.data_utils import get_file import keras import random # TODO implement build_part2_RNN in my_answers.py from my_answers import build_part2_RNN model = build_part2_RNN(window_size, len (chars)) # initialize optimizer optimizer = keras . optimizers . RMSprop(lr = 0.001 , rho = 0.9 , epsilon = 1e-08 , decay = 0.0 ) # compile model --> make sure initialized optimizer and callbacks - as defined above - are used model . compile(loss = 'categorical_crossentropy' , optimizer = optimizer) 2.7 Training our RNN model for text generation With our RNN setup we can now train it! Lets begin by trying it out on a small subset of the larger version. In the next cell we take the first 10,000 input/output pairs from our training database to learn on. # a small subset of our input/output pairs Xsmall = X[: 10000 ,:,:] ysmall = y[: 10000 ,:] Now lets fit our model! # train the model model . fit(Xsmall, ysmall, batch_size = 500 , epochs = 40 ,verbose = 1 ) # save weights model . save_weights( 'model_weights/best_RNN_small_textdata_weights.hdf5' ) Epoch 1/40 10000/10000 [==============================] - 4s - loss: 3.0264 Epoch 2/40 10000/10000 [==============================] - 4s - loss: 2.8770 Epoch 3/40 10000/10000 [==============================] - 4s - loss: 2.8521 Epoch 4/40 10000/10000 [==============================] - 4s - loss: 2.8178 Epoch 5/40 10000/10000 [==============================] - 4s - loss: 2.7683 Epoch 6/40 10000/10000 [==============================] - 4s - loss: 2.7088 Epoch 7/40 10000/10000 [==============================] - 4s - loss: 2.6444 Epoch 8/40 10000/10000 [==============================] - 4s - loss: 2.5756 Epoch 9/40 10000/10000 [==============================] - 4s - loss: 2.5201 Epoch 10/40 10000/10000 [==============================] - 4s - loss: 2.4693 Epoch 11/40 10000/10000 [==============================] - 4s - loss: 2.4174 Epoch 12/40 10000/10000 [==============================] - 4s - loss: 2.3831 Epoch 13/40 10000/10000 [==============================] - 4s - loss: 2.3462 Epoch 14/40 10000/10000 [==============================] - 4s - loss: 2.3109 Epoch 15/40 10000/10000 [==============================] - 4s - loss: 2.2901 Epoch 16/40 10000/10000 [==============================] - 4s - loss: 2.2616 Epoch 17/40 10000/10000 [==============================] - 4s - loss: 2.2389 Epoch 18/40 10000/10000 [==============================] - 4s - loss: 2.2193 Epoch 19/40 10000/10000 [==============================] - 4s - loss: 2.1940 Epoch 20/40 10000/10000 [==============================] - 4s - loss: 2.1783 Epoch 21/40 10000/10000 [==============================] - 4s - loss: 2.1544 Epoch 22/40 10000/10000 [==============================] - 4s - loss: 2.1445 Epoch 23/40 10000/10000 [==============================] - 4s - loss: 2.1245 Epoch 24/40 10000/10000 [==============================] - 4s - loss: 2.0990 Epoch 25/40 10000/10000 [==============================] - 4s - loss: 2.0909 Epoch 26/40 10000/10000 [==============================] - 4s - loss: 2.0710 Epoch 27/40 10000/10000 [==============================] - 4s - loss: 2.0544 Epoch 28/40 10000/10000 [==============================] - 4s - loss: 2.0342 Epoch 29/40 10000/10000 [==============================] - 4s - loss: 2.0137 Epoch 30/40 10000/10000 [==============================] - 4s - loss: 2.0040 Epoch 31/40 10000/10000 [==============================] - 4s - loss: 1.9761 Epoch 32/40 10000/10000 [==============================] - 4s - loss: 1.9636 Epoch 33/40 10000/10000 [==============================] - 4s - loss: 1.9472 Epoch 34/40 10000/10000 [==============================] - 4s - loss: 1.9373 Epoch 35/40 10000/10000 [==============================] - 4s - loss: 1.9106 Epoch 36/40 10000/10000 [==============================] - 4s - loss: 1.8947 Epoch 37/40 10000/10000 [==============================] - 4s - loss: 1.8731 Epoch 38/40 10000/10000 [==============================] - 4s - loss: 1.8560 Epoch 39/40 10000/10000 [==============================] - 4s - loss: 1.8363 Epoch 40/40 10000/10000 [==============================] - 4s - loss: 1.8157 How do we make a given number of predictions (characters) based on this fitted model? First we predict the next character after following any chunk of characters in the text of length equal to our chosen window size. Then we remove the first character in our input sequence and tack our prediction onto the end. This gives us a slightly changed sequence of inputs that still has length equal to the size of our window. We then feed in this updated input sequence into the model to predict the another character. Together then we have two predicted characters following our original input sequence. Repeating this process N times gives us N predicted characters. In the next Python cell we provide you with a completed function that does just this - it makes predictions when given a) a trained RNN model, b) a subset of (window_size) characters from the text, and c) a number of characters to predict (to follow our input subset). # function that uses trained model to predict a desired number of future characters def predict_next_chars (model,input_chars,num_to_predict): # create output predicted_chars = '' for i in range (num_to_predict): # convert this round's predicted characters to numerical input x_test = np . zeros(( 1 , window_size, len (chars))) for t, char in enumerate (input_chars): x_test[ 0 , t, chars_to_indices[char]] = 1. # make this round's prediction test_predict = model . predict(x_test,verbose = 0 )[ 0 ] # translate numerical prediction back to characters r = np . argmax(test_predict) # predict class of each test input d = indices_to_chars[r] # update predicted_chars and input predicted_chars += d input_chars += d input_chars = input_chars[ 1 :] return predicted_chars With your trained model try a few subsets of the complete text as input - note the length of each must be exactly equal to the window size. For each subset use the function above to predict the next 100 characters that follow each input. # TODO: choose an input sequence and use the prediction function in the previous Python cell to predict 100 characters following it # get an appropriately sized chunk of characters from the text start_inds = [] # load in weights model . load_weights( 'model_weights/best_RNN_small_textdata_weights.hdf5' ) for s in start_inds: start_index = s input_chars = text[start_index: start_index + window_size] # use the prediction function predict_input = predict_next_chars(model,input_chars,num_to_predict = 100 ) # print out input characters print ( '------------------' ) input_line = 'input chars = ' + ' \\n ' + input_chars + '\"' + ' \\n ' print (input_line) # print out predicted characters line = 'predicted chars = ' + ' \\n ' + predict_input + '\"' + ' \\n ' print (line) This looks ok, but not great. Now lets try the same experiment with a larger chunk of the data - with the first 100,000 input/output pairs. Tuning RNNs for a typical character dataset like the one we will use here is a computationally intensive endeavour and thus timely on a typical CPU. Using a reasonably sized cloud-based GPU can speed up training by a factor of 10. Also because of the long training time it is highly recommended that you carefully write the output of each step of your process to file. This is so that all of your results are saved even if you close the web browser you're working out of, as the processes will continue processing in the background but variables/output in the notebook system will not update when you open it again. In the next cell we show you how to create a text file in Python and record data to it. This sort of setup can be used to record your final predictions. ### A simple way to write output to file f = open ( 'my_test_output.txt' , 'w' ) # create an output file to write too f . write( 'this is only a test ' + ' \\n ' ) # print some output text x = 2 f . write( 'the value of x is ' + str (x) + ' \\n ' ) # record a variable value f . close() # print out the contents of my_test_output.txt f = open ( 'my_test_output.txt' , 'r' ) # create an output file to write too f . read() 'this is only a test \\nthe value of x is 2\\n' With this recording devices we can now more safely perform experiments on larger portions of the text. In the next cell we will use the first 100,000 input/output pairs to train our RNN model. First we fit our model to the dataset, then generate text using the trained model in precisely the same generation method applied before on the small dataset. Note: your generated words should be - by and large - more realistic than with the small dataset, but you won't be able to generate perfect English sentences even with this amount of data. A rule of thumb: your model is working well if you generate sentences that largely contain real English words. # a small subset of our input/output pairs Xlarge = X[: 100000 ,:,:] ylarge = y[: 100000 ,:] # TODO: fit to our larger dataset model . fit(Xlarge, ylarge, batch_size = 500 , epochs = 30 , verbose = 1 ) # save weights model . save_weights( 'model_weights/best_RNN_large_textdata_weights.hdf5' ) Epoch 1/30 100000/100000 [==============================] - 44s - loss: 2.0428 Epoch 2/30 100000/100000 [==============================] - 44s - loss: 1.9557 Epoch 3/30 100000/100000 [==============================] - 44s - loss: 1.8930 Epoch 4/30 100000/100000 [==============================] - 44s - loss: 1.8399 Epoch 5/30 100000/100000 [==============================] - 44s - loss: 1.7932 Epoch 6/30 100000/100000 [==============================] - 44s - loss: 1.7499 Epoch 7/30 100000/100000 [==============================] - 44s - loss: 1.7120 Epoch 8/30 100000/100000 [==============================] - 44s - loss: 1.6755 Epoch 9/30 100000/100000 [==============================] - 44s - loss: 1.6398 Epoch 10/30 100000/100000 [==============================] - 44s - loss: 1.6081 Epoch 11/30 100000/100000 [==============================] - 44s - loss: 1.5772 Epoch 12/30 100000/100000 [==============================] - 44s - loss: 1.5465 Epoch 13/30 100000/100000 [==============================] - 44s - loss: 1.5174 Epoch 14/30 100000/100000 [==============================] - 44s - loss: 1.4885 Epoch 15/30 100000/100000 [==============================] - 44s - loss: 1.4604 Epoch 16/30 100000/100000 [==============================] - 44s - loss: 1.4321 Epoch 17/30 100000/100000 [==============================] - 44s - loss: 1.4041 Epoch 18/30 100000/100000 [==============================] - 44s - loss: 1.3777 Epoch 19/30 100000/100000 [==============================] - 44s - loss: 1.3497 Epoch 20/30 100000/100000 [==============================] - 44s - loss: 1.3232 Epoch 21/30 100000/100000 [==============================] - 44s - loss: 1.2956 Epoch 22/30 100000/100000 [==============================] - 44s - loss: 1.2689 Epoch 23/30 100000/100000 [==============================] - 44s - loss: 1.2422 Epoch 24/30 100000/100000 [==============================] - 44s - loss: 1.2157 Epoch 25/30 100000/100000 [==============================] - 44s - loss: 1.1894 Epoch 26/30 100000/100000 [==============================] - 44s - loss: 1.1626 Epoch 27/30 100000/100000 [==============================] - 44s - loss: 1.1381 Epoch 28/30 100000/100000 [==============================] - 44s - loss: 1.1118 Epoch 29/30 100000/100000 [==============================] - 44s - loss: 1.0867 Epoch 30/30 100000/100000 [==============================] - 44s - loss: 1.0611 # TODO: choose an input sequence and use the prediction function in the previous Python cell to predict 100 characters following it # get an appropriately sized chunk of characters from the text start_inds = [ 100 , 300 , 500 , 700 , 900 , 1100 , 1300 , 1500 ] # save output f = open ( 'text_gen_output/RNN_large_textdata_output.txt' , 'w' ) # create an output file to write too # load weights model . load_weights( 'model_weights/best_RNN_large_textdata_weights.hdf5' ) for s in start_inds: start_index = s input_chars = text[start_index: start_index + window_size] # use the prediction function predict_input = predict_next_chars(model,input_chars,num_to_predict = 100 ) # print out input characters line = '-------------------' + ' \\n ' print (line) f . write(line) input_line = 'input chars = ' + ' \\n ' + input_chars + '\"' + ' \\n ' print (input_line) f . write(input_line) # print out predicted characters predict_line = 'predicted chars = ' + ' \\n ' + predict_input + '\"' + ' \\n ' print (predict_line) f . write(predict_line) f . close() ------------------- input chars = the whole of her sex. it was not that he felt any emotion akin to love for irene adler. all emotion\" predicted chars = s to the door and had betneed to the dear but sime. there was a small before and a smilion of his h\" ------------------- input chars = s, i take it, the most perfect reasoning and observing machine that the world has seen, but as a lov\" predicted chars = e the hed of the door and a wead, farried, you be a fall the with the side. that is wall go to the m\" ------------------- input chars = h a gibe and a sneer. they were admirable things for the observerexcellent for drawing the veil from\" predicted chars = his eater from his sceathong and the strom in the head of a fellow sourge and losk in the attertion\" ------------------- input chars = te and finely adjusted temperament was to introduce a distracting factor which might throw a doubt u\" predicted chars = pon the door and a wainer, and he said he had been could to froom the offer planss of his suchans an\" ------------------- input chars = enses, would not be more disturbing than a strong emotion in a nature such as his. and yet there was\" predicted chars = a sight and then seadent possible that the ress of sherl of the door and a with a consid and should\" ------------------- input chars = i had seen little of holmes lately. my marriage had drifted us away from each other. my own complet\" predicted chars = ely remarted in the door be the ore of the door of resurest that the tore in our have some have a su\" ------------------- input chars = er of his own establishment, were sufficient to absorb all my attention, while holmes, who loathed e\" predicted chars = ven dead the sounders, and he seet her little to be so down that the strong of the door and a weadli\" ------------------- input chars = among his old books, and alternating from week to week between cocaine and ambition, the drowsiness \" predicted chars = of the door and a wead, from the coolman was he had told me out if his with her looked and lead the \"","title":"Recurrent Neural Network"},{"location":"dl/rnn/RNN_project/#artificial-intelligence-nanodegree","text":"","title":"Artificial Intelligence Nanodegree"},{"location":"dl/rnn/RNN_project/#recurrent-neural-network-projects","text":"Welcome to the Recurrent Neural Network Project in the Artificial Intelligence Nanodegree! In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with 'Implementation' in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a 'Question X' header. Carefully read each question and provide thorough answers in the following text boxes that begin with 'Answer:' . Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide. Note: Code and Markdown cells can be executed using the Shift + Enter keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode.","title":"Recurrent Neural Network Projects"},{"location":"dl/rnn/RNN_project/#implementation-todos-in-this-notebook","text":"This notebook contains two problems, cut into a variety of TODOs. Make sure to complete each section containing a TODO marker throughout the notebook. For convenience we provide links to each of these sections below. TODO #1: Implement a function to window time series TODO #2: Create a simple RNN model using keras to perform regression TODO #3: Finish cleaning a large text corpus TODO #4: Implement a function to window a large text corpus TODO #5: Create a simple RNN model using keras to perform multiclass classification TODO #6: Generate text using a fully trained RNN model and a variety of input sequences","title":"Implementation TODOs in this notebook"},{"location":"dl/rnn/RNN_project/#problem-1-perform-time-series-prediction","text":"In this project you will perform time series prediction using a Recurrent Neural Network regressor. In particular you will re-create the figure shown in the notes - where the stock price of Apple was forecasted (or predicted) 7 days in advance. In completing this exercise you will learn how to construct RNNs using Keras, which will also aid in completing the second project in this notebook. The particular network architecture we will employ for our RNN is known as Long Term Short Memory (LSTM) , which helps significantly avoid technical problems with optimization of RNNs.","title":"Problem 1: Perform time series prediction"},{"location":"dl/rnn/RNN_project/#11-getting-started","text":"First we must load in our time series - a history of around 140 days of Apple's stock price. Then we need to perform a number of pre-processing steps to prepare it for use with an RNN model. First off, it is good practice to normalize time series - by normalizing its range. This helps us avoid serious numerical issues associated how common activation functions (like tanh) transform very large (positive or negative) numbers, as well as helping us to avoid related issues when computing derivatives. Here we normalize the series to lie in the range [0,1] using this scikit function , but it is also commonplace to normalize by a series standard deviation. import seaborn as sns import pandas as pd ### Load in necessary libraries for data input and normalization % matplotlib inline import numpy as np import matplotlib.pyplot as plt % load_ext autoreload % autoreload 2 from my_answers import * % load_ext autoreload % autoreload 2 from my_answers import * ### load in and normalize the dataset dataset = np . loadtxt( 'datasets/normalized_apple_prices.csv' ) The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Lets take a quick look at the (normalized) time series we'll be performing predictions on. # lets take a look at our time series #pd.DataFrame(dataset).plot() plt . plot(dataset) plt . xlabel( 'time period' ) plt . ylabel( 'normalized series value' ) Text(0,0.5,u'normalized series value')","title":"1.1 Getting started"},{"location":"dl/rnn/RNN_project/#12-cutting-our-time-series-into-sequences","text":"Remember, our time series is a sequence of numbers that we can represent in general mathematically as $$s_{0},s_{1},s_{2},...,s_{P}$$ where $s_{p}$ is the numerical value of the time series at time period $p$ and where $P$ is the total length of the series. In order to apply our RNN we treat the time series prediction problem as a regression problem, and so need to use a sliding window to construct a set of associated input/output pairs to regress on. This process is animated in the gif below. For example - using a window of size T = 5 (as illustrated in the gif above) we produce a set of input/output pairs like the one shown in the table below $$\\begin{array}{c|c} \\text{Input} & \\text{Output}\\ \\hline \\color{CornflowerBlue} {\\langle s_{1},s_{2},s_{3},s_{4},s_{5}\\rangle} & \\color{Goldenrod}{ s_{6}} \\ \\ \\color{CornflowerBlue} {\\langle s_{2},s_{3},s_{4},s_{5},s_{6} \\rangle } & \\color{Goldenrod} {s_{7} } \\ \\color{CornflowerBlue} {\\vdots} & \\color{Goldenrod} {\\vdots}\\ \\color{CornflowerBlue} { \\langle s_{P-5},s_{P-4},s_{P-3},s_{P-2},s_{P-1} \\rangle } & \\color{Goldenrod} {s_{P}} \\end{array}$$ Notice here that each input is a sequence (or vector) of length 5 (and in general has length equal to the window size T) while each corresponding output is a scalar value. Notice also how given a time series of length P and window size T = 5 as shown above, we created P - 5 input/output pairs. More generally, for a window size T we create P - T such pairs. Now its time for you to window the input time series as described above! TODO: Implement the function called window_transform_series in my_answers.py so that it runs a sliding window along the input series and creates associated input/output pairs. Note that this function should input a) the series and b) the window length, and return the input/output subsequences. Make sure to format returned input/output as generally shown in table above (where window_size = 5), and make sure your returned input is a numpy array. You can test your function on the list of odd numbers given below odd_nums = np . array([ 1 , 3 , 5 , 7 , 9 , 11 , 13 ]) Here is a hard-coded solution for odd_nums. You can compare its results with what you get from your window_transform_series implementation. # run a window of size 2 over the odd number sequence and display the results window_size = 2 X = [] X . append(odd_nums[ 0 : 2 ]) X . append(odd_nums[ 1 : 3 ]) X . append(odd_nums[ 2 : 4 ]) X . append(odd_nums[ 3 : 5 ]) X . append(odd_nums[ 4 : 6 ]) y = odd_nums[ 2 :] X = np . asarray(X) y = np . asarray(y) y = np . reshape(y, ( len (y), 1 )) #optional assert ( type (X) . __name__ == 'ndarray' ) assert ( type (y) . __name__ == 'ndarray' ) assert (X . shape == ( 5 , 2 )) assert (y . shape in [( 5 , 1 ), ( 5 ,)]) # print out input/output pairs --> here input = X, corresponding output = y print ( '--- the input X will look like ----' ) print (X) print ( '--- the associated output y will look like ----' ) print (y) --- the input X will look like ---- [[ 1 3] [ 3 5] [ 5 7] [ 7 9] [ 9 11]] --- the associated output y will look like ---- [[ 5] [ 7] [ 9] [11] [13]] Again - you can check that your completed window_transform_series function works correctly by trying it on the odd_nums sequence - you should get the above output. ### TODO: implement the function window_transform_series in the file my_answers.py from my_answers import window_transform_series With this function in place apply it to the series in the Python cell below. We use a window_size = 7 for these experiments. # window the data using your windowing function window_size = 7 X,y = window_transform_series(series = dataset, window_size = window_size) X . shape,y . shape ((131, 7), (131, 1))","title":"1.2  Cutting our time series into sequences"},{"location":"dl/rnn/RNN_project/#13-splitting-into-training-and-testing-sets","text":"In order to perform proper testing on our dataset we will lop off the last 1/3 of it for validation (or testing). This is that once we train our model we have something to test it on (like any regression problem!). This splitting into training/testing sets is done in the cell below. Note how here we are not splitting the dataset randomly as one typically would do when validating a regression model. This is because our input/output pairs are related temporally . We don't want to validate our model by training on a random subset of the series and then testing on another random subset, as this simulates the scenario that we receive new points within the timeframe of our training set . We want to train on one solid chunk of the series (in our case, the first full 2/3 of it), and validate on a later chunk (the last 1/3) as this simulates how we would predict future values of a time series. # split our dataset into training / testing sets train_test_split = int (np . ceil( 2 * len (y) / float ( 3 ))) # set the split point print (train_test_split) # partition the training set X_train = X[:train_test_split,:] y_train = y[:train_test_split] # keep the last chunk for testing X_test = X[train_test_split:,:] y_test = y[train_test_split:] # NOTE: to use keras's RNN LSTM module our input must be reshaped to [samples, window size, stepsize] X_train = np . asarray(np . reshape(X_train, (X_train . shape[ 0 ], window_size, 1 ))) X_test = np . asarray(np . reshape(X_test, (X_test . shape[ 0 ], window_size, 1 ))) 88","title":"1.3  Splitting into training and testing sets"},{"location":"dl/rnn/RNN_project/#14-build-and-run-an-rnn-regression-model","text":"Having created input/output pairs out of our time series and cut this into training/testing sets, we can now begin setting up our RNN. We use Keras to quickly build a two hidden layer RNN of the following specifications layer 1 uses an LSTM module with 5 hidden units (note here the input_shape = (window_size,1)) layer 2 uses a fully connected module with one unit the 'mean_squared_error' loss should be used (remember: we are performing regression here) This can be constructed using just a few lines - see e.g., the general Keras documentation and the LSTM documentation in particular for examples of how to quickly use Keras to build neural network models. Make sure you are initializing your optimizer given the keras-recommended approach for RNNs (given in the cell below). (remember to copy your completed function into the script my_answers.py function titled build_part1_RNN before submitting your project) ### TODO: create required RNN model # import keras network libraries from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM import keras # given - fix random seed - so we can all reproduce the same results on our default time series np . random . seed( 0 ) # TODO: implement build_part1_RNN in my_answers.py from my_answers import build_part1_RNN model = build_part1_RNN(window_size) # build model using keras documentation recommended optimizer initialization optimizer = keras . optimizers . RMSprop(lr = 0.001 , rho = 0.9 , epsilon = 1e-08 , decay = 0.0 ) # compile the model model . compile(loss = 'mean_squared_error' , optimizer = optimizer) With your model built you can now fit the model by activating the cell below! Note: the number of epochs (np_epochs) and batch_size are preset (so we can all produce the same results). You can choose to toggle the verbose parameter - which gives you regular updates on the progress of the algorithm - on and off by setting it to 1 or 0 respectively. # run your model! model . fit(X_train, y_train, epochs = 1000 , batch_size = 50 , verbose = 1 ) Epoch 1/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 2/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 3/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 4/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 5/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 6/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 7/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 8/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 9/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 10/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 11/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 12/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 13/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 14/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 15/1000 88/88 [==============================] - 0s - loss: 0.0166 Epoch 16/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 17/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 18/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 19/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 20/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 21/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 22/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 23/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 24/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 25/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 26/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 27/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 28/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 29/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 30/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 31/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 32/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 33/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 34/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 35/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 36/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 37/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 38/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 39/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 40/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 41/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 42/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 43/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 44/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 45/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 46/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 47/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 48/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 49/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 50/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 51/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 52/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 53/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 54/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 55/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 56/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 57/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 58/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 59/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 60/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 61/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 62/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 63/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 64/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 65/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 66/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 67/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 68/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 69/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 70/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 71/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 72/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 73/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 74/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 75/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 76/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 77/1000 88/88 [==============================] - 0s - loss: 0.0166 Epoch 78/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 79/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 80/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 81/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 82/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 83/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 84/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 85/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 86/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 87/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 88/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 89/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 90/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 91/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 92/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 93/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 94/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 95/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 96/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 97/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 98/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 99/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 100/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 101/1000 88/88 [==============================] - ETA: 0s - loss: 0.016 - 0s - loss: 0.0163 Epoch 102/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 103/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 104/1000 88/88 [==============================] - ETA: 0s - loss: 0.016 - 0s - loss: 0.0161 Epoch 105/1000 88/88 [==============================] - 0s - loss: 0.0169 Epoch 106/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 107/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 108/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 109/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 110/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 111/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 112/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 113/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 114/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 115/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 116/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 117/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 118/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 119/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 120/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 121/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 122/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 123/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 124/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 125/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 126/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 127/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 128/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 129/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 130/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 131/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 132/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 133/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 134/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 135/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 136/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 137/1000 88/88 [==============================] - ETA: 0s - loss: 0.017 - 0s - loss: 0.0161 Epoch 138/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 139/1000 88/88 [==============================] - 0s - loss: 0.0166 Epoch 140/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 141/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 142/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 143/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 144/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 145/1000 88/88 [==============================] - 0s - loss: 0.0167 Epoch 146/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 147/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 148/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 149/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 150/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 151/1000 88/88 [==============================] - ETA: 0s - loss: 0.017 - 0s - loss: 0.0161 Epoch 152/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 153/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 154/1000 88/88 [==============================] - ETA: 0s - loss: 0.020 - 0s - loss: 0.0161 Epoch 155/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 156/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 157/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 158/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 159/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 160/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 161/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 162/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 163/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 164/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 165/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 166/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 167/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 168/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 169/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 170/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 171/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 172/1000 88/88 [==============================] - ETA: 0s - loss: 0.015 - 0s - loss: 0.0161 Epoch 173/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 174/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 175/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 176/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 177/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 178/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 179/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 180/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 181/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 182/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 183/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 184/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 185/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 186/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 187/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 188/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 189/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 190/1000 88/88 [==============================] - ETA: 0s - loss: 0.017 - 0s - loss: 0.0161 Epoch 191/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 192/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 193/1000 88/88 [==============================] - 0s - loss: 0.0169 Epoch 194/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 195/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 196/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 197/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 198/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 199/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 200/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 201/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 202/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 203/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 204/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 205/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 206/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 207/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 208/1000 88/88 [==============================] - ETA: 0s - loss: 0.014 - 0s - loss: 0.0162 Epoch 209/1000 88/88 [==============================] - ETA: 0s - loss: 0.019 - 0s - loss: 0.0160 Epoch 210/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 211/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 212/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 213/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 214/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 215/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 216/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 217/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 218/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 219/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 220/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 221/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 222/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 223/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 224/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 225/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 226/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 227/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 228/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 229/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 230/1000 88/88 [==============================] - ETA: 0s - loss: 0.013 - 0s - loss: 0.0161 Epoch 231/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 232/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 233/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 234/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 235/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 236/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 237/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 238/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 239/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 240/1000 88/88 [==============================] - 0s - loss: 0.0165 Epoch 241/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 242/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 243/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 244/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 245/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 246/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 247/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 248/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 249/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 250/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 251/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 252/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 253/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 254/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 255/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 256/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 257/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 258/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 259/1000 88/88 [==============================] - ETA: 0s - loss: 0.013 - 0s - loss: 0.0160 Epoch 260/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 261/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 262/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 263/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 264/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 265/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 266/1000 88/88 [==============================] - 0s - loss: 0.0167 Epoch 267/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 268/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 269/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 270/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 271/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 272/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 273/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 274/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 275/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 276/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 277/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 278/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 279/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 280/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 281/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 282/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 283/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 284/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 285/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 286/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 287/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 288/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 289/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 290/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 291/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 292/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 293/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 294/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 295/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 296/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 297/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 298/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 299/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 300/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 301/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 302/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 303/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 304/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 305/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 306/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 307/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 308/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 309/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 310/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 311/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 312/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 313/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 314/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 315/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 316/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 317/1000 88/88 [==============================] - ETA: 0s - loss: 0.012 - 0s - loss: 0.0161 Epoch 318/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 319/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 320/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 321/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 322/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 323/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 324/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 325/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 326/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 327/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 328/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 329/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 330/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 331/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 332/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 333/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 334/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 335/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 336/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 337/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 338/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 339/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 340/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 341/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 342/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 343/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 344/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 345/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 346/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 347/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 348/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 349/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 350/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 351/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 352/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 353/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 354/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 355/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 356/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 357/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 358/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 359/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 360/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 361/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 362/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 363/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 364/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 365/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 366/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 367/1000 88/88 [==============================] - 0s - loss: 0.0164 Epoch 368/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 369/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 370/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 371/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 372/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 373/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 374/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 375/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 376/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 377/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 378/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 379/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 380/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 381/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 382/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 383/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 384/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 385/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 386/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 387/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 388/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 389/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 390/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 391/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 392/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 393/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 394/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 395/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 396/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 397/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 398/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 399/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 400/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 401/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 402/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 403/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 404/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 405/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 406/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 407/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 408/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 409/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 410/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 411/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 412/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 413/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 414/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 415/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 416/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 417/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 418/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 419/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 420/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 421/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 422/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 423/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 424/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 425/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 426/1000 88/88 [==============================] - ETA: 0s - loss: 0.017 - 0s - loss: 0.0159 Epoch 427/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 428/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 429/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 430/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 431/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 432/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 433/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 434/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 435/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 436/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 437/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 438/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 439/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 440/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 441/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 442/1000 88/88 [==============================] - ETA: 0s - loss: 0.013 - 0s - loss: 0.0160 Epoch 443/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 444/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 445/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 446/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 447/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 448/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 449/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 450/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 451/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 452/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 453/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 454/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 455/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 456/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 457/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 458/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 459/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 460/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 461/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 462/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 463/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 464/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 465/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 466/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 467/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 468/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 469/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 470/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 471/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 472/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 473/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 474/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 475/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 476/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 477/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 478/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 479/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 480/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 481/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 482/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 483/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 484/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 485/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 486/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 487/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 488/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 489/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 490/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 491/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 492/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 493/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 494/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 495/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 496/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 497/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 498/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 499/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 500/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 501/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 502/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 503/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 504/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 505/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 506/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 507/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 508/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 509/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 510/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 511/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 512/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 513/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 514/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 515/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 516/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 517/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 518/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 519/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 520/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 521/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 522/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 523/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 524/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 525/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 526/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 527/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 528/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 529/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 530/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 531/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 532/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 533/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 534/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 535/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 536/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 537/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 538/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 539/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 540/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 541/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 542/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 543/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 544/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 545/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 546/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 547/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 548/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 549/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 550/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 551/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 552/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 553/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 554/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 555/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 556/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 557/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 558/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 559/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 560/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 561/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 562/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 563/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 564/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 565/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 566/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 567/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 568/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 569/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 570/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 571/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 572/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 573/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 574/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 575/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 576/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 577/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 578/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 579/1000 88/88 [==============================] - 0s - loss: 0.0162 Epoch 580/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 581/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 582/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 583/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 584/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 585/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 586/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 587/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 588/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 589/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 590/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 591/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 592/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 593/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 594/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 595/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 596/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 597/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 598/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 599/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 600/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 601/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 602/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 603/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 604/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 605/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 606/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 607/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 608/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 609/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 610/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 611/1000 88/88 [==============================] - ETA: 0s - loss: 0.016 - 0s - loss: 0.0156 Epoch 612/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 613/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 614/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 615/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 616/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 617/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 618/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 619/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 620/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 621/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 622/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 623/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 624/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 625/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 626/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 627/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 628/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 629/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 630/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 631/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 632/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 633/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 634/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 635/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 636/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 637/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 638/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 639/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 640/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 641/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 642/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 643/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 644/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 645/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 646/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 647/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 648/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 649/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 650/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 651/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 652/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 653/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 654/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 655/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 656/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 657/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 658/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 659/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 660/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 661/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 662/1000 88/88 [==============================] - 0s - loss: 0.0160 Epoch 663/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 664/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 665/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 666/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 667/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 668/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 669/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 670/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 671/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 672/1000 88/88 [==============================] - ETA: 0s - loss: 0.014 - 0s - loss: 0.0156 Epoch 673/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 674/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 675/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 676/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 677/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 678/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 679/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 680/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 681/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 682/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 683/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 684/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 685/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 686/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 687/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 688/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 689/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 690/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 691/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 692/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 693/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 694/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 695/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 696/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 697/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 698/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 699/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 700/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 701/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 702/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 703/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 704/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 705/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 706/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 707/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 708/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 709/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 710/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 711/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 712/1000 88/88 [==============================] - ETA: 0s - loss: 0.016 - 0s - loss: 0.0161 Epoch 713/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 714/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 715/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 716/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 717/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 718/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 719/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 720/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 721/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 722/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 723/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 724/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 725/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 726/1000 88/88 [==============================] - ETA: 0s - loss: 0.013 - 0s - loss: 0.0155 Epoch 727/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 728/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 729/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 730/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 731/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 732/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 733/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 734/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 735/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 736/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 737/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 738/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 739/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 740/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 741/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 742/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 743/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 744/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 745/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 746/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 747/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 748/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 749/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 750/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 751/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 752/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 753/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 754/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 755/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 756/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 757/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 758/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 759/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 760/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 761/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 762/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 763/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 764/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 765/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 766/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 767/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 768/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 769/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 770/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 771/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 772/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 773/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 774/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 775/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 776/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 777/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 778/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 779/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 780/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 781/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 782/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 783/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 784/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 785/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 786/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 787/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 788/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 789/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 790/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 791/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 792/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 793/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 794/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 795/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 796/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 797/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 798/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 799/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 800/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 801/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 802/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 803/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 804/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 805/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 806/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 807/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 808/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 809/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 810/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 811/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 812/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 813/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 814/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 815/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 816/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 817/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 818/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 819/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 820/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 821/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 822/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 823/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 824/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 825/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 826/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 827/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 828/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 829/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 830/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 831/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 832/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 833/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 834/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 835/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 836/1000 88/88 [==============================] - 0s - loss: 0.0159 Epoch 837/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 838/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 839/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 840/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 841/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 842/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 843/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 844/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 845/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 846/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 847/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 848/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 849/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 850/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 851/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 852/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 853/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 854/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 855/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 856/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 857/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 858/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 859/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 860/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 861/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 862/1000 88/88 [==============================] - 0s - loss: 0.0158 Epoch 863/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 864/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 865/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 866/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 867/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 868/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 869/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 870/1000 88/88 [==============================] - 0s - loss: 0.0161 Epoch 871/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 872/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 873/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 874/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 875/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 876/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 877/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 878/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 879/1000 88/88 [==============================] - ETA: 0s - loss: 0.013 - 0s - loss: 0.0153 Epoch 880/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 881/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 882/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 883/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 884/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 885/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 886/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 887/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 888/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 889/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 890/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 891/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 892/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 893/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 894/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 895/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 896/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 897/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 898/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 899/1000 88/88 [==============================] - ETA: 0s - loss: 0.013 - 0s - loss: 0.0153 Epoch 900/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 901/1000 88/88 [==============================] - ETA: 0s - loss: 0.015 - 0s - loss: 0.0153 Epoch 902/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 903/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 904/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 905/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 906/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 907/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 908/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 909/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 910/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 911/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 912/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 913/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 914/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 915/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 916/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 917/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 918/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 919/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 920/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 921/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 922/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 923/1000 88/88 [==============================] - 0s - loss: 0.0157 Epoch 924/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 925/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 926/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 927/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 928/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 929/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 930/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 931/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 932/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 933/1000 88/88 [==============================] - ETA: 0s - loss: 0.012 - 0s - loss: 0.0156 Epoch 934/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 935/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 936/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 937/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 938/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 939/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 940/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 941/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 942/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 943/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 944/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 945/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 946/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 947/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 948/1000 88/88 [==============================] - ETA: 0s - loss: 0.016 - 0s - loss: 0.0153 Epoch 949/1000 88/88 [==============================] - ETA: 0s - loss: 0.018 - 0s - loss: 0.0154 Epoch 950/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 951/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 952/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 953/1000 88/88 [==============================] - 0s - loss: 0.0163 Epoch 954/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 955/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 956/1000 88/88 [==============================] - 0s - loss: 0.0156 Epoch 957/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 958/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 959/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 960/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 961/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 962/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 963/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 964/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 965/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 966/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 967/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 968/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 969/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 970/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 971/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 972/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 973/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 974/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 975/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 976/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 977/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 978/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 979/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 980/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 981/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 982/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 983/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 984/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 985/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 986/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 987/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 988/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 989/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 990/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 991/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 992/1000 88/88 [==============================] - 0s - loss: 0.0154 Epoch 993/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 994/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 995/1000 88/88 [==============================] - 0s - loss: 0.0155 Epoch 996/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 997/1000 88/88 [==============================] - 0s - loss: 0.0152 Epoch 998/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 999/1000 88/88 [==============================] - 0s - loss: 0.0153 Epoch 1000/1000 88/88 [==============================] - 0s - loss: 0.0154 &lt;keras.callbacks.History at 0x7f7bdc5c6090&gt;","title":"1.4  Build and run an RNN regression model"},{"location":"dl/rnn/RNN_project/#15-checking-model-performance","text":"With your model fit we can now make predictions on both our training and testing sets. # generate predictions for training train_predict = model . predict(X_train) test_predict = model . predict(X_test) In the next cell we compute training and testing errors using our trained model - you should be able to achieve at least training_error < 0.02 and testing_error < 0.02 with your fully trained model. If either or both of your accuracies are larger than 0.02 re-train your model - increasing the number of epochs you take (a maximum of around 1,000 should do the job) and/or adjusting your batch_size. # print out training and testing errors training_error = model . evaluate(X_train, y_train, verbose = 0 ) print ( 'training error = ' + str (training_error)) testing_error = model . evaluate(X_test, y_test, verbose = 0 ) print ( 'testing error = ' + str (testing_error)) training error = 0.0151829496026 testing error = 0.0146555935444 Activating the next cell plots the original data, as well as both predictions on the training and testing sets. ### Plot everything - the original series as well as predictions on training and testing sets import matplotlib.pyplot as plt % matplotlib inline # plot original series plt . plot(dataset,color = 'k' ) # plot training set prediction split_pt = train_test_split + window_size plt . plot(np . arange(window_size,split_pt, 1 ),train_predict,color = 'b' ) # plot testing set prediction plt . plot(np . arange(split_pt,split_pt + len (test_predict), 1 ),test_predict,color = 'r' ) # pretty up graph plt . xlabel( 'day' ) plt . ylabel( '(normalized) price of Apple stock' ) plt . legend([ 'original series' , 'training fit' , 'testing fit' ],loc = 'center left' , bbox_to_anchor = ( 1 , 0.5 )) plt . show() Note: you can try out any time series for this exercise! If you would like to try another see e.g., this site containing thousands of time series and pick another one!","title":"1.5  Checking model performance"},{"location":"dl/rnn/RNN_project/#problem-2-create-a-sequence-generator","text":"","title":"Problem 2: Create a sequence generator"},{"location":"dl/rnn/RNN_project/#21-getting-started","text":"In this project you will implement a popular Recurrent Neural Network (RNN) architecture to create an English language sequence generator capable of building semi-coherent English sentences from scratch by building them up character-by-character. This will require a substantial amount amount of parameter tuning on a large training corpus (at least 100,000 characters long). In particular for this project we will be using a complete version of Sir Arthur Conan Doyle's classic book The Adventures of Sherlock Holmes. How can we train a machine learning model to generate text automatically, character-by-character? By showing the model many training examples so it can learn a pattern between input and output. With this type of text generation each input is a string of valid characters like this one dogs are grea while the corresponding output is the next character in the sentence - which here is 't' (since the complete sentence is 'dogs are great'). We need to show a model many such examples in order for it to make reasonable predictions. Fun note: For those interested in how text generation is being used check out some of the following fun resources: Generate wacky sentences with this academic RNN text generator Various twitter bots that tweet automatically generated text like this one . the NanoGenMo annual contest to automatically produce a 50,000+ novel automatically Robot Shakespeare a text generator that automatically produces Shakespear-esk sentences","title":"2.1  Getting started"},{"location":"dl/rnn/RNN_project/#22-preprocessing-a-text-dataset","text":"Our first task is to get a large text corpus for use in training, and on it we perform a several light pre-processing tasks. The default corpus we will use is the classic book Sherlock Holmes, but you can use a variety of others as well - so long as they are fairly large (around 100,000 characters or more). # read in the text, transforming everything to lower case text = open ( 'datasets/holmes.txt' ) . read() . lower() print ( 'our original text has ' + str ( len (text)) + ' characters' ) our original text has 594933 characters Next, lets examine a bit of the raw text. Because we are interested in creating sentences of English words automatically by building up each word character-by-character, we only want to train on valid English words. In other words - we need to remove all of the other characters that are not part of English words. ### print out the first 1000 characters of the raw text to get a sense of what we need to throw out text[: 2000 ] \"\\xef\\xbb\\xbfproject gutenberg's the adventures of sherlock holmes, by arthur conan doyle\\r\\n\\r\\nthis ebook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever. you may copy it, give it away or\\r\\nre-use it under the terms of the project gutenberg license included\\r\\nwith this ebook or online at www.gutenberg.net\\r\\n\\r\\n\\r\\ntitle: the adventures of sherlock holmes\\r\\n\\r\\nauthor: arthur conan doyle\\r\\n\\r\\nposting date: april 18, 2011 [ebook #1661]\\r\\nfirst posted: november 29, 2002\\r\\n\\r\\nlanguage: english\\r\\n\\r\\n\\r\\n*** start of this project gutenberg ebook the adventures of sherlock holmes ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nproduced by an anonymous project gutenberg volunteer and jose menendez\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nthe adventures of sherlock holmes\\r\\n\\r\\nby\\r\\n\\r\\nsir arthur conan doyle\\r\\n\\r\\n\\r\\n\\r\\n i. a scandal in bohemia\\r\\n ii. the red-headed league\\r\\n iii. a case of identity\\r\\n iv. the boscombe valley mystery\\r\\n v. the five orange pips\\r\\n vi. the man with the twisted lip\\r\\n vii. the adventure of the blue carbuncle\\r\\nviii. the adventure of the speckled band\\r\\n ix. the adventure of the engineer's thumb\\r\\n x. the adventure of the noble bachelor\\r\\n xi. the adventure of the beryl coronet\\r\\n xii. the adventure of the copper beeches\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nadventure i. a scandal in bohemia\\r\\n\\r\\ni.\\r\\n\\r\\nto sherlock holmes she is always the woman. i have seldom heard\\r\\nhim mention her under any other name. in his eyes she eclipses\\r\\nand predominates the whole of her sex. it was not that he felt\\r\\nany emotion akin to love for irene adler. all emotions, and that\\r\\none particularly, were abhorrent to his cold, precise but\\r\\nadmirably balanced mind. he was, i take it, the most perfect\\r\\nreasoning and observing machine that the world has seen, but as a\\r\\nlover he would have placed himself in a false position. he never\\r\\nspoke of the softer passions, save with a gibe and a sneer. they\\r\\nwere admirable things for the observer--excellent for drawing the\\r\\nveil from men's motives and actions. but for the trained reasoner\\r\\nto admit such intrusions int\" Wow - there's a lot of junk here (i.e., weird uncommon character combinations - as this first character chunk contains the title and author page, as well as table of contents)! To keep things simple, we want to train our RNN on a large chunk of more typical English sentences - we don't want it to start thinking non-english words or strange characters are valid! - so lets clean up the data a bit. First, since the dataset is so large and the first few hundred characters contain a lot of junk, lets cut it out. Lets also find-and-replace those newline tags with empty spaces. ### find and replace '\\n' and '\\r' symbols - replacing them text = text[ 1302 :] text = text . replace( ' \\n ' , ' ' ) # replacing '\\n' with '' simply removes the sequence text = text . replace( ' \\r ' , ' ' ) Lets see how the first 1000 characters of our text looks now! ### print out the first 1000 characters of the raw text to get a sense of what we need to throw out text[: 1000 ] \" i have seldom heard him mention her under any other name. in his eyes she eclipses and predominates the whole of her sex. it was not that he felt any emotion akin to love for irene adler. all emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. he was, i take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position. he never spoke of the softer passions, save with a gibe and a sneer. they were admirable things for the observer--excellent for drawing the veil from men's motives and actions. but for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results. grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as hi\"","title":"2.2  Preprocessing a text dataset"},{"location":"dl/rnn/RNN_project/#todo-finish-cleaning-the-text","text":"Lets make sure we haven't left any other atypical characters (commas, periods, etc., are ok) lurking around in the depths of the text. You can do this by enumerating all the text's unique characters, examining them, and then replacing any unwanted characters with empty spaces! Once we find all of the text's unique characters, we can remove all of the atypical ones in the next cell. Note: don't remove the punctuation marks given in my_answers.py. ### TODO: implement cleaned_text in my_answers.py from my_answers import cleaned_text text = cleaned_text(text) # shorten any extra dead space created above text = text . replace( ' ' , ' ' ) With your chosen characters removed print out the first few hundred lines again just to double check that everything looks good. ### print out the first 2000 characters of the raw text to get a sense of what we need to throw out text[: 2000 ] ' i have seldom heard him mention her under any other name. in his eyes she eclipses and predominates the whole of her sex. it was not that he felt any emotion akin to love for irene adler. all emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. he was, i take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position. he never spoke of the softer passions, save with a gibe and a sneer. they were admirable things for the observerexcellent for drawing the veil from mens motives and actions. but for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results. grit in a sensitive instrument, or a crack in one of his own highpower lenses, would not be more disturbing than a strong emotion in a nature such as his. and yet there was but one woman to him, and that woman was the late irene adler, of dubious and questionable memory. i had seen little of holmes lately. my marriage had drifted us away from each other. my own complete happiness, and the homecentred interests which rise up around the man who first finds himself master of his own establishment, were sufficient to absorb all my attention, while holmes, who loathed every form of society with his whole bohemian soul, remained in our lodgings in baker street, buried among his old books, and alternating from week to week between cocaine and ambition, the drowsiness of the drug, and the fierce energy of his own keen nature. he was still, as ever, deeply attracted by the study of crime, and occupied his immense faculties and extraordinary powers of observation in following out those clues, and clearing up those mysteries which had been abandoned as hopeless by the official police. from time to time i heard some vague account of his doings: of his summons to od' Now that we have thrown out a good number of non-English characters/character sequences lets print out some statistics about the dataset - including number of total characters and number of unique characters. # count the number of unique characters in the text chars = sorted ( list ( set (text))) # print some of the text, as well as statistics print ( \"this corpus has \" + str ( len (text)) + \" total number of characters\" ) print ( \"this corpus has \" + str ( len (chars)) + \" unique characters\" ) this corpus has 571875 total number of characters this corpus has 33 unique characters","title":"TODO: finish cleaning the text"},{"location":"dl/rnn/RNN_project/#23-cutting-data-into-inputoutput-pairs","text":"Now that we have our text all cleaned up, how can we use it to train a model to generate sentences automatically? First we need to train a machine learning model - and in order to do that we need a set of input/output pairs for a model to train on. How can we create a set of input/output pairs from our text to train on? Remember in part 1 of this notebook how we used a sliding window to extract input/output pairs from a time series? We do the same thing here! We slide a window of length $T$ along our giant text corpus - everything in the window becomes one input while the character following becomes its corresponding output. This process of extracting input/output pairs is illustrated in the gif below on a small example text using a window size of T = 5. Notice one aspect of the sliding window in this gif that does not mirror the analogous gif for time series shown in part 1 of the notebook - we do not need to slide the window along one character at a time but can move by a fixed step size $M$ greater than 1 (in the gif indeed $M = 1$). This is done with large input texts (like ours which has over 500,000 characters!) when sliding the window along one character at a time we would create far too many input/output pairs to be able to reasonably compute with. More formally lets denote our text corpus - which is one long string of characters - as follows $$s_{0},s_{1},s_{2},...,s_{P}$$ where $P$ is the length of the text (again for our text $P \\approx 500,000!$). Sliding a window of size T = 5 with a step length of M = 1 (these are the parameters shown in the gif above) over this sequence produces the following list of input/output pairs $$\\begin{array}{c|c} \\text{Input} & \\text{Output}\\ \\hline \\color{CornflowerBlue} {\\langle s_{1},s_{2},s_{3},s_{4},s_{5}\\rangle} & \\color{Goldenrod}{ s_{6}} \\ \\ \\color{CornflowerBlue} {\\langle s_{2},s_{3},s_{4},s_{5},s_{6} \\rangle } & \\color{Goldenrod} {s_{7} } \\ \\color{CornflowerBlue} {\\vdots} & \\color{Goldenrod} {\\vdots}\\ \\color{CornflowerBlue} { \\langle s_{P-5},s_{P-4},s_{P-3},s_{P-2},s_{P-1} \\rangle } & \\color{Goldenrod} {s_{P}} \\end{array}$$ Notice here that each input is a sequence (or vector) of 5 characters (and in general has length equal to the window size T) while each corresponding output is a single character. We created around P total number of input/output pairs (for general step size M we create around ceil(P/M) pairs). Now its time for you to window the input time series as described above! TODO: Create a function that runs a sliding window along the input text and creates associated input/output pairs. A skeleton function has been provided for you. Note that this function should input a) the text b) the window size and c) the step size, and return the input/output sequences. Note: the return items should be lists - not numpy arrays. (remember to copy your completed function into the script my_answers.py function titled window_transform_text before submitting your project) ### TODO: implement window_transform_series in my_answers.py from my_answers import window_transform_series With our function complete we can now use it to produce input/output pairs! We employ the function in the next cell, where the window_size = 50 and step_size = 5. # run your text window-ing function window_size = 100 step_size = 5 inputs, outputs = window_transform_text(text,window_size,step_size) Lets print out a few input/output pairs to verify that we have made the right sort of stuff! # print out a few of the input/output pairs to verify that we've made the right kind of stuff to learn from print ( 'input = ' + inputs[ 2 ]) print ( 'output = ' + outputs[ 2 ]) print ( '--------------' ) print ( 'input = ' + inputs[ 100 ]) print ( 'output = ' + outputs[ 100 ]) input = ldom heard him mention her under any other name. in his eyes she eclipses and predominates the whole output = -------------- input = h a gibe and a sneer. they were admirable things for the observerexcellent for drawing the veil from output = Looks good!","title":"2.3  Cutting data into input/output pairs"},{"location":"dl/rnn/RNN_project/#24-wait-what-kind-of-problem-is-text-generation-again","text":"In part 1 of this notebook we used the same pre-processing technique - the sliding window - to produce a set of training input/output pairs to tackle the problem of time series prediction by treating the problem as one of regression . So what sort of problem do we have here now, with text generation? Well, the time series prediction was a regression problem because the output (one value of the time series) was a continuous value. Here - for character-by-character text generation - each output is a single character . This isn't a continuous value - but a distinct class - therefore character-by-character text generation is a classification problem . How many classes are there in the data? Well, the number of classes is equal to the number of unique characters we have to predict! How many of those were there in our dataset again? Lets print out the value again. # print out the number of unique characters in the dataset chars = sorted ( list ( set (text))) print ( \"this corpus has \" + str ( len (chars)) + \" unique characters\" ) print ( 'and these characters are ' ) print (chars) this corpus has 33 unique characters and these characters are [' ', '!', ',', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] Rockin' - so we have a multiclass classification problem on our hands!","title":"2.4  Wait, what kind of problem is text generation again?"},{"location":"dl/rnn/RNN_project/#25-one-hot-encoding-characters","text":"The last issue we have to deal with is representing our text data as numerical data so that we can use it as an input to a neural network. One of the conceptually simplest ways of doing this is via a 'one-hot encoding' scheme. Here's how it works. We transform each character in our inputs/outputs into a vector with length equal to the number of unique characters in our text. This vector is all zeros except one location where we place a 1 - and this location is unique to each character type. e.g., we transform 'a', 'b', and 'c' as follows $$a\\longleftarrow\\left[\\begin{array}{c} 1\\ 0\\ 0\\ \\vdots\\ 0\\ 0 \\end{array}\\right]\\,\\,\\,\\,\\,\\,\\,b\\longleftarrow\\left[\\begin{array}{c} 0\\ 1\\ 0\\ \\vdots\\ 0\\ 0 \\end{array}\\right]\\,\\,\\,\\,\\,c\\longleftarrow\\left[\\begin{array}{c} 0\\ 0\\ 1\\ \\vdots\\ 0\\ 0 \\end{array}\\right]\\cdots$$ where each vector has 32 entries (or in general: number of entries = number of unique characters in text). The first practical step towards doing this one-hot encoding is to form a dictionary mapping each unique character to a unique integer, and one dictionary to do the reverse mapping. We can then use these dictionaries to quickly make our one-hot encodings, as well as re-translate (from integers to characters) the results of our trained RNN classification model. # this dictionary is a function mapping each unique character to a unique integer chars_to_indices = dict ((c, i) for i, c in enumerate (chars)) # map each unique character to unique integer # this dictionary is a function mapping each unique integer back to a unique character indices_to_chars = dict ((i, c) for i, c in enumerate (chars)) # map each unique integer back to unique character Now we can transform our input/output pairs - consisting of characters - to equivalent input/output pairs made up of one-hot encoded vectors. In the next cell we provide a function for doing just this: it takes in the raw character input/outputs and returns their numerical versions. In particular the numerical input is given as $\\bf{X}$, and numerical output is given as the $\\bf{y}$ # transform character-based input/output into equivalent numerical versions def encode_io_pairs (text,window_size,step_size): # number of unique chars chars = sorted ( list ( set (text))) num_chars = len (chars) # cut up text into character input/output pairs inputs, outputs = window_transform_text(text,window_size,step_size) # create empty vessels for one-hot encoded input/output X = np . zeros(( len (inputs), window_size, num_chars), dtype = np . bool) y = np . zeros(( len (inputs), num_chars), dtype = np . bool) # loop over inputs/outputs and transform and store in X/y for i, sentence in enumerate (inputs): for t, char in enumerate (sentence): X[i, t, chars_to_indices[char]] = 1 y[i, chars_to_indices[outputs[i]]] = 1 return X,y Now run the one-hot encoding function by activating the cell below and transform our input/output pairs! # use your function window_size = 100 step_size = 5 X,y = encode_io_pairs(text,window_size,step_size)","title":"2.5  One-hot encoding characters"},{"location":"dl/rnn/RNN_project/#26-setting-up-our-rnn","text":"With our dataset loaded and the input/output pairs extracted / transformed we can now begin setting up our RNN for training. Again we will use Keras to quickly build a single hidden layer RNN - where our hidden layer consists of LSTM modules. Time to get to work: build a 3 layer RNN model of the following specification layer 1 should be an LSTM module with 200 hidden units --> note this should have input_shape = (window_size,len(chars)) where len(chars) = number of unique characters in your cleaned text layer 2 should be a linear module, fully connected, with len(chars) hidden units --> where len(chars) = number of unique characters in your cleaned text layer 3 should be a softmax activation ( since we are solving a multiclass classification ) Use the categorical_crossentropy loss This network can be constructed using just a few lines - as with the RNN network you made in part 1 of this notebook. See e.g., the general Keras documentation and the LSTM documentation in particular for examples of how to quickly use Keras to build neural network models. ### necessary functions from the keras library from keras.models import Sequential from keras.layers import Dense, Activation, LSTM from keras.optimizers import RMSprop from keras.utils.data_utils import get_file import keras import random # TODO implement build_part2_RNN in my_answers.py from my_answers import build_part2_RNN model = build_part2_RNN(window_size, len (chars)) # initialize optimizer optimizer = keras . optimizers . RMSprop(lr = 0.001 , rho = 0.9 , epsilon = 1e-08 , decay = 0.0 ) # compile model --> make sure initialized optimizer and callbacks - as defined above - are used model . compile(loss = 'categorical_crossentropy' , optimizer = optimizer)","title":"2.6 Setting up our RNN"},{"location":"dl/rnn/RNN_project/#27-training-our-rnn-model-for-text-generation","text":"With our RNN setup we can now train it! Lets begin by trying it out on a small subset of the larger version. In the next cell we take the first 10,000 input/output pairs from our training database to learn on. # a small subset of our input/output pairs Xsmall = X[: 10000 ,:,:] ysmall = y[: 10000 ,:] Now lets fit our model! # train the model model . fit(Xsmall, ysmall, batch_size = 500 , epochs = 40 ,verbose = 1 ) # save weights model . save_weights( 'model_weights/best_RNN_small_textdata_weights.hdf5' ) Epoch 1/40 10000/10000 [==============================] - 4s - loss: 3.0264 Epoch 2/40 10000/10000 [==============================] - 4s - loss: 2.8770 Epoch 3/40 10000/10000 [==============================] - 4s - loss: 2.8521 Epoch 4/40 10000/10000 [==============================] - 4s - loss: 2.8178 Epoch 5/40 10000/10000 [==============================] - 4s - loss: 2.7683 Epoch 6/40 10000/10000 [==============================] - 4s - loss: 2.7088 Epoch 7/40 10000/10000 [==============================] - 4s - loss: 2.6444 Epoch 8/40 10000/10000 [==============================] - 4s - loss: 2.5756 Epoch 9/40 10000/10000 [==============================] - 4s - loss: 2.5201 Epoch 10/40 10000/10000 [==============================] - 4s - loss: 2.4693 Epoch 11/40 10000/10000 [==============================] - 4s - loss: 2.4174 Epoch 12/40 10000/10000 [==============================] - 4s - loss: 2.3831 Epoch 13/40 10000/10000 [==============================] - 4s - loss: 2.3462 Epoch 14/40 10000/10000 [==============================] - 4s - loss: 2.3109 Epoch 15/40 10000/10000 [==============================] - 4s - loss: 2.2901 Epoch 16/40 10000/10000 [==============================] - 4s - loss: 2.2616 Epoch 17/40 10000/10000 [==============================] - 4s - loss: 2.2389 Epoch 18/40 10000/10000 [==============================] - 4s - loss: 2.2193 Epoch 19/40 10000/10000 [==============================] - 4s - loss: 2.1940 Epoch 20/40 10000/10000 [==============================] - 4s - loss: 2.1783 Epoch 21/40 10000/10000 [==============================] - 4s - loss: 2.1544 Epoch 22/40 10000/10000 [==============================] - 4s - loss: 2.1445 Epoch 23/40 10000/10000 [==============================] - 4s - loss: 2.1245 Epoch 24/40 10000/10000 [==============================] - 4s - loss: 2.0990 Epoch 25/40 10000/10000 [==============================] - 4s - loss: 2.0909 Epoch 26/40 10000/10000 [==============================] - 4s - loss: 2.0710 Epoch 27/40 10000/10000 [==============================] - 4s - loss: 2.0544 Epoch 28/40 10000/10000 [==============================] - 4s - loss: 2.0342 Epoch 29/40 10000/10000 [==============================] - 4s - loss: 2.0137 Epoch 30/40 10000/10000 [==============================] - 4s - loss: 2.0040 Epoch 31/40 10000/10000 [==============================] - 4s - loss: 1.9761 Epoch 32/40 10000/10000 [==============================] - 4s - loss: 1.9636 Epoch 33/40 10000/10000 [==============================] - 4s - loss: 1.9472 Epoch 34/40 10000/10000 [==============================] - 4s - loss: 1.9373 Epoch 35/40 10000/10000 [==============================] - 4s - loss: 1.9106 Epoch 36/40 10000/10000 [==============================] - 4s - loss: 1.8947 Epoch 37/40 10000/10000 [==============================] - 4s - loss: 1.8731 Epoch 38/40 10000/10000 [==============================] - 4s - loss: 1.8560 Epoch 39/40 10000/10000 [==============================] - 4s - loss: 1.8363 Epoch 40/40 10000/10000 [==============================] - 4s - loss: 1.8157 How do we make a given number of predictions (characters) based on this fitted model? First we predict the next character after following any chunk of characters in the text of length equal to our chosen window size. Then we remove the first character in our input sequence and tack our prediction onto the end. This gives us a slightly changed sequence of inputs that still has length equal to the size of our window. We then feed in this updated input sequence into the model to predict the another character. Together then we have two predicted characters following our original input sequence. Repeating this process N times gives us N predicted characters. In the next Python cell we provide you with a completed function that does just this - it makes predictions when given a) a trained RNN model, b) a subset of (window_size) characters from the text, and c) a number of characters to predict (to follow our input subset). # function that uses trained model to predict a desired number of future characters def predict_next_chars (model,input_chars,num_to_predict): # create output predicted_chars = '' for i in range (num_to_predict): # convert this round's predicted characters to numerical input x_test = np . zeros(( 1 , window_size, len (chars))) for t, char in enumerate (input_chars): x_test[ 0 , t, chars_to_indices[char]] = 1. # make this round's prediction test_predict = model . predict(x_test,verbose = 0 )[ 0 ] # translate numerical prediction back to characters r = np . argmax(test_predict) # predict class of each test input d = indices_to_chars[r] # update predicted_chars and input predicted_chars += d input_chars += d input_chars = input_chars[ 1 :] return predicted_chars With your trained model try a few subsets of the complete text as input - note the length of each must be exactly equal to the window size. For each subset use the function above to predict the next 100 characters that follow each input. # TODO: choose an input sequence and use the prediction function in the previous Python cell to predict 100 characters following it # get an appropriately sized chunk of characters from the text start_inds = [] # load in weights model . load_weights( 'model_weights/best_RNN_small_textdata_weights.hdf5' ) for s in start_inds: start_index = s input_chars = text[start_index: start_index + window_size] # use the prediction function predict_input = predict_next_chars(model,input_chars,num_to_predict = 100 ) # print out input characters print ( '------------------' ) input_line = 'input chars = ' + ' \\n ' + input_chars + '\"' + ' \\n ' print (input_line) # print out predicted characters line = 'predicted chars = ' + ' \\n ' + predict_input + '\"' + ' \\n ' print (line) This looks ok, but not great. Now lets try the same experiment with a larger chunk of the data - with the first 100,000 input/output pairs. Tuning RNNs for a typical character dataset like the one we will use here is a computationally intensive endeavour and thus timely on a typical CPU. Using a reasonably sized cloud-based GPU can speed up training by a factor of 10. Also because of the long training time it is highly recommended that you carefully write the output of each step of your process to file. This is so that all of your results are saved even if you close the web browser you're working out of, as the processes will continue processing in the background but variables/output in the notebook system will not update when you open it again. In the next cell we show you how to create a text file in Python and record data to it. This sort of setup can be used to record your final predictions. ### A simple way to write output to file f = open ( 'my_test_output.txt' , 'w' ) # create an output file to write too f . write( 'this is only a test ' + ' \\n ' ) # print some output text x = 2 f . write( 'the value of x is ' + str (x) + ' \\n ' ) # record a variable value f . close() # print out the contents of my_test_output.txt f = open ( 'my_test_output.txt' , 'r' ) # create an output file to write too f . read() 'this is only a test \\nthe value of x is 2\\n' With this recording devices we can now more safely perform experiments on larger portions of the text. In the next cell we will use the first 100,000 input/output pairs to train our RNN model. First we fit our model to the dataset, then generate text using the trained model in precisely the same generation method applied before on the small dataset. Note: your generated words should be - by and large - more realistic than with the small dataset, but you won't be able to generate perfect English sentences even with this amount of data. A rule of thumb: your model is working well if you generate sentences that largely contain real English words. # a small subset of our input/output pairs Xlarge = X[: 100000 ,:,:] ylarge = y[: 100000 ,:] # TODO: fit to our larger dataset model . fit(Xlarge, ylarge, batch_size = 500 , epochs = 30 , verbose = 1 ) # save weights model . save_weights( 'model_weights/best_RNN_large_textdata_weights.hdf5' ) Epoch 1/30 100000/100000 [==============================] - 44s - loss: 2.0428 Epoch 2/30 100000/100000 [==============================] - 44s - loss: 1.9557 Epoch 3/30 100000/100000 [==============================] - 44s - loss: 1.8930 Epoch 4/30 100000/100000 [==============================] - 44s - loss: 1.8399 Epoch 5/30 100000/100000 [==============================] - 44s - loss: 1.7932 Epoch 6/30 100000/100000 [==============================] - 44s - loss: 1.7499 Epoch 7/30 100000/100000 [==============================] - 44s - loss: 1.7120 Epoch 8/30 100000/100000 [==============================] - 44s - loss: 1.6755 Epoch 9/30 100000/100000 [==============================] - 44s - loss: 1.6398 Epoch 10/30 100000/100000 [==============================] - 44s - loss: 1.6081 Epoch 11/30 100000/100000 [==============================] - 44s - loss: 1.5772 Epoch 12/30 100000/100000 [==============================] - 44s - loss: 1.5465 Epoch 13/30 100000/100000 [==============================] - 44s - loss: 1.5174 Epoch 14/30 100000/100000 [==============================] - 44s - loss: 1.4885 Epoch 15/30 100000/100000 [==============================] - 44s - loss: 1.4604 Epoch 16/30 100000/100000 [==============================] - 44s - loss: 1.4321 Epoch 17/30 100000/100000 [==============================] - 44s - loss: 1.4041 Epoch 18/30 100000/100000 [==============================] - 44s - loss: 1.3777 Epoch 19/30 100000/100000 [==============================] - 44s - loss: 1.3497 Epoch 20/30 100000/100000 [==============================] - 44s - loss: 1.3232 Epoch 21/30 100000/100000 [==============================] - 44s - loss: 1.2956 Epoch 22/30 100000/100000 [==============================] - 44s - loss: 1.2689 Epoch 23/30 100000/100000 [==============================] - 44s - loss: 1.2422 Epoch 24/30 100000/100000 [==============================] - 44s - loss: 1.2157 Epoch 25/30 100000/100000 [==============================] - 44s - loss: 1.1894 Epoch 26/30 100000/100000 [==============================] - 44s - loss: 1.1626 Epoch 27/30 100000/100000 [==============================] - 44s - loss: 1.1381 Epoch 28/30 100000/100000 [==============================] - 44s - loss: 1.1118 Epoch 29/30 100000/100000 [==============================] - 44s - loss: 1.0867 Epoch 30/30 100000/100000 [==============================] - 44s - loss: 1.0611 # TODO: choose an input sequence and use the prediction function in the previous Python cell to predict 100 characters following it # get an appropriately sized chunk of characters from the text start_inds = [ 100 , 300 , 500 , 700 , 900 , 1100 , 1300 , 1500 ] # save output f = open ( 'text_gen_output/RNN_large_textdata_output.txt' , 'w' ) # create an output file to write too # load weights model . load_weights( 'model_weights/best_RNN_large_textdata_weights.hdf5' ) for s in start_inds: start_index = s input_chars = text[start_index: start_index + window_size] # use the prediction function predict_input = predict_next_chars(model,input_chars,num_to_predict = 100 ) # print out input characters line = '-------------------' + ' \\n ' print (line) f . write(line) input_line = 'input chars = ' + ' \\n ' + input_chars + '\"' + ' \\n ' print (input_line) f . write(input_line) # print out predicted characters predict_line = 'predicted chars = ' + ' \\n ' + predict_input + '\"' + ' \\n ' print (predict_line) f . write(predict_line) f . close() ------------------- input chars = the whole of her sex. it was not that he felt any emotion akin to love for irene adler. all emotion\" predicted chars = s to the door and had betneed to the dear but sime. there was a small before and a smilion of his h\" ------------------- input chars = s, i take it, the most perfect reasoning and observing machine that the world has seen, but as a lov\" predicted chars = e the hed of the door and a wead, farried, you be a fall the with the side. that is wall go to the m\" ------------------- input chars = h a gibe and a sneer. they were admirable things for the observerexcellent for drawing the veil from\" predicted chars = his eater from his sceathong and the strom in the head of a fellow sourge and losk in the attertion\" ------------------- input chars = te and finely adjusted temperament was to introduce a distracting factor which might throw a doubt u\" predicted chars = pon the door and a wainer, and he said he had been could to froom the offer planss of his suchans an\" ------------------- input chars = enses, would not be more disturbing than a strong emotion in a nature such as his. and yet there was\" predicted chars = a sight and then seadent possible that the ress of sherl of the door and a with a consid and should\" ------------------- input chars = i had seen little of holmes lately. my marriage had drifted us away from each other. my own complet\" predicted chars = ely remarted in the door be the ore of the door of resurest that the tore in our have some have a su\" ------------------- input chars = er of his own establishment, were sufficient to absorb all my attention, while holmes, who loathed e\" predicted chars = ven dead the sounders, and he seet her little to be so down that the strong of the door and a weadli\" ------------------- input chars = among his old books, and alternating from week to week between cocaine and ambition, the drowsiness \" predicted chars = of the door and a wead, from the coolman was he had told me out if his with her looked and lead the \"","title":"2.7  Training our RNN model for text generation"},{"location":"dl/stacked/2.Stacked-LSTM/","text":"2. Stacked LSTM from math import sin from math import pi from math import exp from random import random from random import randint from random import uniform from numpy import array from matplotlib import pyplot import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline Sample sequence length = 100 freq = 5 sequence = [sin( 2 * pi * freq * (i / length)) for i in range (length)] Plot sequence pyplot . plot(sequence) pyplot . show() With Damping length = 100 period = 10 decay = 0.05 sequence = [ 0.5 + 0.5 * sin( 2 * pi * i / period) \\ * exp( - decay * i) for i in range (length)] pyplot . plot(sequence) pyplot . show() Sequence Generator Generate damped sine wave in [0,1] def generate_sequence (length, period, decay): return [ 0.5 + 0.5 * sin( 2 * pi * i / period) * \\ exp( - decay * i) for i in range (length)] Generate input and output pairs of damped sine waves def generate_examples (length, n_patterns, output): X, y = list (), list () for _ in range (n_patterns): p = randint( 10 , 20 ) d = uniform( 0.01 , 0.1 ) sequence = generate_sequence(length + output, p, d) X . append(sequence[: - output]) y . append(sequence[ - output:]) X = array(X) . reshape(n_patterns, length, 1 ) y = array(y) . reshape(n_patterns, output) return X, y test problem generation plt . figure(figsize = ( 12 , 12 )) X, y = generate_examples( 100 , 20 , 5 ) for i in range ( len (X)): pyplot . plot([x for x in X[i, :, 0 ]] + [x for x in y[i]], '-o' ) pyplot . show() Model # Example of one output for whole sequence from keras.models import Sequential from keras.layers import LSTM,Dense # configure problem length = 50 output = 5 # define model model = Sequential() model . add(LSTM( 20 , return_sequences = True , input_shape = (length, 1 ))) model . add(LSTM( 20 , return_sequences = True )) model . add(LSTM( 20 , return_sequences = True )) model . add(LSTM( 20 )) model . add(Dense(output)) # compile model . compile(loss = 'mae' , optimizer = 'adam' ) print (model . summary()) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm_13 (LSTM) (None, 50, 20) 1760 _________________________________________________________________ lstm_14 (LSTM) (None, 50, 20) 3280 _________________________________________________________________ lstm_15 (LSTM) (None, 50, 20) 3280 _________________________________________________________________ lstm_16 (LSTM) (None, 20) 3280 _________________________________________________________________ dense_4 (Dense) (None, 5) 105 ================================================================= Total params: 11,705 Trainable params: 11,705 Non-trainable params: 0 _________________________________________________________________ None Fit the model X, y = generate_examples(length, 500 , output) model . fit(X, y, batch_size = 10 , epochs = 5 , verbose = 2 ) Epoch 1/5 20s - loss: 0.0586 Epoch 2/5 20s - loss: 0.0413 Epoch 3/5 21s - loss: 0.0403 Epoch 4/5 20s - loss: 0.0400 Epoch 5/5 20s - loss: 0.0419 &lt;keras.callbacks.History at 0x12cabab70&gt; Evaluate the model # evaluate model X, y = generate_examples(length, 100 , output) loss = model . evaluate(X, y, verbose = 0 ) print ( 'MAE: %f ' % loss) MAE: 0.050917 Prediction # prediction on new data X, y = generate_examples(length, 20 , output) yhat = model . predict(X, verbose = 0 ) pyplot . plot(y[ 0 ], label = 'y' ) pyplot . plot(yhat[ 0 ], label = 'yhat' ) pyplot . legend() pyplot . show()","title":"Stacked LSTM"},{"location":"dl/stacked/2.Stacked-LSTM/#2-stacked-lstm","text":"from math import sin from math import pi from math import exp from random import random from random import randint from random import uniform from numpy import array from matplotlib import pyplot import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline","title":"2. Stacked LSTM"},{"location":"dl/stacked/2.Stacked-LSTM/#sample-sequence","text":"length = 100 freq = 5 sequence = [sin( 2 * pi * freq * (i / length)) for i in range (length)] Plot sequence pyplot . plot(sequence) pyplot . show() With Damping length = 100 period = 10 decay = 0.05 sequence = [ 0.5 + 0.5 * sin( 2 * pi * i / period) \\ * exp( - decay * i) for i in range (length)] pyplot . plot(sequence) pyplot . show()","title":"Sample sequence"},{"location":"dl/stacked/2.Stacked-LSTM/#sequence-generator","text":"Generate damped sine wave in [0,1] def generate_sequence (length, period, decay): return [ 0.5 + 0.5 * sin( 2 * pi * i / period) * \\ exp( - decay * i) for i in range (length)] Generate input and output pairs of damped sine waves def generate_examples (length, n_patterns, output): X, y = list (), list () for _ in range (n_patterns): p = randint( 10 , 20 ) d = uniform( 0.01 , 0.1 ) sequence = generate_sequence(length + output, p, d) X . append(sequence[: - output]) y . append(sequence[ - output:]) X = array(X) . reshape(n_patterns, length, 1 ) y = array(y) . reshape(n_patterns, output) return X, y test problem generation plt . figure(figsize = ( 12 , 12 )) X, y = generate_examples( 100 , 20 , 5 ) for i in range ( len (X)): pyplot . plot([x for x in X[i, :, 0 ]] + [x for x in y[i]], '-o' ) pyplot . show()","title":"Sequence Generator"},{"location":"dl/stacked/2.Stacked-LSTM/#model","text":"# Example of one output for whole sequence from keras.models import Sequential from keras.layers import LSTM,Dense # configure problem length = 50 output = 5 # define model model = Sequential() model . add(LSTM( 20 , return_sequences = True , input_shape = (length, 1 ))) model . add(LSTM( 20 , return_sequences = True )) model . add(LSTM( 20 , return_sequences = True )) model . add(LSTM( 20 )) model . add(Dense(output)) # compile model . compile(loss = 'mae' , optimizer = 'adam' ) print (model . summary()) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm_13 (LSTM) (None, 50, 20) 1760 _________________________________________________________________ lstm_14 (LSTM) (None, 50, 20) 3280 _________________________________________________________________ lstm_15 (LSTM) (None, 50, 20) 3280 _________________________________________________________________ lstm_16 (LSTM) (None, 20) 3280 _________________________________________________________________ dense_4 (Dense) (None, 5) 105 ================================================================= Total params: 11,705 Trainable params: 11,705 Non-trainable params: 0 _________________________________________________________________ None","title":"Model"},{"location":"dl/stacked/2.Stacked-LSTM/#fit-the-model","text":"X, y = generate_examples(length, 500 , output) model . fit(X, y, batch_size = 10 , epochs = 5 , verbose = 2 ) Epoch 1/5 20s - loss: 0.0586 Epoch 2/5 20s - loss: 0.0413 Epoch 3/5 21s - loss: 0.0403 Epoch 4/5 20s - loss: 0.0400 Epoch 5/5 20s - loss: 0.0419 &lt;keras.callbacks.History at 0x12cabab70&gt;","title":"Fit the model"},{"location":"dl/stacked/2.Stacked-LSTM/#evaluate-the-model","text":"# evaluate model X, y = generate_examples(length, 100 , output) loss = model . evaluate(X, y, verbose = 0 ) print ( 'MAE: %f ' % loss) MAE: 0.050917","title":"Evaluate the model"},{"location":"dl/stacked/2.Stacked-LSTM/#prediction","text":"# prediction on new data X, y = generate_examples(length, 20 , output) yhat = model . predict(X, verbose = 0 ) pyplot . plot(y[ 0 ], label = 'y' ) pyplot . plot(yhat[ 0 ], label = 'yhat' ) pyplot . legend() pyplot . show()","title":"Prediction"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/","text":"Self-Driving Car Engineer Nanodegree Deep Learning Project: Build a Traffic Sign Recognition Classifier In this notebook, a template is provided for you to implement your functionality in stages, which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission if necessary. Note : Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to \\n\", \" File -> Download as -> HTML (.html) . Include the finished document along with this notebook as your submission. In addition to implementing code, there is a writeup to complete. The writeup should be completed in a separate file, which can be either a markdown file or a pdf document. There is a write up template that can be used to guide the writing process. Completing the code template and writeup template will cover all of the rubric points for this project. The rubric contains \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. The stand out suggestions are optional. If you decide to pursue the \"stand out suggestions\", you can include the code in this Ipython notebook and also discuss the results in the writeup file. Note: Code and Markdown cells can be executed using the Shift + Enter keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode. Step 0: Load The Data # Load pickled data import pickle # Importing my own choice import numpy as np import tensorflow as tf from sklearn.utils import shuffle # TODO: Fill this in based on where you saved the training and testing data training_file = 'traffic-signs-data/train.p' testing_file = 'traffic-signs-data/test.p' with open (training_file, mode = 'rb' ) as f: train = pickle . load(f) with open (testing_file, mode = 'rb' ) as f: test = pickle . load(f) X_train, y_train = train[ 'features' ], train[ 'labels' ] X_test, y_test = test[ 'features' ], test[ 'labels' ] print () print ( \"Image Shape: {}\" . format(X_train[ 0 ] . shape)) print () print ( \"Training Set: {} samples\" . format( len (X_train))) print ( \"Test Set: {} samples\" . format( len (X_test))) Image Shape: (32, 32, 3) Training Set: 34799 samples Test Set: 12630 samples Step 1: Dataset Summary & Exploration The pickled data is a dictionary with 4 key/value pairs: 'features' is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels). 'labels' is a 1D array containing the label/class id of the traffic sign. The file signnames.csv contains id -> name mappings for each id. 'sizes' is a list containing tuples, (width, height) representing the original width and height the image. 'coords' is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES Complete the basic data summary below. Use python, numpy and/or pandas methods to calculate the data summary rather than hard coding the results. For example, the pandas shape method might be useful for calculating some of the summary results. Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas ### Replace each question mark with the appropriate value. ### Use python, pandas or numpy methods rather than hard coding the results # TODO: Number of training examples n_train = X_train . shape[ 0 ] # TODO: Number of testing examples. n_test = X_test . shape[ 0 ] # TODO: What's the shape of an traffic sign image? image_shape = (X_train . shape[ 1 ], X_train . shape[ 2 ], X_train . shape[ 3 ]) # TODO: How many unique classes/labels there are in the dataset. n_classes = y_train . max() + 1 print ( \"Number of training examples =\" , n_train) print ( \"Number of testing examples =\" , n_test) print ( \"Image data shape =\" , image_shape) print ( \"Number of classes =\" , n_classes) Number of training examples = 34799 Number of testing examples = 12630 Image data shape = (32, 32, 3) Number of classes = 43 Include an exploratory visualization of the dataset Visualize the German Traffic Signs Dataset using the pickled file(s). This is open ended, suggestions include: plotting traffic sign images, plotting the count of each sign, etc. The Matplotlib examples and gallery pages are a great resource for doing visualizations in Python. NOTE: It's recommended you start with something simple first. If you wish to do more, come back to it after you've completed the rest of the sections. ### Data exploration visualization code goes here. ### Feel free to use as many code cells as needed. import matplotlib.pyplot as plt from random import randint # Visualizations will be shown in the notebook. % matplotlib inline num_of_samples = [] plt . figure(figsize = ( 12 , 16.5 )) for i in range ( 0 , n_classes): plt . subplot( 11 , 4 , i + 1 ) x_selected = X_train[y_train == i] plt . imshow(x_selected[ 0 , :, :, :]) #draw the first image of each class plt . title(i) plt . axis( 'off' ) num_of_samples . append( len (x_selected)) plt . show() Plot number of images per class ### Data exploration visualization code goes here. ### Feel free to use as many code cells as needed. import matplotlib.pyplot as plt from random import randint % matplotlib inline #Plot number of images per class plt . figure(figsize = ( 12 , 4 )) plt . bar( range ( 0 , n_classes), num_of_samples) plt . title( \"Distribution of the train dataset\" ) plt . xlabel( \"Class number\" ) plt . ylabel( \"Number of images\" ) plt . show() print ( \"Min number of images per class =\" , min (num_of_samples)) print ( \"Max number of images per class =\" , max (num_of_samples)) Min number of images per class = 180 Max number of images per class = 2010 Step 2: Design and Test a Model Architecture Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the German Traffic Sign Dataset . The LeNet-5 implementation shown in the classroom at the end of the CNN lesson is a solid starting point. You'll have to change the number of classes and possibly the preprocessing, but aside from that it's plug and play! With the LeNet-5 solution from the lecture, you should expect a validation set accuracy of about 0.89. To meet specifications, the validation set accuracy will need to be at least 0.93. It is possible to get an even higher accuracy, but 0.93 is the minimum for a successful project submission. There are various aspects to consider when thinking about this problem: Neural network architecture (is the network over or underfitting?) Play around preprocessing techniques (normalization, rgb to grayscale, etc) Number of examples per label (some have more than others). Generate fake data. Here is an example of a published baseline model on this problem . It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these. 2.1: Preprocess the Data Set (normalization, grayscale, etc.) Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. 2.1.1: Preprocessing Function ### Preprocess the data here. Preprocessing steps could include normalization, converting to grayscale, etc. ### Feel free to use as many code cells as needed. import cv2 from numpy import newaxis # Iterates through grayscale for each image in the data def gray_maker (data): gray_images = [] for image in data: gray = cv2 . cvtColor(image, cv2 . COLOR_BGR2GRAY) gray_images . append(gray) return np . array(gray_images) def preprocess (data,data_name,verbose): if verbose: print ( 'Preprocessing ' + data_name + '...' ) # Iterate through grayscale data = gray_maker(data) data = data[ ... , newaxis] #Normalizes the data between 0.1 and 0.9 instead of 0 to 255 data = data / 255 * 0.8 + 0.1 if verbose: print ( 'Finished preprocessing ' + data_name + '...' ) # Double-check that the image is changed to depth of 1 image_shape = data . shape if verbose: print ( 'Processed ' + data_name + ' shape =' , image_shape) print ( \" \" ) return data 2.1.2: Preprocessing all data X_train = preprocess(X_train, 'train_data' ,verbose = True ) X_test = preprocess(X_test, 'test_data' ,verbose = True ) Preprocessing train_data... Finished preprocessing train_data... Processed train_data shape = (34799, 32, 32, 1) Preprocessing test_data... Finished preprocessing test_data... Processed test_data shape = (12630, 32, 32, 1) 2.1.3: After Preprocessing # Visualizations will be shown in the notebook. % matplotlib inline num_of_samples = [] plt . figure(figsize = ( 12 , 16.5 )) for i in range ( 0 , n_classes): plt . subplot( 11 , 4 , i + 1 ) x_selected_grey = X_train[y_train == i] #draw the first image of each class plt . imshow(x_selected_grey[ 0 , :, :, 0 ], cmap = 'gray' ) plt . title(i) plt . axis( 'off' ) num_of_samples . append( len (x_selected)) plt . show() 2.1.4: Generate fake data I will generate some additional data, then split the data in a later cell. This is to help with the issue identified in the original histogram ### Generate fake data from scipy import ndimage import random # min_desired below is just mean_pics but wanted to # make the code below easier to distinguish def fake_data_generator (X,y,verbose): '''X = feature data , y = label data''' pics_per_class = np . bincount(y) mean_pics = int (np . mean(pics_per_class)) if verbose: print ( 'Generating new data.' ) # Angles to be used to rotate images in additional data made angles = [ - 10 , 10 , - 15 , 15 , - 20 , 20 ] # Iterate through each class for i in range ( len (pics_per_class)): # Check if less data than the mean if pics_per_class[i] < mean_pics: # Count how many additional pictures we want new_wanted = mean_pics - pics_per_class[i] picture = np . where(y == i) more_X = [] more_y = [] # Make the number of additional pictures needed to arrive at the mean for num in range (new_wanted): # Rotate images and append new ones to more_X, append the class to more_y more_X . append(ndimage . rotate(X[picture][random . randint( 0 ,pics_per_class[i] - 1 )],\\ random . choice(angles), reshape = False )) more_y . append(i) # Append the pictures generated for each class back to the original data X = np . append(X, np . array(more_X), axis = 0 ) y = np . append(y, np . array(more_y), axis = 0 ) if verbose: print ( 'Additional data generated. Any classes lacking data now have' ,mean_pics, 'pictures.' ) return X,y X_train, y_train = fake_data_generator(X_train,y_train,verbose = True ) Generating new data. Additional data generated. Any classes lacking data now have 809 pictures. 2.1.5: Histogram representing data distribution in all classes plt . hist(y_train, bins = n_classes) updated_n_train = len (X_train) print ( \"The updated number of training examples =\" , updated_n_train) The updated number of training examples = 46714 Data summary n_train = X_train . shape[ 0 ] n_test = X_test . shape[ 0 ] print ( \"Number of training examples =\" , n_train) print ( \"Number of testing examples =\" , n_test) print ( \"Extra data generated =\" ,n_train - 34799 ) Number of training examples = 46714 Number of testing examples = 12630 Extra data generated = 11915 Splitting Train and Validation data from sklearn.model_selection import train_test_split # shuffleing data X_train, y_train = shuffle(X_train, y_train) # For each epoch, there are separate training data and validation data X_train, X_valid, y_train, y_valid\\ = train_test_split(X_train, y_train,\\ stratify = y_train,\\ test_size = 0.1 ,\\ random_state = 23 ) n_train = X_train . shape[ 0 ] n_valid = X_valid . shape[ 0 ] print ( \"Number of training examples =\" , n_train) print ( \"Number of validation examples =\" , n_valid) Number of training examples = 42042 Number of validation examples = 4672 2.2 : Model Architecture 2.2.1: Neural Network Function ### Define your architecture here. ### Feel free to use as many code cells as needed. # The below is only necessary to reset if the notebook has not been shutdown tf . reset_default_graph() from tensorflow.contrib.layers import flatten def neuralNetwork (x): # Hyperparameters mu = 0 sigma = 0.1 #============================================================== # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6. # Weight and bias # If not using grayscale, the third number in shape would be 3 c1_weight = tf . Variable(tf . truncated_normal(shape = ( 5 , 5 , 1 , 6 ),\\ mean = mu,\\ stddev = sigma)) c1_bias = tf . Variable(tf . zeros( 6 )) # Apply convolution conv_layer1 = tf . nn . conv2d(x, c1_weight,\\ strides = [ 1 , 1 , 1 , 1 ],\\ padding = 'VALID' )\\ + c1_bias # Activation for layer 1 conv_layer1 = tf . nn . relu(conv_layer1) # Pooling. Input = 28x28x6. Output = 14x14x6. conv_layer1 = tf . nn . avg_pool(conv_layer1,\\ ksize = [ 1 , 2 , 2 , 1 ],\\ strides = [ 1 , 2 , 2 , 1 ],\\ padding = 'VALID' ) #================================================================ # Layer 2: Convolutional. Output = 10x10x16. # Note: The second layer is implemented the exact same as layer one, # with layer 1 as input instead of x And then of course changing the #numbers to fit the desired ouput of 10x10x16 Weight and bias c2_weight = tf . Variable(tf . truncated_normal(shape = ( 5 , 5 , 6 , 16 ),\\ mean = mu,\\ stddev = sigma)) c2_bias = tf . Variable(tf . zeros( 16 )) # Apply convolution for layer 2 conv_layer2 = tf . nn . conv2d(conv_layer1, c2_weight,\\ strides = [ 1 , 1 , 1 , 1 ],\\ padding = 'VALID' ) + c2_bias # Activation for layer 2 conv_layer2 = tf . nn . relu(conv_layer2) # Pooling. Input = 10x10x16. Output = 5x5x16. conv_layer2 = tf . nn . avg_pool(conv_layer2,\\ ksize = [ 1 , 2 , 2 , 1 ],\\ strides = [ 1 , 2 , 2 , 1 ],\\ padding = 'VALID' ) # Flatten to get to fully connected layers. Input = 5x5x16. Output = 400. flat = tf . contrib . layers . flatten(conv_layer2) #=============================================================== # Layer 3: Fully Connected. Input = 400. Output = 120. # Although this is fully connected, the weights and biases still are implemented similarly # There is no filter this time, so shape only takes input and output # Weight and bias fc1_weight = tf . Variable(tf . truncated_normal(shape = ( 400 , 200 ),\\ mean = mu,\\ stddev = sigma)) fc1_bias = tf . Variable(tf . zeros( 200 )) # Here is the main change versus a convolutional layer - matrix multiplication instead of 2D convolution fc1 = tf . matmul(flat, fc1_weight) + fc1_bias # Activation for the first fully connected layer. # Same thing as before fc1 = tf . nn . relu(fc1) # Dropout, to prevent overfitting fc1 = tf . nn . dropout(fc1, keep_prob) #================================================================== # Layer 4: Fully Connected. Input = 120. Output = 84. # Same as the fc1 layer, just with updated output numbers fc2_weight = tf . Variable(tf . truncated_normal(shape = ( 200 , 100 ),\\ mean = mu,\\ stddev = sigma)) fc2_bias = tf . Variable(tf . zeros( 100 )) # Again, matrix multiplication fc2 = tf . matmul(fc1, fc2_weight) + fc2_bias # Activation. fc2 = tf . nn . relu(fc2) # Dropout fc2 = tf . nn . dropout(fc2, keep_prob) #======================================================== # Layer 5 Fully Connected. Input = 84. Output = 43. # Since this is the final layer, output needs to match up with the number of classes fc3_weight = tf . Variable(tf . truncated_normal(shape = ( 100 , 43 ),\\ mean = mu, \\ stddev = sigma)) fc3_bias = tf . Variable(tf . zeros( 43 )) # Again, matrix multiplication logits = tf . matmul(fc2, fc3_weight) + fc3_bias return logits 2.2.2: Create Placeholders # Set placeholder variables for x, y, and the keep_prob for dropout # Also, one-hot encode y x = tf . placeholder(tf . float32, ( None , 32 , 32 , 1 )) y = tf . placeholder(tf . int32, ( None )) keep_prob = tf . placeholder(tf . float32) one_hot_y = tf . one_hot(y, 43 ) 2.2.3: Pipeline rate = 0.001 # loss functions, and optimizer logits = neuralNetwork(x) cross_entropy = tf . nn . softmax_cross_entropy_with_logits(logits, one_hot_y) loss_operation = tf . reduce_mean(cross_entropy) optimizer = tf . train . AdamOptimizer(learning_rate = rate) training_operation = optimizer . minimize(loss_operation) 2.2.4: Helper functions for train, validate and test # The below is used in the validation part of the neural network correct_prediction = tf . equal(tf . argmax(logits, 1 ), tf . argmax(one_hot_y, 1 )) accuracy_operation = tf . reduce_mean(tf . cast(correct_prediction, tf . float32)) def evaluate (X_data, y_data): num_examples = len (X_data) total_accuracy = 0 sess = tf . get_default_session() for offset in range ( 0 , num_examples, BATCH_SIZE): batch_x, batch_y = X_data[offset:offset + BATCH_SIZE], y_data[offset:offset + BATCH_SIZE] accuracy = sess . run(accuracy_operation, feed_dict = {x: batch_x, y: batch_y, keep_prob : 1.0 }) total_accuracy += (accuracy * len (batch_x)) return total_accuracy / num_examples 2.3: Train, Validate and Test the Model A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation sets imply underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting. ### Train your model here. ### Calculate and report the accuracy on the training and validation set. ### Once a final model architecture is selected, ### the accuracy on the test set should be calculated and reported as well. ### Feel free to use as many code cells as needed. EPOCHS = 20 BATCH_SIZE = 160 save_file = 'train_model.ckpt' saver = tf . train . Saver() with tf . Session() as sess: sess . run(tf . initialize_all_variables()) num_examples = len (X_train) print ( \"Training...\" ) print () for i in range (EPOCHS): # shuffleing data X_train, y_train = shuffle(X_train, y_train) # run session in each bach of data in a epoch for offset in range ( 0 , num_examples, BATCH_SIZE): end = offset + BATCH_SIZE batch_x, batch_y = X_train[offset:end], y_train[offset:end] loss = sess . run(training_operation, feed_dict = {x: batch_x,\\ y: batch_y,\\ keep_prob : 0.7 }) # calculate validation accuracy once all batches are done in a epoch validation_accuracy = evaluate(X_valid, y_valid) print ( \"EPOCH {} ...\" . format(i + 1 )) print ( \"Validation Accuracy = {:.3f}\" . format(validation_accuracy)) print () # Save the model saver . save(sess, save_file) print ( 'Trained Model Saved.' ) WARNING:tensorflow:From &lt;ipython-input-17-5811dfa87b2d&gt;:17 in &lt;module&gt;.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02. Instructions for updating: Use `tf.global_variables_initializer` instead. Training... EPOCH 1 ... Validation Accuracy = 0.701 EPOCH 2 ... Validation Accuracy = 0.831 EPOCH 3 ... Validation Accuracy = 0.877 EPOCH 4 ... Validation Accuracy = 0.906 EPOCH 5 ... Validation Accuracy = 0.924 EPOCH 6 ... Validation Accuracy = 0.934 EPOCH 7 ... Validation Accuracy = 0.948 EPOCH 8 ... Validation Accuracy = 0.956 EPOCH 9 ... Validation Accuracy = 0.959 EPOCH 10 ... Validation Accuracy = 0.970 EPOCH 11 ... Validation Accuracy = 0.969 EPOCH 12 ... Validation Accuracy = 0.973 EPOCH 13 ... Validation Accuracy = 0.972 EPOCH 14 ... Validation Accuracy = 0.975 EPOCH 15 ... Validation Accuracy = 0.978 EPOCH 16 ... Validation Accuracy = 0.978 EPOCH 17 ... Validation Accuracy = 0.980 EPOCH 18 ... Validation Accuracy = 0.977 EPOCH 19 ... Validation Accuracy = 0.982 EPOCH 20 ... Validation Accuracy = 0.984 Trained Model Saved. Launch the model on the test data,train data and validation data # Remove the previous weights and bias #tf.reset_default_graph() # Launch the model on the test data with tf . Session() as sess: saver . restore(sess, './train_model.ckpt' ) test_accuracy = sess . run(accuracy_operation,\\ feed_dict = {x: X_test,\\ y: y_test,\\ keep_prob : 1.0 }) print ( 'Test Accuracy: {}' . format(test_accuracy)) Test Accuracy: 0.9122723937034607 # Remove the previous weights and bias #tf.reset_default_graph() # Launch the model on the test data with tf . Session() as sess: saver . restore(sess, './train_model.ckpt' ) train_accuracy = sess . run(accuracy_operation,\\ feed_dict = {x:X_train,\\ y: y_train,\\ keep_prob : 1.0 }) print ( 'Train Accuracy: {}' . format(train_accuracy)) Train Accuracy: 0.9920793771743774 # Remove the previous weights and bias #tf.reset_default_graph() # Launch the model on the test data with tf . Session() as sess: saver . restore(sess, './train_model.ckpt' ) valid_accuracy = sess . run(accuracy_operation,\\ feed_dict = {x: X_valid,\\ y: y_valid,\\ keep_prob : 1.0 }) print ( 'validation Accuracy: {}' . format(valid_accuracy)) validation Accuracy: 0.9841609597206116 Step 3: Test a Model on New Images To give yourself more insight into how your model is working, download at least five pictures of German traffic signs from the web and use your model to predict the traffic sign type. You may find signnames.csv useful as it contains mappings from the class id (integer) to the actual sign name. 3.1: Load and Output the Images ### Load the images and plot them here. ### Feel free to use as many code cells as needed. # Importing the images, and let's take a look at what we have! import os import matplotlib.image as mpimg web_pics = os . listdir( \"./web_pics/\" ) # Show the images, add to a list to process for classifying web_pics_data = [] for i in web_pics: # Drop the mac's created '.DS_Store' file if i != '.DS_Store' : i = 'web_pics/' + i image = mpimg . imread(i) web_pics_data . append(image) k = 0 plt . figure(figsize = ( 12 , 14 )) for image in web_pics_data: plt . subplot( 6 , 3 , k + 1 ) plt . axis( 'off' ) plt . imshow(image) plt . title(i) k = k + 1 plt . show() 3.2 : Predict the Sign Type for Each Image ### Run the predictions here and use the model to output the prediction for each image. ### Make sure to pre-process the images with the same pre-processing pipeline used earlier. ### Feel free to use as many code cells as needed. # Make into numpy array for processing web_pics_data = np . array(web_pics_data) # First, double-check the image shape to make sure it #matches the original data's 32x32x3 size print (web_pics_data . shape) (5, 32, 32, 3) 3.2.1: Preprocess the new data web_pics_data = preprocess(web_pics_data, 'web_pics_data' ,verbose = True ) Preprocessing web_pics_data... Finished preprocessing web_pics_data... Processed web_pics_data shape = (5, 32, 32, 1) 3.2.2: Double-check that the image is changed to depth of 1 new_image_shape = web_pics_data . shape print ( \"Processed additional web pictures shape =\" , new_image_shape) Processed additional web pictures shape = (5, 32, 32, 1) 3.2.3: Prediction over new images ### Run the predictions here. ### Feel free to use as many code cells as needed. # Launch the model on the new pictures with tf . Session() as sess: saver . restore(sess, './train_model.ckpt' ) new_pics_classes = sess . run(logits, feed_dict = {x: web_pics_data,\\ keep_prob : 1.0 }) 3.3: Analyze Performance ### Calculate the accuracy for these 5 new images. ### For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images. with tf . Session() as sess: predicts = sess . run(tf . nn . top_k(new_pics_classes, k = 5 , sorted = True )) signId = [] for i in range ( len (predicts[ 0 ])): print ( 'predicted classes:' , predicts[ 1 ][i][ 0 ]) signId . append(predicts[ 1 ][i][ 0 ]) predicted classes: 14 predicted classes: 13 predicted classes: 18 predicted classes: 34 predicted classes: 2 import pandas as pd sign_name = pd . read_csv( 'signnames.csv' ) sign_name = pd . DataFrame(sign_name) sign_name . columns Index(['ClassId', 'SignName'], dtype='object') sign = {} for k in range ( 43 ): sign[sign_name[ 'ClassId' ][k]] = sign_name[ 'SignName' ][k] sign {0: 'Speed limit (20km/h)', 1: 'Speed limit (30km/h)', 2: 'Speed limit (50km/h)', 3: 'Speed limit (60km/h)', 4: 'Speed limit (70km/h)', 5: 'Speed limit (80km/h)', 6: 'End of speed limit (80km/h)', 7: 'Speed limit (100km/h)', 8: 'Speed limit (120km/h)', 9: 'No passing', 10: 'No passing for vehicles over 3.5 metric tons', 11: 'Right-of-way at the next intersection', 12: 'Priority road', 13: 'Yield', 14: 'Stop', 15: 'No vehicles', 16: 'Vehicles over 3.5 metric tons prohibited', 17: 'No entry', 18: 'General caution', 19: 'Dangerous curve to the left', 20: 'Dangerous curve to the right', 21: 'Double curve', 22: 'Bumpy road', 23: 'Slippery road', 24: 'Road narrows on the right', 25: 'Road work', 26: 'Traffic signals', 27: 'Pedestrians', 28: 'Children crossing', 29: 'Bicycles crossing', 30: 'Beware of ice/snow', 31: 'Wild animals crossing', 32: 'End of all speed and passing limits', 33: 'Turn right ahead', 34: 'Turn left ahead', 35: 'Ahead only', 36: 'Go straight or right', 37: 'Go straight or left', 38: 'Keep right', 39: 'Keep left', 40: 'Roundabout mandatory', 41: 'End of no passing', 42: 'End of no passing by vehicles over 3.5 metric tons'} for k in range ( 5 ): print (signId[k] , sign[signId[k]]) print ( \"================================\" ) 14 Stop ================================ 13 Yield ================================ 18 General caution ================================ 34 Turn left ahead ================================ 2 Speed limit (50km/h) ================================ ls - l ./ web_pics total 40 -rw-r--r-- 1 carnd carnd 4725 Mar 25 22:44 \u001b[0m\u001b[01;35m60_kmh.jpg\u001b[0m -rw-r--r-- 1 carnd carnd 4350 Mar 25 22:44 \u001b[01;35mleft_turn.jpeg\u001b[0m -rw-r--r-- 1 carnd carnd 4514 Mar 25 22:44 \u001b[01;35mroad_work.jpg\u001b[0m -rw-r--r-- 1 carnd carnd 4477 Mar 25 22:44 \u001b[01;35mstop_sign.jpg\u001b[0m -rw-r--r-- 1 carnd carnd 4464 Mar 25 22:44 \u001b[01;35myield_sign.jpg\u001b[0m 3.4: Output Top 5 Softmax Probabilities For Each Image Found on the Web For each of the new images, print out the model's softmax probabilities to show the certainty of the model's predictions (limit the output to the top 5 probabilities for each image). tf.nn.top_k could prove helpful here. The example below demonstrates how tf.nn.top_k can be used to find the top k predictions for each image. tf.nn.top_k will return the values and indices (class ids) of the top k predictions. So if k=3, for each sign, it'll return the 3 largest probabilities (out of a possible 43) and the correspoding class ids. Take this numpy array as an example. The values in the array represent predictions. The array contains softmax probabilities for five candidate images with six possible classes. tk.nn.top_k is used to choose the three classes with the highest probability: # (5, 6) array a = np.array([[ 0.24879643, 0.07032244, 0.12641572, 0.34763842, 0.07893497, 0.12789202], [ 0.28086119, 0.27569815, 0.08594638, 0.0178669 , 0.18063401, 0.15899337], [ 0.26076848, 0.23664738, 0.08020603, 0.07001922, 0.1134371 , 0.23892179], [ 0.11943333, 0.29198961, 0.02605103, 0.26234032, 0.1351348 , 0.16505091], [ 0.09561176, 0.34396535, 0.0643941 , 0.16240774, 0.24206137, 0.09155967]]) Running it through sess.run(tf.nn.top_k(tf.constant(a), k=3)) produces: TopKV2(values=array([[ 0.34763842, 0.24879643, 0.12789202], [ 0.28086119, 0.27569815, 0.18063401], [ 0.26076848, 0.23892179, 0.23664738], [ 0.29198961, 0.26234032, 0.16505091], [ 0.34396535, 0.24206137, 0.16240774]]), indices=array([[3, 0, 5], [0, 1, 4], [0, 5, 1], [1, 3, 5], [1, 4, 3]], dtype=int32)) Looking just at the first row we get [ 0.34763842, 0.24879643, 0.12789202] , you can confirm these are the 3 largest probabilities in a . You'll also notice [3, 0, 5] are the corresponding indices. ### Print out the top five softmax probabilities for the predictions on the German traffic sign images found on the web. ### Feel free to use as many code cells as needed. with tf . Session() as sess: predicts = sess . run(tf . nn . top_k(new_pics_classes, k = 5 , sorted = True )) for i in range ( len (predicts[ 0 ])): probabilities = predicts[ 0 ][i] predicted_classes = predicts[ 1 ][i] print ( 'Image' , i,\\ 'probabilities:' , probabilities,\\ ' \\n and predicted classes:' , predicts[ 1 ][i]) Image 0 probabilities: [ 12.23980713 8.32987499 5.29931879 4.51511765 1.64859486] and predicted classes: [14 17 36 38 34] Image 1 probabilities: [ 20.34670448 11.3846302 6.09567165 1.00739896 0.98556668] and predicted classes: [13 35 34 9 3] Image 2 probabilities: [ 12.05598831 9.00940228 8.62032127 6.8418088 3.65508676] and predicted classes: [18 12 40 37 1] Image 3 probabilities: [ 13.12025452 8.07227325 -0.03553319 -1.66574371 -1.75805712] and predicted classes: [34 38 20 35 32] Image 4 probabilities: [ 4.97775316 2.17866445 1.98024666 1.00796783 0.97446799] and predicted classes: [ 2 10 9 5 7] Predicted Result and actual picture visualization plt . figure(figsize = ( 14 , 17 )) for i in range ( 5 ): plt . subplot( 5 , 2 , 2 * i + 1 ) plt . imshow(web_pics_data[i,:,:, 0 ],cmap = 'gray' ) plt . title(i) plt . axis( 'off' ) plt . subplot( 5 , 2 , 2 * i + 2 ) plt . barh(np . arange( 1 , 6 , 1 ), predicts . values[i, :]) labs1 = [predicts[ 1 ][i][j] for j in range ( 5 )] labs = [sign[labs1[j]] for j in range ( 5 )] plt . yticks(np . arange( 1 , 6 , 1 ), labs) plt . show() Step 4: Visualize the Neural Network's State with Test Images This Section is not required to complete but acts as an additional excersise for understaning the output of a neural network's weights. While neural networks can be a great learning device they are often referred to as a black box. We can understand what the weights of a neural network look like better by plotting their feature maps. After successfully training your neural network you can see what it's feature maps look like by plotting the output of the network's weight layers in response to a test stimuli image. From these plotted feature maps, it's possible to see what characteristics of an image the network finds interesting. For a sign, maybe the inner network feature maps react with high activation to the sign's boundary outline or to the contrast in the sign's painted symbol. Provided for you below is the function code that allows you to get the visualization output of any tensorflow weight layer you want. The inputs to the function should be a stimuli image, one used during training or a new one you provided, and then the tensorflow variable name that represents the layer's state during the training process, for instance if you wanted to see what the LeNet lab's feature maps looked like for it's second convolutional layer you could enter conv2 as the tf_activation variable. For an example of what feature map outputs look like, check out NVIDIA's results in their paper End-to-End Deep Learning for Self-Driving Cars in the section Visualization of internal CNN State. NVIDIA was able to show that their network's inner weights had high activations to road boundary lines by comparing feature maps from an image with a clear path to one without. Try experimenting with a similar test to show that your trained network's weights are looking for interesting features, whether it's looking at differences in feature maps from images with or without a sign, or even what feature maps look like in a trained network vs a completely untrained one on the same sign image. Your output should look something like this (above) Setting Function ### Visualize your network's feature maps here. ### Feel free to use as many code cells as needed. # image_input: the test image being fed into the network to produce the feature maps # tf_activation: should be a tf variable name used during your #training procedure that represents the calculated state of a specific weight layer # activation_min/max: can be used to view the activation contrast in more detail, # by default matplot sets min and max to the actual min and max values of the output # plt_num: used to plot out multiple different weight feature map sets on the same block, #just extend the plt number for each new feature map entry def outputFeatureMap (image_input, tf_activation,\\ activation_min =- 1 , activation_max =- 1 ,plt_num = 1 ): # Here make sure to preprocess your image_input in a way your network expects # with size, normalization, ect if needed # image_input = # Note: x should be the same name as your network's tensorflow data placeholder variable # If you get an error tf_activation is not defined it maybe having trouble #accessing the variable from inside a function activation = tf_activation . eval(session = tf . get_default_session(),\\ feed_dict = {x: image_input}) featuremaps = activation . shape[ 3 ] plt . figure(plt_num, figsize = ( 15 , 15 )) for featuremap in range (featuremaps): # sets the number of feature maps to show on each row and column plt . subplot( 6 , 8 , featuremap + 1 ) # displays the feature map number plt . title( 'FeatureMap ' + str (featuremap)) if activation_min != - 1 & activation_max != - 1 : plt . imshow(activation[ 0 ,:,:, featuremap],\\ interpolation = \"nearest\" ,\\ vmin = activation_min,\\ vmax = activation_max,\\ cmap = \"gray\" ) elif activation_max != - 1 : plt . imshow(activation[ 0 ,:,:, featuremap],\\ interpolation = \"nearest\" ,\\ vmax = activation_max,\\ cmap = \"gray\" ) elif activation_min !=- 1 : plt . imshow(activation[ 0 ,:,:, featuremap],\\ interpolation = \"nearest\" ,\\ vmin = activation_min,\\ cmap = \"gray\" ) else : plt . imshow(activation[ 0 ,:,:, featuremap],\\ interpolation = \"nearest\" ,\\ cmap = \"gray\" ) Plot of visual output mu = 0 sigma = 0.1 x = tf . placeholder(tf . float32, ( None , 32 , 32 , 1 )) y = tf . placeholder(tf . int32, ( None )) my_image = web_pics_data[ 0 ] '''first conv-layer''' c1_weight = tf . Variable(tf . truncated_normal(shape = ( 5 , 5 , 1 , 6 ),\\ mean = mu,\\ stddev = sigma)) c1_bias = tf . Variable(tf . zeros( 6 )) conv_layer1 = tf . nn . conv2d(x, c1_weight,\\ strides = [ 1 , 1 , 1 , 1 ],\\ padding = 'VALID' , name = 'conv1' ) + c1_bias '''second conv layer''' c2_weight = tf . Variable(tf . truncated_normal(shape = ( 5 , 5 , 6 , 16 ),\\ mean = mu,\\ stddev = sigma)) c2_bias = tf . Variable(tf . zeros( 16 )) conv_layer2 = tf . nn . conv2d(conv_layer1, c2_weight,\\ strides = [ 1 , 1 , 1 , 1 ],\\ padding = 'VALID' , name = 'conv2' ) + c2_bias with tf . Session() as sess: saver . restore(sess, tf . train . latest_checkpoint( '.' )) sess . run(tf . global_variables_initializer()) my_tensor1 = sess . graph . get_tensor_by_name( 'conv1:0' ) my_tensor2 = sess . graph . get_tensor_by_name( 'conv2:0' ) outputFeatureMap([my_image],my_tensor1) outputFeatureMap([my_image],my_tensor2) Question 9 Discuss how you used the visual output of your trained network's feature maps to show that it had learned to look for interesting characteristics in traffic sign images Answer: Tensorflow allows us to share the variables (ex: sess.graph.get_tensor_by_name('conv1:0') and it becomes possible to draw intermediate data during the training model. The visual output gives us the idea of sharp edge detection and other tendencies in distribution of pixcel intensities. As the layer increases ie. becomes more and more deeper it tries to learn more hidden properties which are not obvious in the single layer. Note : Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to \\n\", \" File -> Download as -> HTML (.html) . Include the finished document along with this notebook as your submission. Project Writeup Once you have completed the code implementation, document your results in a project writeup using this template as a guide. The writeup can be in a markdown or pdf file.","title":"Traffic Sign Detection"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#self-driving-car-engineer-nanodegree","text":"","title":"Self-Driving Car Engineer Nanodegree"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#deep-learning","text":"","title":"Deep Learning"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#project-build-a-traffic-sign-recognition-classifier","text":"In this notebook, a template is provided for you to implement your functionality in stages, which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission if necessary. Note : Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to \\n\", \" File -> Download as -> HTML (.html) . Include the finished document along with this notebook as your submission. In addition to implementing code, there is a writeup to complete. The writeup should be completed in a separate file, which can be either a markdown file or a pdf document. There is a write up template that can be used to guide the writing process. Completing the code template and writeup template will cover all of the rubric points for this project. The rubric contains \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. The stand out suggestions are optional. If you decide to pursue the \"stand out suggestions\", you can include the code in this Ipython notebook and also discuss the results in the writeup file. Note: Code and Markdown cells can be executed using the Shift + Enter keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode.","title":"Project: Build a Traffic Sign Recognition Classifier"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#step-0-load-the-data","text":"# Load pickled data import pickle # Importing my own choice import numpy as np import tensorflow as tf from sklearn.utils import shuffle # TODO: Fill this in based on where you saved the training and testing data training_file = 'traffic-signs-data/train.p' testing_file = 'traffic-signs-data/test.p' with open (training_file, mode = 'rb' ) as f: train = pickle . load(f) with open (testing_file, mode = 'rb' ) as f: test = pickle . load(f) X_train, y_train = train[ 'features' ], train[ 'labels' ] X_test, y_test = test[ 'features' ], test[ 'labels' ] print () print ( \"Image Shape: {}\" . format(X_train[ 0 ] . shape)) print () print ( \"Training Set: {} samples\" . format( len (X_train))) print ( \"Test Set: {} samples\" . format( len (X_test))) Image Shape: (32, 32, 3) Training Set: 34799 samples Test Set: 12630 samples","title":"Step 0: Load The Data"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#step-1-dataset-summary-exploration","text":"The pickled data is a dictionary with 4 key/value pairs: 'features' is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels). 'labels' is a 1D array containing the label/class id of the traffic sign. The file signnames.csv contains id -> name mappings for each id. 'sizes' is a list containing tuples, (width, height) representing the original width and height the image. 'coords' is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES Complete the basic data summary below. Use python, numpy and/or pandas methods to calculate the data summary rather than hard coding the results. For example, the pandas shape method might be useful for calculating some of the summary results.","title":"Step 1: Dataset Summary &amp; Exploration"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#provide-a-basic-summary-of-the-data-set-using-python-numpy-andor-pandas","text":"### Replace each question mark with the appropriate value. ### Use python, pandas or numpy methods rather than hard coding the results # TODO: Number of training examples n_train = X_train . shape[ 0 ] # TODO: Number of testing examples. n_test = X_test . shape[ 0 ] # TODO: What's the shape of an traffic sign image? image_shape = (X_train . shape[ 1 ], X_train . shape[ 2 ], X_train . shape[ 3 ]) # TODO: How many unique classes/labels there are in the dataset. n_classes = y_train . max() + 1 print ( \"Number of training examples =\" , n_train) print ( \"Number of testing examples =\" , n_test) print ( \"Image data shape =\" , image_shape) print ( \"Number of classes =\" , n_classes) Number of training examples = 34799 Number of testing examples = 12630 Image data shape = (32, 32, 3) Number of classes = 43","title":"Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#include-an-exploratory-visualization-of-the-dataset","text":"Visualize the German Traffic Signs Dataset using the pickled file(s). This is open ended, suggestions include: plotting traffic sign images, plotting the count of each sign, etc. The Matplotlib examples and gallery pages are a great resource for doing visualizations in Python. NOTE: It's recommended you start with something simple first. If you wish to do more, come back to it after you've completed the rest of the sections. ### Data exploration visualization code goes here. ### Feel free to use as many code cells as needed. import matplotlib.pyplot as plt from random import randint # Visualizations will be shown in the notebook. % matplotlib inline num_of_samples = [] plt . figure(figsize = ( 12 , 16.5 )) for i in range ( 0 , n_classes): plt . subplot( 11 , 4 , i + 1 ) x_selected = X_train[y_train == i] plt . imshow(x_selected[ 0 , :, :, :]) #draw the first image of each class plt . title(i) plt . axis( 'off' ) num_of_samples . append( len (x_selected)) plt . show()","title":"Include an exploratory visualization of the dataset"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#plot-number-of-images-per-class","text":"### Data exploration visualization code goes here. ### Feel free to use as many code cells as needed. import matplotlib.pyplot as plt from random import randint % matplotlib inline #Plot number of images per class plt . figure(figsize = ( 12 , 4 )) plt . bar( range ( 0 , n_classes), num_of_samples) plt . title( \"Distribution of the train dataset\" ) plt . xlabel( \"Class number\" ) plt . ylabel( \"Number of images\" ) plt . show() print ( \"Min number of images per class =\" , min (num_of_samples)) print ( \"Max number of images per class =\" , max (num_of_samples)) Min number of images per class = 180 Max number of images per class = 2010","title":"Plot number of images per class"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#step-2-design-and-test-a-model-architecture","text":"Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the German Traffic Sign Dataset . The LeNet-5 implementation shown in the classroom at the end of the CNN lesson is a solid starting point. You'll have to change the number of classes and possibly the preprocessing, but aside from that it's plug and play! With the LeNet-5 solution from the lecture, you should expect a validation set accuracy of about 0.89. To meet specifications, the validation set accuracy will need to be at least 0.93. It is possible to get an even higher accuracy, but 0.93 is the minimum for a successful project submission. There are various aspects to consider when thinking about this problem: Neural network architecture (is the network over or underfitting?) Play around preprocessing techniques (normalization, rgb to grayscale, etc) Number of examples per label (some have more than others). Generate fake data. Here is an example of a published baseline model on this problem . It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these.","title":"Step 2: Design and Test a Model Architecture"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#21-preprocess-the-data-set-normalization-grayscale-etc","text":"Use the code cell (or multiple code cells, if necessary) to implement the first step of your project.","title":"2.1:  Preprocess the Data Set (normalization, grayscale, etc.)"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#211-preprocessing-function","text":"### Preprocess the data here. Preprocessing steps could include normalization, converting to grayscale, etc. ### Feel free to use as many code cells as needed. import cv2 from numpy import newaxis # Iterates through grayscale for each image in the data def gray_maker (data): gray_images = [] for image in data: gray = cv2 . cvtColor(image, cv2 . COLOR_BGR2GRAY) gray_images . append(gray) return np . array(gray_images) def preprocess (data,data_name,verbose): if verbose: print ( 'Preprocessing ' + data_name + '...' ) # Iterate through grayscale data = gray_maker(data) data = data[ ... , newaxis] #Normalizes the data between 0.1 and 0.9 instead of 0 to 255 data = data / 255 * 0.8 + 0.1 if verbose: print ( 'Finished preprocessing ' + data_name + '...' ) # Double-check that the image is changed to depth of 1 image_shape = data . shape if verbose: print ( 'Processed ' + data_name + ' shape =' , image_shape) print ( \" \" ) return data","title":"2.1.1: Preprocessing Function"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#212-preprocessing-all-data","text":"X_train = preprocess(X_train, 'train_data' ,verbose = True ) X_test = preprocess(X_test, 'test_data' ,verbose = True ) Preprocessing train_data... Finished preprocessing train_data... Processed train_data shape = (34799, 32, 32, 1) Preprocessing test_data... Finished preprocessing test_data... Processed test_data shape = (12630, 32, 32, 1)","title":"2.1.2: Preprocessing all data"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#213-after-preprocessing","text":"# Visualizations will be shown in the notebook. % matplotlib inline num_of_samples = [] plt . figure(figsize = ( 12 , 16.5 )) for i in range ( 0 , n_classes): plt . subplot( 11 , 4 , i + 1 ) x_selected_grey = X_train[y_train == i] #draw the first image of each class plt . imshow(x_selected_grey[ 0 , :, :, 0 ], cmap = 'gray' ) plt . title(i) plt . axis( 'off' ) num_of_samples . append( len (x_selected)) plt . show()","title":"2.1.3: After Preprocessing"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#214-generate-fake-data","text":"I will generate some additional data, then split the data in a later cell. This is to help with the issue identified in the original histogram ### Generate fake data from scipy import ndimage import random # min_desired below is just mean_pics but wanted to # make the code below easier to distinguish def fake_data_generator (X,y,verbose): '''X = feature data , y = label data''' pics_per_class = np . bincount(y) mean_pics = int (np . mean(pics_per_class)) if verbose: print ( 'Generating new data.' ) # Angles to be used to rotate images in additional data made angles = [ - 10 , 10 , - 15 , 15 , - 20 , 20 ] # Iterate through each class for i in range ( len (pics_per_class)): # Check if less data than the mean if pics_per_class[i] < mean_pics: # Count how many additional pictures we want new_wanted = mean_pics - pics_per_class[i] picture = np . where(y == i) more_X = [] more_y = [] # Make the number of additional pictures needed to arrive at the mean for num in range (new_wanted): # Rotate images and append new ones to more_X, append the class to more_y more_X . append(ndimage . rotate(X[picture][random . randint( 0 ,pics_per_class[i] - 1 )],\\ random . choice(angles), reshape = False )) more_y . append(i) # Append the pictures generated for each class back to the original data X = np . append(X, np . array(more_X), axis = 0 ) y = np . append(y, np . array(more_y), axis = 0 ) if verbose: print ( 'Additional data generated. Any classes lacking data now have' ,mean_pics, 'pictures.' ) return X,y X_train, y_train = fake_data_generator(X_train,y_train,verbose = True ) Generating new data. Additional data generated. Any classes lacking data now have 809 pictures.","title":"2.1.4: Generate fake data"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#215-histogram-representing-data-distribution-in-all-classes","text":"plt . hist(y_train, bins = n_classes) updated_n_train = len (X_train) print ( \"The updated number of training examples =\" , updated_n_train) The updated number of training examples = 46714","title":"2.1.5: Histogram representing data distribution in all classes"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#data-summary","text":"n_train = X_train . shape[ 0 ] n_test = X_test . shape[ 0 ] print ( \"Number of training examples =\" , n_train) print ( \"Number of testing examples =\" , n_test) print ( \"Extra data generated =\" ,n_train - 34799 ) Number of training examples = 46714 Number of testing examples = 12630 Extra data generated = 11915","title":"Data summary"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#splitting-train-and-validation-data","text":"from sklearn.model_selection import train_test_split # shuffleing data X_train, y_train = shuffle(X_train, y_train) # For each epoch, there are separate training data and validation data X_train, X_valid, y_train, y_valid\\ = train_test_split(X_train, y_train,\\ stratify = y_train,\\ test_size = 0.1 ,\\ random_state = 23 ) n_train = X_train . shape[ 0 ] n_valid = X_valid . shape[ 0 ] print ( \"Number of training examples =\" , n_train) print ( \"Number of validation examples =\" , n_valid) Number of training examples = 42042 Number of validation examples = 4672","title":"Splitting Train and Validation data"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#22-model-architecture","text":"","title":"2.2 : Model Architecture"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#221-neural-network-function","text":"### Define your architecture here. ### Feel free to use as many code cells as needed. # The below is only necessary to reset if the notebook has not been shutdown tf . reset_default_graph() from tensorflow.contrib.layers import flatten def neuralNetwork (x): # Hyperparameters mu = 0 sigma = 0.1 #============================================================== # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6. # Weight and bias # If not using grayscale, the third number in shape would be 3 c1_weight = tf . Variable(tf . truncated_normal(shape = ( 5 , 5 , 1 , 6 ),\\ mean = mu,\\ stddev = sigma)) c1_bias = tf . Variable(tf . zeros( 6 )) # Apply convolution conv_layer1 = tf . nn . conv2d(x, c1_weight,\\ strides = [ 1 , 1 , 1 , 1 ],\\ padding = 'VALID' )\\ + c1_bias # Activation for layer 1 conv_layer1 = tf . nn . relu(conv_layer1) # Pooling. Input = 28x28x6. Output = 14x14x6. conv_layer1 = tf . nn . avg_pool(conv_layer1,\\ ksize = [ 1 , 2 , 2 , 1 ],\\ strides = [ 1 , 2 , 2 , 1 ],\\ padding = 'VALID' ) #================================================================ # Layer 2: Convolutional. Output = 10x10x16. # Note: The second layer is implemented the exact same as layer one, # with layer 1 as input instead of x And then of course changing the #numbers to fit the desired ouput of 10x10x16 Weight and bias c2_weight = tf . Variable(tf . truncated_normal(shape = ( 5 , 5 , 6 , 16 ),\\ mean = mu,\\ stddev = sigma)) c2_bias = tf . Variable(tf . zeros( 16 )) # Apply convolution for layer 2 conv_layer2 = tf . nn . conv2d(conv_layer1, c2_weight,\\ strides = [ 1 , 1 , 1 , 1 ],\\ padding = 'VALID' ) + c2_bias # Activation for layer 2 conv_layer2 = tf . nn . relu(conv_layer2) # Pooling. Input = 10x10x16. Output = 5x5x16. conv_layer2 = tf . nn . avg_pool(conv_layer2,\\ ksize = [ 1 , 2 , 2 , 1 ],\\ strides = [ 1 , 2 , 2 , 1 ],\\ padding = 'VALID' ) # Flatten to get to fully connected layers. Input = 5x5x16. Output = 400. flat = tf . contrib . layers . flatten(conv_layer2) #=============================================================== # Layer 3: Fully Connected. Input = 400. Output = 120. # Although this is fully connected, the weights and biases still are implemented similarly # There is no filter this time, so shape only takes input and output # Weight and bias fc1_weight = tf . Variable(tf . truncated_normal(shape = ( 400 , 200 ),\\ mean = mu,\\ stddev = sigma)) fc1_bias = tf . Variable(tf . zeros( 200 )) # Here is the main change versus a convolutional layer - matrix multiplication instead of 2D convolution fc1 = tf . matmul(flat, fc1_weight) + fc1_bias # Activation for the first fully connected layer. # Same thing as before fc1 = tf . nn . relu(fc1) # Dropout, to prevent overfitting fc1 = tf . nn . dropout(fc1, keep_prob) #================================================================== # Layer 4: Fully Connected. Input = 120. Output = 84. # Same as the fc1 layer, just with updated output numbers fc2_weight = tf . Variable(tf . truncated_normal(shape = ( 200 , 100 ),\\ mean = mu,\\ stddev = sigma)) fc2_bias = tf . Variable(tf . zeros( 100 )) # Again, matrix multiplication fc2 = tf . matmul(fc1, fc2_weight) + fc2_bias # Activation. fc2 = tf . nn . relu(fc2) # Dropout fc2 = tf . nn . dropout(fc2, keep_prob) #======================================================== # Layer 5 Fully Connected. Input = 84. Output = 43. # Since this is the final layer, output needs to match up with the number of classes fc3_weight = tf . Variable(tf . truncated_normal(shape = ( 100 , 43 ),\\ mean = mu, \\ stddev = sigma)) fc3_bias = tf . Variable(tf . zeros( 43 )) # Again, matrix multiplication logits = tf . matmul(fc2, fc3_weight) + fc3_bias return logits","title":"2.2.1: Neural Network Function"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#222-create-placeholders","text":"# Set placeholder variables for x, y, and the keep_prob for dropout # Also, one-hot encode y x = tf . placeholder(tf . float32, ( None , 32 , 32 , 1 )) y = tf . placeholder(tf . int32, ( None )) keep_prob = tf . placeholder(tf . float32) one_hot_y = tf . one_hot(y, 43 )","title":"2.2.2: Create Placeholders"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#223-pipeline","text":"rate = 0.001 # loss functions, and optimizer logits = neuralNetwork(x) cross_entropy = tf . nn . softmax_cross_entropy_with_logits(logits, one_hot_y) loss_operation = tf . reduce_mean(cross_entropy) optimizer = tf . train . AdamOptimizer(learning_rate = rate) training_operation = optimizer . minimize(loss_operation)","title":"2.2.3: Pipeline"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#224-helper-functions-for-train-validate-and-test","text":"# The below is used in the validation part of the neural network correct_prediction = tf . equal(tf . argmax(logits, 1 ), tf . argmax(one_hot_y, 1 )) accuracy_operation = tf . reduce_mean(tf . cast(correct_prediction, tf . float32)) def evaluate (X_data, y_data): num_examples = len (X_data) total_accuracy = 0 sess = tf . get_default_session() for offset in range ( 0 , num_examples, BATCH_SIZE): batch_x, batch_y = X_data[offset:offset + BATCH_SIZE], y_data[offset:offset + BATCH_SIZE] accuracy = sess . run(accuracy_operation, feed_dict = {x: batch_x, y: batch_y, keep_prob : 1.0 }) total_accuracy += (accuracy * len (batch_x)) return total_accuracy / num_examples","title":"2.2.4: Helper functions for train, validate and test"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#23-train-validate-and-test-the-model","text":"A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation sets imply underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting. ### Train your model here. ### Calculate and report the accuracy on the training and validation set. ### Once a final model architecture is selected, ### the accuracy on the test set should be calculated and reported as well. ### Feel free to use as many code cells as needed. EPOCHS = 20 BATCH_SIZE = 160 save_file = 'train_model.ckpt' saver = tf . train . Saver() with tf . Session() as sess: sess . run(tf . initialize_all_variables()) num_examples = len (X_train) print ( \"Training...\" ) print () for i in range (EPOCHS): # shuffleing data X_train, y_train = shuffle(X_train, y_train) # run session in each bach of data in a epoch for offset in range ( 0 , num_examples, BATCH_SIZE): end = offset + BATCH_SIZE batch_x, batch_y = X_train[offset:end], y_train[offset:end] loss = sess . run(training_operation, feed_dict = {x: batch_x,\\ y: batch_y,\\ keep_prob : 0.7 }) # calculate validation accuracy once all batches are done in a epoch validation_accuracy = evaluate(X_valid, y_valid) print ( \"EPOCH {} ...\" . format(i + 1 )) print ( \"Validation Accuracy = {:.3f}\" . format(validation_accuracy)) print () # Save the model saver . save(sess, save_file) print ( 'Trained Model Saved.' ) WARNING:tensorflow:From &lt;ipython-input-17-5811dfa87b2d&gt;:17 in &lt;module&gt;.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02. Instructions for updating: Use `tf.global_variables_initializer` instead. Training... EPOCH 1 ... Validation Accuracy = 0.701 EPOCH 2 ... Validation Accuracy = 0.831 EPOCH 3 ... Validation Accuracy = 0.877 EPOCH 4 ... Validation Accuracy = 0.906 EPOCH 5 ... Validation Accuracy = 0.924 EPOCH 6 ... Validation Accuracy = 0.934 EPOCH 7 ... Validation Accuracy = 0.948 EPOCH 8 ... Validation Accuracy = 0.956 EPOCH 9 ... Validation Accuracy = 0.959 EPOCH 10 ... Validation Accuracy = 0.970 EPOCH 11 ... Validation Accuracy = 0.969 EPOCH 12 ... Validation Accuracy = 0.973 EPOCH 13 ... Validation Accuracy = 0.972 EPOCH 14 ... Validation Accuracy = 0.975 EPOCH 15 ... Validation Accuracy = 0.978 EPOCH 16 ... Validation Accuracy = 0.978 EPOCH 17 ... Validation Accuracy = 0.980 EPOCH 18 ... Validation Accuracy = 0.977 EPOCH 19 ... Validation Accuracy = 0.982 EPOCH 20 ... Validation Accuracy = 0.984 Trained Model Saved.","title":"2.3:  Train, Validate and Test the Model"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#launch-the-model-on-the-test-datatrain-data-and-validation-data","text":"# Remove the previous weights and bias #tf.reset_default_graph() # Launch the model on the test data with tf . Session() as sess: saver . restore(sess, './train_model.ckpt' ) test_accuracy = sess . run(accuracy_operation,\\ feed_dict = {x: X_test,\\ y: y_test,\\ keep_prob : 1.0 }) print ( 'Test Accuracy: {}' . format(test_accuracy)) Test Accuracy: 0.9122723937034607 # Remove the previous weights and bias #tf.reset_default_graph() # Launch the model on the test data with tf . Session() as sess: saver . restore(sess, './train_model.ckpt' ) train_accuracy = sess . run(accuracy_operation,\\ feed_dict = {x:X_train,\\ y: y_train,\\ keep_prob : 1.0 }) print ( 'Train Accuracy: {}' . format(train_accuracy)) Train Accuracy: 0.9920793771743774 # Remove the previous weights and bias #tf.reset_default_graph() # Launch the model on the test data with tf . Session() as sess: saver . restore(sess, './train_model.ckpt' ) valid_accuracy = sess . run(accuracy_operation,\\ feed_dict = {x: X_valid,\\ y: y_valid,\\ keep_prob : 1.0 }) print ( 'validation Accuracy: {}' . format(valid_accuracy)) validation Accuracy: 0.9841609597206116","title":"Launch the model on the test data,train data and validation data"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#step-3-test-a-model-on-new-images","text":"To give yourself more insight into how your model is working, download at least five pictures of German traffic signs from the web and use your model to predict the traffic sign type. You may find signnames.csv useful as it contains mappings from the class id (integer) to the actual sign name.","title":"Step 3: Test a Model on New Images"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#31-load-and-output-the-images","text":"### Load the images and plot them here. ### Feel free to use as many code cells as needed. # Importing the images, and let's take a look at what we have! import os import matplotlib.image as mpimg web_pics = os . listdir( \"./web_pics/\" ) # Show the images, add to a list to process for classifying web_pics_data = [] for i in web_pics: # Drop the mac's created '.DS_Store' file if i != '.DS_Store' : i = 'web_pics/' + i image = mpimg . imread(i) web_pics_data . append(image) k = 0 plt . figure(figsize = ( 12 , 14 )) for image in web_pics_data: plt . subplot( 6 , 3 , k + 1 ) plt . axis( 'off' ) plt . imshow(image) plt . title(i) k = k + 1 plt . show()","title":"3.1: Load and Output the Images"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#32-predict-the-sign-type-for-each-image","text":"### Run the predictions here and use the model to output the prediction for each image. ### Make sure to pre-process the images with the same pre-processing pipeline used earlier. ### Feel free to use as many code cells as needed. # Make into numpy array for processing web_pics_data = np . array(web_pics_data) # First, double-check the image shape to make sure it #matches the original data's 32x32x3 size print (web_pics_data . shape) (5, 32, 32, 3)","title":"3.2  : Predict the Sign Type for Each Image"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#321-preprocess-the-new-data","text":"web_pics_data = preprocess(web_pics_data, 'web_pics_data' ,verbose = True ) Preprocessing web_pics_data... Finished preprocessing web_pics_data... Processed web_pics_data shape = (5, 32, 32, 1)","title":"3.2.1: Preprocess the new data"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#322-double-check-that-the-image-is-changed-to-depth-of-1","text":"new_image_shape = web_pics_data . shape print ( \"Processed additional web pictures shape =\" , new_image_shape) Processed additional web pictures shape = (5, 32, 32, 1)","title":"3.2.2: Double-check that the image is changed to depth of 1"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#323-prediction-over-new-images","text":"### Run the predictions here. ### Feel free to use as many code cells as needed. # Launch the model on the new pictures with tf . Session() as sess: saver . restore(sess, './train_model.ckpt' ) new_pics_classes = sess . run(logits, feed_dict = {x: web_pics_data,\\ keep_prob : 1.0 })","title":"3.2.3:  Prediction over new images"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#33-analyze-performance","text":"### Calculate the accuracy for these 5 new images. ### For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images. with tf . Session() as sess: predicts = sess . run(tf . nn . top_k(new_pics_classes, k = 5 , sorted = True )) signId = [] for i in range ( len (predicts[ 0 ])): print ( 'predicted classes:' , predicts[ 1 ][i][ 0 ]) signId . append(predicts[ 1 ][i][ 0 ]) predicted classes: 14 predicted classes: 13 predicted classes: 18 predicted classes: 34 predicted classes: 2 import pandas as pd sign_name = pd . read_csv( 'signnames.csv' ) sign_name = pd . DataFrame(sign_name) sign_name . columns Index(['ClassId', 'SignName'], dtype='object') sign = {} for k in range ( 43 ): sign[sign_name[ 'ClassId' ][k]] = sign_name[ 'SignName' ][k] sign {0: 'Speed limit (20km/h)', 1: 'Speed limit (30km/h)', 2: 'Speed limit (50km/h)', 3: 'Speed limit (60km/h)', 4: 'Speed limit (70km/h)', 5: 'Speed limit (80km/h)', 6: 'End of speed limit (80km/h)', 7: 'Speed limit (100km/h)', 8: 'Speed limit (120km/h)', 9: 'No passing', 10: 'No passing for vehicles over 3.5 metric tons', 11: 'Right-of-way at the next intersection', 12: 'Priority road', 13: 'Yield', 14: 'Stop', 15: 'No vehicles', 16: 'Vehicles over 3.5 metric tons prohibited', 17: 'No entry', 18: 'General caution', 19: 'Dangerous curve to the left', 20: 'Dangerous curve to the right', 21: 'Double curve', 22: 'Bumpy road', 23: 'Slippery road', 24: 'Road narrows on the right', 25: 'Road work', 26: 'Traffic signals', 27: 'Pedestrians', 28: 'Children crossing', 29: 'Bicycles crossing', 30: 'Beware of ice/snow', 31: 'Wild animals crossing', 32: 'End of all speed and passing limits', 33: 'Turn right ahead', 34: 'Turn left ahead', 35: 'Ahead only', 36: 'Go straight or right', 37: 'Go straight or left', 38: 'Keep right', 39: 'Keep left', 40: 'Roundabout mandatory', 41: 'End of no passing', 42: 'End of no passing by vehicles over 3.5 metric tons'} for k in range ( 5 ): print (signId[k] , sign[signId[k]]) print ( \"================================\" ) 14 Stop ================================ 13 Yield ================================ 18 General caution ================================ 34 Turn left ahead ================================ 2 Speed limit (50km/h) ================================ ls - l ./ web_pics total 40 -rw-r--r-- 1 carnd carnd 4725 Mar 25 22:44 \u001b[0m\u001b[01;35m60_kmh.jpg\u001b[0m -rw-r--r-- 1 carnd carnd 4350 Mar 25 22:44 \u001b[01;35mleft_turn.jpeg\u001b[0m -rw-r--r-- 1 carnd carnd 4514 Mar 25 22:44 \u001b[01;35mroad_work.jpg\u001b[0m -rw-r--r-- 1 carnd carnd 4477 Mar 25 22:44 \u001b[01;35mstop_sign.jpg\u001b[0m -rw-r--r-- 1 carnd carnd 4464 Mar 25 22:44 \u001b[01;35myield_sign.jpg\u001b[0m","title":"3.3: Analyze Performance"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#34-output-top-5-softmax-probabilities-for-each-image-found-on-the-web","text":"For each of the new images, print out the model's softmax probabilities to show the certainty of the model's predictions (limit the output to the top 5 probabilities for each image). tf.nn.top_k could prove helpful here. The example below demonstrates how tf.nn.top_k can be used to find the top k predictions for each image. tf.nn.top_k will return the values and indices (class ids) of the top k predictions. So if k=3, for each sign, it'll return the 3 largest probabilities (out of a possible 43) and the correspoding class ids. Take this numpy array as an example. The values in the array represent predictions. The array contains softmax probabilities for five candidate images with six possible classes. tk.nn.top_k is used to choose the three classes with the highest probability: # (5, 6) array a = np.array([[ 0.24879643, 0.07032244, 0.12641572, 0.34763842, 0.07893497, 0.12789202], [ 0.28086119, 0.27569815, 0.08594638, 0.0178669 , 0.18063401, 0.15899337], [ 0.26076848, 0.23664738, 0.08020603, 0.07001922, 0.1134371 , 0.23892179], [ 0.11943333, 0.29198961, 0.02605103, 0.26234032, 0.1351348 , 0.16505091], [ 0.09561176, 0.34396535, 0.0643941 , 0.16240774, 0.24206137, 0.09155967]]) Running it through sess.run(tf.nn.top_k(tf.constant(a), k=3)) produces: TopKV2(values=array([[ 0.34763842, 0.24879643, 0.12789202], [ 0.28086119, 0.27569815, 0.18063401], [ 0.26076848, 0.23892179, 0.23664738], [ 0.29198961, 0.26234032, 0.16505091], [ 0.34396535, 0.24206137, 0.16240774]]), indices=array([[3, 0, 5], [0, 1, 4], [0, 5, 1], [1, 3, 5], [1, 4, 3]], dtype=int32)) Looking just at the first row we get [ 0.34763842, 0.24879643, 0.12789202] , you can confirm these are the 3 largest probabilities in a . You'll also notice [3, 0, 5] are the corresponding indices. ### Print out the top five softmax probabilities for the predictions on the German traffic sign images found on the web. ### Feel free to use as many code cells as needed. with tf . Session() as sess: predicts = sess . run(tf . nn . top_k(new_pics_classes, k = 5 , sorted = True )) for i in range ( len (predicts[ 0 ])): probabilities = predicts[ 0 ][i] predicted_classes = predicts[ 1 ][i] print ( 'Image' , i,\\ 'probabilities:' , probabilities,\\ ' \\n and predicted classes:' , predicts[ 1 ][i]) Image 0 probabilities: [ 12.23980713 8.32987499 5.29931879 4.51511765 1.64859486] and predicted classes: [14 17 36 38 34] Image 1 probabilities: [ 20.34670448 11.3846302 6.09567165 1.00739896 0.98556668] and predicted classes: [13 35 34 9 3] Image 2 probabilities: [ 12.05598831 9.00940228 8.62032127 6.8418088 3.65508676] and predicted classes: [18 12 40 37 1] Image 3 probabilities: [ 13.12025452 8.07227325 -0.03553319 -1.66574371 -1.75805712] and predicted classes: [34 38 20 35 32] Image 4 probabilities: [ 4.97775316 2.17866445 1.98024666 1.00796783 0.97446799] and predicted classes: [ 2 10 9 5 7]","title":"3.4:  Output Top 5 Softmax Probabilities For Each Image Found on the Web"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#predicted-result-and-actual-picture-visualization","text":"plt . figure(figsize = ( 14 , 17 )) for i in range ( 5 ): plt . subplot( 5 , 2 , 2 * i + 1 ) plt . imshow(web_pics_data[i,:,:, 0 ],cmap = 'gray' ) plt . title(i) plt . axis( 'off' ) plt . subplot( 5 , 2 , 2 * i + 2 ) plt . barh(np . arange( 1 , 6 , 1 ), predicts . values[i, :]) labs1 = [predicts[ 1 ][i][j] for j in range ( 5 )] labs = [sign[labs1[j]] for j in range ( 5 )] plt . yticks(np . arange( 1 , 6 , 1 ), labs) plt . show()","title":"Predicted Result  and actual picture visualization"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#step-4-visualize-the-neural-networks-state-with-test-images","text":"This Section is not required to complete but acts as an additional excersise for understaning the output of a neural network's weights. While neural networks can be a great learning device they are often referred to as a black box. We can understand what the weights of a neural network look like better by plotting their feature maps. After successfully training your neural network you can see what it's feature maps look like by plotting the output of the network's weight layers in response to a test stimuli image. From these plotted feature maps, it's possible to see what characteristics of an image the network finds interesting. For a sign, maybe the inner network feature maps react with high activation to the sign's boundary outline or to the contrast in the sign's painted symbol. Provided for you below is the function code that allows you to get the visualization output of any tensorflow weight layer you want. The inputs to the function should be a stimuli image, one used during training or a new one you provided, and then the tensorflow variable name that represents the layer's state during the training process, for instance if you wanted to see what the LeNet lab's feature maps looked like for it's second convolutional layer you could enter conv2 as the tf_activation variable. For an example of what feature map outputs look like, check out NVIDIA's results in their paper End-to-End Deep Learning for Self-Driving Cars in the section Visualization of internal CNN State. NVIDIA was able to show that their network's inner weights had high activations to road boundary lines by comparing feature maps from an image with a clear path to one without. Try experimenting with a similar test to show that your trained network's weights are looking for interesting features, whether it's looking at differences in feature maps from images with or without a sign, or even what feature maps look like in a trained network vs a completely untrained one on the same sign image. Your output should look something like this (above)","title":"Step 4: Visualize the Neural Network's State with Test Images"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#setting-function","text":"### Visualize your network's feature maps here. ### Feel free to use as many code cells as needed. # image_input: the test image being fed into the network to produce the feature maps # tf_activation: should be a tf variable name used during your #training procedure that represents the calculated state of a specific weight layer # activation_min/max: can be used to view the activation contrast in more detail, # by default matplot sets min and max to the actual min and max values of the output # plt_num: used to plot out multiple different weight feature map sets on the same block, #just extend the plt number for each new feature map entry def outputFeatureMap (image_input, tf_activation,\\ activation_min =- 1 , activation_max =- 1 ,plt_num = 1 ): # Here make sure to preprocess your image_input in a way your network expects # with size, normalization, ect if needed # image_input = # Note: x should be the same name as your network's tensorflow data placeholder variable # If you get an error tf_activation is not defined it maybe having trouble #accessing the variable from inside a function activation = tf_activation . eval(session = tf . get_default_session(),\\ feed_dict = {x: image_input}) featuremaps = activation . shape[ 3 ] plt . figure(plt_num, figsize = ( 15 , 15 )) for featuremap in range (featuremaps): # sets the number of feature maps to show on each row and column plt . subplot( 6 , 8 , featuremap + 1 ) # displays the feature map number plt . title( 'FeatureMap ' + str (featuremap)) if activation_min != - 1 & activation_max != - 1 : plt . imshow(activation[ 0 ,:,:, featuremap],\\ interpolation = \"nearest\" ,\\ vmin = activation_min,\\ vmax = activation_max,\\ cmap = \"gray\" ) elif activation_max != - 1 : plt . imshow(activation[ 0 ,:,:, featuremap],\\ interpolation = \"nearest\" ,\\ vmax = activation_max,\\ cmap = \"gray\" ) elif activation_min !=- 1 : plt . imshow(activation[ 0 ,:,:, featuremap],\\ interpolation = \"nearest\" ,\\ vmin = activation_min,\\ cmap = \"gray\" ) else : plt . imshow(activation[ 0 ,:,:, featuremap],\\ interpolation = \"nearest\" ,\\ cmap = \"gray\" )","title":"Setting Function"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#plot-of-visual-output","text":"mu = 0 sigma = 0.1 x = tf . placeholder(tf . float32, ( None , 32 , 32 , 1 )) y = tf . placeholder(tf . int32, ( None )) my_image = web_pics_data[ 0 ] '''first conv-layer''' c1_weight = tf . Variable(tf . truncated_normal(shape = ( 5 , 5 , 1 , 6 ),\\ mean = mu,\\ stddev = sigma)) c1_bias = tf . Variable(tf . zeros( 6 )) conv_layer1 = tf . nn . conv2d(x, c1_weight,\\ strides = [ 1 , 1 , 1 , 1 ],\\ padding = 'VALID' , name = 'conv1' ) + c1_bias '''second conv layer''' c2_weight = tf . Variable(tf . truncated_normal(shape = ( 5 , 5 , 6 , 16 ),\\ mean = mu,\\ stddev = sigma)) c2_bias = tf . Variable(tf . zeros( 16 )) conv_layer2 = tf . nn . conv2d(conv_layer1, c2_weight,\\ strides = [ 1 , 1 , 1 , 1 ],\\ padding = 'VALID' , name = 'conv2' ) + c2_bias with tf . Session() as sess: saver . restore(sess, tf . train . latest_checkpoint( '.' )) sess . run(tf . global_variables_initializer()) my_tensor1 = sess . graph . get_tensor_by_name( 'conv1:0' ) my_tensor2 = sess . graph . get_tensor_by_name( 'conv2:0' ) outputFeatureMap([my_image],my_tensor1) outputFeatureMap([my_image],my_tensor2)","title":"Plot of visual output"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#question-9","text":"Discuss how you used the visual output of your trained network's feature maps to show that it had learned to look for interesting characteristics in traffic sign images Answer: Tensorflow allows us to share the variables (ex: sess.graph.get_tensor_by_name('conv1:0') and it becomes possible to draw intermediate data during the training model. The visual output gives us the idea of sharp edge detection and other tendencies in distribution of pixcel intensities. As the layer increases ie. becomes more and more deeper it tries to learn more hidden properties which are not obvious in the single layer. Note : Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to \\n\", \" File -> Download as -> HTML (.html) . Include the finished document along with this notebook as your submission.","title":"Question 9"},{"location":"dl/traffic-sign/Traffic_Sign_Classifier/#project-writeup","text":"Once you have completed the code implementation, document your results in a project writeup using this template as a guide. The writeup can be in a markdown or pdf file.","title":"Project Writeup"},{"location":"dl/translator/dlnd_language_translation/","text":"Language Translation In this project, you\u2019re going to take a peek into the realm of neural network machine translation. You\u2019ll be training a sequence to sequence model on a dataset of English and French sentences that can translate new sentences from English to French. Get the Data Since translating the whole language of English to French will take lots of time to train, we have provided you with a small portion of the English corpus. \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" import helper import problem_unittests as tests source_path = 'data/small_vocab_en' target_path = 'data/small_vocab_fr' source_text = helper . load_data(source_path) target_text = helper . load_data(target_path) Explore the Data Play around with view_sentence_range to view different parts of the data. view_sentence_range = ( 0 , 10 ) \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" import numpy as np print ( 'Dataset Stats' ) print ( 'Roughly the number of unique words: {}' . format( len ({word: None for word in source_text . split()}))) sentences = source_text . split( ' \\n ' ) word_counts = [ len (sentence . split()) for sentence in sentences] print ( 'Number of sentences: {}' . format( len (sentences))) print ( 'Average number of words in a sentence: {}' . format(np . average(word_counts))) print () print ( 'English sentences {} to {}:' . format( * view_sentence_range)) print ( ' \\n ' . join(source_text . split( ' \\n ' )[view_sentence_range[ 0 ]:view_sentence_range[ 1 ]])) print () print ( 'French sentences {} to {}:' . format( * view_sentence_range)) print ( ' \\n ' . join(target_text . split( ' \\n ' )[view_sentence_range[ 0 ]:view_sentence_range[ 1 ]])) Dataset Stats Roughly the number of unique words: 227 Number of sentences: 137861 Average number of words in a sentence: 13.225277634719028 English sentences 0 to 10: new jersey is sometimes quiet during autumn , and it is snowy in april . the united states is usually chilly during july , and it is usually freezing in november . california is usually quiet during march , and it is usually hot in june . the united states is sometimes mild during june , and it is cold in september . your least liked fruit is the grape , but my least liked is the apple . his favorite fruit is the orange , but my favorite is the grape . paris is relaxing during december , but it is usually chilly in july . new jersey is busy during spring , and it is never hot in march . our least liked fruit is the lemon , but my least liked is the grape . the united states is sometimes busy during january , and it is sometimes warm in november . French sentences 0 to 10: new jersey est parfois calme pendant l' automne , et il est neigeux en avril . les \u00e9tats-unis est g\u00e9n\u00e9ralement froid en juillet , et il g\u00e8le habituellement en novembre . california est g\u00e9n\u00e9ralement calme en mars , et il est g\u00e9n\u00e9ralement chaud en juin . les \u00e9tats-unis est parfois l\u00e9g\u00e8re en juin , et il fait froid en septembre . votre moins aim\u00e9 fruit est le raisin , mais mon moins aim\u00e9 est la pomme . son fruit pr\u00e9f\u00e9r\u00e9 est l'orange , mais mon pr\u00e9f\u00e9r\u00e9 est le raisin . paris est relaxant en d\u00e9cembre , mais il est g\u00e9n\u00e9ralement froid en juillet . new jersey est occup\u00e9 au printemps , et il est jamais chaude en mars . notre fruit est moins aim\u00e9 le citron , mais mon moins aim\u00e9 est le raisin . les \u00e9tats-unis est parfois occup\u00e9 en janvier , et il est parfois chaud en novembre . Implement Preprocessing Function Text to Word Ids As you did with other RNNs, you must turn the text into a number so the computer can understand it. In the function text_to_ids() , you'll turn source_text and target_text from words to ids. However, you need to add the <EOS> word id at the end of each sentence from target_text . This will help the neural network predict when the sentence should end. You can get the <EOS> word id by doing: target_vocab_to_int[ '<EOS>' ] You can get other word ids using source_vocab_to_int and target_vocab_to_int . def text_to_ids (source_text, target_text, source_vocab_to_int, target_vocab_to_int): \"\"\" Convert source and target text to proper word ids :param source_text: String that contains all the source text. :param target_text: String that contains all the target text. :param source_vocab_to_int: Dictionary to go from the source words to an id :param target_vocab_to_int: Dictionary to go from the target words to an id :return: A tuple of lists (source_id_text, target_id_text) \"\"\" # TODO: Implement Function '''st = sentence; wd = word''' st_source = [st for st in source_text . split( ' \\n ' )] st_target = [st + ' <EOS>' for st in target_text . split( ' \\n ' )] source_id_text = [[source_vocab_to_int[wd] for wd in st . split()] for st in st_source] target_id_text = [[target_vocab_to_int[wd] for wd in st . split()] for st in st_target] return source_id_text, target_id_text \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_text_to_ids(text_to_ids) Tests Passed Preprocess all the data and save it Running the code cell below will preprocess all the data and save it to file. \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" helper . preprocess_and_save_data(source_path, target_path, text_to_ids) Check Point This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk. \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" import numpy as np import helper (source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper . load_preprocess() Check the Version of TensorFlow and Access to GPU This will check to make sure you have the correct version of TensorFlow and access to a GPU \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" from distutils.version import LooseVersion import warnings import tensorflow as tf # Check TensorFlow Version assert LooseVersion(tf . __version__) in [LooseVersion( '1.0.0' ), LooseVersion( '1.0.1' )], 'This project requires TensorFlow version 1.0 You are using {}' . format(tf . __version__) print ( 'TensorFlow Version: {}' . format(tf . __version__)) # Check for a GPU if not tf . test . gpu_device_name(): warnings . warn( 'No GPU found. Please use a GPU to train your neural network.' ) else : print ( 'Default GPU Device: {}' . format(tf . test . gpu_device_name())) TensorFlow Version: 1.0.0 Default GPU Device: /gpu:0 Build the Neural Network You'll build the components necessary to build a Sequence-to-Sequence model by implementing the following functions below: - model_inputs - process_decoding_input - encoding_layer - decoding_layer_train - decoding_layer_infer - decoding_layer - seq2seq_model Input Implement the model_inputs() function to create TF Placeholders for the Neural Network. It should create the following placeholders: Input text placeholder named \"input\" using the TF Placeholder name parameter with rank 2. Targets placeholder with rank 2. Learning rate placeholder with rank 0. Keep probability placeholder named \"keep_prob\" using the TF Placeholder name parameter with rank 0. Return the placeholders in the following the tuple (Input, Targets, Learing Rate, Keep Probability) def model_inputs (): \"\"\" Create TF Placeholders for input, targets, and learning rate. :return: Tuple (input, targets, learning rate, keep probability) \"\"\" # TODO: Implement Function input_ = tf . placeholder(tf . int32, [ None , None ], 'input' ) target_ = tf . placeholder(tf . int32, [ None , None ], 'target' ) learning_rate_ = tf . placeholder(tf . float32, None , 'lr' ) keep_prob_ = tf . placeholder(tf . float32, None , 'keep_prob' ) return input_, target_, learning_rate_, keep_prob_ \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_model_inputs(model_inputs) Tests Passed Process Decoding Input Implement process_decoding_input using TensorFlow to remove the last word id from each batch in target_data and concat the GO ID to the begining of each batch. def process_decoding_input (target_data, target_vocab_to_int, batch_size): \"\"\" Preprocess target data for dencoding :param target_data: Target Placehoder :param target_vocab_to_int: Dictionary to go from the target words to an id :param batch_size: Batch Size :return: Preprocessed target data \"\"\" # TODO: Implement Function A = tf . strided_slice(target_data, [ 0 , 0 ], [batch_size, - 1 ], [ 1 , 1 ]) B = tf . fill([batch_size, 1 ], target_vocab_to_int[ '<GO>' ]) C = tf . concat([B, A], 1 ) return C \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_process_decoding_input(process_decoding_input) Tests Passed Encoding Implement encoding_layer() to create a Encoder RNN layer using tf.nn.dynamic_rnn() . def encoding_layer (rnn_inputs, rnn_size, num_layers, keep_prob): \"\"\" Create encoding layer :param rnn_inputs: Inputs for the RNN :param rnn_size: RNN Size :param num_layers: Number of layers :param keep_prob: Dropout keep probability :return: RNN state \"\"\" # TODO: Implement Function lstm = tf . contrib . rnn . BasicLSTMCell(rnn_size) dropout = tf . contrib . rnn . DropoutWrapper(lstm, keep_prob) cell = tf . contrib . rnn . MultiRNNCell([dropout] * num_layers) outputs, final_state = tf . nn . dynamic_rnn(cell, rnn_inputs, dtype = tf . float32) return final_state \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_encoding_layer(encoding_layer) Tests Passed Decoding - Training Create training logits using tf.contrib.seq2seq.simple_decoder_fn_train() and tf.contrib.seq2seq.dynamic_rnn_decoder() . Apply the output_fn to the tf.contrib.seq2seq.dynamic_rnn_decoder() outputs. def decoding_layer_train (encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob): \"\"\" Create a decoding layer for training :param encoder_state: Encoder State :param dec_cell: Decoder RNN Cell :param dec_embed_input: Decoder embedded input :param sequence_length: Sequence Length :param decoding_scope: TenorFlow Variable Scope for decoding :param output_fn: Function to apply the output layer :param keep_prob: Dropout keep probability :return: Train Logits \"\"\" # TODO: Implement Function # 1. Create training logits using tf.contrib.seq2seq.simple_decoder_fn_train() simple_dec_fn_train = tf . contrib . seq2seq . simple_decoder_fn_train(encoder_state) # 2. and tf.contrib.seq2seq.dynamic_rnn_decoder() and # Apply the output_fn to the tf.contrib.seq2seq.dynamic_rnn_decoder() outputs. outputs_train, final_state, final_context_state = \\ tf . contrib . seq2seq . dynamic_rnn_decoder(cell = dec_cell, decoder_fn = simple_dec_fn_train, inputs = dec_embed_input, sequence_length = sequence_length, scope = decoding_scope) logits_train = output_fn(outputs_train) logits = tf . nn . dropout(logits_train, keep_prob) return logits \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_decoding_layer_train(decoding_layer_train) Tests Passed Decoding - Inference Create inference logits using tf.contrib.seq2seq.simple_decoder_fn_inference() and tf.contrib.seq2seq.dynamic_rnn_decoder() . def decoding_layer_infer (encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size, decoding_scope, output_fn, keep_prob): \"\"\" Create a decoding layer for inference :param encoder_state: Encoder state :param dec_cell: Decoder RNN Cell :param dec_embeddings: Decoder embeddings :param start_of_sequence_id: GO ID :param end_of_sequence_id: EOS Id :param maximum_length: The maximum allowed time steps to decode :param vocab_size: Size of vocabulary :param decoding_scope: TensorFlow Variable Scope for decoding :param output_fn: Function to apply the output layer :param keep_prob: Dropout keep probability :return: Inference Logits \"\"\" # TODO: Implement Function # Create inference logits using tf.contrib.seq2seq.simple_decoder_fn_inference() simple_dec_fn_infer = tf . contrib . seq2seq . simple_decoder_fn_inference(output_fn, encoder_state, dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size) # add dropout layer dropout = tf . contrib . rnn . DropoutWrapper(dec_cell, keep_prob) # and use tf.contrib.seq2seq.dynamic_rnn_decoder(). logits_infer,final_state, final_context_state = \\ tf . contrib . seq2seq . dynamic_rnn_decoder(dropout, simple_dec_fn_infer, sequence_length = maximum_length, scope = decoding_scope) return logits_infer \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_decoding_layer_infer(decoding_layer_infer) Tests Passed Build the Decoding Layer Implement decoding_layer() to create a Decoder RNN layer. Create RNN cell for decoding using rnn_size and num_layers . Create the output fuction using lambda to transform it's input, logits, to class logits. Use the your decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob) function to get the training logits. Use your decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size, decoding_scope, output_fn, keep_prob) function to get the inference logits. Note: You'll need to use tf.variable_scope to share variables between training and inference. def decoding_layer (dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size, num_layers, target_vocab_to_int, keep_prob): \"\"\" Create decoding layer :param dec_embed_input: Decoder embedded input :param dec_embeddings: Decoder embeddings :param encoder_state: The encoded state :param vocab_size: Size of vocabulary :param sequence_length: Sequence Length :param rnn_size: RNN Size :param num_layers: Number of layers :param target_vocab_to_int: Dictionary to go from the target words to an id :param keep_prob: Dropout keep probability :return: Tuple of (Training Logits, Inference Logits) \"\"\" # TODO: Implement Function # 1. Create RNN cell for decoding using rnn_size and num_layers. lstm = tf . contrib . rnn . BasicLSTMCell(rnn_size) dropout = tf . contrib . rnn . DropoutWrapper(lstm, keep_prob) dec_cell = tf . contrib . rnn . MultiRNNCell([dropout] * num_layers) max_target_sentence_length = max ([ len (sentence) for sentence in source_int_text]) # 2. Create the output fuction using lambda to transform it's input, logits, to class logits. with tf . variable_scope( 'decoding_layer' ) as decoding_scope: output_fn = lambda x: tf . contrib . layers . fully_connected(x,\\ num_outputs = vocab_size,\\ activation_fn = None ,\\ scope = decoding_scope) # 3. Use the your decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, # decoding_scope, output_fn, keep_prob) function to get the training logits. with tf . variable_scope( 'decoding_layer' ) as decoding_scope: logits_train = decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob) # 4. Use your decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, # end_of_sequence_id, maximum_length, vocab_size, decoding_scope, output_fn, keep_prob) # function to get the inference logits. with tf . variable_scope( 'decoding_layer' , reuse = True ) as decoding_scope: logits_infer = decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, target_vocab_to_int[ '<GO>' ], target_vocab_to_int[ '<EOS>' ], max_target_sentence_length, vocab_size, decoding_scope, output_fn, keep_prob) return logits_train, logits_infer \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_decoding_layer(decoding_layer) Tests Passed Build the Neural Network Apply the functions you implemented above to: Apply embedding to the input data for the encoder. Encode the input using your encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob) . Process target data using your process_decoding_input(target_data, target_vocab_to_int, batch_size) function. Apply embedding to the target data for the decoder. Decode the encoded input using your decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size, num_layers, target_vocab_to_int, keep_prob) . def seq2seq_model (input_data, target_data, keep_prob, batch_size, sequence_length, source_vocab_size, target_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers, target_vocab_to_int): \"\"\" Build the Sequence-to-Sequence part of the neural network :param input_data: Input placeholder :param target_data: Target placeholder :param keep_prob: Dropout keep probability placeholder :param batch_size: Batch Size :param sequence_length: Sequence Length :param source_vocab_size: Source vocabulary size :param target_vocab_size: Target vocabulary size :param enc_embedding_size: Decoder embedding size :param dec_embedding_size: Encoder embedding size :param rnn_size: RNN Size :param num_layers: Number of layers :param target_vocab_to_int: Dictionary to go from the target words to an id :return: Tuple of (Training Logits, Inference Logits) \"\"\" # TODO: Implement Function # 1. Apply embedding to the input data for the encoder. enc_embed_input = tf . contrib . layers . embed_sequence(input_data,\\ source_vocab_size,\\ enc_embedding_size) # 2. Encode the input using your encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob). enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob) # 3. Process target data using your process_decoding_input(target_data, target_vocab_to_int, batch_size) function. dec_input = process_decoding_input(target_data, target_vocab_to_int, batch_size) # 4. Apply embedding to the target data for the decoder. dec_embeddings = tf . Variable(tf . truncated_normal([target_vocab_size, dec_embedding_size], stddev = 0.01 )) # 5. Decode the encoded input using your decoding_layer(dec_embed_input, dec_embeddings, encoder_state, # vocab_size, sequence_length, rnn_size, num_layers, target_vocab_to_int, keep_prob). dec_embed_input = tf . nn . embedding_lookup(dec_embeddings, dec_input) logits_train, logits_infer = decoding_layer(dec_embed_input, dec_embeddings, enc_state, target_vocab_size, sequence_length, rnn_size, num_layers, target_vocab_to_int, keep_prob) return logits_train, logits_infer \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_seq2seq_model(seq2seq_model) Tests Passed Neural Network Training Hyperparameters Tune the following parameters: Set epochs to the number of epochs. Set batch_size to the batch size. Set rnn_size to the size of the RNNs. Set num_layers to the number of layers. Set encoding_embedding_size to the size of the embedding for the encoder. Set decoding_embedding_size to the size of the embedding for the decoder. Set learning_rate to the learning rate. Set keep_probability to the Dropout keep probability # Number of Epochs epochs = 10 # Batch Size batch_size = 128 # RNN Size rnn_size = 300 # Number of Layers num_layers = 3 # Embedding Size encoding_embedding_size = 160 decoding_embedding_size = 160 # Learning Rate learning_rate = 0.001 # Dropout Keep Probability keep_probability = 0.70 Build the Graph Build the graph using the neural network you implemented. \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" save_path = 'checkpoints/dev' (source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper . load_preprocess() max_source_sentence_length = max ([ len (sentence) for sentence in source_int_text]) train_graph = tf . Graph() with train_graph . as_default(): input_data, targets, lr, keep_prob = model_inputs() sequence_length = tf . placeholder_with_default(max_source_sentence_length, None , name = 'sequence_length' ) input_shape = tf . shape(input_data) train_logits, inference_logits = seq2seq_model( tf . reverse(input_data, [ - 1 ]), targets, keep_prob, batch_size, sequence_length, len (source_vocab_to_int), len (target_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, target_vocab_to_int) tf . identity(inference_logits, 'logits' ) with tf . name_scope( \"optimization\" ): # Loss function cost = tf . contrib . seq2seq . sequence_loss( train_logits, targets, tf . ones([input_shape[ 0 ], sequence_length])) # Optimizer optimizer = tf . train . AdamOptimizer(lr) # Gradient Clipping gradients = optimizer . compute_gradients(cost) capped_gradients = [(tf . clip_by_value(grad, - 1. , 1. ), var) for grad, var in gradients if grad is not None ] train_op = optimizer . apply_gradients(capped_gradients) Train Train the neural network on the preprocessed data. If you have a hard time getting a good loss, check the forms to see if anyone is having the same problem. \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" import time def get_accuracy (target, logits): \"\"\" Calculate accuracy \"\"\" max_seq = max (target . shape[ 1 ], logits . shape[ 1 ]) if max_seq - target . shape[ 1 ]: target = np . pad( target, [( 0 , 0 ),( 0 ,max_seq - target . shape[ 1 ])], 'constant' ) if max_seq - logits . shape[ 1 ]: logits = np . pad( logits, [( 0 , 0 ),( 0 ,max_seq - logits . shape[ 1 ]), ( 0 , 0 )], 'constant' ) return np . mean(np . equal(target, np . argmax(logits, 2 ))) train_source = source_int_text[batch_size:] train_target = target_int_text[batch_size:] valid_source = helper . pad_sentence_batch(source_int_text[:batch_size]) valid_target = helper . pad_sentence_batch(target_int_text[:batch_size]) with tf . Session(graph = train_graph) as sess: sess . run(tf . global_variables_initializer()) for epoch_i in range (epochs): for batch_i, (source_batch, target_batch) in enumerate ( helper . batch_data(train_source, train_target, batch_size)): start_time = time . time() _, loss = sess . run( [train_op, cost], {input_data: source_batch, targets: target_batch, lr: learning_rate, sequence_length: target_batch . shape[ 1 ], keep_prob: keep_probability}) batch_train_logits = sess . run( inference_logits, {input_data: source_batch, keep_prob: 1.0 }) batch_valid_logits = sess . run( inference_logits, {input_data: valid_source, keep_prob: 1.0 }) train_acc = get_accuracy(target_batch, batch_train_logits) valid_acc = get_accuracy(np . array(valid_target), batch_valid_logits) end_time = time . time() print ( 'Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.3f}, Validation Accuracy: {:>6.3f}, Loss: {:>6.3f}' . format(epoch_i, batch_i, len (source_int_text) // batch_size, train_acc, valid_acc, loss)) # Save Model saver = tf . train . Saver() saver . save(sess, save_path) print ( 'Model Trained and Saved' ) Epoch 0 Batch 0/1077 - Train Accuracy: 0.324, Validation Accuracy: 0.335, Loss: 5.881 Epoch 0 Batch 1/1077 - Train Accuracy: 0.255, Validation Accuracy: 0.335, Loss: 5.863 Epoch 0 Batch 2/1077 - Train Accuracy: 0.244, Validation Accuracy: 0.335, Loss: 5.794 Epoch 0 Batch 3/1077 - Train Accuracy: 0.250, Validation Accuracy: 0.319, Loss: 5.441 Epoch 0 Batch 4/1077 - Train Accuracy: 0.262, Validation Accuracy: 0.336, Loss: 4.921 Epoch 0 Batch 5/1077 - Train Accuracy: 0.296, Validation Accuracy: 0.339, Loss: 4.560 Epoch 0 Batch 6/1077 - Train Accuracy: 0.283, Validation Accuracy: 0.343, Loss: 4.586 Epoch 0 Batch 7/1077 - Train Accuracy: 0.262, Validation Accuracy: 0.337, Loss: 4.475 Epoch 0 Batch 8/1077 - Train Accuracy: 0.279, Validation Accuracy: 0.347, Loss: 4.324 Epoch 0 Batch 9/1077 - Train Accuracy: 0.315, Validation Accuracy: 0.375, Loss: 4.254 Epoch 0 Batch 10/1077 - Train Accuracy: 0.270, Validation Accuracy: 0.368, Loss: 4.311 Epoch 0 Batch 11/1077 - Train Accuracy: 0.329, Validation Accuracy: 0.368, Loss: 4.150 Epoch 0 Batch 12/1077 - Train Accuracy: 0.311, Validation Accuracy: 0.376, Loss: 4.131 Epoch 0 Batch 13/1077 - Train Accuracy: 0.358, Validation Accuracy: 0.377, Loss: 3.920 Epoch 0 Batch 14/1077 - Train Accuracy: 0.331, Validation Accuracy: 0.374, Loss: 3.901 Epoch 0 Batch 15/1077 - Train Accuracy: 0.337, Validation Accuracy: 0.401, Loss: 4.001 Epoch 0 Batch 16/1077 - Train Accuracy: 0.352, Validation Accuracy: 0.397, Loss: 3.933 Epoch 0 Batch 17/1077 - Train Accuracy: 0.368, Validation Accuracy: 0.412, Loss: 3.860 Epoch 0 Batch 18/1077 - Train Accuracy: 0.354, Validation Accuracy: 0.419, Loss: 3.938 Epoch 0 Batch 19/1077 - Train Accuracy: 0.378, Validation Accuracy: 0.422, Loss: 3.882 Epoch 0 Batch 20/1077 - Train Accuracy: 0.355, Validation Accuracy: 0.411, Loss: 3.861 Epoch 0 Batch 21/1077 - Train Accuracy: 0.333, Validation Accuracy: 0.413, Loss: 3.866 Epoch 0 Batch 22/1077 - Train Accuracy: 0.355, Validation Accuracy: 0.421, Loss: 3.920 Epoch 0 Batch 23/1077 - Train Accuracy: 0.365, Validation Accuracy: 0.425, Loss: 3.797 Epoch 0 Batch 24/1077 - Train Accuracy: 0.371, Validation Accuracy: 0.424, Loss: 3.814 Epoch 0 Batch 25/1077 - Train Accuracy: 0.364, Validation Accuracy: 0.422, Loss: 3.780 Epoch 0 Batch 26/1077 - Train Accuracy: 0.361, Validation Accuracy: 0.429, Loss: 3.723 Epoch 0 Batch 27/1077 - Train Accuracy: 0.420, Validation Accuracy: 0.438, Loss: 3.564 Epoch 0 Batch 28/1077 - Train Accuracy: 0.398, Validation Accuracy: 0.436, Loss: 3.627 Epoch 0 Batch 29/1077 - Train Accuracy: 0.384, Validation Accuracy: 0.434, Loss: 3.669 Epoch 0 Batch 30/1077 - Train Accuracy: 0.391, Validation Accuracy: 0.453, Loss: 3.635 Epoch 0 Batch 31/1077 - Train Accuracy: 0.388, Validation Accuracy: 0.457, Loss: 3.665 Epoch 0 Batch 32/1077 - Train Accuracy: 0.435, Validation Accuracy: 0.454, Loss: 3.496 Epoch 0 Batch 33/1077 - Train Accuracy: 0.422, Validation Accuracy: 0.454, Loss: 3.503 Epoch 0 Batch 34/1077 - Train Accuracy: 0.398, Validation Accuracy: 0.458, Loss: 3.511 Epoch 0 Batch 35/1077 - Train Accuracy: 0.406, Validation Accuracy: 0.456, Loss: 3.537 Epoch 0 Batch 36/1077 - Train Accuracy: 0.407, Validation Accuracy: 0.459, Loss: 3.471 Epoch 0 Batch 37/1077 - Train Accuracy: 0.387, Validation Accuracy: 0.445, Loss: 3.545 Epoch 0 Batch 38/1077 - Train Accuracy: 0.378, Validation Accuracy: 0.469, Loss: 3.708 Epoch 0 Batch 39/1077 - Train Accuracy: 0.395, Validation Accuracy: 0.453, Loss: 3.466 Epoch 0 Batch 40/1077 - Train Accuracy: 0.411, Validation Accuracy: 0.467, Loss: 3.501 Epoch 0 Batch 41/1077 - Train Accuracy: 0.420, Validation Accuracy: 0.454, Loss: 3.426 Epoch 0 Batch 42/1077 - Train Accuracy: 0.416, Validation Accuracy: 0.473, Loss: 3.434 Epoch 0 Batch 43/1077 - Train Accuracy: 0.396, Validation Accuracy: 0.462, Loss: 3.523 Epoch 0 Batch 44/1077 - Train Accuracy: 0.372, Validation Accuracy: 0.474, Loss: 3.563 Epoch 0 Batch 45/1077 - Train Accuracy: 0.403, Validation Accuracy: 0.471, Loss: 3.408 Epoch 0 Batch 46/1077 - Train Accuracy: 0.396, Validation Accuracy: 0.465, Loss: 3.455 Epoch 0 Batch 47/1077 - Train Accuracy: 0.430, Validation Accuracy: 0.475, Loss: 3.303 Epoch 0 Batch 48/1077 - Train Accuracy: 0.410, Validation Accuracy: 0.464, Loss: 3.370 Epoch 0 Batch 49/1077 - Train Accuracy: 0.411, Validation Accuracy: 0.477, Loss: 3.365 Epoch 0 Batch 50/1077 - Train Accuracy: 0.363, Validation Accuracy: 0.445, Loss: 3.501 Epoch 0 Batch 51/1077 - Train Accuracy: 0.435, Validation Accuracy: 0.479, Loss: 3.434 Epoch 0 Batch 52/1077 - Train Accuracy: 0.407, Validation Accuracy: 0.471, Loss: 3.370 Epoch 0 Batch 53/1077 - Train Accuracy: 0.393, Validation Accuracy: 0.440, Loss: 3.417 Epoch 0 Batch 54/1077 - Train Accuracy: 0.375, Validation Accuracy: 0.467, Loss: 3.545 Epoch 0 Batch 55/1077 - Train Accuracy: 0.452, Validation Accuracy: 0.478, Loss: 3.253 Epoch 0 Batch 56/1077 - Train Accuracy: 0.405, Validation Accuracy: 0.466, Loss: 3.331 Epoch 0 Batch 57/1077 - Train Accuracy: 0.454, Validation Accuracy: 0.459, Loss: 3.199 Epoch 0 Batch 58/1077 - Train Accuracy: 0.424, Validation Accuracy: 0.474, Loss: 3.389 Epoch 0 Batch 59/1077 - Train Accuracy: 0.395, Validation Accuracy: 0.485, Loss: 3.356 Epoch 0 Batch 60/1077 - Train Accuracy: 0.406, Validation Accuracy: 0.461, Loss: 3.284 Epoch 0 Batch 61/1077 - Train Accuracy: 0.406, Validation Accuracy: 0.477, Loss: 3.289 Epoch 0 Batch 62/1077 - Train Accuracy: 0.410, Validation Accuracy: 0.486, Loss: 3.302 Epoch 0 Batch 63/1077 - Train Accuracy: 0.446, Validation Accuracy: 0.473, Loss: 3.155 Epoch 0 Batch 64/1077 - Train Accuracy: 0.389, Validation Accuracy: 0.466, Loss: 3.280 Epoch 0 Batch 65/1077 - Train Accuracy: 0.400, Validation Accuracy: 0.480, Loss: 3.324 Epoch 0 Batch 66/1077 - Train Accuracy: 0.415, Validation Accuracy: 0.485, Loss: 3.242 Epoch 0 Batch 67/1077 - Train Accuracy: 0.429, Validation Accuracy: 0.462, Loss: 3.176 Epoch 0 Batch 68/1077 - Train Accuracy: 0.396, Validation Accuracy: 0.484, Loss: 3.244 Epoch 0 Batch 69/1077 - Train Accuracy: 0.459, Validation Accuracy: 0.488, Loss: 3.250 Epoch 0 Batch 70/1077 - Train Accuracy: 0.377, Validation Accuracy: 0.480, Loss: 3.334 Epoch 0 Batch 71/1077 - Train Accuracy: 0.388, Validation Accuracy: 0.459, Loss: 3.225 Epoch 0 Batch 72/1077 - Train Accuracy: 0.424, Validation Accuracy: 0.483, Loss: 3.248 Epoch 0 Batch 73/1077 - Train Accuracy: 0.427, Validation Accuracy: 0.479, Loss: 3.168 Epoch 0 Batch 74/1077 - Train Accuracy: 0.420, Validation Accuracy: 0.466, Loss: 3.127 Epoch 0 Batch 75/1077 - Train Accuracy: 0.424, Validation Accuracy: 0.471, Loss: 3.063 Epoch 0 Batch 76/1077 - Train Accuracy: 0.435, Validation Accuracy: 0.494, Loss: 3.139 Epoch 0 Batch 77/1077 - Train Accuracy: 0.416, Validation Accuracy: 0.492, Loss: 3.183 Epoch 0 Batch 78/1077 - Train Accuracy: 0.386, Validation Accuracy: 0.482, Loss: 3.269 Epoch 0 Batch 79/1077 - Train Accuracy: 0.417, Validation Accuracy: 0.482, Loss: 3.213 Epoch 0 Batch 80/1077 - Train Accuracy: 0.440, Validation Accuracy: 0.484, Loss: 3.123 Epoch 0 Batch 81/1077 - Train Accuracy: 0.452, Validation Accuracy: 0.488, Loss: 3.096 Epoch 0 Batch 82/1077 - Train Accuracy: 0.473, Validation Accuracy: 0.484, Loss: 3.003 Epoch 0 Batch 83/1077 - Train Accuracy: 0.428, Validation Accuracy: 0.505, Loss: 3.298 Epoch 0 Batch 84/1077 - Train Accuracy: 0.454, Validation Accuracy: 0.504, Loss: 3.154 Epoch 0 Batch 85/1077 - Train Accuracy: 0.443, Validation Accuracy: 0.487, Loss: 3.131 Epoch 0 Batch 86/1077 - Train Accuracy: 0.425, Validation Accuracy: 0.484, Loss: 3.144 Epoch 0 Batch 87/1077 - Train Accuracy: 0.430, Validation Accuracy: 0.496, Loss: 3.128 Epoch 0 Batch 88/1077 - Train Accuracy: 0.425, Validation Accuracy: 0.491, Loss: 3.140 Epoch 0 Batch 89/1077 - Train Accuracy: 0.417, Validation Accuracy: 0.488, Loss: 3.143 Epoch 0 Batch 90/1077 - Train Accuracy: 0.424, Validation Accuracy: 0.493, Loss: 3.137 Epoch 0 Batch 91/1077 - Train Accuracy: 0.483, Validation Accuracy: 0.493, Loss: 2.936 Epoch 0 Batch 92/1077 - Train Accuracy: 0.439, Validation Accuracy: 0.485, Loss: 3.099 Epoch 0 Batch 93/1077 - Train Accuracy: 0.439, Validation Accuracy: 0.500, Loss: 3.179 Epoch 0 Batch 94/1077 - Train Accuracy: 0.467, Validation Accuracy: 0.517, Loss: 3.060 Epoch 0 Batch 95/1077 - Train Accuracy: 0.480, Validation Accuracy: 0.504, Loss: 3.046 Epoch 0 Batch 96/1077 - Train Accuracy: 0.445, Validation Accuracy: 0.487, Loss: 3.108 Epoch 0 Batch 97/1077 - Train Accuracy: 0.441, Validation Accuracy: 0.484, Loss: 3.093 Epoch 0 Batch 98/1077 - Train Accuracy: 0.469, Validation Accuracy: 0.480, Loss: 2.999 Epoch 0 Batch 99/1077 - Train Accuracy: 0.414, Validation Accuracy: 0.489, Loss: 3.196 Epoch 0 Batch 100/1077 - Train Accuracy: 0.444, Validation Accuracy: 0.500, Loss: 3.061 Epoch 0 Batch 101/1077 - Train Accuracy: 0.444, Validation Accuracy: 0.505, Loss: 3.070 Epoch 0 Batch 102/1077 - Train Accuracy: 0.478, Validation Accuracy: 0.513, Loss: 3.038 Epoch 0 Batch 103/1077 - Train Accuracy: 0.391, Validation Accuracy: 0.493, Loss: 3.172 Epoch 0 Batch 104/1077 - Train Accuracy: 0.373, Validation Accuracy: 0.495, Loss: 3.147 Epoch 0 Batch 105/1077 - Train Accuracy: 0.450, Validation Accuracy: 0.496, Loss: 3.061 Epoch 0 Batch 106/1077 - Train Accuracy: 0.440, Validation Accuracy: 0.494, Loss: 3.163 Epoch 0 Batch 107/1077 - Train Accuracy: 0.467, Validation Accuracy: 0.494, Loss: 3.001 Epoch 0 Batch 108/1077 - Train Accuracy: 0.483, Validation Accuracy: 0.505, Loss: 2.941 Epoch 0 Batch 109/1077 - Train Accuracy: 0.459, Validation Accuracy: 0.512, Loss: 3.009 Epoch 0 Batch 110/1077 - Train Accuracy: 0.471, Validation Accuracy: 0.503, Loss: 3.016 Epoch 0 Batch 111/1077 - Train Accuracy: 0.430, Validation Accuracy: 0.494, Loss: 2.948 Epoch 0 Batch 112/1077 - Train Accuracy: 0.453, Validation Accuracy: 0.507, Loss: 3.041 Epoch 0 Batch 113/1077 - Train Accuracy: 0.444, Validation Accuracy: 0.517, Loss: 3.017 Epoch 0 Batch 114/1077 - Train Accuracy: 0.431, Validation Accuracy: 0.475, Loss: 2.991 Epoch 0 Batch 115/1077 - Train Accuracy: 0.449, Validation Accuracy: 0.504, Loss: 3.087 Epoch 0 Batch 116/1077 - Train Accuracy: 0.423, Validation Accuracy: 0.512, Loss: 3.088 Epoch 0 Batch 117/1077 - Train Accuracy: 0.400, Validation Accuracy: 0.502, Loss: 3.048 Epoch 0 Batch 118/1077 - Train Accuracy: 0.410, Validation Accuracy: 0.507, Loss: 3.071 Epoch 0 Batch 119/1077 - Train Accuracy: 0.425, Validation Accuracy: 0.494, Loss: 2.953 Epoch 0 Batch 120/1077 - Train Accuracy: 0.450, Validation Accuracy: 0.506, Loss: 2.916 Epoch 0 Batch 121/1077 - Train Accuracy: 0.450, Validation Accuracy: 0.501, Loss: 2.997 Epoch 0 Batch 122/1077 - Train Accuracy: 0.455, Validation Accuracy: 0.513, Loss: 2.959 Epoch 0 Batch 123/1077 - Train Accuracy: 0.471, Validation Accuracy: 0.503, Loss: 2.831 Epoch 0 Batch 124/1077 - Train Accuracy: 0.461, Validation Accuracy: 0.515, Loss: 2.974 Epoch 0 Batch 125/1077 - Train Accuracy: 0.480, Validation Accuracy: 0.516, Loss: 2.979 Epoch 0 Batch 126/1077 - Train Accuracy: 0.460, Validation Accuracy: 0.512, Loss: 2.883 Epoch 0 Batch 127/1077 - Train Accuracy: 0.473, Validation Accuracy: 0.508, Loss: 2.933 Epoch 0 Batch 128/1077 - Train Accuracy: 0.510, Validation Accuracy: 0.516, Loss: 2.865 Epoch 0 Batch 129/1077 - Train Accuracy: 0.476, Validation Accuracy: 0.520, Loss: 2.884 Epoch 0 Batch 130/1077 - Train Accuracy: 0.464, Validation Accuracy: 0.495, Loss: 2.766 Epoch 0 Batch 131/1077 - Train Accuracy: 0.412, Validation Accuracy: 0.485, Loss: 3.086 Epoch 0 Batch 132/1077 - Train Accuracy: 0.438, Validation Accuracy: 0.496, Loss: 3.078 Epoch 0 Batch 133/1077 - Train Accuracy: 0.402, Validation Accuracy: 0.478, Loss: 2.878 Epoch 0 Batch 134/1077 - Train Accuracy: 0.442, Validation Accuracy: 0.505, Loss: 2.896 Epoch 0 Batch 135/1077 - Train Accuracy: 0.422, Validation Accuracy: 0.496, Loss: 2.939 Epoch 0 Batch 136/1077 - Train Accuracy: 0.441, Validation Accuracy: 0.500, Loss: 2.915 Epoch 0 Batch 137/1077 - Train Accuracy: 0.482, Validation Accuracy: 0.512, Loss: 2.830 Epoch 0 Batch 138/1077 - Train Accuracy: 0.400, Validation Accuracy: 0.458, Loss: 2.795 Epoch 0 Batch 139/1077 - Train Accuracy: 0.457, Validation Accuracy: 0.523, Loss: 2.912 Epoch 0 Batch 140/1077 - Train Accuracy: 0.435, Validation Accuracy: 0.517, Loss: 2.929 Epoch 0 Batch 141/1077 - Train Accuracy: 0.428, Validation Accuracy: 0.495, Loss: 2.901 Epoch 0 Batch 142/1077 - Train Accuracy: 0.451, Validation Accuracy: 0.491, Loss: 2.780 Epoch 0 Batch 143/1077 - Train Accuracy: 0.459, Validation Accuracy: 0.503, Loss: 2.844 Epoch 0 Batch 144/1077 - Train Accuracy: 0.431, Validation Accuracy: 0.517, Loss: 2.852 Epoch 0 Batch 145/1077 - Train Accuracy: 0.458, Validation Accuracy: 0.494, Loss: 2.820 Epoch 0 Batch 146/1077 - Train Accuracy: 0.475, Validation Accuracy: 0.511, Loss: 2.766 Epoch 0 Batch 147/1077 - Train Accuracy: 0.454, Validation Accuracy: 0.520, Loss: 2.858 Epoch 0 Batch 148/1077 - Train Accuracy: 0.473, Validation Accuracy: 0.526, Loss: 2.847 Epoch 0 Batch 149/1077 - Train Accuracy: 0.447, Validation Accuracy: 0.520, Loss: 2.797 Epoch 0 Batch 150/1077 - Train Accuracy: 0.474, Validation Accuracy: 0.504, Loss: 2.802 Epoch 0 Batch 151/1077 - Train Accuracy: 0.465, Validation Accuracy: 0.493, Loss: 2.763 Epoch 0 Batch 152/1077 - Train Accuracy: 0.457, Validation Accuracy: 0.515, Loss: 2.793 Epoch 0 Batch 153/1077 - Train Accuracy: 0.464, Validation Accuracy: 0.527, Loss: 2.790 Epoch 0 Batch 154/1077 - Train Accuracy: 0.429, Validation Accuracy: 0.517, Loss: 2.857 Epoch 0 Batch 155/1077 - Train Accuracy: 0.461, Validation Accuracy: 0.519, Loss: 2.711 Epoch 0 Batch 156/1077 - Train Accuracy: 0.440, Validation Accuracy: 0.487, Loss: 2.786 Epoch 0 Batch 157/1077 - Train Accuracy: 0.481, Validation Accuracy: 0.522, Loss: 2.722 Epoch 0 Batch 158/1077 - Train Accuracy: 0.465, Validation Accuracy: 0.513, Loss: 2.696 Epoch 0 Batch 159/1077 - Train Accuracy: 0.475, Validation Accuracy: 0.514, Loss: 2.706 Epoch 0 Batch 160/1077 - Train Accuracy: 0.461, Validation Accuracy: 0.516, Loss: 2.727 Epoch 0 Batch 161/1077 - Train Accuracy: 0.421, Validation Accuracy: 0.500, Loss: 2.726 Epoch 0 Batch 162/1077 - Train Accuracy: 0.410, Validation Accuracy: 0.458, Loss: 2.779 Epoch 0 Batch 163/1077 - Train Accuracy: 0.404, Validation Accuracy: 0.494, Loss: 2.744 Epoch 0 Batch 164/1077 - Train Accuracy: 0.452, Validation Accuracy: 0.501, Loss: 2.767 Epoch 0 Batch 165/1077 - Train Accuracy: 0.413, Validation Accuracy: 0.482, Loss: 2.738 Epoch 0 Batch 166/1077 - Train Accuracy: 0.440, Validation Accuracy: 0.460, Loss: 2.720 Epoch 0 Batch 167/1077 - Train Accuracy: 0.421, Validation Accuracy: 0.503, Loss: 2.758 Epoch 0 Batch 168/1077 - Train Accuracy: 0.417, Validation Accuracy: 0.486, Loss: 2.875 Epoch 0 Batch 169/1077 - Train Accuracy: 0.462, Validation Accuracy: 0.467, Loss: 2.651 Epoch 0 Batch 170/1077 - Train Accuracy: 0.417, Validation Accuracy: 0.501, Loss: 2.728 Epoch 0 Batch 171/1077 - Train Accuracy: 0.492, Validation Accuracy: 0.490, Loss: 2.636 Epoch 0 Batch 172/1077 - Train Accuracy: 0.447, Validation Accuracy: 0.485, Loss: 2.644 Epoch 0 Batch 173/1077 - Train Accuracy: 0.397, Validation Accuracy: 0.507, Loss: 2.783 Epoch 0 Batch 174/1077 - Train Accuracy: 0.464, Validation Accuracy: 0.500, Loss: 2.660 Epoch 0 Batch 175/1077 - Train Accuracy: 0.451, Validation Accuracy: 0.500, Loss: 2.614 Epoch 0 Batch 176/1077 - Train Accuracy: 0.420, Validation Accuracy: 0.504, Loss: 2.628 Epoch 0 Batch 177/1077 - Train Accuracy: 0.409, Validation Accuracy: 0.479, Loss: 2.617 Epoch 0 Batch 178/1077 - Train Accuracy: 0.461, Validation Accuracy: 0.495, Loss: 2.629 Epoch 0 Batch 179/1077 - Train Accuracy: 0.457, Validation Accuracy: 0.503, Loss: 2.729 Epoch 0 Batch 180/1077 - Train Accuracy: 0.427, Validation Accuracy: 0.499, Loss: 2.572 Epoch 0 Batch 181/1077 - Train Accuracy: 0.448, Validation Accuracy: 0.490, Loss: 2.650 Epoch 0 Batch 182/1077 - Train Accuracy: 0.453, Validation Accuracy: 0.494, Loss: 2.675 Epoch 0 Batch 183/1077 - Train Accuracy: 0.405, Validation Accuracy: 0.473, Loss: 2.686 Epoch 0 Batch 184/1077 - Train Accuracy: 0.418, Validation Accuracy: 0.471, Loss: 2.578 Epoch 0 Batch 185/1077 - Train Accuracy: 0.429, Validation Accuracy: 0.488, Loss: 2.613 Epoch 0 Batch 186/1077 - Train Accuracy: 0.467, Validation Accuracy: 0.491, Loss: 2.695 Epoch 0 Batch 187/1077 - Train Accuracy: 0.441, Validation Accuracy: 0.511, Loss: 2.600 Epoch 0 Batch 188/1077 - Train Accuracy: 0.488, Validation Accuracy: 0.522, Loss: 2.568 Epoch 0 Batch 189/1077 - Train Accuracy: 0.470, Validation Accuracy: 0.513, Loss: 2.628 Epoch 0 Batch 190/1077 - Train Accuracy: 0.447, Validation Accuracy: 0.495, Loss: 2.651 Epoch 0 Batch 191/1077 - Train Accuracy: 0.483, Validation Accuracy: 0.499, Loss: 2.526 Epoch 0 Batch 192/1077 - Train Accuracy: 0.487, Validation Accuracy: 0.508, Loss: 2.561 Epoch 0 Batch 193/1077 - Train Accuracy: 0.483, Validation Accuracy: 0.500, Loss: 2.508 Epoch 0 Batch 194/1077 - Train Accuracy: 0.474, Validation Accuracy: 0.498, Loss: 2.527 Epoch 0 Batch 195/1077 - Train Accuracy: 0.432, Validation Accuracy: 0.494, Loss: 2.622 Epoch 0 Batch 196/1077 - Train Accuracy: 0.463, Validation Accuracy: 0.493, Loss: 2.579 Epoch 0 Batch 197/1077 - Train Accuracy: 0.470, Validation Accuracy: 0.512, Loss: 2.551 Epoch 0 Batch 198/1077 - Train Accuracy: 0.465, Validation Accuracy: 0.516, Loss: 2.452 Epoch 0 Batch 199/1077 - Train Accuracy: 0.446, Validation Accuracy: 0.502, Loss: 2.616 Epoch 0 Batch 200/1077 - Train Accuracy: 0.427, Validation Accuracy: 0.502, Loss: 2.638 Epoch 0 Batch 201/1077 - Train Accuracy: 0.465, Validation Accuracy: 0.506, Loss: 2.516 Epoch 0 Batch 202/1077 - Train Accuracy: 0.466, Validation Accuracy: 0.512, Loss: 2.592 Epoch 0 Batch 203/1077 - Train Accuracy: 0.444, Validation Accuracy: 0.510, Loss: 2.571 Epoch 0 Batch 204/1077 - Train Accuracy: 0.453, Validation Accuracy: 0.503, Loss: 2.534 Epoch 0 Batch 205/1077 - Train Accuracy: 0.501, Validation Accuracy: 0.520, Loss: 2.555 Epoch 0 Batch 206/1077 - Train Accuracy: 0.440, Validation Accuracy: 0.498, Loss: 2.576 Epoch 0 Batch 207/1077 - Train Accuracy: 0.416, Validation Accuracy: 0.498, Loss: 2.555 Epoch 0 Batch 208/1077 - Train Accuracy: 0.456, Validation Accuracy: 0.498, Loss: 2.488 Epoch 0 Batch 209/1077 - Train Accuracy: 0.487, Validation Accuracy: 0.517, Loss: 2.456 Epoch 0 Batch 210/1077 - Train Accuracy: 0.489, Validation Accuracy: 0.507, Loss: 2.489 Epoch 0 Batch 211/1077 - Train Accuracy: 0.452, Validation Accuracy: 0.506, Loss: 2.500 Epoch 0 Batch 212/1077 - Train Accuracy: 0.492, Validation Accuracy: 0.509, Loss: 2.491 Epoch 0 Batch 213/1077 - Train Accuracy: 0.466, Validation Accuracy: 0.495, Loss: 2.541 Epoch 0 Batch 214/1077 - Train Accuracy: 0.458, Validation Accuracy: 0.522, Loss: 2.521 Epoch 0 Batch 215/1077 - Train Accuracy: 0.441, Validation Accuracy: 0.511, Loss: 2.559 Epoch 0 Batch 216/1077 - Train Accuracy: 0.447, Validation Accuracy: 0.514, Loss: 2.551 Epoch 0 Batch 217/1077 - Train Accuracy: 0.507, Validation Accuracy: 0.526, Loss: 2.466 Epoch 0 Batch 218/1077 - Train Accuracy: 0.436, Validation Accuracy: 0.511, Loss: 2.608 Epoch 0 Batch 219/1077 - Train Accuracy: 0.478, Validation Accuracy: 0.501, Loss: 2.484 Epoch 0 Batch 220/1077 - Train Accuracy: 0.387, Validation Accuracy: 0.466, Loss: 2.624 Epoch 0 Batch 221/1077 - Train Accuracy: 0.428, Validation Accuracy: 0.475, Loss: 2.521 Epoch 0 Batch 222/1077 - Train Accuracy: 0.400, Validation Accuracy: 0.472, Loss: 2.802 Epoch 0 Batch 223/1077 - Train Accuracy: 0.456, Validation Accuracy: 0.489, Loss: 2.458 Epoch 0 Batch 224/1077 - Train Accuracy: 0.460, Validation Accuracy: 0.489, Loss: 2.531 Epoch 0 Batch 225/1077 - Train Accuracy: 0.421, Validation Accuracy: 0.474, Loss: 2.490 Epoch 0 Batch 226/1077 - Train Accuracy: 0.454, Validation Accuracy: 0.479, Loss: 2.438 Epoch 0 Batch 227/1077 - Train Accuracy: 0.430, Validation Accuracy: 0.481, Loss: 2.647 Epoch 0 Batch 228/1077 - Train Accuracy: 0.474, Validation Accuracy: 0.491, Loss: 2.517 Epoch 0 Batch 229/1077 - Train Accuracy: 0.488, Validation Accuracy: 0.495, Loss: 2.410 Epoch 0 Batch 230/1077 - Train Accuracy: 0.469, Validation Accuracy: 0.500, Loss: 2.465 Epoch 0 Batch 231/1077 - Train Accuracy: 0.479, Validation Accuracy: 0.509, Loss: 2.504 Epoch 0 Batch 232/1077 - Train Accuracy: 0.441, Validation Accuracy: 0.521, Loss: 2.536 Epoch 0 Batch 233/1077 - Train Accuracy: 0.448, Validation Accuracy: 0.520, Loss: 2.596 Epoch 0 Batch 234/1077 - Train Accuracy: 0.504, Validation Accuracy: 0.517, Loss: 2.499 Epoch 0 Batch 235/1077 - Train Accuracy: 0.487, Validation Accuracy: 0.489, Loss: 2.387 Epoch 0 Batch 236/1077 - Train Accuracy: 0.439, Validation Accuracy: 0.487, Loss: 2.506 Epoch 0 Batch 237/1077 - Train Accuracy: 0.474, Validation Accuracy: 0.485, Loss: 2.439 Epoch 0 Batch 238/1077 - Train Accuracy: 0.495, Validation Accuracy: 0.511, Loss: 2.472 Epoch 0 Batch 239/1077 - Train Accuracy: 0.475, Validation Accuracy: 0.500, Loss: 2.405 Epoch 0 Batch 240/1077 - Train Accuracy: 0.464, Validation Accuracy: 0.518, Loss: 2.482 Epoch 0 Batch 241/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.523, Loss: 2.443 Epoch 0 Batch 242/1077 - Train Accuracy: 0.439, Validation Accuracy: 0.515, Loss: 2.458 Epoch 0 Batch 243/1077 - Train Accuracy: 0.425, Validation Accuracy: 0.510, Loss: 2.519 Epoch 0 Batch 244/1077 - Train Accuracy: 0.505, Validation Accuracy: 0.489, Loss: 2.400 Epoch 0 Batch 245/1077 - Train Accuracy: 0.462, Validation Accuracy: 0.491, Loss: 2.388 Epoch 0 Batch 246/1077 - Train Accuracy: 0.416, Validation Accuracy: 0.485, Loss: 2.460 Epoch 0 Batch 247/1077 - Train Accuracy: 0.448, Validation Accuracy: 0.469, Loss: 2.405 Epoch 0 Batch 248/1077 - Train Accuracy: 0.433, Validation Accuracy: 0.477, Loss: 2.421 Epoch 0 Batch 249/1077 - Train Accuracy: 0.478, Validation Accuracy: 0.498, Loss: 2.431 Epoch 0 Batch 250/1077 - Train Accuracy: 0.487, Validation Accuracy: 0.509, Loss: 2.365 Epoch 0 Batch 251/1077 - Train Accuracy: 0.485, Validation Accuracy: 0.493, Loss: 2.412 Epoch 0 Batch 252/1077 - Train Accuracy: 0.460, Validation Accuracy: 0.480, Loss: 2.385 Epoch 0 Batch 253/1077 - Train Accuracy: 0.498, Validation Accuracy: 0.508, Loss: 2.384 Epoch 0 Batch 254/1077 - Train Accuracy: 0.448, Validation Accuracy: 0.495, Loss: 2.477 Epoch 0 Batch 255/1077 - Train Accuracy: 0.437, Validation Accuracy: 0.461, Loss: 2.371 Epoch 0 Batch 256/1077 - Train Accuracy: 0.415, Validation Accuracy: 0.483, Loss: 2.489 Epoch 0 Batch 257/1077 - Train Accuracy: 0.465, Validation Accuracy: 0.498, Loss: 2.416 Epoch 0 Batch 258/1077 - Train Accuracy: 0.471, Validation Accuracy: 0.495, Loss: 2.289 Epoch 0 Batch 259/1077 - Train Accuracy: 0.425, Validation Accuracy: 0.482, Loss: 2.361 Epoch 0 Batch 260/1077 - Train Accuracy: 0.443, Validation Accuracy: 0.450, Loss: 2.374 Epoch 0 Batch 261/1077 - Train Accuracy: 0.448, Validation Accuracy: 0.451, Loss: 2.415 Epoch 0 Batch 262/1077 - Train Accuracy: 0.438, Validation Accuracy: 0.473, Loss: 2.495 Epoch 0 Batch 263/1077 - Train Accuracy: 0.496, Validation Accuracy: 0.478, Loss: 2.344 Epoch 0 Batch 264/1077 - Train Accuracy: 0.420, Validation Accuracy: 0.467, Loss: 2.482 Epoch 0 Batch 265/1077 - Train Accuracy: 0.441, Validation Accuracy: 0.464, Loss: 2.425 Epoch 0 Batch 266/1077 - Train Accuracy: 0.453, Validation Accuracy: 0.472, Loss: 2.342 Epoch 0 Batch 267/1077 - Train Accuracy: 0.464, Validation Accuracy: 0.483, Loss: 2.280 Epoch 0 Batch 268/1077 - Train Accuracy: 0.470, Validation Accuracy: 0.497, Loss: 2.363 Epoch 0 Batch 269/1077 - Train Accuracy: 0.423, Validation Accuracy: 0.506, Loss: 2.423 Epoch 0 Batch 270/1077 - Train Accuracy: 0.441, Validation Accuracy: 0.499, Loss: 2.378 Epoch 0 Batch 271/1077 - Train Accuracy: 0.432, Validation Accuracy: 0.489, Loss: 2.392 Epoch 0 Batch 272/1077 - Train Accuracy: 0.457, Validation Accuracy: 0.490, Loss: 2.350 Epoch 0 Batch 273/1077 - Train Accuracy: 0.458, Validation Accuracy: 0.494, Loss: 2.377 Epoch 0 Batch 274/1077 - Train Accuracy: 0.464, Validation Accuracy: 0.497, Loss: 2.381 Epoch 0 Batch 275/1077 - Train Accuracy: 0.472, Validation Accuracy: 0.495, Loss: 2.283 Epoch 0 Batch 276/1077 - Train Accuracy: 0.459, Validation Accuracy: 0.518, Loss: 2.413 Epoch 0 Batch 277/1077 - Train Accuracy: 0.482, Validation Accuracy: 0.476, Loss: 2.297 Epoch 0 Batch 278/1077 - Train Accuracy: 0.420, Validation Accuracy: 0.465, Loss: 2.365 Epoch 0 Batch 279/1077 - Train Accuracy: 0.433, Validation Accuracy: 0.482, Loss: 2.397 Epoch 0 Batch 280/1077 - Train Accuracy: 0.452, Validation Accuracy: 0.494, Loss: 2.420 Epoch 0 Batch 281/1077 - Train Accuracy: 0.447, Validation Accuracy: 0.496, Loss: 2.414 Epoch 0 Batch 282/1077 - Train Accuracy: 0.436, Validation Accuracy: 0.487, Loss: 2.399 Epoch 0 Batch 283/1077 - Train Accuracy: 0.459, Validation Accuracy: 0.489, Loss: 2.398 Epoch 0 Batch 284/1077 - Train Accuracy: 0.457, Validation Accuracy: 0.483, Loss: 2.361 Epoch 0 Batch 285/1077 - Train Accuracy: 0.466, Validation Accuracy: 0.471, Loss: 2.320 Epoch 0 Batch 286/1077 - Train Accuracy: 0.468, Validation Accuracy: 0.479, Loss: 2.293 Epoch 0 Batch 287/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.538, Loss: 2.251 Epoch 0 Batch 288/1077 - Train Accuracy: 0.470, Validation Accuracy: 0.534, Loss: 2.360 Epoch 0 Batch 289/1077 - Train Accuracy: 0.491, Validation Accuracy: 0.528, Loss: 2.336 Epoch 0 Batch 290/1077 - Train Accuracy: 0.446, Validation Accuracy: 0.513, Loss: 2.392 Epoch 0 Batch 291/1077 - Train Accuracy: 0.477, Validation Accuracy: 0.510, Loss: 2.332 Epoch 0 Batch 292/1077 - Train Accuracy: 0.512, Validation Accuracy: 0.501, Loss: 2.338 Epoch 0 Batch 293/1077 - Train Accuracy: 0.407, Validation Accuracy: 0.480, Loss: 2.418 Epoch 0 Batch 294/1077 - Train Accuracy: 0.498, Validation Accuracy: 0.492, Loss: 2.256 Epoch 0 Batch 295/1077 - Train Accuracy: 0.455, Validation Accuracy: 0.487, Loss: 2.356 Epoch 0 Batch 296/1077 - Train Accuracy: 0.481, Validation Accuracy: 0.492, Loss: 2.292 Epoch 0 Batch 297/1077 - Train Accuracy: 0.431, Validation Accuracy: 0.489, Loss: 2.382 Epoch 0 Batch 298/1077 - Train Accuracy: 0.470, Validation Accuracy: 0.484, Loss: 2.368 Epoch 0 Batch 299/1077 - Train Accuracy: 0.447, Validation Accuracy: 0.476, Loss: 2.335 Epoch 0 Batch 300/1077 - Train Accuracy: 0.450, Validation Accuracy: 0.491, Loss: 2.421 Epoch 0 Batch 301/1077 - Train Accuracy: 0.448, Validation Accuracy: 0.491, Loss: 2.342 Epoch 0 Batch 302/1077 - Train Accuracy: 0.483, Validation Accuracy: 0.482, Loss: 2.246 Epoch 0 Batch 303/1077 - Train Accuracy: 0.436, Validation Accuracy: 0.496, Loss: 2.305 Epoch 0 Batch 304/1077 - Train Accuracy: 0.478, Validation Accuracy: 0.539, Loss: 2.279 Epoch 0 Batch 305/1077 - Train Accuracy: 0.463, Validation Accuracy: 0.519, Loss: 2.369 Epoch 0 Batch 306/1077 - Train Accuracy: 0.453, Validation Accuracy: 0.501, Loss: 2.239 Epoch 0 Batch 307/1077 - Train Accuracy: 0.444, Validation Accuracy: 0.517, Loss: 2.376 Epoch 0 Batch 308/1077 - Train Accuracy: 0.430, Validation Accuracy: 0.509, Loss: 2.343 Epoch 0 Batch 309/1077 - Train Accuracy: 0.470, Validation Accuracy: 0.514, Loss: 2.239 Epoch 0 Batch 310/1077 - Train Accuracy: 0.446, Validation Accuracy: 0.525, Loss: 2.325 Epoch 0 Batch 311/1077 - Train Accuracy: 0.553, Validation Accuracy: 0.540, Loss: 2.274 Epoch 0 Batch 312/1077 - Train Accuracy: 0.488, Validation Accuracy: 0.508, Loss: 2.362 Epoch 0 Batch 313/1077 - Train Accuracy: 0.446, Validation Accuracy: 0.498, Loss: 2.364 Epoch 0 Batch 314/1077 - Train Accuracy: 0.509, Validation Accuracy: 0.495, Loss: 2.359 Epoch 0 Batch 315/1077 - Train Accuracy: 0.487, Validation Accuracy: 0.499, Loss: 2.248 Epoch 0 Batch 316/1077 - Train Accuracy: 0.478, Validation Accuracy: 0.505, Loss: 2.327 Epoch 0 Batch 317/1077 - Train Accuracy: 0.452, Validation Accuracy: 0.502, Loss: 2.404 Epoch 0 Batch 318/1077 - Train Accuracy: 0.448, Validation Accuracy: 0.489, Loss: 2.357 Epoch 0 Batch 319/1077 - Train Accuracy: 0.439, Validation Accuracy: 0.477, Loss: 2.330 Epoch 0 Batch 320/1077 - Train Accuracy: 0.445, Validation Accuracy: 0.489, Loss: 2.394 Epoch 0 Batch 321/1077 - Train Accuracy: 0.450, Validation Accuracy: 0.496, Loss: 2.359 Epoch 0 Batch 322/1077 - Train Accuracy: 0.469, Validation Accuracy: 0.500, Loss: 2.224 Epoch 0 Batch 323/1077 - Train Accuracy: 0.518, Validation Accuracy: 0.512, Loss: 2.424 Epoch 0 Batch 324/1077 - Train Accuracy: 0.468, Validation Accuracy: 0.532, Loss: 2.284 Epoch 0 Batch 325/1077 - Train Accuracy: 0.528, Validation Accuracy: 0.544, Loss: 2.197 Epoch 0 Batch 326/1077 - Train Accuracy: 0.510, Validation Accuracy: 0.526, Loss: 2.215 Epoch 0 Batch 327/1077 - Train Accuracy: 0.444, Validation Accuracy: 0.496, Loss: 2.323 Epoch 0 Batch 328/1077 - Train Accuracy: 0.523, Validation Accuracy: 0.520, Loss: 2.237 Epoch 0 Batch 329/1077 - Train Accuracy: 0.491, Validation Accuracy: 0.556, Loss: 2.298 Epoch 0 Batch 330/1077 - Train Accuracy: 0.525, Validation Accuracy: 0.549, Loss: 2.376 Epoch 0 Batch 331/1077 - Train Accuracy: 0.482, Validation Accuracy: 0.536, Loss: 2.231 Epoch 0 Batch 332/1077 - Train Accuracy: 0.486, Validation Accuracy: 0.530, Loss: 2.238 Epoch 0 Batch 333/1077 - Train Accuracy: 0.512, Validation Accuracy: 0.536, Loss: 2.331 Epoch 0 Batch 334/1077 - Train Accuracy: 0.498, Validation Accuracy: 0.544, Loss: 2.271 Epoch 0 Batch 335/1077 - Train Accuracy: 0.544, Validation Accuracy: 0.538, Loss: 2.192 Epoch 0 Batch 336/1077 - Train Accuracy: 0.494, Validation Accuracy: 0.511, Loss: 2.264 Epoch 0 Batch 337/1077 - Train Accuracy: 0.463, Validation Accuracy: 0.500, Loss: 2.247 Epoch 0 Batch 338/1077 - Train Accuracy: 0.475, Validation Accuracy: 0.506, Loss: 2.264 Epoch 0 Batch 339/1077 - Train Accuracy: 0.489, Validation Accuracy: 0.509, Loss: 2.251 Epoch 0 Batch 340/1077 - Train Accuracy: 0.470, Validation Accuracy: 0.534, Loss: 2.299 Epoch 0 Batch 341/1077 - Train Accuracy: 0.509, Validation Accuracy: 0.542, Loss: 2.303 Epoch 0 Batch 342/1077 - Train Accuracy: 0.473, Validation Accuracy: 0.506, Loss: 2.243 Epoch 0 Batch 343/1077 - Train Accuracy: 0.438, Validation Accuracy: 0.494, Loss: 2.252 Epoch 0 Batch 344/1077 - Train Accuracy: 0.458, Validation Accuracy: 0.500, Loss: 2.273 Epoch 0 Batch 345/1077 - Train Accuracy: 0.485, Validation Accuracy: 0.499, Loss: 2.182 Epoch 0 Batch 346/1077 - Train Accuracy: 0.493, Validation Accuracy: 0.552, Loss: 2.240 Epoch 0 Batch 347/1077 - Train Accuracy: 0.508, Validation Accuracy: 0.536, Loss: 2.215 Epoch 0 Batch 348/1077 - Train Accuracy: 0.515, Validation Accuracy: 0.542, Loss: 2.172 Epoch 0 Batch 349/1077 - Train Accuracy: 0.477, Validation Accuracy: 0.523, Loss: 2.300 Epoch 0 Batch 350/1077 - Train Accuracy: 0.475, Validation Accuracy: 0.554, Loss: 2.260 Epoch 0 Batch 351/1077 - Train Accuracy: 0.468, Validation Accuracy: 0.542, Loss: 2.328 Epoch 0 Batch 352/1077 - Train Accuracy: 0.431, Validation Accuracy: 0.539, Loss: 2.226 Epoch 0 Batch 353/1077 - Train Accuracy: 0.456, Validation Accuracy: 0.534, Loss: 2.236 Epoch 0 Batch 354/1077 - Train Accuracy: 0.490, Validation Accuracy: 0.546, Loss: 2.213 Epoch 0 Batch 355/1077 - Train Accuracy: 0.501, Validation Accuracy: 0.549, Loss: 2.221 Epoch 0 Batch 356/1077 - Train Accuracy: 0.490, Validation Accuracy: 0.539, Loss: 2.203 Epoch 0 Batch 357/1077 - Train Accuracy: 0.530, Validation Accuracy: 0.534, Loss: 2.147 Epoch 0 Batch 358/1077 - Train Accuracy: 0.463, Validation Accuracy: 0.543, Loss: 2.345 Epoch 0 Batch 359/1077 - Train Accuracy: 0.507, Validation Accuracy: 0.553, Loss: 2.317 Epoch 0 Batch 360/1077 - Train Accuracy: 0.514, Validation Accuracy: 0.550, Loss: 2.274 Epoch 0 Batch 361/1077 - Train Accuracy: 0.499, Validation Accuracy: 0.555, Loss: 2.307 Epoch 0 Batch 362/1077 - Train Accuracy: 0.475, Validation Accuracy: 0.535, Loss: 2.233 Epoch 0 Batch 363/1077 - Train Accuracy: 0.471, Validation Accuracy: 0.525, Loss: 2.198 Epoch 0 Batch 364/1077 - Train Accuracy: 0.486, Validation Accuracy: 0.527, Loss: 2.281 Epoch 0 Batch 365/1077 - Train Accuracy: 0.491, Validation Accuracy: 0.544, Loss: 2.219 Epoch 0 Batch 366/1077 - Train Accuracy: 0.491, Validation Accuracy: 0.555, Loss: 2.373 Epoch 0 Batch 367/1077 - Train Accuracy: 0.519, Validation Accuracy: 0.564, Loss: 2.201 Epoch 0 Batch 368/1077 - Train Accuracy: 0.511, Validation Accuracy: 0.554, Loss: 2.171 Epoch 0 Batch 369/1077 - Train Accuracy: 0.509, Validation Accuracy: 0.547, Loss: 2.245 Epoch 0 Batch 370/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.554, Loss: 2.164 Epoch 0 Batch 371/1077 - Train Accuracy: 0.534, Validation Accuracy: 0.558, Loss: 2.288 Epoch 0 Batch 372/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.550, Loss: 2.217 Epoch 0 Batch 373/1077 - Train Accuracy: 0.552, Validation Accuracy: 0.531, Loss: 2.193 Epoch 0 Batch 374/1077 - Train Accuracy: 0.439, Validation Accuracy: 0.543, Loss: 2.278 Epoch 0 Batch 375/1077 - Train Accuracy: 0.541, Validation Accuracy: 0.557, Loss: 2.150 Epoch 0 Batch 376/1077 - Train Accuracy: 0.544, Validation Accuracy: 0.537, Loss: 2.112 Epoch 0 Batch 377/1077 - Train Accuracy: 0.507, Validation Accuracy: 0.523, Loss: 2.180 Epoch 0 Batch 378/1077 - Train Accuracy: 0.507, Validation Accuracy: 0.542, Loss: 2.212 Epoch 0 Batch 379/1077 - Train Accuracy: 0.491, Validation Accuracy: 0.538, Loss: 2.274 Epoch 0 Batch 380/1077 - Train Accuracy: 0.511, Validation Accuracy: 0.547, Loss: 2.132 Epoch 0 Batch 381/1077 - Train Accuracy: 0.500, Validation Accuracy: 0.551, Loss: 2.150 Epoch 0 Batch 382/1077 - Train Accuracy: 0.505, Validation Accuracy: 0.549, Loss: 2.265 Epoch 0 Batch 383/1077 - Train Accuracy: 0.523, Validation Accuracy: 0.549, Loss: 2.174 Epoch 0 Batch 384/1077 - Train Accuracy: 0.515, Validation Accuracy: 0.553, Loss: 2.188 Epoch 0 Batch 385/1077 - Train Accuracy: 0.531, Validation Accuracy: 0.551, Loss: 2.206 Epoch 0 Batch 386/1077 - Train Accuracy: 0.500, Validation Accuracy: 0.540, Loss: 2.135 Epoch 0 Batch 387/1077 - Train Accuracy: 0.530, Validation Accuracy: 0.554, Loss: 2.195 Epoch 0 Batch 388/1077 - Train Accuracy: 0.530, Validation Accuracy: 0.553, Loss: 2.247 Epoch 0 Batch 389/1077 - Train Accuracy: 0.539, Validation Accuracy: 0.553, Loss: 2.228 Epoch 0 Batch 390/1077 - Train Accuracy: 0.464, Validation Accuracy: 0.555, Loss: 2.190 Epoch 0 Batch 391/1077 - Train Accuracy: 0.531, Validation Accuracy: 0.551, Loss: 2.212 Epoch 0 Batch 392/1077 - Train Accuracy: 0.457, Validation Accuracy: 0.556, Loss: 2.197 Epoch 0 Batch 393/1077 - Train Accuracy: 0.529, Validation Accuracy: 0.559, Loss: 2.222 Epoch 0 Batch 394/1077 - Train Accuracy: 0.477, Validation Accuracy: 0.544, Loss: 2.177 Epoch 0 Batch 395/1077 - Train Accuracy: 0.533, Validation Accuracy: 0.544, Loss: 2.170 Epoch 0 Batch 396/1077 - Train Accuracy: 0.514, Validation Accuracy: 0.552, Loss: 2.242 Epoch 0 Batch 397/1077 - Train Accuracy: 0.560, Validation Accuracy: 0.557, Loss: 2.166 Epoch 0 Batch 398/1077 - Train Accuracy: 0.515, Validation Accuracy: 0.547, Loss: 2.205 Epoch 0 Batch 399/1077 - Train Accuracy: 0.463, Validation Accuracy: 0.553, Loss: 2.229 Epoch 0 Batch 400/1077 - Train Accuracy: 0.520, Validation Accuracy: 0.554, Loss: 2.170 Epoch 0 Batch 401/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.555, Loss: 2.228 Epoch 0 Batch 402/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.528, Loss: 2.159 Epoch 0 Batch 403/1077 - Train Accuracy: 0.474, Validation Accuracy: 0.527, Loss: 2.200 Epoch 0 Batch 404/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.529, Loss: 2.165 Epoch 0 Batch 405/1077 - Train Accuracy: 0.462, Validation Accuracy: 0.535, Loss: 2.183 Epoch 0 Batch 406/1077 - Train Accuracy: 0.528, Validation Accuracy: 0.553, Loss: 2.143 Epoch 0 Batch 407/1077 - Train Accuracy: 0.520, Validation Accuracy: 0.559, Loss: 2.238 Epoch 0 Batch 408/1077 - Train Accuracy: 0.513, Validation Accuracy: 0.550, Loss: 2.210 Epoch 0 Batch 409/1077 - Train Accuracy: 0.514, Validation Accuracy: 0.554, Loss: 2.156 Epoch 0 Batch 410/1077 - Train Accuracy: 0.483, Validation Accuracy: 0.565, Loss: 2.255 Epoch 0 Batch 411/1077 - Train Accuracy: 0.544, Validation Accuracy: 0.566, Loss: 2.158 Epoch 0 Batch 412/1077 - Train Accuracy: 0.540, Validation Accuracy: 0.571, Loss: 2.127 Epoch 0 Batch 413/1077 - Train Accuracy: 0.556, Validation Accuracy: 0.571, Loss: 2.174 Epoch 0 Batch 414/1077 - Train Accuracy: 0.506, Validation Accuracy: 0.562, Loss: 2.203 Epoch 0 Batch 415/1077 - Train Accuracy: 0.530, Validation Accuracy: 0.564, Loss: 2.221 Epoch 0 Batch 416/1077 - Train Accuracy: 0.511, Validation Accuracy: 0.557, Loss: 2.133 Epoch 0 Batch 417/1077 - Train Accuracy: 0.510, Validation Accuracy: 0.556, Loss: 2.099 Epoch 0 Batch 418/1077 - Train Accuracy: 0.486, Validation Accuracy: 0.548, Loss: 2.120 Epoch 0 Batch 419/1077 - Train Accuracy: 0.504, Validation Accuracy: 0.562, Loss: 2.142 Epoch 0 Batch 420/1077 - Train Accuracy: 0.479, Validation Accuracy: 0.553, Loss: 2.110 Epoch 0 Batch 421/1077 - Train Accuracy: 0.506, Validation Accuracy: 0.556, Loss: 2.139 Epoch 0 Batch 422/1077 - Train Accuracy: 0.503, Validation Accuracy: 0.548, Loss: 2.130 Epoch 0 Batch 423/1077 - Train Accuracy: 0.512, Validation Accuracy: 0.543, Loss: 2.096 Epoch 0 Batch 424/1077 - Train Accuracy: 0.459, Validation Accuracy: 0.556, Loss: 2.185 Epoch 0 Batch 425/1077 - Train Accuracy: 0.561, Validation Accuracy: 0.554, Loss: 2.184 Epoch 0 Batch 426/1077 - Train Accuracy: 0.525, Validation Accuracy: 0.558, Loss: 2.142 Epoch 0 Batch 427/1077 - Train Accuracy: 0.496, Validation Accuracy: 0.565, Loss: 2.161 Epoch 0 Batch 428/1077 - Train Accuracy: 0.555, Validation Accuracy: 0.569, Loss: 2.037 Epoch 0 Batch 429/1077 - Train Accuracy: 0.529, Validation Accuracy: 0.574, Loss: 2.160 Epoch 0 Batch 430/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.567, Loss: 2.159 Epoch 0 Batch 431/1077 - Train Accuracy: 0.506, Validation Accuracy: 0.562, Loss: 2.226 Epoch 0 Batch 432/1077 - Train Accuracy: 0.540, Validation Accuracy: 0.582, Loss: 2.166 Epoch 0 Batch 433/1077 - Train Accuracy: 0.515, Validation Accuracy: 0.567, Loss: 2.175 Epoch 0 Batch 434/1077 - Train Accuracy: 0.508, Validation Accuracy: 0.572, Loss: 2.168 Epoch 0 Batch 435/1077 - Train Accuracy: 0.548, Validation Accuracy: 0.569, Loss: 2.124 Epoch 0 Batch 436/1077 - Train Accuracy: 0.542, Validation Accuracy: 0.575, Loss: 2.146 Epoch 0 Batch 437/1077 - Train Accuracy: 0.517, Validation Accuracy: 0.588, Loss: 2.149 Epoch 0 Batch 438/1077 - Train Accuracy: 0.543, Validation Accuracy: 0.582, Loss: 2.138 Epoch 0 Batch 439/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.580, Loss: 2.161 Epoch 0 Batch 440/1077 - Train Accuracy: 0.536, Validation Accuracy: 0.581, Loss: 2.128 Epoch 0 Batch 441/1077 - Train Accuracy: 0.504, Validation Accuracy: 0.581, Loss: 2.185 Epoch 0 Batch 442/1077 - Train Accuracy: 0.501, Validation Accuracy: 0.578, Loss: 2.166 Epoch 0 Batch 443/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.569, Loss: 2.092 Epoch 0 Batch 444/1077 - Train Accuracy: 0.538, Validation Accuracy: 0.560, Loss: 2.196 Epoch 0 Batch 445/1077 - Train Accuracy: 0.502, Validation Accuracy: 0.545, Loss: 2.273 Epoch 0 Batch 446/1077 - Train Accuracy: 0.527, Validation Accuracy: 0.542, Loss: 2.082 Epoch 0 Batch 447/1077 - Train Accuracy: 0.504, Validation Accuracy: 0.523, Loss: 2.085 Epoch 0 Batch 448/1077 - Train Accuracy: 0.523, Validation Accuracy: 0.527, Loss: 2.149 Epoch 0 Batch 449/1077 - Train Accuracy: 0.489, Validation Accuracy: 0.526, Loss: 2.148 Epoch 0 Batch 450/1077 - Train Accuracy: 0.496, Validation Accuracy: 0.530, Loss: 2.198 Epoch 0 Batch 451/1077 - Train Accuracy: 0.524, Validation Accuracy: 0.520, Loss: 2.057 Epoch 0 Batch 452/1077 - Train Accuracy: 0.519, Validation Accuracy: 0.534, Loss: 2.167 Epoch 0 Batch 453/1077 - Train Accuracy: 0.563, Validation Accuracy: 0.553, Loss: 2.099 Epoch 0 Batch 454/1077 - Train Accuracy: 0.548, Validation Accuracy: 0.562, Loss: 2.150 Epoch 0 Batch 455/1077 - Train Accuracy: 0.549, Validation Accuracy: 0.558, Loss: 2.114 Epoch 0 Batch 456/1077 - Train Accuracy: 0.514, Validation Accuracy: 0.554, Loss: 2.141 Epoch 0 Batch 457/1077 - Train Accuracy: 0.531, Validation Accuracy: 0.550, Loss: 2.110 Epoch 0 Batch 458/1077 - Train Accuracy: 0.489, Validation Accuracy: 0.570, Loss: 2.151 Epoch 0 Batch 459/1077 - Train Accuracy: 0.570, Validation Accuracy: 0.571, Loss: 2.081 Epoch 0 Batch 460/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.572, Loss: 2.107 Epoch 0 Batch 461/1077 - Train Accuracy: 0.537, Validation Accuracy: 0.583, Loss: 2.085 Epoch 0 Batch 462/1077 - Train Accuracy: 0.524, Validation Accuracy: 0.581, Loss: 2.070 Epoch 0 Batch 463/1077 - Train Accuracy: 0.503, Validation Accuracy: 0.583, Loss: 2.159 Epoch 0 Batch 464/1077 - Train Accuracy: 0.569, Validation Accuracy: 0.584, Loss: 2.106 Epoch 0 Batch 465/1077 - Train Accuracy: 0.532, Validation Accuracy: 0.581, Loss: 2.158 Epoch 0 Batch 466/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.580, Loss: 2.169 Epoch 0 Batch 467/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.569, Loss: 2.080 Epoch 0 Batch 468/1077 - Train Accuracy: 0.522, Validation Accuracy: 0.559, Loss: 2.160 Epoch 0 Batch 469/1077 - Train Accuracy: 0.482, Validation Accuracy: 0.548, Loss: 2.177 Epoch 0 Batch 470/1077 - Train Accuracy: 0.498, Validation Accuracy: 0.561, Loss: 2.082 Epoch 0 Batch 471/1077 - Train Accuracy: 0.561, Validation Accuracy: 0.569, Loss: 2.100 Epoch 0 Batch 472/1077 - Train Accuracy: 0.531, Validation Accuracy: 0.582, Loss: 2.192 Epoch 0 Batch 473/1077 - Train Accuracy: 0.534, Validation Accuracy: 0.582, Loss: 2.207 Epoch 0 Batch 474/1077 - Train Accuracy: 0.543, Validation Accuracy: 0.580, Loss: 2.151 Epoch 0 Batch 475/1077 - Train Accuracy: 0.544, Validation Accuracy: 0.575, Loss: 2.046 Epoch 0 Batch 476/1077 - Train Accuracy: 0.530, Validation Accuracy: 0.582, Loss: 2.122 Epoch 0 Batch 477/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.582, Loss: 2.046 Epoch 0 Batch 478/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.585, Loss: 2.135 Epoch 0 Batch 479/1077 - Train Accuracy: 0.535, Validation Accuracy: 0.588, Loss: 2.150 Epoch 0 Batch 480/1077 - Train Accuracy: 0.543, Validation Accuracy: 0.589, Loss: 2.163 Epoch 0 Batch 481/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.591, Loss: 2.089 Epoch 0 Batch 482/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.590, Loss: 2.180 Epoch 0 Batch 483/1077 - Train Accuracy: 0.536, Validation Accuracy: 0.591, Loss: 2.126 Epoch 0 Batch 484/1077 - Train Accuracy: 0.560, Validation Accuracy: 0.589, Loss: 2.107 Epoch 0 Batch 485/1077 - Train Accuracy: 0.577, Validation Accuracy: 0.575, Loss: 2.115 Epoch 0 Batch 486/1077 - Train Accuracy: 0.542, Validation Accuracy: 0.574, Loss: 2.156 Epoch 0 Batch 487/1077 - Train Accuracy: 0.529, Validation Accuracy: 0.591, Loss: 2.153 Epoch 0 Batch 488/1077 - Train Accuracy: 0.524, Validation Accuracy: 0.582, Loss: 2.163 Epoch 0 Batch 489/1077 - Train Accuracy: 0.584, Validation Accuracy: 0.583, Loss: 2.060 Epoch 0 Batch 490/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.587, Loss: 2.153 Epoch 0 Batch 491/1077 - Train Accuracy: 0.579, Validation Accuracy: 0.572, Loss: 2.153 Epoch 0 Batch 492/1077 - Train Accuracy: 0.539, Validation Accuracy: 0.572, Loss: 2.121 Epoch 0 Batch 493/1077 - Train Accuracy: 0.574, Validation Accuracy: 0.567, Loss: 2.067 Epoch 0 Batch 494/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.579, Loss: 2.118 Epoch 0 Batch 495/1077 - Train Accuracy: 0.534, Validation Accuracy: 0.577, Loss: 2.110 Epoch 0 Batch 496/1077 - Train Accuracy: 0.561, Validation Accuracy: 0.579, Loss: 2.163 Epoch 0 Batch 497/1077 - Train Accuracy: 0.513, Validation Accuracy: 0.580, Loss: 2.175 Epoch 0 Batch 498/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.578, Loss: 2.026 Epoch 0 Batch 499/1077 - Train Accuracy: 0.552, Validation Accuracy: 0.584, Loss: 2.059 Epoch 0 Batch 500/1077 - Train Accuracy: 0.561, Validation Accuracy: 0.582, Loss: 2.045 Epoch 0 Batch 501/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.585, Loss: 2.027 Epoch 0 Batch 502/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.582, Loss: 2.121 Epoch 0 Batch 503/1077 - Train Accuracy: 0.572, Validation Accuracy: 0.584, Loss: 2.185 Epoch 0 Batch 504/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.590, Loss: 2.151 Epoch 0 Batch 505/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.597, Loss: 1.995 Epoch 0 Batch 506/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.593, Loss: 2.108 Epoch 0 Batch 507/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.589, Loss: 2.076 Epoch 0 Batch 508/1077 - Train Accuracy: 0.584, Validation Accuracy: 0.594, Loss: 2.039 Epoch 0 Batch 509/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.584, Loss: 2.075 Epoch 0 Batch 510/1077 - Train Accuracy: 0.564, Validation Accuracy: 0.571, Loss: 2.151 Epoch 0 Batch 511/1077 - Train Accuracy: 0.533, Validation Accuracy: 0.575, Loss: 2.188 Epoch 0 Batch 512/1077 - Train Accuracy: 0.560, Validation Accuracy: 0.581, Loss: 2.146 Epoch 0 Batch 513/1077 - Train Accuracy: 0.580, Validation Accuracy: 0.576, Loss: 2.152 Epoch 0 Batch 514/1077 - Train Accuracy: 0.535, Validation Accuracy: 0.568, Loss: 2.109 Epoch 0 Batch 515/1077 - Train Accuracy: 0.540, Validation Accuracy: 0.567, Loss: 2.081 Epoch 0 Batch 516/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.567, Loss: 2.066 Epoch 0 Batch 517/1077 - Train Accuracy: 0.599, Validation Accuracy: 0.571, Loss: 2.058 Epoch 0 Batch 518/1077 - Train Accuracy: 0.575, Validation Accuracy: 0.577, Loss: 2.153 Epoch 0 Batch 519/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.579, Loss: 2.167 Epoch 0 Batch 520/1077 - Train Accuracy: 0.597, Validation Accuracy: 0.584, Loss: 2.027 Epoch 0 Batch 521/1077 - Train Accuracy: 0.571, Validation Accuracy: 0.591, Loss: 2.084 Epoch 0 Batch 522/1077 - Train Accuracy: 0.547, Validation Accuracy: 0.587, Loss: 2.165 Epoch 0 Batch 523/1077 - Train Accuracy: 0.555, Validation Accuracy: 0.585, Loss: 2.202 Epoch 0 Batch 524/1077 - Train Accuracy: 0.555, Validation Accuracy: 0.591, Loss: 2.189 Epoch 0 Batch 525/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.590, Loss: 2.069 Epoch 0 Batch 526/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.594, Loss: 2.025 Epoch 0 Batch 527/1077 - Train Accuracy: 0.550, Validation Accuracy: 0.596, Loss: 2.127 Epoch 0 Batch 528/1077 - Train Accuracy: 0.562, Validation Accuracy: 0.583, Loss: 2.090 Epoch 0 Batch 529/1077 - Train Accuracy: 0.545, Validation Accuracy: 0.581, Loss: 2.063 Epoch 0 Batch 530/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.587, Loss: 2.017 Epoch 0 Batch 531/1077 - Train Accuracy: 0.543, Validation Accuracy: 0.586, Loss: 2.059 Epoch 0 Batch 532/1077 - Train Accuracy: 0.496, Validation Accuracy: 0.576, Loss: 2.085 Epoch 0 Batch 533/1077 - Train Accuracy: 0.567, Validation Accuracy: 0.580, Loss: 2.125 Epoch 0 Batch 534/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.578, Loss: 2.091 Epoch 0 Batch 535/1077 - Train Accuracy: 0.548, Validation Accuracy: 0.573, Loss: 2.139 Epoch 0 Batch 536/1077 - Train Accuracy: 0.557, Validation Accuracy: 0.583, Loss: 2.044 Epoch 0 Batch 537/1077 - Train Accuracy: 0.555, Validation Accuracy: 0.582, Loss: 2.043 Epoch 0 Batch 538/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.575, Loss: 2.051 Epoch 0 Batch 539/1077 - Train Accuracy: 0.528, Validation Accuracy: 0.565, Loss: 2.124 Epoch 0 Batch 540/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.570, Loss: 2.080 Epoch 0 Batch 541/1077 - Train Accuracy: 0.550, Validation Accuracy: 0.577, Loss: 2.081 Epoch 0 Batch 542/1077 - Train Accuracy: 0.561, Validation Accuracy: 0.583, Loss: 2.099 Epoch 0 Batch 543/1077 - Train Accuracy: 0.543, Validation Accuracy: 0.582, Loss: 2.082 Epoch 0 Batch 544/1077 - Train Accuracy: 0.567, Validation Accuracy: 0.583, Loss: 2.052 Epoch 0 Batch 545/1077 - Train Accuracy: 0.565, Validation Accuracy: 0.589, Loss: 2.169 Epoch 0 Batch 546/1077 - Train Accuracy: 0.535, Validation Accuracy: 0.589, Loss: 2.086 Epoch 0 Batch 547/1077 - Train Accuracy: 0.573, Validation Accuracy: 0.586, Loss: 2.052 Epoch 0 Batch 548/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.575, Loss: 2.094 Epoch 0 Batch 549/1077 - Train Accuracy: 0.521, Validation Accuracy: 0.573, Loss: 2.115 Epoch 0 Batch 550/1077 - Train Accuracy: 0.526, Validation Accuracy: 0.577, Loss: 2.000 Epoch 0 Batch 551/1077 - Train Accuracy: 0.555, Validation Accuracy: 0.592, Loss: 2.059 Epoch 0 Batch 552/1077 - Train Accuracy: 0.558, Validation Accuracy: 0.591, Loss: 2.094 Epoch 0 Batch 553/1077 - Train Accuracy: 0.594, Validation Accuracy: 0.593, Loss: 2.119 Epoch 0 Batch 554/1077 - Train Accuracy: 0.570, Validation Accuracy: 0.593, Loss: 2.106 Epoch 0 Batch 555/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.585, Loss: 2.122 Epoch 0 Batch 556/1077 - Train Accuracy: 0.547, Validation Accuracy: 0.593, Loss: 2.023 Epoch 0 Batch 557/1077 - Train Accuracy: 0.576, Validation Accuracy: 0.600, Loss: 2.107 Epoch 0 Batch 558/1077 - Train Accuracy: 0.566, Validation Accuracy: 0.600, Loss: 1.993 Epoch 0 Batch 559/1077 - Train Accuracy: 0.577, Validation Accuracy: 0.591, Loss: 2.043 Epoch 0 Batch 560/1077 - Train Accuracy: 0.534, Validation Accuracy: 0.586, Loss: 2.054 Epoch 0 Batch 561/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.596, Loss: 2.113 Epoch 0 Batch 562/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.592, Loss: 2.099 Epoch 0 Batch 563/1077 - Train Accuracy: 0.565, Validation Accuracy: 0.599, Loss: 2.114 Epoch 0 Batch 564/1077 - Train Accuracy: 0.574, Validation Accuracy: 0.596, Loss: 2.107 Epoch 0 Batch 565/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.583, Loss: 2.034 Epoch 0 Batch 566/1077 - Train Accuracy: 0.563, Validation Accuracy: 0.580, Loss: 2.104 Epoch 0 Batch 567/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.574, Loss: 2.014 Epoch 0 Batch 568/1077 - Train Accuracy: 0.574, Validation Accuracy: 0.579, Loss: 2.074 Epoch 0 Batch 569/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.575, Loss: 2.087 Epoch 0 Batch 570/1077 - Train Accuracy: 0.564, Validation Accuracy: 0.577, Loss: 2.130 Epoch 0 Batch 571/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.571, Loss: 2.047 Epoch 0 Batch 572/1077 - Train Accuracy: 0.562, Validation Accuracy: 0.568, Loss: 2.096 Epoch 0 Batch 573/1077 - Train Accuracy: 0.558, Validation Accuracy: 0.565, Loss: 2.081 Epoch 0 Batch 574/1077 - Train Accuracy: 0.537, Validation Accuracy: 0.570, Loss: 2.142 Epoch 0 Batch 575/1077 - Train Accuracy: 0.581, Validation Accuracy: 0.585, Loss: 2.120 Epoch 0 Batch 576/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.595, Loss: 2.038 Epoch 0 Batch 577/1077 - Train Accuracy: 0.539, Validation Accuracy: 0.596, Loss: 2.167 Epoch 0 Batch 578/1077 - Train Accuracy: 0.534, Validation Accuracy: 0.593, Loss: 2.077 Epoch 0 Batch 579/1077 - Train Accuracy: 0.599, Validation Accuracy: 0.591, Loss: 2.025 Epoch 0 Batch 580/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.602, Loss: 1.969 Epoch 0 Batch 581/1077 - Train Accuracy: 0.563, Validation Accuracy: 0.596, Loss: 2.008 Epoch 0 Batch 582/1077 - Train Accuracy: 0.550, Validation Accuracy: 0.600, Loss: 2.101 Epoch 0 Batch 583/1077 - Train Accuracy: 0.532, Validation Accuracy: 0.599, Loss: 2.079 Epoch 0 Batch 584/1077 - Train Accuracy: 0.582, Validation Accuracy: 0.592, Loss: 2.105 Epoch 0 Batch 585/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.595, Loss: 2.032 Epoch 0 Batch 586/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.597, Loss: 2.097 Epoch 0 Batch 587/1077 - Train Accuracy: 0.537, Validation Accuracy: 0.587, Loss: 2.084 Epoch 0 Batch 588/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.582, Loss: 2.046 Epoch 0 Batch 589/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.544, Loss: 2.115 Epoch 0 Batch 590/1077 - Train Accuracy: 0.505, Validation Accuracy: 0.555, Loss: 2.104 Epoch 0 Batch 591/1077 - Train Accuracy: 0.558, Validation Accuracy: 0.564, Loss: 2.060 Epoch 0 Batch 592/1077 - Train Accuracy: 0.574, Validation Accuracy: 0.579, Loss: 2.100 Epoch 0 Batch 593/1077 - Train Accuracy: 0.570, Validation Accuracy: 0.581, Loss: 2.053 Epoch 0 Batch 594/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.588, Loss: 2.117 Epoch 0 Batch 595/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.584, Loss: 2.052 Epoch 0 Batch 596/1077 - Train Accuracy: 0.560, Validation Accuracy: 0.594, Loss: 2.056 Epoch 0 Batch 597/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.601, Loss: 2.088 Epoch 0 Batch 598/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.598, Loss: 2.010 Epoch 0 Batch 599/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.606, Loss: 2.071 Epoch 0 Batch 600/1077 - Train Accuracy: 0.599, Validation Accuracy: 0.599, Loss: 2.023 Epoch 0 Batch 601/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.598, Loss: 2.022 Epoch 0 Batch 602/1077 - Train Accuracy: 0.597, Validation Accuracy: 0.588, Loss: 2.070 Epoch 0 Batch 603/1077 - Train Accuracy: 0.591, Validation Accuracy: 0.584, Loss: 2.040 Epoch 0 Batch 604/1077 - Train Accuracy: 0.530, Validation Accuracy: 0.589, Loss: 2.085 Epoch 0 Batch 605/1077 - Train Accuracy: 0.545, Validation Accuracy: 0.572, Loss: 2.105 Epoch 0 Batch 606/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.571, Loss: 1.968 Epoch 0 Batch 607/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.568, Loss: 1.973 Epoch 0 Batch 608/1077 - Train Accuracy: 0.540, Validation Accuracy: 0.566, Loss: 2.129 Epoch 0 Batch 609/1077 - Train Accuracy: 0.550, Validation Accuracy: 0.572, Loss: 2.071 Epoch 0 Batch 610/1077 - Train Accuracy: 0.536, Validation Accuracy: 0.575, Loss: 2.102 Epoch 0 Batch 611/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.600, Loss: 2.024 Epoch 0 Batch 612/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.602, Loss: 2.023 Epoch 0 Batch 613/1077 - Train Accuracy: 0.563, Validation Accuracy: 0.604, Loss: 2.147 Epoch 0 Batch 614/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.601, Loss: 2.024 Epoch 0 Batch 615/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.601, Loss: 2.114 Epoch 0 Batch 616/1077 - Train Accuracy: 0.547, Validation Accuracy: 0.603, Loss: 2.086 Epoch 0 Batch 617/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.610, Loss: 2.014 Epoch 0 Batch 618/1077 - Train Accuracy: 0.579, Validation Accuracy: 0.613, Loss: 2.011 Epoch 0 Batch 619/1077 - Train Accuracy: 0.558, Validation Accuracy: 0.605, Loss: 2.056 Epoch 0 Batch 620/1077 - Train Accuracy: 0.573, Validation Accuracy: 0.609, Loss: 2.067 Epoch 0 Batch 621/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.612, Loss: 2.096 Epoch 0 Batch 622/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.614, Loss: 2.094 Epoch 0 Batch 623/1077 - Train Accuracy: 0.564, Validation Accuracy: 0.617, Loss: 2.044 Epoch 0 Batch 624/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.610, Loss: 2.039 Epoch 0 Batch 625/1077 - Train Accuracy: 0.594, Validation Accuracy: 0.608, Loss: 2.047 Epoch 0 Batch 626/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.611, Loss: 1.959 Epoch 0 Batch 627/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.605, Loss: 2.089 Epoch 0 Batch 628/1077 - Train Accuracy: 0.557, Validation Accuracy: 0.599, Loss: 2.069 Epoch 0 Batch 629/1077 - Train Accuracy: 0.558, Validation Accuracy: 0.609, Loss: 2.041 Epoch 0 Batch 630/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.610, Loss: 2.039 Epoch 0 Batch 631/1077 - Train Accuracy: 0.578, Validation Accuracy: 0.611, Loss: 2.014 Epoch 0 Batch 632/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.613, Loss: 2.080 Epoch 0 Batch 633/1077 - Train Accuracy: 0.591, Validation Accuracy: 0.614, Loss: 1.969 Epoch 0 Batch 634/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.609, Loss: 1.960 Epoch 0 Batch 635/1077 - Train Accuracy: 0.562, Validation Accuracy: 0.610, Loss: 2.038 Epoch 0 Batch 636/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.614, Loss: 1.974 Epoch 0 Batch 637/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.606, Loss: 2.107 Epoch 0 Batch 638/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.601, Loss: 1.991 Epoch 0 Batch 639/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.601, Loss: 2.007 Epoch 0 Batch 640/1077 - Train Accuracy: 0.562, Validation Accuracy: 0.599, Loss: 1.958 Epoch 0 Batch 641/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.597, Loss: 1.989 Epoch 0 Batch 642/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.594, Loss: 1.999 Epoch 0 Batch 643/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.602, Loss: 2.050 Epoch 0 Batch 644/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.602, Loss: 2.063 Epoch 0 Batch 645/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.603, Loss: 1.971 Epoch 0 Batch 646/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.603, Loss: 2.058 Epoch 0 Batch 647/1077 - Train Accuracy: 0.592, Validation Accuracy: 0.598, Loss: 1.987 Epoch 0 Batch 648/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.585, Loss: 2.003 Epoch 0 Batch 649/1077 - Train Accuracy: 0.543, Validation Accuracy: 0.572, Loss: 2.169 Epoch 0 Batch 650/1077 - Train Accuracy: 0.542, Validation Accuracy: 0.577, Loss: 2.095 Epoch 0 Batch 651/1077 - Train Accuracy: 0.579, Validation Accuracy: 0.575, Loss: 1.993 Epoch 0 Batch 652/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.584, Loss: 2.103 Epoch 0 Batch 653/1077 - Train Accuracy: 0.608, Validation Accuracy: 0.599, Loss: 2.041 Epoch 0 Batch 654/1077 - Train Accuracy: 0.579, Validation Accuracy: 0.612, Loss: 2.041 Epoch 0 Batch 655/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.617, Loss: 2.012 Epoch 0 Batch 656/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.618, Loss: 2.066 Epoch 0 Batch 657/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.619, Loss: 2.065 Epoch 0 Batch 658/1077 - Train Accuracy: 0.594, Validation Accuracy: 0.621, Loss: 1.904 Epoch 0 Batch 659/1077 - Train Accuracy: 0.635, Validation Accuracy: 0.618, Loss: 2.021 Epoch 0 Batch 660/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.614, Loss: 2.073 Epoch 0 Batch 661/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.613, Loss: 1.984 Epoch 0 Batch 662/1077 - Train Accuracy: 0.608, Validation Accuracy: 0.616, Loss: 2.005 Epoch 0 Batch 663/1077 - Train Accuracy: 0.571, Validation Accuracy: 0.615, Loss: 1.985 Epoch 0 Batch 664/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.602, Loss: 2.127 Epoch 0 Batch 665/1077 - Train Accuracy: 0.573, Validation Accuracy: 0.601, Loss: 2.071 Epoch 0 Batch 666/1077 - Train Accuracy: 0.542, Validation Accuracy: 0.606, Loss: 2.058 Epoch 0 Batch 667/1077 - Train Accuracy: 0.569, Validation Accuracy: 0.613, Loss: 2.069 Epoch 0 Batch 668/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.617, Loss: 2.003 Epoch 0 Batch 669/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.616, Loss: 2.027 Epoch 0 Batch 670/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.616, Loss: 2.003 Epoch 0 Batch 671/1077 - Train Accuracy: 0.561, Validation Accuracy: 0.613, Loss: 2.078 Epoch 0 Batch 672/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.597, Loss: 1.994 Epoch 0 Batch 673/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.585, Loss: 2.049 Epoch 0 Batch 674/1077 - Train Accuracy: 0.566, Validation Accuracy: 0.581, Loss: 2.091 Epoch 0 Batch 675/1077 - Train Accuracy: 0.548, Validation Accuracy: 0.582, Loss: 2.094 Epoch 0 Batch 676/1077 - Train Accuracy: 0.583, Validation Accuracy: 0.600, Loss: 2.021 Epoch 0 Batch 677/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.593, Loss: 2.042 Epoch 0 Batch 678/1077 - Train Accuracy: 0.578, Validation Accuracy: 0.613, Loss: 2.058 Epoch 0 Batch 679/1077 - Train Accuracy: 0.558, Validation Accuracy: 0.615, Loss: 2.022 Epoch 0 Batch 680/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.609, Loss: 2.016 Epoch 0 Batch 681/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.602, Loss: 2.049 Epoch 0 Batch 682/1077 - Train Accuracy: 0.535, Validation Accuracy: 0.608, Loss: 1.949 Epoch 0 Batch 683/1077 - Train Accuracy: 0.568, Validation Accuracy: 0.619, Loss: 2.022 Epoch 0 Batch 684/1077 - Train Accuracy: 0.580, Validation Accuracy: 0.600, Loss: 2.068 Epoch 0 Batch 685/1077 - Train Accuracy: 0.555, Validation Accuracy: 0.592, Loss: 2.022 Epoch 0 Batch 686/1077 - Train Accuracy: 0.567, Validation Accuracy: 0.589, Loss: 2.035 Epoch 0 Batch 687/1077 - Train Accuracy: 0.580, Validation Accuracy: 0.600, Loss: 2.035 Epoch 0 Batch 688/1077 - Train Accuracy: 0.568, Validation Accuracy: 0.606, Loss: 2.044 Epoch 0 Batch 689/1077 - Train Accuracy: 0.584, Validation Accuracy: 0.615, Loss: 1.971 Epoch 0 Batch 690/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.600, Loss: 2.050 Epoch 0 Batch 691/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.607, Loss: 2.001 Epoch 0 Batch 692/1077 - Train Accuracy: 0.583, Validation Accuracy: 0.610, Loss: 1.979 Epoch 0 Batch 693/1077 - Train Accuracy: 0.538, Validation Accuracy: 0.618, Loss: 2.053 Epoch 0 Batch 694/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.621, Loss: 2.005 Epoch 0 Batch 695/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.625, Loss: 2.021 Epoch 0 Batch 696/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.624, Loss: 2.038 Epoch 0 Batch 697/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.623, Loss: 1.988 Epoch 0 Batch 698/1077 - Train Accuracy: 0.567, Validation Accuracy: 0.608, Loss: 2.044 Epoch 0 Batch 699/1077 - Train Accuracy: 0.514, Validation Accuracy: 0.612, Loss: 2.093 Epoch 0 Batch 700/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.624, Loss: 1.990 Epoch 0 Batch 701/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.621, Loss: 2.020 Epoch 0 Batch 702/1077 - Train Accuracy: 0.574, Validation Accuracy: 0.607, Loss: 1.989 Epoch 0 Batch 703/1077 - Train Accuracy: 0.569, Validation Accuracy: 0.604, Loss: 2.056 Epoch 0 Batch 704/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.601, Loss: 2.059 Epoch 0 Batch 705/1077 - Train Accuracy: 0.566, Validation Accuracy: 0.619, Loss: 2.061 Epoch 0 Batch 706/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.609, Loss: 1.978 Epoch 0 Batch 707/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.610, Loss: 2.023 Epoch 0 Batch 708/1077 - Train Accuracy: 0.592, Validation Accuracy: 0.621, Loss: 2.035 Epoch 0 Batch 709/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.618, Loss: 1.967 Epoch 0 Batch 710/1077 - Train Accuracy: 0.562, Validation Accuracy: 0.615, Loss: 2.013 Epoch 0 Batch 711/1077 - Train Accuracy: 0.579, Validation Accuracy: 0.588, Loss: 1.998 Epoch 0 Batch 712/1077 - Train Accuracy: 0.571, Validation Accuracy: 0.586, Loss: 2.015 Epoch 0 Batch 713/1077 - Train Accuracy: 0.608, Validation Accuracy: 0.588, Loss: 1.962 Epoch 0 Batch 714/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.599, Loss: 2.050 Epoch 0 Batch 715/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.612, Loss: 2.024 Epoch 0 Batch 716/1077 - Train Accuracy: 0.581, Validation Accuracy: 0.612, Loss: 1.973 Epoch 0 Batch 717/1077 - Train Accuracy: 0.577, Validation Accuracy: 0.618, Loss: 1.986 Epoch 0 Batch 718/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.613, Loss: 2.009 Epoch 0 Batch 719/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.614, Loss: 1.980 Epoch 0 Batch 720/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.607, Loss: 2.072 Epoch 0 Batch 721/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.610, Loss: 1.949 Epoch 0 Batch 722/1077 - Train Accuracy: 0.568, Validation Accuracy: 0.612, Loss: 1.989 Epoch 0 Batch 723/1077 - Train Accuracy: 0.557, Validation Accuracy: 0.602, Loss: 1.982 Epoch 0 Batch 724/1077 - Train Accuracy: 0.566, Validation Accuracy: 0.613, Loss: 1.983 Epoch 0 Batch 725/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.626, Loss: 1.951 Epoch 0 Batch 726/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.626, Loss: 1.968 Epoch 0 Batch 727/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.620, Loss: 1.973 Epoch 0 Batch 728/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.622, Loss: 1.971 Epoch 0 Batch 729/1077 - Train Accuracy: 0.557, Validation Accuracy: 0.626, Loss: 2.003 Epoch 0 Batch 730/1077 - Train Accuracy: 0.582, Validation Accuracy: 0.614, Loss: 1.954 Epoch 0 Batch 731/1077 - Train Accuracy: 0.582, Validation Accuracy: 0.613, Loss: 1.961 Epoch 0 Batch 732/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.611, Loss: 2.028 Epoch 0 Batch 733/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.606, Loss: 2.026 Epoch 0 Batch 734/1077 - Train Accuracy: 0.583, Validation Accuracy: 0.614, Loss: 2.013 Epoch 0 Batch 735/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.613, Loss: 2.007 Epoch 0 Batch 736/1077 - Train Accuracy: 0.608, Validation Accuracy: 0.616, Loss: 2.029 Epoch 0 Batch 737/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.612, Loss: 1.967 Epoch 0 Batch 738/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.602, Loss: 1.916 Epoch 0 Batch 739/1077 - Train Accuracy: 0.582, Validation Accuracy: 0.604, Loss: 2.023 Epoch 0 Batch 740/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.608, Loss: 1.972 Epoch 0 Batch 741/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.615, Loss: 2.020 Epoch 0 Batch 742/1077 - Train Accuracy: 0.579, Validation Accuracy: 0.615, Loss: 2.027 Epoch 0 Batch 743/1077 - Train Accuracy: 0.584, Validation Accuracy: 0.621, Loss: 1.953 Epoch 0 Batch 744/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.624, Loss: 1.947 Epoch 0 Batch 745/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.623, Loss: 2.007 Epoch 0 Batch 746/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.605, Loss: 2.009 Epoch 0 Batch 747/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.601, Loss: 1.969 Epoch 0 Batch 748/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.604, Loss: 1.927 Epoch 0 Batch 749/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.600, Loss: 2.023 Epoch 0 Batch 750/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.599, Loss: 2.044 Epoch 0 Batch 751/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.601, Loss: 1.969 Epoch 0 Batch 752/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.604, Loss: 1.889 Epoch 0 Batch 753/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.623, Loss: 1.967 Epoch 0 Batch 754/1077 - Train Accuracy: 0.574, Validation Accuracy: 0.618, Loss: 2.048 Epoch 0 Batch 755/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.613, Loss: 1.971 Epoch 0 Batch 756/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.613, Loss: 1.927 Epoch 0 Batch 757/1077 - Train Accuracy: 0.592, Validation Accuracy: 0.613, Loss: 2.004 Epoch 0 Batch 758/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.608, Loss: 1.902 Epoch 0 Batch 759/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.621, Loss: 1.923 Epoch 0 Batch 760/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.630, Loss: 2.042 Epoch 0 Batch 761/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.637, Loss: 1.962 Epoch 0 Batch 762/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.634, Loss: 1.969 Epoch 0 Batch 763/1077 - Train Accuracy: 0.585, Validation Accuracy: 0.629, Loss: 1.930 Epoch 0 Batch 764/1077 - Train Accuracy: 0.565, Validation Accuracy: 0.629, Loss: 2.080 Epoch 0 Batch 765/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.629, Loss: 1.981 Epoch 0 Batch 766/1077 - Train Accuracy: 0.582, Validation Accuracy: 0.622, Loss: 2.083 Epoch 0 Batch 767/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.630, Loss: 2.004 Epoch 0 Batch 768/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.625, Loss: 1.986 Epoch 0 Batch 769/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.617, Loss: 1.956 Epoch 0 Batch 770/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.603, Loss: 1.960 Epoch 0 Batch 771/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.606, Loss: 2.014 Epoch 0 Batch 772/1077 - Train Accuracy: 0.574, Validation Accuracy: 0.603, Loss: 1.982 Epoch 0 Batch 773/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.609, Loss: 2.018 Epoch 0 Batch 774/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.618, Loss: 2.001 Epoch 0 Batch 775/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.620, Loss: 1.991 Epoch 0 Batch 776/1077 - Train Accuracy: 0.585, Validation Accuracy: 0.616, Loss: 1.930 Epoch 0 Batch 777/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.615, Loss: 1.957 Epoch 0 Batch 778/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.618, Loss: 1.962 Epoch 0 Batch 779/1077 - Train Accuracy: 0.594, Validation Accuracy: 0.624, Loss: 1.911 Epoch 0 Batch 780/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.614, Loss: 1.979 Epoch 0 Batch 781/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.614, Loss: 1.931 Epoch 0 Batch 782/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.626, Loss: 2.027 Epoch 0 Batch 783/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.623, Loss: 1.949 Epoch 0 Batch 784/1077 - Train Accuracy: 0.612, Validation Accuracy: 0.625, Loss: 1.938 Epoch 0 Batch 785/1077 - Train Accuracy: 0.650, Validation Accuracy: 0.617, Loss: 1.925 Epoch 0 Batch 786/1077 - Train Accuracy: 0.568, Validation Accuracy: 0.612, Loss: 2.030 Epoch 0 Batch 787/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.608, Loss: 1.980 Epoch 0 Batch 788/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.616, Loss: 1.976 Epoch 0 Batch 789/1077 - Train Accuracy: 0.556, Validation Accuracy: 0.623, Loss: 1.977 Epoch 0 Batch 790/1077 - Train Accuracy: 0.562, Validation Accuracy: 0.626, Loss: 2.077 Epoch 0 Batch 791/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.621, Loss: 2.000 Epoch 0 Batch 792/1077 - Train Accuracy: 0.571, Validation Accuracy: 0.613, Loss: 2.009 Epoch 0 Batch 793/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.617, Loss: 1.992 Epoch 0 Batch 794/1077 - Train Accuracy: 0.573, Validation Accuracy: 0.615, Loss: 1.947 Epoch 0 Batch 795/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.617, Loss: 2.045 Epoch 0 Batch 796/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.615, Loss: 1.897 Epoch 0 Batch 797/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.613, Loss: 1.939 Epoch 0 Batch 798/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.614, Loss: 1.963 Epoch 0 Batch 799/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.616, Loss: 2.002 Epoch 0 Batch 800/1077 - Train Accuracy: 0.584, Validation Accuracy: 0.619, Loss: 1.914 Epoch 0 Batch 801/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.615, Loss: 2.008 Epoch 0 Batch 802/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.618, Loss: 2.015 Epoch 0 Batch 803/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.619, Loss: 1.916 Epoch 0 Batch 804/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.609, Loss: 2.032 Epoch 0 Batch 805/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.617, Loss: 1.934 Epoch 0 Batch 806/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.625, Loss: 1.964 Epoch 0 Batch 807/1077 - Train Accuracy: 0.570, Validation Accuracy: 0.633, Loss: 1.991 Epoch 0 Batch 808/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.627, Loss: 1.924 Epoch 0 Batch 809/1077 - Train Accuracy: 0.581, Validation Accuracy: 0.609, Loss: 2.009 Epoch 0 Batch 810/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.608, Loss: 1.947 Epoch 0 Batch 811/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.611, Loss: 1.958 Epoch 0 Batch 812/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.626, Loss: 2.004 Epoch 0 Batch 813/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.627, Loss: 2.063 Epoch 0 Batch 814/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.618, Loss: 1.948 Epoch 0 Batch 815/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.626, Loss: 2.024 Epoch 0 Batch 816/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.621, Loss: 1.962 Epoch 0 Batch 817/1077 - Train Accuracy: 0.568, Validation Accuracy: 0.625, Loss: 1.952 Epoch 0 Batch 818/1077 - Train Accuracy: 0.573, Validation Accuracy: 0.626, Loss: 1.957 Epoch 0 Batch 819/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.620, Loss: 1.954 Epoch 0 Batch 820/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.629, Loss: 1.949 Epoch 0 Batch 821/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.631, Loss: 1.877 Epoch 0 Batch 822/1077 - Train Accuracy: 0.612, Validation Accuracy: 0.631, Loss: 2.014 Epoch 0 Batch 823/1077 - Train Accuracy: 0.580, Validation Accuracy: 0.615, Loss: 2.004 Epoch 0 Batch 824/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.615, Loss: 1.948 Epoch 0 Batch 825/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.616, Loss: 1.978 Epoch 0 Batch 826/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.619, Loss: 1.983 Epoch 0 Batch 827/1077 - Train Accuracy: 0.577, Validation Accuracy: 0.617, Loss: 1.966 Epoch 0 Batch 828/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.624, Loss: 2.014 Epoch 0 Batch 829/1077 - Train Accuracy: 0.592, Validation Accuracy: 0.625, Loss: 1.962 Epoch 0 Batch 830/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.626, Loss: 2.005 Epoch 0 Batch 831/1077 - Train Accuracy: 0.591, Validation Accuracy: 0.639, Loss: 1.982 Epoch 0 Batch 832/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.632, Loss: 2.023 Epoch 0 Batch 833/1077 - Train Accuracy: 0.612, Validation Accuracy: 0.632, Loss: 1.966 Epoch 0 Batch 834/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.635, Loss: 1.928 Epoch 0 Batch 835/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.632, Loss: 1.949 Epoch 0 Batch 836/1077 - Train Accuracy: 0.568, Validation Accuracy: 0.632, Loss: 1.994 Epoch 0 Batch 837/1077 - Train Accuracy: 0.576, Validation Accuracy: 0.634, Loss: 2.035 Epoch 0 Batch 838/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.620, Loss: 2.006 Epoch 0 Batch 839/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.619, Loss: 1.907 Epoch 0 Batch 840/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.616, Loss: 2.008 Epoch 0 Batch 841/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.605, Loss: 1.989 Epoch 0 Batch 842/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.620, Loss: 1.963 Epoch 0 Batch 843/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.620, Loss: 1.919 Epoch 0 Batch 844/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.623, Loss: 2.029 Epoch 0 Batch 845/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.629, Loss: 2.064 Epoch 0 Batch 846/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.618, Loss: 1.955 Epoch 0 Batch 847/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.620, Loss: 2.034 Epoch 0 Batch 848/1077 - Train Accuracy: 0.612, Validation Accuracy: 0.632, Loss: 1.881 Epoch 0 Batch 849/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.630, Loss: 1.925 Epoch 0 Batch 850/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.629, Loss: 1.954 Epoch 0 Batch 851/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.631, Loss: 1.903 Epoch 0 Batch 852/1077 - Train Accuracy: 0.597, Validation Accuracy: 0.624, Loss: 2.033 Epoch 0 Batch 853/1077 - Train Accuracy: 0.640, Validation Accuracy: 0.629, Loss: 1.995 Epoch 0 Batch 854/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.632, Loss: 1.906 Epoch 0 Batch 855/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.630, Loss: 1.898 Epoch 0 Batch 856/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.623, Loss: 1.940 Epoch 0 Batch 857/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.620, Loss: 1.963 Epoch 0 Batch 858/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.625, Loss: 1.962 Epoch 0 Batch 859/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.622, Loss: 2.053 Epoch 0 Batch 860/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.621, Loss: 1.915 Epoch 0 Batch 861/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.615, Loss: 1.905 Epoch 0 Batch 862/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.611, Loss: 1.960 Epoch 0 Batch 863/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.621, Loss: 1.957 Epoch 0 Batch 864/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.623, Loss: 1.952 Epoch 0 Batch 865/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.620, Loss: 1.891 Epoch 0 Batch 866/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.612, Loss: 1.963 Epoch 0 Batch 867/1077 - Train Accuracy: 0.566, Validation Accuracy: 0.612, Loss: 1.974 Epoch 0 Batch 868/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.612, Loss: 1.977 Epoch 0 Batch 869/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.627, Loss: 1.979 Epoch 0 Batch 870/1077 - Train Accuracy: 0.584, Validation Accuracy: 0.629, Loss: 2.062 Epoch 0 Batch 871/1077 - Train Accuracy: 0.583, Validation Accuracy: 0.634, Loss: 1.970 Epoch 0 Batch 872/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.630, Loss: 1.922 Epoch 0 Batch 873/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.631, Loss: 1.898 Epoch 0 Batch 874/1077 - Train Accuracy: 0.577, Validation Accuracy: 0.622, Loss: 2.007 Epoch 0 Batch 875/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.631, Loss: 2.016 Epoch 0 Batch 876/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.628, Loss: 1.949 Epoch 0 Batch 877/1077 - Train Accuracy: 0.592, Validation Accuracy: 0.627, Loss: 1.929 Epoch 0 Batch 878/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.638, Loss: 1.862 Epoch 0 Batch 879/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.640, Loss: 1.939 Epoch 0 Batch 880/1077 - Train Accuracy: 0.628, Validation Accuracy: 0.637, Loss: 1.947 Epoch 0 Batch 881/1077 - Train Accuracy: 0.569, Validation Accuracy: 0.627, Loss: 2.040 Epoch 0 Batch 882/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.629, Loss: 2.043 Epoch 0 Batch 883/1077 - Train Accuracy: 0.556, Validation Accuracy: 0.636, Loss: 2.033 Epoch 0 Batch 884/1077 - Train Accuracy: 0.577, Validation Accuracy: 0.651, Loss: 1.930 Epoch 0 Batch 885/1077 - Train Accuracy: 0.653, Validation Accuracy: 0.641, Loss: 1.845 Epoch 0 Batch 886/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.639, Loss: 2.011 Epoch 0 Batch 887/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.645, Loss: 2.036 Epoch 0 Batch 888/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.619, Loss: 2.009 Epoch 0 Batch 889/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.629, Loss: 1.979 Epoch 0 Batch 890/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.626, Loss: 1.952 Epoch 0 Batch 891/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.626, Loss: 1.931 Epoch 0 Batch 892/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.645, Loss: 1.892 Epoch 0 Batch 893/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.636, Loss: 1.923 Epoch 0 Batch 894/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.622, Loss: 1.911 Epoch 0 Batch 895/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.627, Loss: 1.961 Epoch 0 Batch 896/1077 - Train Accuracy: 0.556, Validation Accuracy: 0.624, Loss: 1.940 Epoch 0 Batch 897/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.621, Loss: 1.937 Epoch 0 Batch 898/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.640, Loss: 1.910 Epoch 0 Batch 899/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.641, Loss: 1.988 Epoch 0 Batch 900/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.638, Loss: 1.870 Epoch 0 Batch 901/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.642, Loss: 1.876 Epoch 0 Batch 902/1077 - Train Accuracy: 0.641, Validation Accuracy: 0.635, Loss: 1.928 Epoch 0 Batch 903/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.636, Loss: 1.838 Epoch 0 Batch 904/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.638, Loss: 1.973 Epoch 0 Batch 905/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.635, Loss: 1.957 Epoch 0 Batch 906/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.638, Loss: 1.878 Epoch 0 Batch 907/1077 - Train Accuracy: 0.608, Validation Accuracy: 0.643, Loss: 1.902 Epoch 0 Batch 908/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.638, Loss: 1.917 Epoch 0 Batch 909/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.641, Loss: 1.911 Epoch 0 Batch 910/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.639, Loss: 1.945 Epoch 0 Batch 911/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.637, Loss: 1.926 Epoch 0 Batch 912/1077 - Train Accuracy: 0.572, Validation Accuracy: 0.636, Loss: 1.970 Epoch 0 Batch 913/1077 - Train Accuracy: 0.580, Validation Accuracy: 0.635, Loss: 2.009 Epoch 0 Batch 914/1077 - Train Accuracy: 0.681, Validation Accuracy: 0.643, Loss: 1.932 Epoch 0 Batch 915/1077 - Train Accuracy: 0.582, Validation Accuracy: 0.637, Loss: 2.037 Epoch 0 Batch 916/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.633, Loss: 2.001 Epoch 0 Batch 917/1077 - Train Accuracy: 0.569, Validation Accuracy: 0.635, Loss: 1.898 Epoch 0 Batch 918/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.637, Loss: 1.879 Epoch 0 Batch 919/1077 - Train Accuracy: 0.654, Validation Accuracy: 0.637, Loss: 1.954 Epoch 0 Batch 920/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.642, Loss: 1.919 Epoch 0 Batch 921/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.638, Loss: 1.919 Epoch 0 Batch 922/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.644, Loss: 1.982 Epoch 0 Batch 923/1077 - Train Accuracy: 0.547, Validation Accuracy: 0.646, Loss: 1.962 Epoch 0 Batch 924/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.640, Loss: 1.934 Epoch 0 Batch 925/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.638, Loss: 1.903 Epoch 0 Batch 926/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.620, Loss: 1.981 Epoch 0 Batch 927/1077 - Train Accuracy: 0.550, Validation Accuracy: 0.620, Loss: 1.992 Epoch 0 Batch 928/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.607, Loss: 1.947 Epoch 0 Batch 929/1077 - Train Accuracy: 0.568, Validation Accuracy: 0.613, Loss: 2.081 Epoch 0 Batch 930/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.637, Loss: 2.003 Epoch 0 Batch 931/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.649, Loss: 1.943 Epoch 0 Batch 932/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.652, Loss: 1.984 Epoch 0 Batch 933/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.647, Loss: 1.923 Epoch 0 Batch 934/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.629, Loss: 1.951 Epoch 0 Batch 935/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.630, Loss: 1.997 Epoch 0 Batch 936/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.633, Loss: 1.948 Epoch 0 Batch 937/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.636, Loss: 1.967 Epoch 0 Batch 938/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.642, Loss: 1.898 Epoch 0 Batch 939/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.642, Loss: 1.903 Epoch 0 Batch 940/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.650, Loss: 1.957 Epoch 0 Batch 941/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.646, Loss: 2.016 Epoch 0 Batch 942/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.641, Loss: 2.012 Epoch 0 Batch 943/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.649, Loss: 2.000 Epoch 0 Batch 944/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.637, Loss: 1.917 Epoch 0 Batch 945/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.631, Loss: 1.901 Epoch 0 Batch 946/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.635, Loss: 1.908 Epoch 0 Batch 947/1077 - Train Accuracy: 0.564, Validation Accuracy: 0.636, Loss: 1.939 Epoch 0 Batch 948/1077 - Train Accuracy: 0.575, Validation Accuracy: 0.634, Loss: 1.885 Epoch 0 Batch 949/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.649, Loss: 1.901 Epoch 0 Batch 950/1077 - Train Accuracy: 0.584, Validation Accuracy: 0.656, Loss: 1.892 Epoch 0 Batch 951/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.650, Loss: 1.871 Epoch 0 Batch 952/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.650, Loss: 1.915 Epoch 0 Batch 953/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.645, Loss: 1.866 Epoch 0 Batch 954/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.647, Loss: 1.933 Epoch 0 Batch 955/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.641, Loss: 1.879 Epoch 0 Batch 956/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.636, Loss: 1.861 Epoch 0 Batch 957/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.631, Loss: 1.911 Epoch 0 Batch 958/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.631, Loss: 1.916 Epoch 0 Batch 959/1077 - Train Accuracy: 0.652, Validation Accuracy: 0.632, Loss: 1.885 Epoch 0 Batch 960/1077 - Train Accuracy: 0.622, Validation Accuracy: 0.632, Loss: 1.959 Epoch 0 Batch 961/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.620, Loss: 1.803 Epoch 0 Batch 962/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.613, Loss: 1.881 Epoch 0 Batch 963/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.627, Loss: 1.934 Epoch 0 Batch 964/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.631, Loss: 1.902 Epoch 0 Batch 965/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.637, Loss: 1.974 Epoch 0 Batch 966/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.623, Loss: 1.874 Epoch 0 Batch 967/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.617, Loss: 1.943 Epoch 0 Batch 968/1077 - Train Accuracy: 0.580, Validation Accuracy: 0.632, Loss: 1.937 Epoch 0 Batch 969/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.643, Loss: 1.920 Epoch 0 Batch 970/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.650, Loss: 1.895 Epoch 0 Batch 971/1077 - Train Accuracy: 0.633, Validation Accuracy: 0.651, Loss: 1.923 Epoch 0 Batch 972/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.642, Loss: 1.961 Epoch 0 Batch 973/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.635, Loss: 1.893 Epoch 0 Batch 974/1077 - Train Accuracy: 0.581, Validation Accuracy: 0.635, Loss: 1.884 Epoch 0 Batch 975/1077 - Train Accuracy: 0.669, Validation Accuracy: 0.627, Loss: 1.942 Epoch 0 Batch 976/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.635, Loss: 1.941 Epoch 0 Batch 977/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.645, Loss: 1.884 Epoch 0 Batch 978/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.651, Loss: 1.927 Epoch 0 Batch 979/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.646, Loss: 2.018 Epoch 0 Batch 980/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.644, Loss: 2.002 Epoch 0 Batch 981/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.654, Loss: 1.936 Epoch 0 Batch 982/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.657, Loss: 1.888 Epoch 0 Batch 983/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.660, Loss: 1.974 Epoch 0 Batch 984/1077 - Train Accuracy: 0.599, Validation Accuracy: 0.660, Loss: 1.880 Epoch 0 Batch 985/1077 - Train Accuracy: 0.661, Validation Accuracy: 0.655, Loss: 1.836 Epoch 0 Batch 986/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.660, Loss: 1.925 Epoch 0 Batch 987/1077 - Train Accuracy: 0.628, Validation Accuracy: 0.659, Loss: 1.866 Epoch 0 Batch 988/1077 - Train Accuracy: 0.578, Validation Accuracy: 0.660, Loss: 1.934 Epoch 0 Batch 989/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.646, Loss: 1.945 Epoch 0 Batch 990/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.643, Loss: 1.856 Epoch 0 Batch 991/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.642, Loss: 1.891 Epoch 0 Batch 992/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.643, Loss: 1.943 Epoch 0 Batch 993/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.642, Loss: 1.884 Epoch 0 Batch 994/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.647, Loss: 1.935 Epoch 0 Batch 995/1077 - Train Accuracy: 0.658, Validation Accuracy: 0.654, Loss: 1.870 Epoch 0 Batch 996/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.656, Loss: 1.843 Epoch 0 Batch 997/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.647, Loss: 1.883 Epoch 0 Batch 998/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.646, Loss: 1.912 Epoch 0 Batch 999/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.639, Loss: 1.976 Epoch 0 Batch 1000/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.639, Loss: 1.872 Epoch 0 Batch 1001/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.637, Loss: 1.876 Epoch 0 Batch 1002/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.631, Loss: 1.856 Epoch 0 Batch 1003/1077 - Train Accuracy: 0.635, Validation Accuracy: 0.640, Loss: 1.973 Epoch 0 Batch 1004/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.638, Loss: 1.994 Epoch 0 Batch 1005/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.638, Loss: 1.951 Epoch 0 Batch 1006/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.643, Loss: 1.872 Epoch 0 Batch 1007/1077 - Train Accuracy: 0.661, Validation Accuracy: 0.635, Loss: 1.871 Epoch 0 Batch 1008/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.638, Loss: 1.910 Epoch 0 Batch 1009/1077 - Train Accuracy: 0.627, Validation Accuracy: 0.644, Loss: 1.914 Epoch 0 Batch 1010/1077 - Train Accuracy: 0.633, Validation Accuracy: 0.653, Loss: 1.921 Epoch 0 Batch 1011/1077 - Train Accuracy: 0.627, Validation Accuracy: 0.650, Loss: 1.927 Epoch 0 Batch 1012/1077 - Train Accuracy: 0.628, Validation Accuracy: 0.632, Loss: 1.901 Epoch 0 Batch 1013/1077 - Train Accuracy: 0.658, Validation Accuracy: 0.625, Loss: 1.910 Epoch 0 Batch 1014/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.633, Loss: 1.909 Epoch 0 Batch 1015/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.645, Loss: 1.964 Epoch 0 Batch 1016/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.647, Loss: 1.931 Epoch 0 Batch 1017/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.652, Loss: 1.853 Epoch 0 Batch 1018/1077 - Train Accuracy: 0.637, Validation Accuracy: 0.659, Loss: 1.882 Epoch 0 Batch 1019/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.661, Loss: 1.908 Epoch 0 Batch 1020/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.656, Loss: 1.874 Epoch 0 Batch 1021/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.658, Loss: 1.881 Epoch 0 Batch 1022/1077 - Train Accuracy: 0.666, Validation Accuracy: 0.651, Loss: 1.902 Epoch 0 Batch 1023/1077 - Train Accuracy: 0.653, Validation Accuracy: 0.662, Loss: 1.868 Epoch 0 Batch 1024/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.650, Loss: 1.937 Epoch 0 Batch 1025/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.648, Loss: 1.874 Epoch 0 Batch 1026/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.642, Loss: 1.883 Epoch 0 Batch 1027/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.632, Loss: 1.881 Epoch 0 Batch 1028/1077 - Train Accuracy: 0.597, Validation Accuracy: 0.634, Loss: 1.886 Epoch 0 Batch 1029/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.640, Loss: 1.878 Epoch 0 Batch 1030/1077 - Train Accuracy: 0.612, Validation Accuracy: 0.649, Loss: 1.938 Epoch 0 Batch 1031/1077 - Train Accuracy: 0.583, Validation Accuracy: 0.642, Loss: 1.911 Epoch 0 Batch 1032/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.639, Loss: 1.896 Epoch 0 Batch 1033/1077 - Train Accuracy: 0.580, Validation Accuracy: 0.637, Loss: 1.933 Epoch 0 Batch 1034/1077 - Train Accuracy: 0.576, Validation Accuracy: 0.636, Loss: 1.996 Epoch 0 Batch 1035/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.641, Loss: 1.904 Epoch 0 Batch 1036/1077 - Train Accuracy: 0.622, Validation Accuracy: 0.641, Loss: 1.880 Epoch 0 Batch 1037/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.645, Loss: 1.911 Epoch 0 Batch 1038/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.645, Loss: 1.868 Epoch 0 Batch 1039/1077 - Train Accuracy: 0.669, Validation Accuracy: 0.651, Loss: 1.848 Epoch 0 Batch 1040/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.647, Loss: 1.899 Epoch 0 Batch 1041/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.636, Loss: 1.946 Epoch 0 Batch 1042/1077 - Train Accuracy: 0.665, Validation Accuracy: 0.647, Loss: 1.949 Epoch 0 Batch 1043/1077 - Train Accuracy: 0.652, Validation Accuracy: 0.653, Loss: 1.840 Epoch 0 Batch 1044/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.654, Loss: 1.938 Epoch 0 Batch 1045/1077 - Train Accuracy: 0.583, Validation Accuracy: 0.653, Loss: 1.860 Epoch 0 Batch 1046/1077 - Train Accuracy: 0.647, Validation Accuracy: 0.650, Loss: 1.965 Epoch 0 Batch 1047/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.643, Loss: 1.951 Epoch 0 Batch 1048/1077 - Train Accuracy: 0.599, Validation Accuracy: 0.641, Loss: 1.890 Epoch 0 Batch 1049/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.646, Loss: 1.945 Epoch 0 Batch 1050/1077 - Train Accuracy: 0.575, Validation Accuracy: 0.643, Loss: 1.837 Epoch 0 Batch 1051/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.647, Loss: 1.866 Epoch 0 Batch 1052/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.651, Loss: 1.975 Epoch 0 Batch 1053/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.642, Loss: 1.866 Epoch 0 Batch 1054/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.647, Loss: 1.909 Epoch 0 Batch 1055/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.645, Loss: 1.976 Epoch 0 Batch 1056/1077 - Train Accuracy: 0.640, Validation Accuracy: 0.640, Loss: 1.865 Epoch 0 Batch 1057/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.642, Loss: 1.836 Epoch 0 Batch 1058/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.640, Loss: 2.008 Epoch 0 Batch 1059/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.657, Loss: 1.932 Epoch 0 Batch 1060/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.650, Loss: 1.920 Epoch 0 Batch 1061/1077 - Train Accuracy: 0.656, Validation Accuracy: 0.650, Loss: 1.947 Epoch 0 Batch 1062/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.653, Loss: 1.896 Epoch 0 Batch 1063/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.647, Loss: 1.923 Epoch 0 Batch 1064/1077 - Train Accuracy: 0.654, Validation Accuracy: 0.656, Loss: 1.868 Epoch 0 Batch 1065/1077 - Train Accuracy: 0.622, Validation Accuracy: 0.651, Loss: 1.893 Epoch 0 Batch 1066/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.648, Loss: 1.869 Epoch 0 Batch 1067/1077 - Train Accuracy: 0.663, Validation Accuracy: 0.644, Loss: 1.842 Epoch 0 Batch 1068/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.640, Loss: 1.907 Epoch 0 Batch 1069/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.639, Loss: 1.854 Epoch 0 Batch 1070/1077 - Train Accuracy: 0.576, Validation Accuracy: 0.643, Loss: 1.930 Epoch 0 Batch 1071/1077 - Train Accuracy: 0.628, Validation Accuracy: 0.648, Loss: 1.908 Epoch 0 Batch 1072/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.647, Loss: 1.820 Epoch 0 Batch 1073/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.649, Loss: 1.886 Epoch 0 Batch 1074/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.635, Loss: 1.956 Epoch 0 Batch 1075/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.625, Loss: 1.960 Epoch 1 Batch 0/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.611, Loss: 1.839 Epoch 1 Batch 1/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.624, Loss: 1.906 Epoch 1 Batch 2/1077 - Train Accuracy: 0.536, Validation Accuracy: 0.634, Loss: 1.928 Epoch 1 Batch 3/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.650, Loss: 1.872 Epoch 1 Batch 4/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.646, Loss: 1.937 Epoch 1 Batch 5/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.640, Loss: 1.965 Epoch 1 Batch 6/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.641, Loss: 1.941 Epoch 1 Batch 7/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.641, Loss: 1.950 Epoch 1 Batch 8/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.639, Loss: 1.858 Epoch 1 Batch 9/1077 - Train Accuracy: 0.666, Validation Accuracy: 0.642, Loss: 1.886 Epoch 1 Batch 10/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.647, Loss: 1.944 Epoch 1 Batch 11/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.657, Loss: 1.888 Epoch 1 Batch 12/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.652, Loss: 1.838 Epoch 1 Batch 13/1077 - Train Accuracy: 0.664, Validation Accuracy: 0.645, Loss: 1.867 Epoch 1 Batch 14/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.641, Loss: 1.823 Epoch 1 Batch 15/1077 - Train Accuracy: 0.627, Validation Accuracy: 0.630, Loss: 1.833 Epoch 1 Batch 16/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.626, Loss: 1.943 Epoch 1 Batch 17/1077 - Train Accuracy: 0.656, Validation Accuracy: 0.638, Loss: 1.885 Epoch 1 Batch 18/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.634, Loss: 2.027 Epoch 1 Batch 19/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.633, Loss: 1.883 Epoch 1 Batch 20/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.629, Loss: 1.928 Epoch 1 Batch 21/1077 - Train Accuracy: 0.561, Validation Accuracy: 0.627, Loss: 1.908 Epoch 1 Batch 22/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.619, Loss: 1.927 Epoch 1 Batch 23/1077 - Train Accuracy: 0.622, Validation Accuracy: 0.625, Loss: 1.981 Epoch 1 Batch 24/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.634, Loss: 1.884 Epoch 1 Batch 25/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.625, Loss: 1.845 Epoch 1 Batch 26/1077 - Train Accuracy: 0.556, Validation Accuracy: 0.625, Loss: 1.919 Epoch 1 Batch 27/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.628, Loss: 1.826 Epoch 1 Batch 28/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.619, Loss: 1.955 Epoch 1 Batch 29/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.628, Loss: 1.844 Epoch 1 Batch 30/1077 - Train Accuracy: 0.612, Validation Accuracy: 0.635, Loss: 1.853 Epoch 1 Batch 31/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.644, Loss: 1.837 Epoch 1 Batch 32/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.647, Loss: 1.860 Epoch 1 Batch 33/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.651, Loss: 1.794 Epoch 1 Batch 34/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.651, Loss: 1.917 Epoch 1 Batch 35/1077 - Train Accuracy: 0.661, Validation Accuracy: 0.646, Loss: 1.773 Epoch 1 Batch 36/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.650, Loss: 1.931 Epoch 1 Batch 37/1077 - Train Accuracy: 0.635, Validation Accuracy: 0.648, Loss: 1.872 Epoch 1 Batch 38/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.647, Loss: 1.939 Epoch 1 Batch 39/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.650, Loss: 1.905 Epoch 1 Batch 40/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.650, Loss: 1.900 Epoch 1 Batch 41/1077 - Train Accuracy: 0.627, Validation Accuracy: 0.649, Loss: 1.957 Epoch 1 Batch 42/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.657, Loss: 1.896 Epoch 1 Batch 43/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.657, Loss: 1.838 Epoch 1 Batch 44/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.646, Loss: 1.965 Epoch 1 Batch 45/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.636, Loss: 1.903 Epoch 1 Batch 46/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.631, Loss: 1.920 Epoch 1 Batch 47/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.628, Loss: 1.920 Epoch 1 Batch 48/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.625, Loss: 1.967 Epoch 1 Batch 49/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.626, Loss: 1.838 Epoch 1 Batch 50/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.617, Loss: 1.851 Epoch 1 Batch 51/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.630, Loss: 1.838 Epoch 1 Batch 52/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.612, Loss: 1.965 Epoch 1 Batch 53/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.632, Loss: 1.940 Epoch 1 Batch 54/1077 - Train Accuracy: 0.576, Validation Accuracy: 0.637, Loss: 1.908 Epoch 1 Batch 55/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.634, Loss: 1.829 Epoch 1 Batch 56/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.630, Loss: 1.874 Epoch 1 Batch 57/1077 - Train Accuracy: 0.652, Validation Accuracy: 0.626, Loss: 1.911 Epoch 1 Batch 58/1077 - Train Accuracy: 0.627, Validation Accuracy: 0.633, Loss: 1.856 Epoch 1 Batch 59/1077 - Train Accuracy: 0.635, Validation Accuracy: 0.634, Loss: 1.955 Epoch 1 Batch 60/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.633, Loss: 1.841 Epoch 1 Batch 61/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.629, Loss: 1.870 Epoch 1 Batch 62/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.645, Loss: 1.920 Epoch 1 Batch 63/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.651, Loss: 1.813 Epoch 1 Batch 64/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.645, Loss: 1.886 Epoch 1 Batch 65/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.640, Loss: 1.942 Epoch 1 Batch 66/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.630, Loss: 1.848 Epoch 1 Batch 67/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.632, Loss: 1.845 Epoch 1 Batch 68/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.635, Loss: 1.871 Epoch 1 Batch 69/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.641, Loss: 1.906 Epoch 1 Batch 70/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.640, Loss: 1.801 Epoch 1 Batch 71/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.640, Loss: 1.875 Epoch 1 Batch 72/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.641, Loss: 1.924 Epoch 1 Batch 73/1077 - Train Accuracy: 0.628, Validation Accuracy: 0.644, Loss: 1.916 Epoch 1 Batch 74/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.646, Loss: 1.826 Epoch 1 Batch 75/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.646, Loss: 1.906 Epoch 1 Batch 76/1077 - Train Accuracy: 0.650, Validation Accuracy: 0.650, Loss: 1.880 Epoch 1 Batch 77/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.649, Loss: 1.887 Epoch 1 Batch 78/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.642, Loss: 1.895 Epoch 1 Batch 79/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.649, Loss: 1.914 Epoch 1 Batch 80/1077 - Train Accuracy: 0.650, Validation Accuracy: 0.646, Loss: 1.888 Epoch 1 Batch 81/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.639, Loss: 1.867 Epoch 1 Batch 82/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.650, Loss: 1.832 Epoch 1 Batch 83/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.635, Loss: 1.850 Epoch 1 Batch 84/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.627, Loss: 1.821 Epoch 1 Batch 85/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.635, Loss: 1.911 Epoch 1 Batch 86/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.642, Loss: 1.940 Epoch 1 Batch 87/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.650, Loss: 1.945 Epoch 1 Batch 88/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.659, Loss: 1.901 Epoch 1 Batch 89/1077 - Train Accuracy: 0.633, Validation Accuracy: 0.653, Loss: 1.902 Epoch 1 Batch 90/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.651, Loss: 1.889 Epoch 1 Batch 91/1077 - Train Accuracy: 0.669, Validation Accuracy: 0.651, Loss: 1.801 Epoch 1 Batch 92/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.646, Loss: 1.836 Epoch 1 Batch 93/1077 - Train Accuracy: 0.622, Validation Accuracy: 0.649, Loss: 1.906 Epoch 1 Batch 94/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.661, Loss: 1.905 Epoch 1 Batch 95/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.657, Loss: 1.900 Epoch 1 Batch 96/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.646, Loss: 1.895 Epoch 1 Batch 97/1077 - Train Accuracy: 0.640, Validation Accuracy: 0.640, Loss: 1.912 Epoch 1 Batch 98/1077 - Train Accuracy: 0.683, Validation Accuracy: 0.642, Loss: 1.837 Epoch 1 Batch 99/1077 - Train Accuracy: 0.637, Validation Accuracy: 0.646, Loss: 1.872 Epoch 1 Batch 100/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.637, Loss: 1.845 Epoch 1 Batch 101/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.637, Loss: 1.885 Epoch 1 Batch 102/1077 - Train Accuracy: 0.661, Validation Accuracy: 0.640, Loss: 1.901 Epoch 1 Batch 103/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.635, Loss: 1.917 Epoch 1 Batch 104/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.636, Loss: 1.971 Epoch 1 Batch 105/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.622, Loss: 1.874 Epoch 1 Batch 106/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.635, Loss: 1.894 Epoch 1 Batch 107/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.627, Loss: 1.864 Epoch 1 Batch 108/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.639, Loss: 1.847 Epoch 1 Batch 109/1077 - Train Accuracy: 0.637, Validation Accuracy: 0.648, Loss: 1.936 Epoch 1 Batch 110/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.647, Loss: 1.830 Epoch 1 Batch 111/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.631, Loss: 1.944 Epoch 1 Batch 112/1077 - Train Accuracy: 0.637, Validation Accuracy: 0.629, Loss: 1.938 Epoch 1 Batch 113/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.634, Loss: 1.901 Epoch 1 Batch 114/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.638, Loss: 1.890 Epoch 1 Batch 115/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.644, Loss: 1.914 Epoch 1 Batch 116/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.640, Loss: 1.940 Epoch 1 Batch 117/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.642, Loss: 1.897 Epoch 1 Batch 118/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.637, Loss: 1.840 Epoch 1 Batch 119/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.643, Loss: 1.897 Epoch 1 Batch 120/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.634, Loss: 1.853 Epoch 1 Batch 121/1077 - Train Accuracy: 0.649, Validation Accuracy: 0.643, Loss: 1.845 Epoch 1 Batch 122/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.637, Loss: 1.902 Epoch 1 Batch 123/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.632, Loss: 1.885 Epoch 1 Batch 124/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.647, Loss: 1.905 Epoch 1 Batch 125/1077 - Train Accuracy: 0.653, Validation Accuracy: 0.643, Loss: 1.824 Epoch 1 Batch 126/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.646, Loss: 1.846 Epoch 1 Batch 127/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.642, Loss: 1.899 Epoch 1 Batch 128/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.642, Loss: 1.846 Epoch 1 Batch 129/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.643, Loss: 1.857 Epoch 1 Batch 130/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.632, Loss: 1.839 Epoch 1 Batch 131/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.627, Loss: 1.918 Epoch 1 Batch 132/1077 - Train Accuracy: 0.633, Validation Accuracy: 0.625, Loss: 1.947 Epoch 1 Batch 133/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.621, Loss: 1.842 Epoch 1 Batch 134/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.618, Loss: 1.848 Epoch 1 Batch 135/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.627, Loss: 1.838 Epoch 1 Batch 136/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.625, Loss: 1.900 Epoch 1 Batch 137/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.637, Loss: 1.780 Epoch 1 Batch 138/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.641, Loss: 1.895 Epoch 1 Batch 139/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.658, Loss: 1.803 Epoch 1 Batch 140/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.658, Loss: 1.803 Epoch 1 Batch 141/1077 - Train Accuracy: 0.647, Validation Accuracy: 0.656, Loss: 1.804 Epoch 1 Batch 142/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.659, Loss: 1.889 Epoch 1 Batch 143/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.661, Loss: 1.854 Epoch 1 Batch 144/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.662, Loss: 1.868 Epoch 1 Batch 145/1077 - Train Accuracy: 0.686, Validation Accuracy: 0.664, Loss: 1.949 Epoch 1 Batch 146/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.655, Loss: 1.877 Epoch 1 Batch 147/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.655, Loss: 1.861 Epoch 1 Batch 148/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.657, Loss: 1.803 Epoch 1 Batch 149/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.642, Loss: 1.814 Epoch 1 Batch 150/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.646, Loss: 1.911 Epoch 1 Batch 151/1077 - Train Accuracy: 0.674, Validation Accuracy: 0.638, Loss: 1.836 Epoch 1 Batch 152/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.633, Loss: 1.844 Epoch 1 Batch 153/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.615, Loss: 1.901 Epoch 1 Batch 154/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.613, Loss: 1.952 Epoch 1 Batch 155/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.608, Loss: 1.912 Epoch 1 Batch 156/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.623, Loss: 1.869 Epoch 1 Batch 157/1077 - Train Accuracy: 0.660, Validation Accuracy: 0.643, Loss: 1.937 Epoch 1 Batch 158/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.653, Loss: 1.908 Epoch 1 Batch 159/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.656, Loss: 1.840 Epoch 1 Batch 160/1077 - Train Accuracy: 0.660, Validation Accuracy: 0.658, Loss: 1.845 Epoch 1 Batch 161/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.662, Loss: 1.798 Epoch 1 Batch 162/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.663, Loss: 1.975 Epoch 1 Batch 163/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.665, Loss: 1.864 Epoch 1 Batch 164/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.662, Loss: 1.911 Epoch 1 Batch 165/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.668, Loss: 1.836 Epoch 1 Batch 166/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.656, Loss: 1.864 Epoch 1 Batch 167/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.662, Loss: 1.837 Epoch 1 Batch 168/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.659, Loss: 1.872 Epoch 1 Batch 169/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.650, Loss: 1.894 Epoch 1 Batch 170/1077 - Train Accuracy: 0.579, Validation Accuracy: 0.642, Loss: 1.811 Epoch 1 Batch 171/1077 - Train Accuracy: 0.650, Validation Accuracy: 0.635, Loss: 1.864 Epoch 1 Batch 172/1077 - Train Accuracy: 0.658, Validation Accuracy: 0.630, Loss: 1.869 Epoch 1 Batch 173/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.635, Loss: 1.941 Epoch 1 Batch 174/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.635, Loss: 1.892 Epoch 1 Batch 175/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.646, Loss: 1.868 Epoch 1 Batch 176/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.645, Loss: 1.835 Epoch 1 Batch 177/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.652, Loss: 1.850 Epoch 1 Batch 178/1077 - Train Accuracy: 0.663, Validation Accuracy: 0.656, Loss: 1.828 Epoch 1 Batch 179/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.661, Loss: 1.845 Epoch 1 Batch 180/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.656, Loss: 1.836 Epoch 1 Batch 181/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.658, Loss: 1.941 Epoch 1 Batch 182/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.657, Loss: 1.826 Epoch 1 Batch 183/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.641, Loss: 1.873 Epoch 1 Batch 184/1077 - Train Accuracy: 0.682, Validation Accuracy: 0.637, Loss: 1.982 Epoch 1 Batch 185/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.648, Loss: 1.786 Epoch 1 Batch 186/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.652, Loss: 1.821 Epoch 1 Batch 187/1077 - Train Accuracy: 0.640, Validation Accuracy: 0.652, Loss: 1.843 Epoch 1 Batch 188/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.638, Loss: 1.827 Epoch 1 Batch 189/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.614, Loss: 1.828 Epoch 1 Batch 190/1077 - Train Accuracy: 0.666, Validation Accuracy: 0.620, Loss: 1.902 Epoch 1 Batch 191/1077 - Train Accuracy: 0.656, Validation Accuracy: 0.630, Loss: 1.794 Epoch 1 Batch 192/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.646, Loss: 1.894 Epoch 1 Batch 193/1077 - Train Accuracy: 0.681, Validation Accuracy: 0.648, Loss: 1.796 Epoch 1 Batch 194/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.643, Loss: 1.800 Epoch 1 Batch 195/1077 - Train Accuracy: 0.622, Validation Accuracy: 0.646, Loss: 1.817 Epoch 1 Batch 196/1077 - Train Accuracy: 0.688, Validation Accuracy: 0.653, Loss: 1.894 Epoch 1 Batch 197/1077 - Train Accuracy: 0.664, Validation Accuracy: 0.658, Loss: 1.851 Epoch 1 Batch 198/1077 - Train Accuracy: 0.674, Validation Accuracy: 0.646, Loss: 1.857 Epoch 1 Batch 199/1077 - Train Accuracy: 0.641, Validation Accuracy: 0.647, Loss: 1.827 Epoch 1 Batch 200/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.644, Loss: 1.886 Epoch 1 Batch 201/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.646, Loss: 1.815 Epoch 1 Batch 202/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.643, Loss: 1.848 Epoch 1 Batch 203/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.630, Loss: 1.797 Epoch 1 Batch 204/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.624, Loss: 1.815 Epoch 1 Batch 205/1077 - Train Accuracy: 0.641, Validation Accuracy: 0.619, Loss: 1.797 Epoch 1 Batch 206/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.644, Loss: 1.894 Epoch 1 Batch 207/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.645, Loss: 1.846 Epoch 1 Batch 208/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.642, Loss: 1.796 Epoch 1 Batch 209/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.648, Loss: 1.749 Epoch 1 Batch 210/1077 - Train Accuracy: 0.653, Validation Accuracy: 0.650, Loss: 1.788 Epoch 1 Batch 211/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.659, Loss: 1.862 Epoch 1 Batch 212/1077 - Train Accuracy: 0.663, Validation Accuracy: 0.663, Loss: 1.890 Epoch 1 Batch 213/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.664, Loss: 1.793 Epoch 1 Batch 214/1077 - Train Accuracy: 0.627, Validation Accuracy: 0.655, Loss: 1.810 Epoch 1 Batch 215/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.653, Loss: 1.810 Epoch 1 Batch 216/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.647, Loss: 1.847 Epoch 1 Batch 217/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.642, Loss: 1.860 Epoch 1 Batch 218/1077 - Train Accuracy: 0.653, Validation Accuracy: 0.650, Loss: 1.931 Epoch 1 Batch 219/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.649, Loss: 1.853 Epoch 1 Batch 220/1077 - Train Accuracy: 0.653, Validation Accuracy: 0.650, Loss: 1.902 Epoch 1 Batch 221/1077 - Train Accuracy: 0.695, Validation Accuracy: 0.656, Loss: 1.846 Epoch 1 Batch 222/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.661, Loss: 1.813 Epoch 1 Batch 223/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.658, Loss: 1.851 Epoch 1 Batch 224/1077 - Train Accuracy: 0.666, Validation Accuracy: 0.664, Loss: 1.864 Epoch 1 Batch 225/1077 - Train Accuracy: 0.656, Validation Accuracy: 0.665, Loss: 1.855 Epoch 1 Batch 226/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.660, Loss: 1.814 Epoch 1 Batch 227/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.668, Loss: 1.868 Epoch 1 Batch 228/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.660, Loss: 1.834 Epoch 1 Batch 229/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.659, Loss: 1.763 Epoch 1 Batch 230/1077 - Train Accuracy: 0.672, Validation Accuracy: 0.650, Loss: 1.857 Epoch 1 Batch 231/1077 - Train Accuracy: 0.650, Validation Accuracy: 0.653, Loss: 1.932 Epoch 1 Batch 232/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.662, Loss: 1.760 Epoch 1 Batch 233/1077 - Train Accuracy: 0.637, Validation Accuracy: 0.656, Loss: 1.842 Epoch 1 Batch 234/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.662, Loss: 1.877 Epoch 1 Batch 235/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.672, Loss: 1.819 Epoch 1 Batch 236/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.653, Loss: 1.893 Epoch 1 Batch 237/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.646, Loss: 1.831 Epoch 1 Batch 238/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.640, Loss: 1.785 Epoch 1 Batch 239/1077 - Train Accuracy: 0.661, Validation Accuracy: 0.650, Loss: 1.811 Epoch 1 Batch 240/1077 - Train Accuracy: 0.682, Validation Accuracy: 0.653, Loss: 1.800 Epoch 1 Batch 241/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.663, Loss: 1.806 Epoch 1 Batch 242/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.667, Loss: 1.739 Epoch 1 Batch 243/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.662, Loss: 1.800 Epoch 1 Batch 244/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.662, Loss: 1.880 Epoch 1 Batch 245/1077 - Train Accuracy: 0.660, Validation Accuracy: 0.658, Loss: 1.816 Epoch 1 Batch 246/1077 - Train Accuracy: 0.647, Validation Accuracy: 0.641, Loss: 1.831 Epoch 1 Batch 247/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.632, Loss: 1.809 Epoch 1 Batch 248/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.633, Loss: 1.773 Epoch 1 Batch 249/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.623, Loss: 1.826 Epoch 1 Batch 250/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.623, Loss: 1.813 Epoch 1 Batch 251/1077 - Train Accuracy: 0.635, Validation Accuracy: 0.618, Loss: 1.831 Epoch 1 Batch 252/1077 - Train Accuracy: 0.633, Validation Accuracy: 0.620, Loss: 1.836 Epoch 1 Batch 253/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.642, Loss: 1.844 Epoch 1 Batch 254/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.631, Loss: 1.881 Epoch 1 Batch 255/1077 - Train Accuracy: 0.660, Validation Accuracy: 0.636, Loss: 1.813 Epoch 1 Batch 256/1077 - Train Accuracy: 0.647, Validation Accuracy: 0.629, Loss: 1.859 Epoch 1 Batch 257/1077 - Train Accuracy: 0.665, Validation Accuracy: 0.648, Loss: 1.845 Epoch 1 Batch 258/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.644, Loss: 1.782 Epoch 1 Batch 259/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.643, Loss: 1.823 Epoch 1 Batch 260/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.635, Loss: 1.778 Epoch 1 Batch 261/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.643, Loss: 1.805 Epoch 1 Batch 262/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.638, Loss: 1.828 Epoch 1 Batch 263/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.649, Loss: 1.852 Epoch 1 Batch 264/1077 - Train Accuracy: 0.650, Validation Accuracy: 0.659, Loss: 1.750 Epoch 1 Batch 265/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.643, Loss: 1.746 Epoch 1 Batch 266/1077 - Train Accuracy: 0.649, Validation Accuracy: 0.640, Loss: 1.876 Epoch 1 Batch 267/1077 - Train Accuracy: 0.633, Validation Accuracy: 0.640, Loss: 1.785 Epoch 1 Batch 268/1077 - Train Accuracy: 0.654, Validation Accuracy: 0.639, Loss: 1.780 Epoch 1 Batch 269/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.652, Loss: 1.861 Epoch 1 Batch 270/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.650, Loss: 1.813 Epoch 1 Batch 271/1077 - Train Accuracy: 0.666, Validation Accuracy: 0.626, Loss: 1.862 Epoch 1 Batch 272/1077 - Train Accuracy: 0.640, Validation Accuracy: 0.622, Loss: 1.841 Epoch 1 Batch 273/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.620, Loss: 1.854 Epoch 1 Batch 274/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.625, Loss: 1.742 Epoch 1 Batch 275/1077 - Train Accuracy: 0.641, Validation Accuracy: 0.631, Loss: 1.850 Epoch 1 Batch 276/1077 - Train Accuracy: 0.608, Validation Accuracy: 0.632, Loss: 1.934 Epoch 1 Batch 277/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.643, Loss: 1.861 Epoch 1 Batch 278/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.661, Loss: 1.918 Epoch 1 Batch 279/1077 - Train Accuracy: 0.640, Validation Accuracy: 0.661, Loss: 1.854 Epoch 1 Batch 280/1077 - Train Accuracy: 0.686, Validation Accuracy: 0.664, Loss: 1.786 Epoch 1 Batch 281/1077 - Train Accuracy: 0.628, Validation Accuracy: 0.663, Loss: 1.844 Epoch 1 Batch 282/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.667, Loss: 1.798 Epoch 1 Batch 283/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.663, Loss: 1.884 Epoch 1 Batch 284/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.675, Loss: 1.827 Epoch 1 Batch 285/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.661, Loss: 1.819 Epoch 1 Batch 286/1077 - Train Accuracy: 0.719, Validation Accuracy: 0.670, Loss: 1.796 Epoch 1 Batch 287/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.661, Loss: 1.749 Epoch 1 Batch 288/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.661, Loss: 1.820 Epoch 1 Batch 289/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.668, Loss: 1.884 Epoch 1 Batch 290/1077 - Train Accuracy: 0.661, Validation Accuracy: 0.672, Loss: 1.861 Epoch 1 Batch 291/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.657, Loss: 1.876 Epoch 1 Batch 292/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.651, Loss: 1.864 Epoch 1 Batch 293/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.636, Loss: 1.839 Epoch 1 Batch 294/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.637, Loss: 1.787 Epoch 1 Batch 295/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.659, Loss: 1.904 Epoch 1 Batch 296/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.670, Loss: 1.824 Epoch 1 Batch 297/1077 - Train Accuracy: 0.649, Validation Accuracy: 0.672, Loss: 1.825 Epoch 1 Batch 298/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.678, Loss: 1.822 Epoch 1 Batch 299/1077 - Train Accuracy: 0.656, Validation Accuracy: 0.676, Loss: 1.824 Epoch 1 Batch 300/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.679, Loss: 1.815 Epoch 1 Batch 301/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.679, Loss: 1.836 Epoch 1 Batch 302/1077 - Train Accuracy: 0.678, Validation Accuracy: 0.667, Loss: 1.861 Epoch 1 Batch 303/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.651, Loss: 1.823 Epoch 1 Batch 304/1077 - Train Accuracy: 0.635, Validation Accuracy: 0.673, Loss: 1.867 Epoch 1 Batch 305/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.670, Loss: 1.806 Epoch 1 Batch 306/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.664, Loss: 1.769 Epoch 1 Batch 307/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.658, Loss: 1.803 Epoch 1 Batch 308/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.654, Loss: 1.824 Epoch 1 Batch 309/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.640, Loss: 1.745 Epoch 1 Batch 310/1077 - Train Accuracy: 0.633, Validation Accuracy: 0.638, Loss: 1.817 Epoch 1 Batch 311/1077 - Train Accuracy: 0.703, Validation Accuracy: 0.648, Loss: 1.819 Epoch 1 Batch 312/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.657, Loss: 1.846 Epoch 1 Batch 313/1077 - Train Accuracy: 0.652, Validation Accuracy: 0.667, Loss: 1.842 Epoch 1 Batch 314/1077 - Train Accuracy: 0.680, Validation Accuracy: 0.673, Loss: 1.796 Epoch 1 Batch 315/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.669, Loss: 1.754 Epoch 1 Batch 316/1077 - Train Accuracy: 0.659, Validation Accuracy: 0.670, Loss: 1.801 Epoch 1 Batch 317/1077 - Train Accuracy: 0.678, Validation Accuracy: 0.678, Loss: 1.816 Epoch 1 Batch 318/1077 - Train Accuracy: 0.663, Validation Accuracy: 0.667, Loss: 1.815 Epoch 1 Batch 319/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.657, Loss: 1.802 Epoch 1 Batch 320/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.660, Loss: 1.782 Epoch 1 Batch 321/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.676, Loss: 1.902 Epoch 1 Batch 322/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.672, Loss: 1.737 Epoch 1 Batch 323/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.670, Loss: 1.755 Epoch 1 Batch 324/1077 - Train Accuracy: 0.665, Validation Accuracy: 0.668, Loss: 1.828 Epoch 1 Batch 325/1077 - Train Accuracy: 0.703, Validation Accuracy: 0.673, Loss: 1.790 Epoch 1 Batch 326/1077 - Train Accuracy: 0.688, Validation Accuracy: 0.678, Loss: 1.806 Epoch 1 Batch 327/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.667, Loss: 1.845 Epoch 1 Batch 328/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.657, Loss: 1.844 Epoch 1 Batch 329/1077 - Train Accuracy: 0.659, Validation Accuracy: 0.666, Loss: 1.870 Epoch 1 Batch 330/1077 - Train Accuracy: 0.710, Validation Accuracy: 0.663, Loss: 1.841 Epoch 1 Batch 331/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.656, Loss: 1.771 Epoch 1 Batch 332/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.651, Loss: 1.791 Epoch 1 Batch 333/1077 - Train Accuracy: 0.717, Validation Accuracy: 0.656, Loss: 1.808 Epoch 1 Batch 334/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.662, Loss: 1.737 Epoch 1 Batch 335/1077 - Train Accuracy: 0.697, Validation Accuracy: 0.653, Loss: 1.721 Epoch 1 Batch 336/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.651, Loss: 1.811 Epoch 1 Batch 337/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.656, Loss: 1.824 Epoch 1 Batch 338/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.649, Loss: 1.741 Epoch 1 Batch 339/1077 - Train Accuracy: 0.684, Validation Accuracy: 0.647, Loss: 1.756 Epoch 1 Batch 340/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.652, Loss: 1.830 Epoch 1 Batch 341/1077 - Train Accuracy: 0.707, Validation Accuracy: 0.660, Loss: 1.874 Epoch 1 Batch 342/1077 - Train Accuracy: 0.674, Validation Accuracy: 0.654, Loss: 1.808 Epoch 1 Batch 343/1077 - Train Accuracy: 0.675, Validation Accuracy: 0.650, Loss: 1.854 Epoch 1 Batch 344/1077 - Train Accuracy: 0.693, Validation Accuracy: 0.650, Loss: 1.817 Epoch 1 Batch 345/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.650, Loss: 1.847 Epoch 1 Batch 346/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.645, Loss: 1.796 Epoch 1 Batch 347/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.657, Loss: 1.755 Epoch 1 Batch 348/1077 - Train Accuracy: 0.658, Validation Accuracy: 0.652, Loss: 1.792 Epoch 1 Batch 349/1077 - Train Accuracy: 0.664, Validation Accuracy: 0.642, Loss: 1.832 Epoch 1 Batch 350/1077 - Train Accuracy: 0.681, Validation Accuracy: 0.649, Loss: 1.832 Epoch 1 Batch 351/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.646, Loss: 1.759 Epoch 1 Batch 352/1077 - Train Accuracy: 0.660, Validation Accuracy: 0.656, Loss: 1.769 Epoch 1 Batch 353/1077 - Train Accuracy: 0.641, Validation Accuracy: 0.651, Loss: 1.786 Epoch 1 Batch 354/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.654, Loss: 1.797 Epoch 1 Batch 355/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.672, Loss: 1.877 Epoch 1 Batch 356/1077 - Train Accuracy: 0.680, Validation Accuracy: 0.670, Loss: 1.815 Epoch 1 Batch 357/1077 - Train Accuracy: 0.675, Validation Accuracy: 0.677, Loss: 1.769 Epoch 1 Batch 358/1077 - Train Accuracy: 0.659, Validation Accuracy: 0.678, Loss: 1.814 Epoch 1 Batch 359/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.672, Loss: 1.809 Epoch 1 Batch 360/1077 - Train Accuracy: 0.693, Validation Accuracy: 0.680, Loss: 1.768 Epoch 1 Batch 361/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.656, Loss: 1.839 Epoch 1 Batch 362/1077 - Train Accuracy: 0.702, Validation Accuracy: 0.631, Loss: 1.822 Epoch 1 Batch 363/1077 - Train Accuracy: 0.647, Validation Accuracy: 0.634, Loss: 1.839 Epoch 1 Batch 364/1077 - Train Accuracy: 0.664, Validation Accuracy: 0.643, Loss: 1.803 Epoch 1 Batch 365/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.652, Loss: 1.801 Epoch 1 Batch 366/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.664, Loss: 1.812 Epoch 1 Batch 367/1077 - Train Accuracy: 0.722, Validation Accuracy: 0.662, Loss: 1.780 Epoch 1 Batch 368/1077 - Train Accuracy: 0.684, Validation Accuracy: 0.653, Loss: 1.825 Epoch 1 Batch 369/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.658, Loss: 1.900 Epoch 1 Batch 370/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.665, Loss: 1.831 Epoch 1 Batch 371/1077 - Train Accuracy: 0.700, Validation Accuracy: 0.665, Loss: 1.840 Epoch 1 Batch 372/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.661, Loss: 1.801 Epoch 1 Batch 373/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.664, Loss: 1.863 Epoch 1 Batch 374/1077 - Train Accuracy: 0.652, Validation Accuracy: 0.658, Loss: 1.829 Epoch 1 Batch 375/1077 - Train Accuracy: 0.684, Validation Accuracy: 0.651, Loss: 1.822 Epoch 1 Batch 376/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.637, Loss: 1.845 Epoch 1 Batch 377/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.643, Loss: 1.805 Epoch 1 Batch 378/1077 - Train Accuracy: 0.665, Validation Accuracy: 0.647, Loss: 1.765 Epoch 1 Batch 379/1077 - Train Accuracy: 0.686, Validation Accuracy: 0.653, Loss: 1.798 Epoch 1 Batch 380/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.662, Loss: 1.754 Epoch 1 Batch 381/1077 - Train Accuracy: 0.686, Validation Accuracy: 0.656, Loss: 1.802 Epoch 1 Batch 382/1077 - Train Accuracy: 0.680, Validation Accuracy: 0.656, Loss: 1.891 Epoch 1 Batch 383/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.661, Loss: 1.821 Epoch 1 Batch 384/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.667, Loss: 1.905 Epoch 1 Batch 385/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.677, Loss: 1.814 Epoch 1 Batch 386/1077 - Train Accuracy: 0.654, Validation Accuracy: 0.667, Loss: 1.791 Epoch 1 Batch 387/1077 - Train Accuracy: 0.721, Validation Accuracy: 0.670, Loss: 1.777 Epoch 1 Batch 388/1077 - Train Accuracy: 0.664, Validation Accuracy: 0.671, Loss: 1.850 Epoch 1 Batch 389/1077 - Train Accuracy: 0.691, Validation Accuracy: 0.679, Loss: 1.809 Epoch 1 Batch 390/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.675, Loss: 1.815 Epoch 1 Batch 391/1077 - Train Accuracy: 0.701, Validation Accuracy: 0.672, Loss: 1.775 Epoch 1 Batch 392/1077 - Train Accuracy: 0.681, Validation Accuracy: 0.674, Loss: 1.756 Epoch 1 Batch 393/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.673, Loss: 1.743 Epoch 1 Batch 394/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.678, Loss: 1.856 Epoch 1 Batch 395/1077 - Train Accuracy: 0.675, Validation Accuracy: 0.671, Loss: 1.815 Epoch 1 Batch 396/1077 - Train Accuracy: 0.660, Validation Accuracy: 0.667, Loss: 1.766 Epoch 1 Batch 397/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.662, Loss: 1.809 Epoch 1 Batch 398/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.671, Loss: 1.798 Epoch 1 Batch 399/1077 - Train Accuracy: 0.635, Validation Accuracy: 0.673, Loss: 1.832 Epoch 1 Batch 400/1077 - Train Accuracy: 0.707, Validation Accuracy: 0.672, Loss: 1.760 Epoch 1 Batch 401/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.678, Loss: 1.767 Epoch 1 Batch 402/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.687, Loss: 1.783 Epoch 1 Batch 403/1077 - Train Accuracy: 0.675, Validation Accuracy: 0.673, Loss: 1.809 Epoch 1 Batch 404/1077 - Train Accuracy: 0.698, Validation Accuracy: 0.679, Loss: 1.882 Epoch 1 Batch 405/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.663, Loss: 1.826 Epoch 1 Batch 406/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.658, Loss: 1.820 Epoch 1 Batch 407/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.660, Loss: 1.831 Epoch 1 Batch 408/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.668, Loss: 1.762 Epoch 1 Batch 409/1077 - Train Accuracy: 0.688, Validation Accuracy: 0.670, Loss: 1.836 Epoch 1 Batch 410/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.672, Loss: 1.861 Epoch 1 Batch 411/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.675, Loss: 1.729 Epoch 1 Batch 412/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.677, Loss: 1.730 Epoch 1 Batch 413/1077 - Train Accuracy: 0.674, Validation Accuracy: 0.677, Loss: 1.759 Epoch 1 Batch 414/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.675, Loss: 1.888 Epoch 1 Batch 415/1077 - Train Accuracy: 0.699, Validation Accuracy: 0.673, Loss: 1.744 Epoch 1 Batch 416/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.670, Loss: 1.872 Epoch 1 Batch 417/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.667, Loss: 1.820 Epoch 1 Batch 418/1077 - Train Accuracy: 0.695, Validation Accuracy: 0.670, Loss: 1.879 Epoch 1 Batch 419/1077 - Train Accuracy: 0.675, Validation Accuracy: 0.671, Loss: 1.702 Epoch 1 Batch 420/1077 - Train Accuracy: 0.714, Validation Accuracy: 0.662, Loss: 1.767 Epoch 1 Batch 421/1077 - Train Accuracy: 0.665, Validation Accuracy: 0.680, Loss: 1.758 Epoch 1 Batch 422/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.668, Loss: 1.736 Epoch 1 Batch 423/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.656, Loss: 1.836 Epoch 1 Batch 424/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.650, Loss: 1.793 Epoch 1 Batch 425/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.663, Loss: 1.777 Epoch 1 Batch 426/1077 - Train Accuracy: 0.699, Validation Accuracy: 0.669, Loss: 1.747 Epoch 1 Batch 427/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.665, Loss: 1.787 Epoch 1 Batch 428/1077 - Train Accuracy: 0.688, Validation Accuracy: 0.650, Loss: 1.721 Epoch 1 Batch 429/1077 - Train Accuracy: 0.706, Validation Accuracy: 0.641, Loss: 1.750 Epoch 1 Batch 430/1077 - Train Accuracy: 0.652, Validation Accuracy: 0.661, Loss: 1.848 Epoch 1 Batch 431/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.661, Loss: 1.767 Epoch 1 Batch 432/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.657, Loss: 1.683 Epoch 1 Batch 433/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.664, Loss: 1.812 Epoch 1 Batch 434/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.654, Loss: 1.822 Epoch 1 Batch 435/1077 - Train Accuracy: 0.726, Validation Accuracy: 0.658, Loss: 1.737 Epoch 1 Batch 436/1077 - Train Accuracy: 0.695, Validation Accuracy: 0.663, Loss: 1.691 Epoch 1 Batch 437/1077 - Train Accuracy: 0.665, Validation Accuracy: 0.681, Loss: 1.843 Epoch 1 Batch 438/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.670, Loss: 1.824 Epoch 1 Batch 439/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.657, Loss: 1.778 Epoch 1 Batch 440/1077 - Train Accuracy: 0.693, Validation Accuracy: 0.655, Loss: 1.658 Epoch 1 Batch 441/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.650, Loss: 1.782 Epoch 1 Batch 442/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.642, Loss: 1.778 Epoch 1 Batch 443/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.657, Loss: 1.784 Epoch 1 Batch 444/1077 - Train Accuracy: 0.698, Validation Accuracy: 0.660, Loss: 1.712 Epoch 1 Batch 445/1077 - Train Accuracy: 0.669, Validation Accuracy: 0.644, Loss: 1.851 Epoch 1 Batch 446/1077 - Train Accuracy: 0.687, Validation Accuracy: 0.643, Loss: 1.749 Epoch 1 Batch 447/1077 - Train Accuracy: 0.701, Validation Accuracy: 0.635, Loss: 1.793 Epoch 1 Batch 448/1077 - Train Accuracy: 0.710, Validation Accuracy: 0.637, Loss: 1.777 Epoch 1 Batch 449/1077 - Train Accuracy: 0.669, Validation Accuracy: 0.652, Loss: 1.810 Epoch 1 Batch 450/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.654, Loss: 1.803 Epoch 1 Batch 451/1077 - Train Accuracy: 0.687, Validation Accuracy: 0.655, Loss: 1.717 Epoch 1 Batch 452/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.653, Loss: 1.903 Epoch 1 Batch 453/1077 - Train Accuracy: 0.698, Validation Accuracy: 0.650, Loss: 1.759 Epoch 1 Batch 454/1077 - Train Accuracy: 0.702, Validation Accuracy: 0.644, Loss: 1.837 Epoch 1 Batch 455/1077 - Train Accuracy: 0.693, Validation Accuracy: 0.642, Loss: 1.720 Epoch 1 Batch 456/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.654, Loss: 1.810 Epoch 1 Batch 457/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.667, Loss: 1.765 Epoch 1 Batch 458/1077 - Train Accuracy: 0.652, Validation Accuracy: 0.658, Loss: 1.856 Epoch 1 Batch 459/1077 - Train Accuracy: 0.706, Validation Accuracy: 0.670, Loss: 1.750 Epoch 1 Batch 460/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.675, Loss: 1.822 Epoch 1 Batch 461/1077 - Train Accuracy: 0.703, Validation Accuracy: 0.677, Loss: 1.807 Epoch 1 Batch 462/1077 - Train Accuracy: 0.691, Validation Accuracy: 0.661, Loss: 1.797 Epoch 1 Batch 463/1077 - Train Accuracy: 0.695, Validation Accuracy: 0.665, Loss: 1.830 Epoch 1 Batch 464/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.672, Loss: 1.786 Epoch 1 Batch 465/1077 - Train Accuracy: 0.693, Validation Accuracy: 0.669, Loss: 1.753 Epoch 1 Batch 466/1077 - Train Accuracy: 0.675, Validation Accuracy: 0.664, Loss: 1.843 Epoch 1 Batch 467/1077 - Train Accuracy: 0.739, Validation Accuracy: 0.670, Loss: 1.788 Epoch 1 Batch 468/1077 - Train Accuracy: 0.715, Validation Accuracy: 0.675, Loss: 1.848 Epoch 1 Batch 469/1077 - Train Accuracy: 0.665, Validation Accuracy: 0.667, Loss: 1.828 Epoch 1 Batch 470/1077 - Train Accuracy: 0.713, Validation Accuracy: 0.668, Loss: 1.850 Epoch 1 Batch 471/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.676, Loss: 1.803 Epoch 1 Batch 472/1077 - Train Accuracy: 0.712, Validation Accuracy: 0.677, Loss: 1.713 Epoch 1 Batch 473/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.677, Loss: 1.796 Epoch 1 Batch 474/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.659, Loss: 1.793 Epoch 1 Batch 475/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.667, Loss: 1.780 Epoch 1 Batch 476/1077 - Train Accuracy: 0.687, Validation Accuracy: 0.667, Loss: 1.726 Epoch 1 Batch 477/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.677, Loss: 1.795 Epoch 1 Batch 478/1077 - Train Accuracy: 0.697, Validation Accuracy: 0.670, Loss: 1.737 Epoch 1 Batch 479/1077 - Train Accuracy: 0.686, Validation Accuracy: 0.668, Loss: 1.689 Epoch 1 Batch 480/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.666, Loss: 1.762 Epoch 1 Batch 481/1077 - Train Accuracy: 0.678, Validation Accuracy: 0.675, Loss: 1.820 Epoch 1 Batch 482/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.667, Loss: 1.727 Epoch 1 Batch 483/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.670, Loss: 1.815 Epoch 1 Batch 484/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.674, Loss: 1.742 Epoch 1 Batch 485/1077 - Train Accuracy: 0.730, Validation Accuracy: 0.670, Loss: 1.872 Epoch 1 Batch 486/1077 - Train Accuracy: 0.721, Validation Accuracy: 0.671, Loss: 1.821 Epoch 1 Batch 487/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.678, Loss: 1.887 Epoch 1 Batch 488/1077 - Train Accuracy: 0.687, Validation Accuracy: 0.674, Loss: 1.809 Epoch 1 Batch 489/1077 - Train Accuracy: 0.720, Validation Accuracy: 0.659, Loss: 1.794 Epoch 1 Batch 490/1077 - Train Accuracy: 0.720, Validation Accuracy: 0.665, Loss: 1.789 Epoch 1 Batch 491/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.676, Loss: 1.769 Epoch 1 Batch 492/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.674, Loss: 1.764 Epoch 1 Batch 493/1077 - Train Accuracy: 0.697, Validation Accuracy: 0.676, Loss: 1.692 Epoch 1 Batch 494/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.668, Loss: 1.761 Epoch 1 Batch 495/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.665, Loss: 1.756 Epoch 1 Batch 496/1077 - Train Accuracy: 0.707, Validation Accuracy: 0.662, Loss: 1.762 Epoch 1 Batch 497/1077 - Train Accuracy: 0.678, Validation Accuracy: 0.657, Loss: 1.822 Epoch 1 Batch 498/1077 - Train Accuracy: 0.721, Validation Accuracy: 0.656, Loss: 1.794 Epoch 1 Batch 499/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.659, Loss: 1.786 Epoch 1 Batch 500/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.664, Loss: 1.738 Epoch 1 Batch 501/1077 - Train Accuracy: 0.721, Validation Accuracy: 0.679, Loss: 1.720 Epoch 1 Batch 502/1077 - Train Accuracy: 0.735, Validation Accuracy: 0.673, Loss: 1.817 Epoch 1 Batch 503/1077 - Train Accuracy: 0.741, Validation Accuracy: 0.674, Loss: 1.740 Epoch 1 Batch 504/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.675, Loss: 1.809 Epoch 1 Batch 505/1077 - Train Accuracy: 0.737, Validation Accuracy: 0.676, Loss: 1.705 Epoch 1 Batch 506/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.674, Loss: 1.724 Epoch 1 Batch 507/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.680, Loss: 1.731 Epoch 1 Batch 508/1077 - Train Accuracy: 0.710, Validation Accuracy: 0.679, Loss: 1.754 Epoch 1 Batch 509/1077 - Train Accuracy: 0.714, Validation Accuracy: 0.678, Loss: 1.810 Epoch 1 Batch 510/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.677, Loss: 1.756 Epoch 1 Batch 511/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.678, Loss: 1.804 Epoch 1 Batch 512/1077 - Train Accuracy: 0.753, Validation Accuracy: 0.676, Loss: 1.716 Epoch 1 Batch 513/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.670, Loss: 1.763 Epoch 1 Batch 514/1077 - Train Accuracy: 0.680, Validation Accuracy: 0.669, Loss: 1.711 Epoch 1 Batch 515/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.670, Loss: 1.786 Epoch 1 Batch 516/1077 - Train Accuracy: 0.752, Validation Accuracy: 0.673, Loss: 1.782 Epoch 1 Batch 517/1077 - Train Accuracy: 0.723, Validation Accuracy: 0.685, Loss: 1.784 Epoch 1 Batch 518/1077 - Train Accuracy: 0.709, Validation Accuracy: 0.688, Loss: 1.726 Epoch 1 Batch 519/1077 - Train Accuracy: 0.716, Validation Accuracy: 0.683, Loss: 1.728 Epoch 1 Batch 520/1077 - Train Accuracy: 0.774, Validation Accuracy: 0.682, Loss: 1.694 Epoch 1 Batch 521/1077 - Train Accuracy: 0.716, Validation Accuracy: 0.682, Loss: 1.816 Epoch 1 Batch 522/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.688, Loss: 1.817 Epoch 1 Batch 523/1077 - Train Accuracy: 0.716, Validation Accuracy: 0.686, Loss: 1.798 Epoch 1 Batch 524/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.686, Loss: 1.802 Epoch 1 Batch 525/1077 - Train Accuracy: 0.707, Validation Accuracy: 0.685, Loss: 1.799 Epoch 1 Batch 526/1077 - Train Accuracy: 0.731, Validation Accuracy: 0.679, Loss: 1.774 Epoch 1 Batch 527/1077 - Train Accuracy: 0.691, Validation Accuracy: 0.688, Loss: 1.811 Epoch 1 Batch 528/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.695, Loss: 1.742 Epoch 1 Batch 529/1077 - Train Accuracy: 0.717, Validation Accuracy: 0.692, Loss: 1.778 Epoch 1 Batch 530/1077 - Train Accuracy: 0.694, Validation Accuracy: 0.683, Loss: 1.826 Epoch 1 Batch 531/1077 - Train Accuracy: 0.701, Validation Accuracy: 0.686, Loss: 1.808 Epoch 1 Batch 532/1077 - Train Accuracy: 0.682, Validation Accuracy: 0.678, Loss: 1.820 Epoch 1 Batch 533/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.675, Loss: 1.824 Epoch 1 Batch 534/1077 - Train Accuracy: 0.730, Validation Accuracy: 0.684, Loss: 1.718 Epoch 1 Batch 535/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.682, Loss: 1.794 Epoch 1 Batch 536/1077 - Train Accuracy: 0.742, Validation Accuracy: 0.673, Loss: 1.759 Epoch 1 Batch 537/1077 - Train Accuracy: 0.714, Validation Accuracy: 0.673, Loss: 1.763 Epoch 1 Batch 538/1077 - Train Accuracy: 0.730, Validation Accuracy: 0.677, Loss: 1.757 Epoch 1 Batch 539/1077 - Train Accuracy: 0.693, Validation Accuracy: 0.670, Loss: 1.800 Epoch 1 Batch 540/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.661, Loss: 1.771 Epoch 1 Batch 541/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.679, Loss: 1.738 Epoch 1 Batch 542/1077 - Train Accuracy: 0.713, Validation Accuracy: 0.673, Loss: 1.824 Epoch 1 Batch 543/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.667, Loss: 1.744 Epoch 1 Batch 544/1077 - Train Accuracy: 0.702, Validation Accuracy: 0.672, Loss: 1.767 Epoch 1 Batch 545/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.681, Loss: 1.739 Epoch 1 Batch 546/1077 - Train Accuracy: 0.684, Validation Accuracy: 0.688, Loss: 1.776 Epoch 1 Batch 547/1077 - Train Accuracy: 0.733, Validation Accuracy: 0.694, Loss: 1.791 Epoch 1 Batch 548/1077 - Train Accuracy: 0.723, Validation Accuracy: 0.689, Loss: 1.746 Epoch 1 Batch 549/1077 - Train Accuracy: 0.661, Validation Accuracy: 0.692, Loss: 1.781 Epoch 1 Batch 550/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.687, Loss: 1.804 Epoch 1 Batch 551/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.691, Loss: 1.780 Epoch 1 Batch 552/1077 - Train Accuracy: 0.719, Validation Accuracy: 0.690, Loss: 1.802 Epoch 1 Batch 553/1077 - Train Accuracy: 0.737, Validation Accuracy: 0.699, Loss: 1.793 Epoch 1 Batch 554/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.690, Loss: 1.693 Epoch 1 Batch 555/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.688, Loss: 1.729 Epoch 1 Batch 556/1077 - Train Accuracy: 0.709, Validation Accuracy: 0.694, Loss: 1.728 Epoch 1 Batch 557/1077 - Train Accuracy: 0.726, Validation Accuracy: 0.678, Loss: 1.797 Epoch 1 Batch 558/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.672, Loss: 1.766 Epoch 1 Batch 559/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.673, Loss: 1.828 Epoch 1 Batch 560/1077 - Train Accuracy: 0.702, Validation Accuracy: 0.679, Loss: 1.802 Epoch 1 Batch 561/1077 - Train Accuracy: 0.724, Validation Accuracy: 0.685, Loss: 1.696 Epoch 1 Batch 562/1077 - Train Accuracy: 0.751, Validation Accuracy: 0.682, Loss: 1.712 Epoch 1 Batch 563/1077 - Train Accuracy: 0.682, Validation Accuracy: 0.673, Loss: 1.739 Epoch 1 Batch 564/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.670, Loss: 1.832 Epoch 1 Batch 565/1077 - Train Accuracy: 0.694, Validation Accuracy: 0.672, Loss: 1.787 Epoch 1 Batch 566/1077 - Train Accuracy: 0.714, Validation Accuracy: 0.685, Loss: 1.820 Epoch 1 Batch 567/1077 - Train Accuracy: 0.712, Validation Accuracy: 0.678, Loss: 1.757 Epoch 1 Batch 568/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.687, Loss: 1.801 Epoch 1 Batch 569/1077 - Train Accuracy: 0.733, Validation Accuracy: 0.687, Loss: 1.720 Epoch 1 Batch 570/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.696, Loss: 1.741 Epoch 1 Batch 571/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.697, Loss: 1.713 Epoch 1 Batch 572/1077 - Train Accuracy: 0.728, Validation Accuracy: 0.702, Loss: 1.787 Epoch 1 Batch 573/1077 - Train Accuracy: 0.715, Validation Accuracy: 0.703, Loss: 1.768 Epoch 1 Batch 574/1077 - Train Accuracy: 0.678, Validation Accuracy: 0.688, Loss: 1.750 Epoch 1 Batch 575/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.694, Loss: 1.819 Epoch 1 Batch 576/1077 - Train Accuracy: 0.756, Validation Accuracy: 0.686, Loss: 1.772 Epoch 1 Batch 577/1077 - Train Accuracy: 0.695, Validation Accuracy: 0.684, Loss: 1.810 Epoch 1 Batch 578/1077 - Train Accuracy: 0.698, Validation Accuracy: 0.688, Loss: 1.834 Epoch 1 Batch 579/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.690, Loss: 1.764 Epoch 1 Batch 580/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.692, Loss: 1.747 Epoch 1 Batch 581/1077 - Train Accuracy: 0.731, Validation Accuracy: 0.680, Loss: 1.744 Epoch 1 Batch 582/1077 - Train Accuracy: 0.733, Validation Accuracy: 0.681, Loss: 1.701 Epoch 1 Batch 583/1077 - Train Accuracy: 0.712, Validation Accuracy: 0.680, Loss: 1.798 Epoch 1 Batch 584/1077 - Train Accuracy: 0.713, Validation Accuracy: 0.694, Loss: 1.770 Epoch 1 Batch 585/1077 - Train Accuracy: 0.751, Validation Accuracy: 0.698, Loss: 1.711 Epoch 1 Batch 586/1077 - Train Accuracy: 0.697, Validation Accuracy: 0.705, Loss: 1.769 Epoch 1 Batch 587/1077 - Train Accuracy: 0.723, Validation Accuracy: 0.694, Loss: 1.707 Epoch 1 Batch 588/1077 - Train Accuracy: 0.716, Validation Accuracy: 0.688, Loss: 1.708 Epoch 1 Batch 589/1077 - Train Accuracy: 0.725, Validation Accuracy: 0.683, Loss: 1.780 Epoch 1 Batch 590/1077 - Train Accuracy: 0.698, Validation Accuracy: 0.683, Loss: 1.703 Epoch 1 Batch 591/1077 - Train Accuracy: 0.732, Validation Accuracy: 0.677, Loss: 1.749 Epoch 1 Batch 592/1077 - Train Accuracy: 0.739, Validation Accuracy: 0.673, Loss: 1.736 Epoch 1 Batch 593/1077 - Train Accuracy: 0.713, Validation Accuracy: 0.678, Loss: 1.757 Epoch 1 Batch 594/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.688, Loss: 1.752 Epoch 1 Batch 595/1077 - Train Accuracy: 0.701, Validation Accuracy: 0.689, Loss: 1.666 Epoch 1 Batch 596/1077 - Train Accuracy: 0.693, Validation Accuracy: 0.692, Loss: 1.794 Epoch 1 Batch 597/1077 - Train Accuracy: 0.659, Validation Accuracy: 0.697, Loss: 1.773 Epoch 1 Batch 598/1077 - Train Accuracy: 0.695, Validation Accuracy: 0.705, Loss: 1.700 Epoch 1 Batch 599/1077 - Train Accuracy: 0.698, Validation Accuracy: 0.704, Loss: 1.756 Epoch 1 Batch 600/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.709, Loss: 1.789 Epoch 1 Batch 601/1077 - Train Accuracy: 0.731, Validation Accuracy: 0.706, Loss: 1.755 Epoch 1 Batch 602/1077 - Train Accuracy: 0.720, Validation Accuracy: 0.705, Loss: 1.786 Epoch 1 Batch 603/1077 - Train Accuracy: 0.714, Validation Accuracy: 0.704, Loss: 1.716 Epoch 1 Batch 604/1077 - Train Accuracy: 0.700, Validation Accuracy: 0.704, Loss: 1.829 Epoch 1 Batch 605/1077 - Train Accuracy: 0.725, Validation Accuracy: 0.707, Loss: 1.705 Epoch 1 Batch 606/1077 - Train Accuracy: 0.734, Validation Accuracy: 0.702, Loss: 1.732 Epoch 1 Batch 607/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.700, Loss: 1.728 Epoch 1 Batch 608/1077 - Train Accuracy: 0.734, Validation Accuracy: 0.701, Loss: 1.785 Epoch 1 Batch 609/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.692, Loss: 1.661 Epoch 1 Batch 610/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.685, Loss: 1.800 Epoch 1 Batch 611/1077 - Train Accuracy: 0.717, Validation Accuracy: 0.691, Loss: 1.706 Epoch 1 Batch 612/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.690, Loss: 1.692 Epoch 1 Batch 613/1077 - Train Accuracy: 0.686, Validation Accuracy: 0.683, Loss: 1.836 Epoch 1 Batch 614/1077 - Train Accuracy: 0.717, Validation Accuracy: 0.681, Loss: 1.758 Epoch 1 Batch 615/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.686, Loss: 1.826 Epoch 1 Batch 616/1077 - Train Accuracy: 0.710, Validation Accuracy: 0.678, Loss: 1.758 Epoch 1 Batch 617/1077 - Train Accuracy: 0.709, Validation Accuracy: 0.697, Loss: 1.735 Epoch 1 Batch 618/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.707, Loss: 1.784 Epoch 1 Batch 619/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.698, Loss: 1.756 Epoch 1 Batch 620/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.701, Loss: 1.759 Epoch 1 Batch 621/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.697, Loss: 1.772 Epoch 1 Batch 622/1077 - Train Accuracy: 0.736, Validation Accuracy: 0.700, Loss: 1.682 Epoch 1 Batch 623/1077 - Train Accuracy: 0.691, Validation Accuracy: 0.687, Loss: 1.751 Epoch 1 Batch 624/1077 - Train Accuracy: 0.720, Validation Accuracy: 0.691, Loss: 1.771 Epoch 1 Batch 625/1077 - Train Accuracy: 0.722, Validation Accuracy: 0.695, Loss: 1.762 Epoch 1 Batch 626/1077 - Train Accuracy: 0.761, Validation Accuracy: 0.686, Loss: 1.749 Epoch 1 Batch 627/1077 - Train Accuracy: 0.724, Validation Accuracy: 0.693, Loss: 1.768 Epoch 1 Batch 628/1077 - Train Accuracy: 0.695, Validation Accuracy: 0.703, Loss: 1.739 Epoch 1 Batch 629/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.715, Loss: 1.834 Epoch 1 Batch 630/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.723, Loss: 1.802 Epoch 1 Batch 631/1077 - Train Accuracy: 0.709, Validation Accuracy: 0.731, Loss: 1.706 Epoch 1 Batch 632/1077 - Train Accuracy: 0.733, Validation Accuracy: 0.727, Loss: 1.670 Epoch 1 Batch 633/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.718, Loss: 1.789 Epoch 1 Batch 634/1077 - Train Accuracy: 0.726, Validation Accuracy: 0.699, Loss: 1.825 Epoch 1 Batch 635/1077 - Train Accuracy: 0.712, Validation Accuracy: 0.698, Loss: 1.724 Epoch 1 Batch 636/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.701, Loss: 1.681 Epoch 1 Batch 637/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.702, Loss: 1.671 Epoch 1 Batch 638/1077 - Train Accuracy: 0.710, Validation Accuracy: 0.712, Loss: 1.734 Epoch 1 Batch 639/1077 - Train Accuracy: 0.739, Validation Accuracy: 0.695, Loss: 1.750 Epoch 1 Batch 640/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.697, Loss: 1.784 Epoch 1 Batch 641/1077 - Train Accuracy: 0.720, Validation Accuracy: 0.694, Loss: 1.776 Epoch 1 Batch 642/1077 - Train Accuracy: 0.701, Validation Accuracy: 0.692, Loss: 1.746 Epoch 1 Batch 643/1077 - Train Accuracy: 0.748, Validation Accuracy: 0.682, Loss: 1.746 Epoch 1 Batch 644/1077 - Train Accuracy: 0.732, Validation Accuracy: 0.683, Loss: 1.759 Epoch 1 Batch 645/1077 - Train Accuracy: 0.702, Validation Accuracy: 0.690, Loss: 1.733 Epoch 1 Batch 646/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.687, Loss: 1.704 Epoch 1 Batch 647/1077 - Train Accuracy: 0.723, Validation Accuracy: 0.699, Loss: 1.678 Epoch 1 Batch 648/1077 - Train Accuracy: 0.706, Validation Accuracy: 0.699, Loss: 1.751 Epoch 1 Batch 649/1077 - Train Accuracy: 0.715, Validation Accuracy: 0.694, Loss: 1.793 Epoch 1 Batch 650/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.708, Loss: 1.723 Epoch 1 Batch 651/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.712, Loss: 1.663 Epoch 1 Batch 652/1077 - Train Accuracy: 0.740, Validation Accuracy: 0.717, Loss: 1.666 Epoch 1 Batch 653/1077 - Train Accuracy: 0.709, Validation Accuracy: 0.704, Loss: 1.729 Epoch 1 Batch 654/1077 - Train Accuracy: 0.736, Validation Accuracy: 0.700, Loss: 1.746 Epoch 1 Batch 655/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.705, Loss: 1.726 Epoch 1 Batch 656/1077 - Train Accuracy: 0.684, Validation Accuracy: 0.706, Loss: 1.724 Epoch 1 Batch 657/1077 - Train Accuracy: 0.740, Validation Accuracy: 0.700, Loss: 1.730 Epoch 1 Batch 658/1077 - Train Accuracy: 0.684, Validation Accuracy: 0.692, Loss: 1.771 Epoch 1 Batch 659/1077 - Train Accuracy: 0.737, Validation Accuracy: 0.693, Loss: 1.789 Epoch 1 Batch 660/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.695, Loss: 1.750 Epoch 1 Batch 661/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.695, Loss: 1.706 Epoch 1 Batch 662/1077 - Train Accuracy: 0.727, Validation Accuracy: 0.697, Loss: 1.745 Epoch 1 Batch 663/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.694, Loss: 1.725 Epoch 1 Batch 664/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.690, Loss: 1.809 Epoch 1 Batch 665/1077 - Train Accuracy: 0.709, Validation Accuracy: 0.691, Loss: 1.726 Epoch 1 Batch 666/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.696, Loss: 1.863 Epoch 1 Batch 667/1077 - Train Accuracy: 0.702, Validation Accuracy: 0.699, Loss: 1.703 Epoch 1 Batch 668/1077 - Train Accuracy: 0.717, Validation Accuracy: 0.713, Loss: 1.741 Epoch 1 Batch 669/1077 - Train Accuracy: 0.712, Validation Accuracy: 0.701, Loss: 1.688 Epoch 1 Batch 670/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.700, Loss: 1.698 Epoch 1 Batch 671/1077 - Train Accuracy: 0.715, Validation Accuracy: 0.696, Loss: 1.767 Epoch 1 Batch 672/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.688, Loss: 1.635 Epoch 1 Batch 673/1077 - Train Accuracy: 0.736, Validation Accuracy: 0.692, Loss: 1.731 Epoch 1 Batch 674/1077 - Train Accuracy: 0.764, Validation Accuracy: 0.709, Loss: 1.763 Epoch 1 Batch 675/1077 - Train Accuracy: 0.751, Validation Accuracy: 0.703, Loss: 1.700 Epoch 1 Batch 676/1077 - Train Accuracy: 0.756, Validation Accuracy: 0.702, Loss: 1.766 Epoch 1 Batch 677/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.700, Loss: 1.817 Epoch 1 Batch 678/1077 - Train Accuracy: 0.712, Validation Accuracy: 0.697, Loss: 1.709 Epoch 1 Batch 679/1077 - Train Accuracy: 0.731, Validation Accuracy: 0.699, Loss: 1.685 Epoch 1 Batch 680/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.696, Loss: 1.764 Epoch 1 Batch 681/1077 - Train Accuracy: 0.740, Validation Accuracy: 0.714, Loss: 1.772 Epoch 1 Batch 682/1077 - Train Accuracy: 0.737, Validation Accuracy: 0.714, Loss: 1.735 Epoch 1 Batch 683/1077 - Train Accuracy: 0.740, Validation Accuracy: 0.714, Loss: 1.646 Epoch 1 Batch 684/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.716, Loss: 1.707 Epoch 1 Batch 685/1077 - Train Accuracy: 0.725, Validation Accuracy: 0.720, Loss: 1.656 Epoch 1 Batch 686/1077 - Train Accuracy: 0.760, Validation Accuracy: 0.703, Loss: 1.791 Epoch 1 Batch 687/1077 - Train Accuracy: 0.761, Validation Accuracy: 0.707, Loss: 1.726 Epoch 1 Batch 688/1077 - Train Accuracy: 0.744, Validation Accuracy: 0.702, Loss: 1.682 Epoch 1 Batch 689/1077 - Train Accuracy: 0.719, Validation Accuracy: 0.708, Loss: 1.756 Epoch 1 Batch 690/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.716, Loss: 1.748 Epoch 1 Batch 691/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.716, Loss: 1.734 Epoch 1 Batch 692/1077 - Train Accuracy: 0.741, Validation Accuracy: 0.711, Loss: 1.783 Epoch 1 Batch 693/1077 - Train Accuracy: 0.669, Validation Accuracy: 0.704, Loss: 1.824 Epoch 1 Batch 694/1077 - Train Accuracy: 0.773, Validation Accuracy: 0.718, Loss: 1.764 Epoch 1 Batch 695/1077 - Train Accuracy: 0.740, Validation Accuracy: 0.711, Loss: 1.704 Epoch 1 Batch 696/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.705, Loss: 1.723 Epoch 1 Batch 697/1077 - Train Accuracy: 0.737, Validation Accuracy: 0.700, Loss: 1.719 Epoch 1 Batch 698/1077 - Train Accuracy: 0.747, Validation Accuracy: 0.701, Loss: 1.660 Epoch 1 Batch 699/1077 - Train Accuracy: 0.725, Validation Accuracy: 0.672, Loss: 1.787 Epoch 1 Batch 700/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.678, Loss: 1.717 Epoch 1 Batch 701/1077 - Train Accuracy: 0.730, Validation Accuracy: 0.686, Loss: 1.761 Epoch 1 Batch 702/1077 - Train Accuracy: 0.740, Validation Accuracy: 0.693, Loss: 1.687 Epoch 1 Batch 703/1077 - Train Accuracy: 0.758, Validation Accuracy: 0.706, Loss: 1.712 Epoch 1 Batch 704/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.702, Loss: 1.760 Epoch 1 Batch 705/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.696, Loss: 1.771 Epoch 1 Batch 706/1077 - Train Accuracy: 0.726, Validation Accuracy: 0.706, Loss: 1.726 Epoch 1 Batch 707/1077 - Train Accuracy: 0.752, Validation Accuracy: 0.700, Loss: 1.751 Epoch 1 Batch 708/1077 - Train Accuracy: 0.727, Validation Accuracy: 0.714, Loss: 1.702 Epoch 1 Batch 709/1077 - Train Accuracy: 0.709, Validation Accuracy: 0.713, Loss: 1.718 Epoch 1 Batch 710/1077 - Train Accuracy: 0.716, Validation Accuracy: 0.721, Loss: 1.742 Epoch 1 Batch 711/1077 - Train Accuracy: 0.707, Validation Accuracy: 0.714, Loss: 1.806 Epoch 1 Batch 712/1077 - Train Accuracy: 0.751, Validation Accuracy: 0.707, Loss: 1.684 Epoch 1 Batch 713/1077 - Train Accuracy: 0.740, Validation Accuracy: 0.703, Loss: 1.678 Epoch 1 Batch 714/1077 - Train Accuracy: 0.721, Validation Accuracy: 0.713, Loss: 1.809 Epoch 1 Batch 715/1077 - Train Accuracy: 0.694, Validation Accuracy: 0.734, Loss: 1.741 Epoch 1 Batch 716/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.723, Loss: 1.723 Epoch 1 Batch 717/1077 - Train Accuracy: 0.760, Validation Accuracy: 0.731, Loss: 1.699 Epoch 1 Batch 718/1077 - Train Accuracy: 0.721, Validation Accuracy: 0.704, Loss: 1.799 Epoch 1 Batch 719/1077 - Train Accuracy: 0.715, Validation Accuracy: 0.715, Loss: 1.715 Epoch 1 Batch 720/1077 - Train Accuracy: 0.752, Validation Accuracy: 0.718, Loss: 1.735 Epoch 1 Batch 721/1077 - Train Accuracy: 0.736, Validation Accuracy: 0.736, Loss: 1.696 Epoch 1 Batch 722/1077 - Train Accuracy: 0.727, Validation Accuracy: 0.723, Loss: 1.701 Epoch 1 Batch 723/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.712, Loss: 1.688 Epoch 1 Batch 724/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.714, Loss: 1.756 Epoch 1 Batch 725/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.734, Loss: 1.652 Epoch 1 Batch 726/1077 - Train Accuracy: 0.725, Validation Accuracy: 0.728, Loss: 1.727 Epoch 1 Batch 727/1077 - Train Accuracy: 0.753, Validation Accuracy: 0.738, Loss: 1.740 Epoch 1 Batch 728/1077 - Train Accuracy: 0.733, Validation Accuracy: 0.732, Loss: 1.693 Epoch 1 Batch 729/1077 - Train Accuracy: 0.727, Validation Accuracy: 0.713, Loss: 1.778 Epoch 1 Batch 730/1077 - Train Accuracy: 0.732, Validation Accuracy: 0.710, Loss: 1.843 Epoch 1 Batch 731/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.705, Loss: 1.698 Epoch 1 Batch 732/1077 - Train Accuracy: 0.735, Validation Accuracy: 0.703, Loss: 1.811 Epoch 1 Batch 733/1077 - Train Accuracy: 0.760, Validation Accuracy: 0.710, Loss: 1.784 Epoch 1 Batch 734/1077 - Train Accuracy: 0.724, Validation Accuracy: 0.713, Loss: 1.701 Epoch 1 Batch 735/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.711, Loss: 1.729 Epoch 1 Batch 736/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.704, Loss: 1.725 Epoch 1 Batch 737/1077 - Train Accuracy: 0.700, Validation Accuracy: 0.714, Loss: 1.699 Epoch 1 Batch 738/1077 - Train Accuracy: 0.766, Validation Accuracy: 0.711, Loss: 1.634 Epoch 1 Batch 739/1077 - Train Accuracy: 0.751, Validation Accuracy: 0.713, Loss: 1.755 Epoch 1 Batch 740/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.728, Loss: 1.750 Epoch 1 Batch 741/1077 - Train Accuracy: 0.755, Validation Accuracy: 0.730, Loss: 1.677 Epoch 1 Batch 742/1077 - Train Accuracy: 0.735, Validation Accuracy: 0.719, Loss: 1.775 Epoch 1 Batch 743/1077 - Train Accuracy: 0.722, Validation Accuracy: 0.708, Loss: 1.825 Epoch 1 Batch 744/1077 - Train Accuracy: 0.737, Validation Accuracy: 0.730, Loss: 1.663 Epoch 1 Batch 745/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.723, Loss: 1.699 Epoch 1 Batch 746/1077 - Train Accuracy: 0.758, Validation Accuracy: 0.714, Loss: 1.756 Epoch 1 Batch 747/1077 - Train Accuracy: 0.779, Validation Accuracy: 0.733, Loss: 1.677 Epoch 1 Batch 748/1077 - Train Accuracy: 0.726, Validation Accuracy: 0.734, Loss: 1.725 Epoch 1 Batch 749/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.744, Loss: 1.806 Epoch 1 Batch 750/1077 - Train Accuracy: 0.735, Validation Accuracy: 0.735, Loss: 1.720 Epoch 1 Batch 751/1077 - Train Accuracy: 0.755, Validation Accuracy: 0.741, Loss: 1.846 Epoch 1 Batch 752/1077 - Train Accuracy: 0.726, Validation Accuracy: 0.738, Loss: 1.745 Epoch 1 Batch 753/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.750, Loss: 1.689 Epoch 1 Batch 754/1077 - Train Accuracy: 0.723, Validation Accuracy: 0.739, Loss: 1.830 Epoch 1 Batch 755/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.728, Loss: 1.752 Epoch 1 Batch 756/1077 - Train Accuracy: 0.744, Validation Accuracy: 0.722, Loss: 1.724 Epoch 1 Batch 757/1077 - Train Accuracy: 0.748, Validation Accuracy: 0.707, Loss: 1.712 Epoch 1 Batch 758/1077 - Train Accuracy: 0.747, Validation Accuracy: 0.716, Loss: 1.689 Epoch 1 Batch 759/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.714, Loss: 1.689 Epoch 1 Batch 760/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.721, Loss: 1.698 Epoch 1 Batch 761/1077 - Train Accuracy: 0.694, Validation Accuracy: 0.724, Loss: 1.821 Epoch 1 Batch 762/1077 - Train Accuracy: 0.767, Validation Accuracy: 0.718, Loss: 1.683 Epoch 1 Batch 763/1077 - Train Accuracy: 0.739, Validation Accuracy: 0.723, Loss: 1.701 Epoch 1 Batch 764/1077 - Train Accuracy: 0.750, Validation Accuracy: 0.717, Loss: 1.753 Epoch 1 Batch 765/1077 - Train Accuracy: 0.759, Validation Accuracy: 0.732, Loss: 1.716 Epoch 1 Batch 766/1077 - Train Accuracy: 0.721, Validation Accuracy: 0.733, Loss: 1.704 Epoch 1 Batch 767/1077 - Train Accuracy: 0.781, Validation Accuracy: 0.743, Loss: 1.730 Epoch 1 Batch 768/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.747, Loss: 1.672 Epoch 1 Batch 769/1077 - Train Accuracy: 0.799, Validation Accuracy: 0.756, Loss: 1.664 Epoch 1 Batch 770/1077 - Train Accuracy: 0.747, Validation Accuracy: 0.750, Loss: 1.679 Epoch 1 Batch 771/1077 - Train Accuracy: 0.771, Validation Accuracy: 0.735, Loss: 1.681 Epoch 1 Batch 772/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.721, Loss: 1.692 Epoch 1 Batch 773/1077 - Train Accuracy: 0.758, Validation Accuracy: 0.725, Loss: 1.678 Epoch 1 Batch 774/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.713, Loss: 1.677 Epoch 1 Batch 775/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.730, Loss: 1.772 Epoch 1 Batch 776/1077 - Train Accuracy: 0.741, Validation Accuracy: 0.730, Loss: 1.674 Epoch 1 Batch 777/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.723, Loss: 1.758 Epoch 1 Batch 778/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.717, Loss: 1.672 Epoch 1 Batch 779/1077 - Train Accuracy: 0.736, Validation Accuracy: 0.714, Loss: 1.650 Epoch 1 Batch 780/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.739, Loss: 1.793 Epoch 1 Batch 781/1077 - Train Accuracy: 0.779, Validation Accuracy: 0.731, Loss: 1.648 Epoch 1 Batch 782/1077 - Train Accuracy: 0.772, Validation Accuracy: 0.734, Loss: 1.740 Epoch 1 Batch 783/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.733, Loss: 1.648 Epoch 1 Batch 784/1077 - Train Accuracy: 0.756, Validation Accuracy: 0.732, Loss: 1.660 Epoch 1 Batch 785/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.740, Loss: 1.580 Epoch 1 Batch 786/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.745, Loss: 1.685 Epoch 1 Batch 787/1077 - Train Accuracy: 0.769, Validation Accuracy: 0.748, Loss: 1.726 Epoch 1 Batch 788/1077 - Train Accuracy: 0.799, Validation Accuracy: 0.750, Loss: 1.677 Epoch 1 Batch 789/1077 - Train Accuracy: 0.687, Validation Accuracy: 0.748, Loss: 1.766 Epoch 1 Batch 790/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.740, Loss: 1.764 Epoch 1 Batch 791/1077 - Train Accuracy: 0.756, Validation Accuracy: 0.739, Loss: 1.743 Epoch 1 Batch 792/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.754, Loss: 1.756 Epoch 1 Batch 793/1077 - Train Accuracy: 0.769, Validation Accuracy: 0.732, Loss: 1.702 Epoch 1 Batch 794/1077 - Train Accuracy: 0.723, Validation Accuracy: 0.733, Loss: 1.729 Epoch 1 Batch 795/1077 - Train Accuracy: 0.760, Validation Accuracy: 0.737, Loss: 1.685 Epoch 1 Batch 796/1077 - Train Accuracy: 0.755, Validation Accuracy: 0.743, Loss: 1.710 Epoch 1 Batch 797/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.758, Loss: 1.695 Epoch 1 Batch 798/1077 - Train Accuracy: 0.713, Validation Accuracy: 0.754, Loss: 1.691 Epoch 1 Batch 799/1077 - Train Accuracy: 0.722, Validation Accuracy: 0.762, Loss: 1.736 Epoch 1 Batch 800/1077 - Train Accuracy: 0.776, Validation Accuracy: 0.750, Loss: 1.704 Epoch 1 Batch 801/1077 - Train Accuracy: 0.727, Validation Accuracy: 0.752, Loss: 1.694 Epoch 1 Batch 802/1077 - Train Accuracy: 0.766, Validation Accuracy: 0.753, Loss: 1.730 Epoch 1 Batch 803/1077 - Train Accuracy: 0.741, Validation Accuracy: 0.736, Loss: 1.668 Epoch 1 Batch 804/1077 - Train Accuracy: 0.766, Validation Accuracy: 0.732, Loss: 1.724 Epoch 1 Batch 805/1077 - Train Accuracy: 0.730, Validation Accuracy: 0.737, Loss: 1.737 Epoch 1 Batch 806/1077 - Train Accuracy: 0.749, Validation Accuracy: 0.739, Loss: 1.740 Epoch 1 Batch 807/1077 - Train Accuracy: 0.732, Validation Accuracy: 0.734, Loss: 1.654 Epoch 1 Batch 808/1077 - Train Accuracy: 0.750, Validation Accuracy: 0.723, Loss: 1.719 Epoch 1 Batch 809/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.727, Loss: 1.762 Epoch 1 Batch 810/1077 - Train Accuracy: 0.759, Validation Accuracy: 0.731, Loss: 1.634 Epoch 1 Batch 811/1077 - Train Accuracy: 0.788, Validation Accuracy: 0.721, Loss: 1.703 Epoch 1 Batch 812/1077 - Train Accuracy: 0.717, Validation Accuracy: 0.711, Loss: 1.678 Epoch 1 Batch 813/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.723, Loss: 1.742 Epoch 1 Batch 814/1077 - Train Accuracy: 0.734, Validation Accuracy: 0.741, Loss: 1.752 Epoch 1 Batch 815/1077 - Train Accuracy: 0.762, Validation Accuracy: 0.734, Loss: 1.695 Epoch 1 Batch 816/1077 - Train Accuracy: 0.762, Validation Accuracy: 0.735, Loss: 1.685 Epoch 1 Batch 817/1077 - Train Accuracy: 0.749, Validation Accuracy: 0.739, Loss: 1.696 Epoch 1 Batch 818/1077 - Train Accuracy: 0.719, Validation Accuracy: 0.747, Loss: 1.720 Epoch 1 Batch 819/1077 - Train Accuracy: 0.791, Validation Accuracy: 0.741, Loss: 1.723 Epoch 1 Batch 820/1077 - Train Accuracy: 0.732, Validation Accuracy: 0.730, Loss: 1.653 Epoch 1 Batch 821/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.731, Loss: 1.718 Epoch 1 Batch 822/1077 - Train Accuracy: 0.751, Validation Accuracy: 0.730, Loss: 1.718 Epoch 1 Batch 823/1077 - Train Accuracy: 0.757, Validation Accuracy: 0.735, Loss: 1.621 Epoch 1 Batch 824/1077 - Train Accuracy: 0.764, Validation Accuracy: 0.748, Loss: 1.694 Epoch 1 Batch 825/1077 - Train Accuracy: 0.764, Validation Accuracy: 0.749, Loss: 1.725 Epoch 1 Batch 826/1077 - Train Accuracy: 0.760, Validation Accuracy: 0.747, Loss: 1.664 Epoch 1 Batch 827/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.740, Loss: 1.646 Epoch 1 Batch 828/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.747, Loss: 1.697 Epoch 1 Batch 829/1077 - Train Accuracy: 0.748, Validation Accuracy: 0.748, Loss: 1.680 Epoch 1 Batch 830/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.738, Loss: 1.708 Epoch 1 Batch 831/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.739, Loss: 1.744 Epoch 1 Batch 832/1077 - Train Accuracy: 0.775, Validation Accuracy: 0.729, Loss: 1.712 Epoch 1 Batch 833/1077 - Train Accuracy: 0.735, Validation Accuracy: 0.729, Loss: 1.690 Epoch 1 Batch 834/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.732, Loss: 1.650 Epoch 1 Batch 835/1077 - Train Accuracy: 0.781, Validation Accuracy: 0.725, Loss: 1.721 Epoch 1 Batch 836/1077 - Train Accuracy: 0.759, Validation Accuracy: 0.719, Loss: 1.665 Epoch 1 Batch 837/1077 - Train Accuracy: 0.744, Validation Accuracy: 0.731, Loss: 1.665 Epoch 1 Batch 838/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.734, Loss: 1.686 Epoch 1 Batch 839/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.725, Loss: 1.723 Epoch 1 Batch 840/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.722, Loss: 1.722 Epoch 1 Batch 841/1077 - Train Accuracy: 0.788, Validation Accuracy: 0.723, Loss: 1.694 Epoch 1 Batch 842/1077 - Train Accuracy: 0.795, Validation Accuracy: 0.728, Loss: 1.700 Epoch 1 Batch 843/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.731, Loss: 1.719 Epoch 1 Batch 844/1077 - Train Accuracy: 0.763, Validation Accuracy: 0.743, Loss: 1.731 Epoch 1 Batch 845/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.734, Loss: 1.728 Epoch 1 Batch 846/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.732, Loss: 1.655 Epoch 1 Batch 847/1077 - Train Accuracy: 0.781, Validation Accuracy: 0.748, Loss: 1.712 Epoch 1 Batch 848/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.740, Loss: 1.635 Epoch 1 Batch 849/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.724, Loss: 1.725 Epoch 1 Batch 850/1077 - Train Accuracy: 0.762, Validation Accuracy: 0.730, Loss: 1.804 Epoch 1 Batch 851/1077 - Train Accuracy: 0.791, Validation Accuracy: 0.733, Loss: 1.696 Epoch 1 Batch 852/1077 - Train Accuracy: 0.752, Validation Accuracy: 0.745, Loss: 1.718 Epoch 1 Batch 853/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.746, Loss: 1.717 Epoch 1 Batch 854/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.748, Loss: 1.691 Epoch 1 Batch 855/1077 - Train Accuracy: 0.727, Validation Accuracy: 0.740, Loss: 1.658 Epoch 1 Batch 856/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.748, Loss: 1.708 Epoch 1 Batch 857/1077 - Train Accuracy: 0.787, Validation Accuracy: 0.742, Loss: 1.707 Epoch 1 Batch 858/1077 - Train Accuracy: 0.757, Validation Accuracy: 0.743, Loss: 1.741 Epoch 1 Batch 859/1077 - Train Accuracy: 0.769, Validation Accuracy: 0.747, Loss: 1.742 Epoch 1 Batch 860/1077 - Train Accuracy: 0.749, Validation Accuracy: 0.745, Loss: 1.740 Epoch 1 Batch 861/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.734, Loss: 1.621 Epoch 1 Batch 862/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.740, Loss: 1.691 Epoch 1 Batch 863/1077 - Train Accuracy: 0.750, Validation Accuracy: 0.736, Loss: 1.716 Epoch 1 Batch 864/1077 - Train Accuracy: 0.752, Validation Accuracy: 0.743, Loss: 1.691 Epoch 1 Batch 865/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.749, Loss: 1.637 Epoch 1 Batch 866/1077 - Train Accuracy: 0.794, Validation Accuracy: 0.752, Loss: 1.770 Epoch 1 Batch 867/1077 - Train Accuracy: 0.731, Validation Accuracy: 0.748, Loss: 1.751 Epoch 1 Batch 868/1077 - Train Accuracy: 0.803, Validation Accuracy: 0.746, Loss: 1.688 Epoch 1 Batch 869/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.746, Loss: 1.679 Epoch 1 Batch 870/1077 - Train Accuracy: 0.747, Validation Accuracy: 0.775, Loss: 1.717 Epoch 1 Batch 871/1077 - Train Accuracy: 0.759, Validation Accuracy: 0.778, Loss: 1.729 Epoch 1 Batch 872/1077 - Train Accuracy: 0.786, Validation Accuracy: 0.782, Loss: 1.654 Epoch 1 Batch 873/1077 - Train Accuracy: 0.763, Validation Accuracy: 0.793, Loss: 1.675 Epoch 1 Batch 874/1077 - Train Accuracy: 0.752, Validation Accuracy: 0.792, Loss: 1.659 Epoch 1 Batch 875/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.773, Loss: 1.701 Epoch 1 Batch 876/1077 - Train Accuracy: 0.775, Validation Accuracy: 0.766, Loss: 1.716 Epoch 1 Batch 877/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.776, Loss: 1.608 Epoch 1 Batch 878/1077 - Train Accuracy: 0.808, Validation Accuracy: 0.757, Loss: 1.722 Epoch 1 Batch 879/1077 - Train Accuracy: 0.800, Validation Accuracy: 0.740, Loss: 1.582 Epoch 1 Batch 880/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.744, Loss: 1.724 Epoch 1 Batch 881/1077 - Train Accuracy: 0.727, Validation Accuracy: 0.746, Loss: 1.654 Epoch 1 Batch 882/1077 - Train Accuracy: 0.786, Validation Accuracy: 0.744, Loss: 1.741 Epoch 1 Batch 883/1077 - Train Accuracy: 0.698, Validation Accuracy: 0.751, Loss: 1.709 Epoch 1 Batch 884/1077 - Train Accuracy: 0.767, Validation Accuracy: 0.742, Loss: 1.715 Epoch 1 Batch 885/1077 - Train Accuracy: 0.809, Validation Accuracy: 0.747, Loss: 1.647 Epoch 1 Batch 886/1077 - Train Accuracy: 0.760, Validation Accuracy: 0.744, Loss: 1.739 Epoch 1 Batch 887/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.727, Loss: 1.639 Epoch 1 Batch 888/1077 - Train Accuracy: 0.772, Validation Accuracy: 0.731, Loss: 1.718 Epoch 1 Batch 889/1077 - Train Accuracy: 0.772, Validation Accuracy: 0.732, Loss: 1.699 Epoch 1 Batch 890/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.738, Loss: 1.634 Epoch 1 Batch 891/1077 - Train Accuracy: 0.795, Validation Accuracy: 0.749, Loss: 1.662 Epoch 1 Batch 892/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.746, Loss: 1.685 Epoch 1 Batch 893/1077 - Train Accuracy: 0.774, Validation Accuracy: 0.746, Loss: 1.631 Epoch 1 Batch 894/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.764, Loss: 1.678 Epoch 1 Batch 895/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.758, Loss: 1.623 Epoch 1 Batch 896/1077 - Train Accuracy: 0.731, Validation Accuracy: 0.767, Loss: 1.628 Epoch 1 Batch 897/1077 - Train Accuracy: 0.795, Validation Accuracy: 0.771, Loss: 1.636 Epoch 1 Batch 898/1077 - Train Accuracy: 0.788, Validation Accuracy: 0.752, Loss: 1.681 Epoch 1 Batch 899/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.755, Loss: 1.649 Epoch 1 Batch 900/1077 - Train Accuracy: 0.775, Validation Accuracy: 0.750, Loss: 1.660 Epoch 1 Batch 901/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.756, Loss: 1.688 Epoch 1 Batch 902/1077 - Train Accuracy: 0.786, Validation Accuracy: 0.752, Loss: 1.710 Epoch 1 Batch 903/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.767, Loss: 1.707 Epoch 1 Batch 904/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.759, Loss: 1.657 Epoch 1 Batch 905/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.753, Loss: 1.600 Epoch 1 Batch 906/1077 - Train Accuracy: 0.791, Validation Accuracy: 0.756, Loss: 1.718 Epoch 1 Batch 907/1077 - Train Accuracy: 0.803, Validation Accuracy: 0.751, Loss: 1.694 Epoch 1 Batch 908/1077 - Train Accuracy: 0.777, Validation Accuracy: 0.752, Loss: 1.738 Epoch 1 Batch 909/1077 - Train Accuracy: 0.774, Validation Accuracy: 0.749, Loss: 1.706 Epoch 1 Batch 910/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.750, Loss: 1.609 Epoch 1 Batch 911/1077 - Train Accuracy: 0.813, Validation Accuracy: 0.735, Loss: 1.661 Epoch 1 Batch 912/1077 - Train Accuracy: 0.761, Validation Accuracy: 0.743, Loss: 1.703 Epoch 1 Batch 913/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.751, Loss: 1.679 Epoch 1 Batch 914/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.746, Loss: 1.635 Epoch 1 Batch 915/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.757, Loss: 1.654 Epoch 1 Batch 916/1077 - Train Accuracy: 0.774, Validation Accuracy: 0.763, Loss: 1.690 Epoch 1 Batch 917/1077 - Train Accuracy: 0.773, Validation Accuracy: 0.785, Loss: 1.578 Epoch 1 Batch 918/1077 - Train Accuracy: 0.795, Validation Accuracy: 0.773, Loss: 1.652 Epoch 1 Batch 919/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.786, Loss: 1.667 Epoch 1 Batch 920/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.784, Loss: 1.659 Epoch 1 Batch 921/1077 - Train Accuracy: 0.771, Validation Accuracy: 0.770, Loss: 1.665 Epoch 1 Batch 922/1077 - Train Accuracy: 0.737, Validation Accuracy: 0.774, Loss: 1.634 Epoch 1 Batch 923/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.775, Loss: 1.601 Epoch 1 Batch 924/1077 - Train Accuracy: 0.767, Validation Accuracy: 0.763, Loss: 1.638 Epoch 1 Batch 925/1077 - Train Accuracy: 0.753, Validation Accuracy: 0.770, Loss: 1.641 Epoch 1 Batch 926/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.756, Loss: 1.594 Epoch 1 Batch 927/1077 - Train Accuracy: 0.753, Validation Accuracy: 0.757, Loss: 1.730 Epoch 1 Batch 928/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.759, Loss: 1.662 Epoch 1 Batch 929/1077 - Train Accuracy: 0.748, Validation Accuracy: 0.758, Loss: 1.660 Epoch 1 Batch 930/1077 - Train Accuracy: 0.763, Validation Accuracy: 0.755, Loss: 1.659 Epoch 1 Batch 931/1077 - Train Accuracy: 0.769, Validation Accuracy: 0.758, Loss: 1.728 Epoch 1 Batch 932/1077 - Train Accuracy: 0.744, Validation Accuracy: 0.755, Loss: 1.684 Epoch 1 Batch 933/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.750, Loss: 1.676 Epoch 1 Batch 934/1077 - Train Accuracy: 0.771, Validation Accuracy: 0.740, Loss: 1.646 Epoch 1 Batch 935/1077 - Train Accuracy: 0.786, Validation Accuracy: 0.737, Loss: 1.697 Epoch 1 Batch 936/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.750, Loss: 1.681 Epoch 1 Batch 937/1077 - Train Accuracy: 0.807, Validation Accuracy: 0.766, Loss: 1.737 Epoch 1 Batch 938/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.760, Loss: 1.674 Epoch 1 Batch 939/1077 - Train Accuracy: 0.773, Validation Accuracy: 0.771, Loss: 1.671 Epoch 1 Batch 940/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.759, Loss: 1.608 Epoch 1 Batch 941/1077 - Train Accuracy: 0.778, Validation Accuracy: 0.745, Loss: 1.662 Epoch 1 Batch 942/1077 - Train Accuracy: 0.779, Validation Accuracy: 0.759, Loss: 1.678 Epoch 1 Batch 943/1077 - Train Accuracy: 0.764, Validation Accuracy: 0.756, Loss: 1.681 Epoch 1 Batch 944/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.780, Loss: 1.666 Epoch 1 Batch 945/1077 - Train Accuracy: 0.821, Validation Accuracy: 0.773, Loss: 1.623 Epoch 1 Batch 946/1077 - Train Accuracy: 0.778, Validation Accuracy: 0.776, Loss: 1.696 Epoch 1 Batch 947/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.768, Loss: 1.698 Epoch 1 Batch 948/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.769, Loss: 1.795 Epoch 1 Batch 949/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.761, Loss: 1.712 Epoch 1 Batch 950/1077 - Train Accuracy: 0.795, Validation Accuracy: 0.773, Loss: 1.667 Epoch 1 Batch 951/1077 - Train Accuracy: 0.775, Validation Accuracy: 0.762, Loss: 1.660 Epoch 1 Batch 952/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.768, Loss: 1.642 Epoch 1 Batch 953/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.756, Loss: 1.655 Epoch 1 Batch 954/1077 - Train Accuracy: 0.716, Validation Accuracy: 0.755, Loss: 1.746 Epoch 1 Batch 955/1077 - Train Accuracy: 0.735, Validation Accuracy: 0.757, Loss: 1.640 Epoch 1 Batch 956/1077 - Train Accuracy: 0.793, Validation Accuracy: 0.772, Loss: 1.638 Epoch 1 Batch 957/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.772, Loss: 1.662 Epoch 1 Batch 958/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.776, Loss: 1.658 Epoch 1 Batch 959/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.773, Loss: 1.638 Epoch 1 Batch 960/1077 - Train Accuracy: 0.797, Validation Accuracy: 0.767, Loss: 1.708 Epoch 1 Batch 961/1077 - Train Accuracy: 0.777, Validation Accuracy: 0.776, Loss: 1.613 Epoch 1 Batch 962/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.786, Loss: 1.650 Epoch 1 Batch 963/1077 - Train Accuracy: 0.794, Validation Accuracy: 0.783, Loss: 1.715 Epoch 1 Batch 964/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.779, Loss: 1.659 Epoch 1 Batch 965/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.781, Loss: 1.640 Epoch 1 Batch 966/1077 - Train Accuracy: 0.813, Validation Accuracy: 0.784, Loss: 1.676 Epoch 1 Batch 967/1077 - Train Accuracy: 0.795, Validation Accuracy: 0.767, Loss: 1.656 Epoch 1 Batch 968/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.773, Loss: 1.731 Epoch 1 Batch 969/1077 - Train Accuracy: 0.785, Validation Accuracy: 0.764, Loss: 1.650 Epoch 1 Batch 970/1077 - Train Accuracy: 0.786, Validation Accuracy: 0.770, Loss: 1.709 Epoch 1 Batch 971/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.754, Loss: 1.729 Epoch 1 Batch 972/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.760, Loss: 1.661 Epoch 1 Batch 973/1077 - Train Accuracy: 0.809, Validation Accuracy: 0.765, Loss: 1.597 Epoch 1 Batch 974/1077 - Train Accuracy: 0.756, Validation Accuracy: 0.784, Loss: 1.736 Epoch 1 Batch 975/1077 - Train Accuracy: 0.769, Validation Accuracy: 0.785, Loss: 1.671 Epoch 1 Batch 976/1077 - Train Accuracy: 0.769, Validation Accuracy: 0.777, Loss: 1.616 Epoch 1 Batch 977/1077 - Train Accuracy: 0.796, Validation Accuracy: 0.772, Loss: 1.670 Epoch 1 Batch 978/1077 - Train Accuracy: 0.822, Validation Accuracy: 0.783, Loss: 1.719 Epoch 1 Batch 979/1077 - Train Accuracy: 0.769, Validation Accuracy: 0.791, Loss: 1.682 Epoch 1 Batch 980/1077 - Train Accuracy: 0.781, Validation Accuracy: 0.792, Loss: 1.702 Epoch 1 Batch 981/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.807, Loss: 1.705 Epoch 1 Batch 982/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.800, Loss: 1.662 Epoch 1 Batch 983/1077 - Train Accuracy: 0.756, Validation Accuracy: 0.801, Loss: 1.736 Epoch 1 Batch 984/1077 - Train Accuracy: 0.748, Validation Accuracy: 0.812, Loss: 1.651 Epoch 1 Batch 985/1077 - Train Accuracy: 0.810, Validation Accuracy: 0.808, Loss: 1.619 Epoch 1 Batch 986/1077 - Train Accuracy: 0.766, Validation Accuracy: 0.803, Loss: 1.611 Epoch 1 Batch 987/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.782, Loss: 1.657 Epoch 1 Batch 988/1077 - Train Accuracy: 0.779, Validation Accuracy: 0.784, Loss: 1.708 Epoch 1 Batch 989/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.773, Loss: 1.681 Epoch 1 Batch 990/1077 - Train Accuracy: 0.761, Validation Accuracy: 0.776, Loss: 1.668 Epoch 1 Batch 991/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.769, Loss: 1.648 Epoch 1 Batch 992/1077 - Train Accuracy: 0.793, Validation Accuracy: 0.762, Loss: 1.606 Epoch 1 Batch 993/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.765, Loss: 1.608 Epoch 1 Batch 994/1077 - Train Accuracy: 0.779, Validation Accuracy: 0.773, Loss: 1.636 Epoch 1 Batch 995/1077 - Train Accuracy: 0.807, Validation Accuracy: 0.772, Loss: 1.606 Epoch 1 Batch 996/1077 - Train Accuracy: 0.809, Validation Accuracy: 0.781, Loss: 1.676 Epoch 1 Batch 997/1077 - Train Accuracy: 0.763, Validation Accuracy: 0.783, Loss: 1.713 Epoch 1 Batch 998/1077 - Train Accuracy: 0.742, Validation Accuracy: 0.788, Loss: 1.704 Epoch 1 Batch 999/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.795, Loss: 1.638 Epoch 1 Batch 1000/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.784, Loss: 1.631 Epoch 1 Batch 1001/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.788, Loss: 1.628 Epoch 1 Batch 1002/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.784, Loss: 1.665 Epoch 1 Batch 1003/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.769, Loss: 1.660 Epoch 1 Batch 1004/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.773, Loss: 1.670 Epoch 1 Batch 1005/1077 - Train Accuracy: 0.797, Validation Accuracy: 0.788, Loss: 1.659 Epoch 1 Batch 1006/1077 - Train Accuracy: 0.809, Validation Accuracy: 0.784, Loss: 1.612 Epoch 1 Batch 1007/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.778, Loss: 1.628 Epoch 1 Batch 1008/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.772, Loss: 1.683 Epoch 1 Batch 1009/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.771, Loss: 1.617 Epoch 1 Batch 1010/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.785, Loss: 1.681 Epoch 1 Batch 1011/1077 - Train Accuracy: 0.771, Validation Accuracy: 0.782, Loss: 1.644 Epoch 1 Batch 1012/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.779, Loss: 1.618 Epoch 1 Batch 1013/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.780, Loss: 1.617 Epoch 1 Batch 1014/1077 - Train Accuracy: 0.772, Validation Accuracy: 0.778, Loss: 1.750 Epoch 1 Batch 1015/1077 - Train Accuracy: 0.734, Validation Accuracy: 0.779, Loss: 1.663 Epoch 1 Batch 1016/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.782, Loss: 1.675 Epoch 1 Batch 1017/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.784, Loss: 1.696 Epoch 1 Batch 1018/1077 - Train Accuracy: 0.747, Validation Accuracy: 0.780, Loss: 1.672 Epoch 1 Batch 1019/1077 - Train Accuracy: 0.749, Validation Accuracy: 0.783, Loss: 1.680 Epoch 1 Batch 1020/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.788, Loss: 1.694 Epoch 1 Batch 1021/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.804, Loss: 1.681 Epoch 1 Batch 1022/1077 - Train Accuracy: 0.815, Validation Accuracy: 0.800, Loss: 1.636 Epoch 1 Batch 1023/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.805, Loss: 1.675 Epoch 1 Batch 1024/1077 - Train Accuracy: 0.796, Validation Accuracy: 0.792, Loss: 1.615 Epoch 1 Batch 1025/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.794, Loss: 1.674 Epoch 1 Batch 1026/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.796, Loss: 1.667 Epoch 1 Batch 1027/1077 - Train Accuracy: 0.753, Validation Accuracy: 0.792, Loss: 1.714 Epoch 1 Batch 1028/1077 - Train Accuracy: 0.749, Validation Accuracy: 0.799, Loss: 1.687 Epoch 1 Batch 1029/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.802, Loss: 1.647 Epoch 1 Batch 1030/1077 - Train Accuracy: 0.759, Validation Accuracy: 0.783, Loss: 1.735 Epoch 1 Batch 1031/1077 - Train Accuracy: 0.763, Validation Accuracy: 0.778, Loss: 1.674 Epoch 1 Batch 1032/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.773, Loss: 1.719 Epoch 1 Batch 1033/1077 - Train Accuracy: 0.774, Validation Accuracy: 0.769, Loss: 1.609 Epoch 1 Batch 1034/1077 - Train Accuracy: 0.766, Validation Accuracy: 0.771, Loss: 1.640 Epoch 1 Batch 1035/1077 - Train Accuracy: 0.822, Validation Accuracy: 0.763, Loss: 1.698 Epoch 1 Batch 1036/1077 - Train Accuracy: 0.763, Validation Accuracy: 0.752, Loss: 1.627 Epoch 1 Batch 1037/1077 - Train Accuracy: 0.778, Validation Accuracy: 0.763, Loss: 1.646 Epoch 1 Batch 1038/1077 - Train Accuracy: 0.813, Validation Accuracy: 0.776, Loss: 1.633 Epoch 1 Batch 1039/1077 - Train Accuracy: 0.785, Validation Accuracy: 0.783, Loss: 1.695 Epoch 1 Batch 1040/1077 - Train Accuracy: 0.784, Validation Accuracy: 0.798, Loss: 1.655 Epoch 1 Batch 1041/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.801, Loss: 1.625 Epoch 1 Batch 1042/1077 - Train Accuracy: 0.793, Validation Accuracy: 0.801, Loss: 1.638 Epoch 1 Batch 1043/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.809, Loss: 1.767 Epoch 1 Batch 1044/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.799, Loss: 1.670 Epoch 1 Batch 1045/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.793, Loss: 1.748 Epoch 1 Batch 1046/1077 - Train Accuracy: 0.800, Validation Accuracy: 0.787, Loss: 1.655 Epoch 1 Batch 1047/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.784, Loss: 1.676 Epoch 1 Batch 1048/1077 - Train Accuracy: 0.766, Validation Accuracy: 0.784, Loss: 1.645 Epoch 1 Batch 1049/1077 - Train Accuracy: 0.778, Validation Accuracy: 0.797, Loss: 1.607 Epoch 1 Batch 1050/1077 - Train Accuracy: 0.778, Validation Accuracy: 0.801, Loss: 1.665 Epoch 1 Batch 1051/1077 - Train Accuracy: 0.815, Validation Accuracy: 0.798, Loss: 1.645 Epoch 1 Batch 1052/1077 - Train Accuracy: 0.826, Validation Accuracy: 0.784, Loss: 1.638 Epoch 1 Batch 1053/1077 - Train Accuracy: 0.813, Validation Accuracy: 0.797, Loss: 1.593 Epoch 1 Batch 1054/1077 - Train Accuracy: 0.799, Validation Accuracy: 0.793, Loss: 1.612 Epoch 1 Batch 1055/1077 - Train Accuracy: 0.803, Validation Accuracy: 0.796, Loss: 1.673 Epoch 1 Batch 1056/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.784, Loss: 1.681 Epoch 1 Batch 1057/1077 - Train Accuracy: 0.791, Validation Accuracy: 0.785, Loss: 1.618 Epoch 1 Batch 1058/1077 - Train Accuracy: 0.796, Validation Accuracy: 0.793, Loss: 1.680 Epoch 1 Batch 1059/1077 - Train Accuracy: 0.771, Validation Accuracy: 0.784, Loss: 1.668 Epoch 1 Batch 1060/1077 - Train Accuracy: 0.810, Validation Accuracy: 0.786, Loss: 1.617 Epoch 1 Batch 1061/1077 - Train Accuracy: 0.742, Validation Accuracy: 0.776, Loss: 1.578 Epoch 1 Batch 1062/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.782, Loss: 1.663 Epoch 1 Batch 1063/1077 - Train Accuracy: 0.762, Validation Accuracy: 0.787, Loss: 1.630 Epoch 1 Batch 1064/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.798, Loss: 1.670 Epoch 1 Batch 1065/1077 - Train Accuracy: 0.820, Validation Accuracy: 0.800, Loss: 1.655 Epoch 1 Batch 1066/1077 - Train Accuracy: 0.807, Validation Accuracy: 0.800, Loss: 1.767 Epoch 1 Batch 1067/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.799, Loss: 1.725 Epoch 1 Batch 1068/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.798, Loss: 1.678 Epoch 1 Batch 1069/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.801, Loss: 1.662 Epoch 1 Batch 1070/1077 - Train Accuracy: 0.752, Validation Accuracy: 0.797, Loss: 1.674 Epoch 1 Batch 1071/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.797, Loss: 1.662 Epoch 1 Batch 1072/1077 - Train Accuracy: 0.793, Validation Accuracy: 0.796, Loss: 1.611 Epoch 1 Batch 1073/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.795, Loss: 1.641 Epoch 1 Batch 1074/1077 - Train Accuracy: 0.810, Validation Accuracy: 0.783, Loss: 1.622 Epoch 1 Batch 1075/1077 - Train Accuracy: 0.785, Validation Accuracy: 0.778, Loss: 1.662 Epoch 2 Batch 0/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.791, Loss: 1.564 Epoch 2 Batch 1/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.790, Loss: 1.646 Epoch 2 Batch 2/1077 - Train Accuracy: 0.762, Validation Accuracy: 0.794, Loss: 1.639 Epoch 2 Batch 3/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.796, Loss: 1.661 Epoch 2 Batch 4/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.794, Loss: 1.712 Epoch 2 Batch 5/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.790, Loss: 1.669 Epoch 2 Batch 6/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.800, Loss: 1.584 Epoch 2 Batch 7/1077 - Train Accuracy: 0.813, Validation Accuracy: 0.811, Loss: 1.666 Epoch 2 Batch 8/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.805, Loss: 1.697 Epoch 2 Batch 9/1077 - Train Accuracy: 0.807, Validation Accuracy: 0.790, Loss: 1.630 Epoch 2 Batch 10/1077 - Train Accuracy: 0.803, Validation Accuracy: 0.778, Loss: 1.651 Epoch 2 Batch 11/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.790, Loss: 1.623 Epoch 2 Batch 12/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.809, Loss: 1.645 Epoch 2 Batch 13/1077 - Train Accuracy: 0.800, Validation Accuracy: 0.808, Loss: 1.551 Epoch 2 Batch 14/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.823, Loss: 1.581 Epoch 2 Batch 15/1077 - Train Accuracy: 0.820, Validation Accuracy: 0.818, Loss: 1.646 Epoch 2 Batch 16/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.805, Loss: 1.601 Epoch 2 Batch 17/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.791, Loss: 1.615 Epoch 2 Batch 18/1077 - Train Accuracy: 0.775, Validation Accuracy: 0.796, Loss: 1.686 Epoch 2 Batch 19/1077 - Train Accuracy: 0.806, Validation Accuracy: 0.795, Loss: 1.624 Epoch 2 Batch 20/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.791, Loss: 1.549 Epoch 2 Batch 21/1077 - Train Accuracy: 0.755, Validation Accuracy: 0.789, Loss: 1.598 Epoch 2 Batch 22/1077 - Train Accuracy: 0.803, Validation Accuracy: 0.788, Loss: 1.563 Epoch 2 Batch 23/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.785, Loss: 1.688 Epoch 2 Batch 24/1077 - Train Accuracy: 0.819, Validation Accuracy: 0.794, Loss: 1.656 Epoch 2 Batch 25/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.771, Loss: 1.584 Epoch 2 Batch 26/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.778, Loss: 1.731 Epoch 2 Batch 27/1077 - Train Accuracy: 0.826, Validation Accuracy: 0.775, Loss: 1.651 Epoch 2 Batch 28/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.787, Loss: 1.618 Epoch 2 Batch 29/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.798, Loss: 1.636 Epoch 2 Batch 30/1077 - Train Accuracy: 0.793, Validation Accuracy: 0.788, Loss: 1.620 Epoch 2 Batch 31/1077 - Train Accuracy: 0.781, Validation Accuracy: 0.790, Loss: 1.609 Epoch 2 Batch 32/1077 - Train Accuracy: 0.796, Validation Accuracy: 0.792, Loss: 1.683 Epoch 2 Batch 33/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.793, Loss: 1.572 Epoch 2 Batch 34/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.802, Loss: 1.625 Epoch 2 Batch 35/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.793, Loss: 1.573 Epoch 2 Batch 36/1077 - Train Accuracy: 0.773, Validation Accuracy: 0.800, Loss: 1.655 Epoch 2 Batch 37/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.807, Loss: 1.633 Epoch 2 Batch 38/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.817, Loss: 1.738 Epoch 2 Batch 39/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.829, Loss: 1.645 Epoch 2 Batch 40/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.836, Loss: 1.692 Epoch 2 Batch 41/1077 - Train Accuracy: 0.807, Validation Accuracy: 0.835, Loss: 1.653 Epoch 2 Batch 42/1077 - Train Accuracy: 0.795, Validation Accuracy: 0.825, Loss: 1.593 Epoch 2 Batch 43/1077 - Train Accuracy: 0.837, Validation Accuracy: 0.821, Loss: 1.639 Epoch 2 Batch 44/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.815, Loss: 1.764 Epoch 2 Batch 45/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.816, Loss: 1.659 Epoch 2 Batch 46/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.811, Loss: 1.668 Epoch 2 Batch 47/1077 - Train Accuracy: 0.834, Validation Accuracy: 0.805, Loss: 1.683 Epoch 2 Batch 48/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.806, Loss: 1.684 Epoch 2 Batch 49/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.809, Loss: 1.621 Epoch 2 Batch 50/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.809, Loss: 1.639 Epoch 2 Batch 51/1077 - Train Accuracy: 0.806, Validation Accuracy: 0.805, Loss: 1.560 Epoch 2 Batch 52/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.814, Loss: 1.589 Epoch 2 Batch 53/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.814, Loss: 1.612 Epoch 2 Batch 54/1077 - Train Accuracy: 0.757, Validation Accuracy: 0.806, Loss: 1.631 Epoch 2 Batch 55/1077 - Train Accuracy: 0.797, Validation Accuracy: 0.817, Loss: 1.587 Epoch 2 Batch 56/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.815, Loss: 1.672 Epoch 2 Batch 57/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.797, Loss: 1.729 Epoch 2 Batch 58/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.788, Loss: 1.625 Epoch 2 Batch 59/1077 - Train Accuracy: 0.766, Validation Accuracy: 0.803, Loss: 1.566 Epoch 2 Batch 60/1077 - Train Accuracy: 0.796, Validation Accuracy: 0.802, Loss: 1.650 Epoch 2 Batch 61/1077 - Train Accuracy: 0.788, Validation Accuracy: 0.799, Loss: 1.662 Epoch 2 Batch 62/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.811, Loss: 1.656 Epoch 2 Batch 63/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.815, Loss: 1.654 Epoch 2 Batch 64/1077 - Train Accuracy: 0.814, Validation Accuracy: 0.773, Loss: 1.684 Epoch 2 Batch 65/1077 - Train Accuracy: 0.788, Validation Accuracy: 0.759, Loss: 1.601 Epoch 2 Batch 66/1077 - Train Accuracy: 0.786, Validation Accuracy: 0.756, Loss: 1.586 Epoch 2 Batch 67/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.774, Loss: 1.680 Epoch 2 Batch 68/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.778, Loss: 1.633 Epoch 2 Batch 69/1077 - Train Accuracy: 0.803, Validation Accuracy: 0.781, Loss: 1.688 Epoch 2 Batch 70/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.788, Loss: 1.657 Epoch 2 Batch 71/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.794, Loss: 1.552 Epoch 2 Batch 72/1077 - Train Accuracy: 0.797, Validation Accuracy: 0.782, Loss: 1.642 Epoch 2 Batch 73/1077 - Train Accuracy: 0.832, Validation Accuracy: 0.776, Loss: 1.679 Epoch 2 Batch 74/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.776, Loss: 1.545 Epoch 2 Batch 75/1077 - Train Accuracy: 0.799, Validation Accuracy: 0.782, Loss: 1.635 Epoch 2 Batch 76/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.805, Loss: 1.598 Epoch 2 Batch 77/1077 - Train Accuracy: 0.791, Validation Accuracy: 0.799, Loss: 1.697 Epoch 2 Batch 78/1077 - Train Accuracy: 0.810, Validation Accuracy: 0.807, Loss: 1.721 Epoch 2 Batch 79/1077 - Train Accuracy: 0.837, Validation Accuracy: 0.807, Loss: 1.584 Epoch 2 Batch 80/1077 - Train Accuracy: 0.790, Validation Accuracy: 0.800, Loss: 1.619 Epoch 2 Batch 81/1077 - Train Accuracy: 0.821, Validation Accuracy: 0.793, Loss: 1.610 Epoch 2 Batch 82/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.800, Loss: 1.653 Epoch 2 Batch 83/1077 - Train Accuracy: 0.803, Validation Accuracy: 0.805, Loss: 1.568 Epoch 2 Batch 84/1077 - Train Accuracy: 0.806, Validation Accuracy: 0.812, Loss: 1.642 Epoch 2 Batch 85/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.814, Loss: 1.618 Epoch 2 Batch 86/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.827, Loss: 1.653 Epoch 2 Batch 87/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.818, Loss: 1.622 Epoch 2 Batch 88/1077 - Train Accuracy: 0.827, Validation Accuracy: 0.820, Loss: 1.590 Epoch 2 Batch 89/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.815, Loss: 1.651 Epoch 2 Batch 90/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.816, Loss: 1.605 Epoch 2 Batch 91/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.816, Loss: 1.651 Epoch 2 Batch 92/1077 - Train Accuracy: 0.817, Validation Accuracy: 0.810, Loss: 1.603 Epoch 2 Batch 93/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.808, Loss: 1.634 Epoch 2 Batch 94/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.822, Loss: 1.589 Epoch 2 Batch 95/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.834, Loss: 1.647 Epoch 2 Batch 96/1077 - Train Accuracy: 0.808, Validation Accuracy: 0.833, Loss: 1.670 Epoch 2 Batch 97/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.834, Loss: 1.595 Epoch 2 Batch 98/1077 - Train Accuracy: 0.826, Validation Accuracy: 0.826, Loss: 1.680 Epoch 2 Batch 99/1077 - Train Accuracy: 0.809, Validation Accuracy: 0.822, Loss: 1.619 Epoch 2 Batch 100/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.812, Loss: 1.669 Epoch 2 Batch 101/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.809, Loss: 1.590 Epoch 2 Batch 102/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.798, Loss: 1.705 Epoch 2 Batch 103/1077 - Train Accuracy: 0.777, Validation Accuracy: 0.801, Loss: 1.637 Epoch 2 Batch 104/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.777, Loss: 1.725 Epoch 2 Batch 105/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.756, Loss: 1.570 Epoch 2 Batch 106/1077 - Train Accuracy: 0.799, Validation Accuracy: 0.756, Loss: 1.635 Epoch 2 Batch 107/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.759, Loss: 1.494 Epoch 2 Batch 108/1077 - Train Accuracy: 0.826, Validation Accuracy: 0.789, Loss: 1.629 Epoch 2 Batch 109/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.801, Loss: 1.626 Epoch 2 Batch 110/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.816, Loss: 1.621 Epoch 2 Batch 111/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.814, Loss: 1.567 Epoch 2 Batch 112/1077 - Train Accuracy: 0.806, Validation Accuracy: 0.810, Loss: 1.600 Epoch 2 Batch 113/1077 - Train Accuracy: 0.791, Validation Accuracy: 0.809, Loss: 1.655 Epoch 2 Batch 114/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.810, Loss: 1.542 Epoch 2 Batch 115/1077 - Train Accuracy: 0.793, Validation Accuracy: 0.814, Loss: 1.681 Epoch 2 Batch 116/1077 - Train Accuracy: 0.773, Validation Accuracy: 0.822, Loss: 1.661 Epoch 2 Batch 117/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.833, Loss: 1.656 Epoch 2 Batch 118/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.836, Loss: 1.651 Epoch 2 Batch 119/1077 - Train Accuracy: 0.813, Validation Accuracy: 0.834, Loss: 1.596 Epoch 2 Batch 120/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.828, Loss: 1.692 Epoch 2 Batch 121/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.829, Loss: 1.595 Epoch 2 Batch 122/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.835, Loss: 1.652 Epoch 2 Batch 123/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.839, Loss: 1.605 Epoch 2 Batch 124/1077 - Train Accuracy: 0.844, Validation Accuracy: 0.846, Loss: 1.650 Epoch 2 Batch 125/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.853, Loss: 1.667 Epoch 2 Batch 126/1077 - Train Accuracy: 0.858, Validation Accuracy: 0.841, Loss: 1.552 Epoch 2 Batch 127/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.844, Loss: 1.579 Epoch 2 Batch 128/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.839, Loss: 1.590 Epoch 2 Batch 129/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.831, Loss: 1.546 Epoch 2 Batch 130/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.817, Loss: 1.631 Epoch 2 Batch 131/1077 - Train Accuracy: 0.820, Validation Accuracy: 0.804, Loss: 1.627 Epoch 2 Batch 132/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.797, Loss: 1.693 Epoch 2 Batch 133/1077 - Train Accuracy: 0.781, Validation Accuracy: 0.797, Loss: 1.628 Epoch 2 Batch 134/1077 - Train Accuracy: 0.843, Validation Accuracy: 0.792, Loss: 1.567 Epoch 2 Batch 135/1077 - Train Accuracy: 0.785, Validation Accuracy: 0.798, Loss: 1.640 Epoch 2 Batch 136/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.812, Loss: 1.583 Epoch 2 Batch 137/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.816, Loss: 1.626 Epoch 2 Batch 138/1077 - Train Accuracy: 0.814, Validation Accuracy: 0.819, Loss: 1.667 Epoch 2 Batch 139/1077 - Train Accuracy: 0.793, Validation Accuracy: 0.808, Loss: 1.585 Epoch 2 Batch 140/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.810, Loss: 1.589 Epoch 2 Batch 141/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.811, Loss: 1.557 Epoch 2 Batch 142/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.817, Loss: 1.602 Epoch 2 Batch 143/1077 - Train Accuracy: 0.827, Validation Accuracy: 0.821, Loss: 1.631 Epoch 2 Batch 144/1077 - Train Accuracy: 0.834, Validation Accuracy: 0.821, Loss: 1.546 Epoch 2 Batch 145/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.818, Loss: 1.682 Epoch 2 Batch 146/1077 - Train Accuracy: 0.839, Validation Accuracy: 0.822, Loss: 1.723 Epoch 2 Batch 147/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.815, Loss: 1.590 Epoch 2 Batch 148/1077 - Train Accuracy: 0.832, Validation Accuracy: 0.824, Loss: 1.657 Epoch 2 Batch 149/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.835, Loss: 1.598 Epoch 2 Batch 150/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.830, Loss: 1.665 Epoch 2 Batch 151/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.824, Loss: 1.619 Epoch 2 Batch 152/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.822, Loss: 1.692 Epoch 2 Batch 153/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.823, Loss: 1.597 Epoch 2 Batch 154/1077 - Train Accuracy: 0.829, Validation Accuracy: 0.827, Loss: 1.662 Epoch 2 Batch 155/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.826, Loss: 1.629 Epoch 2 Batch 156/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.813, Loss: 1.691 Epoch 2 Batch 157/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.807, Loss: 1.587 Epoch 2 Batch 158/1077 - Train Accuracy: 0.820, Validation Accuracy: 0.806, Loss: 1.607 Epoch 2 Batch 159/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.808, Loss: 1.566 Epoch 2 Batch 160/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.794, Loss: 1.613 Epoch 2 Batch 161/1077 - Train Accuracy: 0.814, Validation Accuracy: 0.804, Loss: 1.589 Epoch 2 Batch 162/1077 - Train Accuracy: 0.827, Validation Accuracy: 0.820, Loss: 1.636 Epoch 2 Batch 163/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.831, Loss: 1.674 Epoch 2 Batch 164/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.831, Loss: 1.573 Epoch 2 Batch 165/1077 - Train Accuracy: 0.785, Validation Accuracy: 0.823, Loss: 1.603 Epoch 2 Batch 166/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.822, Loss: 1.650 Epoch 2 Batch 167/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.823, Loss: 1.613 Epoch 2 Batch 168/1077 - Train Accuracy: 0.810, Validation Accuracy: 0.830, Loss: 1.630 Epoch 2 Batch 169/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.826, Loss: 1.671 Epoch 2 Batch 170/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.814, Loss: 1.615 Epoch 2 Batch 171/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.807, Loss: 1.552 Epoch 2 Batch 172/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.809, Loss: 1.630 Epoch 2 Batch 173/1077 - Train Accuracy: 0.822, Validation Accuracy: 0.808, Loss: 1.725 Epoch 2 Batch 174/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.813, Loss: 1.614 Epoch 2 Batch 175/1077 - Train Accuracy: 0.843, Validation Accuracy: 0.823, Loss: 1.551 Epoch 2 Batch 176/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.829, Loss: 1.628 Epoch 2 Batch 177/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.828, Loss: 1.607 Epoch 2 Batch 178/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.834, Loss: 1.596 Epoch 2 Batch 179/1077 - Train Accuracy: 0.808, Validation Accuracy: 0.832, Loss: 1.676 Epoch 2 Batch 180/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.830, Loss: 1.543 Epoch 2 Batch 181/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.832, Loss: 1.609 Epoch 2 Batch 182/1077 - Train Accuracy: 0.839, Validation Accuracy: 0.832, Loss: 1.595 Epoch 2 Batch 183/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.833, Loss: 1.667 Epoch 2 Batch 184/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.817, Loss: 1.645 Epoch 2 Batch 185/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.813, Loss: 1.610 Epoch 2 Batch 186/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.812, Loss: 1.645 Epoch 2 Batch 187/1077 - Train Accuracy: 0.834, Validation Accuracy: 0.806, Loss: 1.634 Epoch 2 Batch 188/1077 - Train Accuracy: 0.839, Validation Accuracy: 0.817, Loss: 1.546 Epoch 2 Batch 189/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.821, Loss: 1.615 Epoch 2 Batch 190/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.822, Loss: 1.593 Epoch 2 Batch 191/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.810, Loss: 1.588 Epoch 2 Batch 192/1077 - Train Accuracy: 0.817, Validation Accuracy: 0.789, Loss: 1.592 Epoch 2 Batch 193/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.781, Loss: 1.531 Epoch 2 Batch 194/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.792, Loss: 1.525 Epoch 2 Batch 195/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.812, Loss: 1.559 Epoch 2 Batch 196/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.824, Loss: 1.569 Epoch 2 Batch 197/1077 - Train Accuracy: 0.837, Validation Accuracy: 0.821, Loss: 1.602 Epoch 2 Batch 198/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.832, Loss: 1.670 Epoch 2 Batch 199/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.842, Loss: 1.645 Epoch 2 Batch 200/1077 - Train Accuracy: 0.806, Validation Accuracy: 0.840, Loss: 1.549 Epoch 2 Batch 201/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.834, Loss: 1.539 Epoch 2 Batch 202/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.827, Loss: 1.500 Epoch 2 Batch 203/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.826, Loss: 1.629 Epoch 2 Batch 204/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.833, Loss: 1.645 Epoch 2 Batch 205/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.827, Loss: 1.607 Epoch 2 Batch 206/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.830, Loss: 1.595 Epoch 2 Batch 207/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.826, Loss: 1.628 Epoch 2 Batch 208/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.825, Loss: 1.573 Epoch 2 Batch 209/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.806, Loss: 1.639 Epoch 2 Batch 210/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.805, Loss: 1.655 Epoch 2 Batch 211/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.799, Loss: 1.615 Epoch 2 Batch 212/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.793, Loss: 1.572 Epoch 2 Batch 213/1077 - Train Accuracy: 0.814, Validation Accuracy: 0.801, Loss: 1.573 Epoch 2 Batch 214/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.808, Loss: 1.653 Epoch 2 Batch 215/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.815, Loss: 1.618 Epoch 2 Batch 216/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.828, Loss: 1.592 Epoch 2 Batch 217/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.835, Loss: 1.656 Epoch 2 Batch 218/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.838, Loss: 1.641 Epoch 2 Batch 219/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.834, Loss: 1.648 Epoch 2 Batch 220/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.833, Loss: 1.715 Epoch 2 Batch 221/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.833, Loss: 1.553 Epoch 2 Batch 222/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.836, Loss: 1.627 Epoch 2 Batch 223/1077 - Train Accuracy: 0.843, Validation Accuracy: 0.835, Loss: 1.621 Epoch 2 Batch 224/1077 - Train Accuracy: 0.837, Validation Accuracy: 0.835, Loss: 1.614 Epoch 2 Batch 225/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.834, Loss: 1.634 Epoch 2 Batch 226/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.839, Loss: 1.616 Epoch 2 Batch 227/1077 - Train Accuracy: 0.794, Validation Accuracy: 0.833, Loss: 1.632 Epoch 2 Batch 228/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.806, Loss: 1.570 Epoch 2 Batch 229/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.805, Loss: 1.560 Epoch 2 Batch 230/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.817, Loss: 1.606 Epoch 2 Batch 231/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.823, Loss: 1.689 Epoch 2 Batch 232/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.841, Loss: 1.620 Epoch 2 Batch 233/1077 - Train Accuracy: 0.854, Validation Accuracy: 0.855, Loss: 1.647 Epoch 2 Batch 234/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.847, Loss: 1.666 Epoch 2 Batch 235/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.846, Loss: 1.513 Epoch 2 Batch 236/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.847, Loss: 1.609 Epoch 2 Batch 237/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.846, Loss: 1.575 Epoch 2 Batch 238/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.846, Loss: 1.683 Epoch 2 Batch 239/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.840, Loss: 1.588 Epoch 2 Batch 240/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.836, Loss: 1.655 Epoch 2 Batch 241/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.828, Loss: 1.623 Epoch 2 Batch 242/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.825, Loss: 1.603 Epoch 2 Batch 243/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.837, Loss: 1.662 Epoch 2 Batch 244/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.839, Loss: 1.622 Epoch 2 Batch 245/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.836, Loss: 1.637 Epoch 2 Batch 246/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.826, Loss: 1.607 Epoch 2 Batch 247/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.821, Loss: 1.582 Epoch 2 Batch 248/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.831, Loss: 1.615 Epoch 2 Batch 249/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.829, Loss: 1.647 Epoch 2 Batch 250/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.832, Loss: 1.617 Epoch 2 Batch 251/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.831, Loss: 1.573 Epoch 2 Batch 252/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.833, Loss: 1.641 Epoch 2 Batch 253/1077 - Train Accuracy: 0.827, Validation Accuracy: 0.847, Loss: 1.616 Epoch 2 Batch 254/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.854, Loss: 1.612 Epoch 2 Batch 255/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.852, Loss: 1.548 Epoch 2 Batch 256/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.853, Loss: 1.554 Epoch 2 Batch 257/1077 - Train Accuracy: 0.832, Validation Accuracy: 0.849, Loss: 1.588 Epoch 2 Batch 258/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.849, Loss: 1.601 Epoch 2 Batch 259/1077 - Train Accuracy: 0.807, Validation Accuracy: 0.846, Loss: 1.598 Epoch 2 Batch 260/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.850, Loss: 1.588 Epoch 2 Batch 261/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.850, Loss: 1.591 Epoch 2 Batch 262/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.865, Loss: 1.579 Epoch 2 Batch 263/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.860, Loss: 1.594 Epoch 2 Batch 264/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.862, Loss: 1.691 Epoch 2 Batch 265/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.862, Loss: 1.572 Epoch 2 Batch 266/1077 - Train Accuracy: 0.827, Validation Accuracy: 0.851, Loss: 1.566 Epoch 2 Batch 267/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.849, Loss: 1.606 Epoch 2 Batch 268/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.847, Loss: 1.561 Epoch 2 Batch 269/1077 - Train Accuracy: 0.822, Validation Accuracy: 0.830, Loss: 1.568 Epoch 2 Batch 270/1077 - Train Accuracy: 0.791, Validation Accuracy: 0.825, Loss: 1.547 Epoch 2 Batch 271/1077 - Train Accuracy: 0.876, Validation Accuracy: 0.825, Loss: 1.645 Epoch 2 Batch 272/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.833, Loss: 1.587 Epoch 2 Batch 273/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.828, Loss: 1.618 Epoch 2 Batch 274/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.825, Loss: 1.593 Epoch 2 Batch 275/1077 - Train Accuracy: 0.827, Validation Accuracy: 0.829, Loss: 1.603 Epoch 2 Batch 276/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.844, Loss: 1.650 Epoch 2 Batch 277/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.841, Loss: 1.603 Epoch 2 Batch 278/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.835, Loss: 1.521 Epoch 2 Batch 279/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.832, Loss: 1.582 Epoch 2 Batch 280/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.835, Loss: 1.636 Epoch 2 Batch 281/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.843, Loss: 1.644 Epoch 2 Batch 282/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.848, Loss: 1.697 Epoch 2 Batch 283/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.860, Loss: 1.605 Epoch 2 Batch 284/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.864, Loss: 1.594 Epoch 2 Batch 285/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.859, Loss: 1.557 Epoch 2 Batch 286/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.858, Loss: 1.598 Epoch 2 Batch 287/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.851, Loss: 1.549 Epoch 2 Batch 288/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.829, Loss: 1.624 Epoch 2 Batch 289/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.831, Loss: 1.641 Epoch 2 Batch 290/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.815, Loss: 1.533 Epoch 2 Batch 291/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.805, Loss: 1.601 Epoch 2 Batch 292/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.799, Loss: 1.648 Epoch 2 Batch 293/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.816, Loss: 1.571 Epoch 2 Batch 294/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.836, Loss: 1.574 Epoch 2 Batch 295/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.841, Loss: 1.630 Epoch 2 Batch 296/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.841, Loss: 1.670 Epoch 2 Batch 297/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.831, Loss: 1.594 Epoch 2 Batch 298/1077 - Train Accuracy: 0.814, Validation Accuracy: 0.833, Loss: 1.564 Epoch 2 Batch 299/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.838, Loss: 1.615 Epoch 2 Batch 300/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.837, Loss: 1.628 Epoch 2 Batch 301/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.828, Loss: 1.569 Epoch 2 Batch 302/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.834, Loss: 1.682 Epoch 2 Batch 303/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.832, Loss: 1.541 Epoch 2 Batch 304/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.833, Loss: 1.564 Epoch 2 Batch 305/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.844, Loss: 1.550 Epoch 2 Batch 306/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.846, Loss: 1.600 Epoch 2 Batch 307/1077 - Train Accuracy: 0.829, Validation Accuracy: 0.843, Loss: 1.650 Epoch 2 Batch 308/1077 - Train Accuracy: 0.844, Validation Accuracy: 0.836, Loss: 1.532 Epoch 2 Batch 309/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.831, Loss: 1.551 Epoch 2 Batch 310/1077 - Train Accuracy: 0.843, Validation Accuracy: 0.837, Loss: 1.603 Epoch 2 Batch 311/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.844, Loss: 1.538 Epoch 2 Batch 312/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.847, Loss: 1.627 Epoch 2 Batch 313/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.846, Loss: 1.520 Epoch 2 Batch 314/1077 - Train Accuracy: 0.876, Validation Accuracy: 0.851, Loss: 1.564 Epoch 2 Batch 315/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.843, Loss: 1.549 Epoch 2 Batch 316/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.842, Loss: 1.612 Epoch 2 Batch 317/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.842, Loss: 1.581 Epoch 2 Batch 318/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.853, Loss: 1.617 Epoch 2 Batch 319/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.855, Loss: 1.617 Epoch 2 Batch 320/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.866, Loss: 1.608 Epoch 2 Batch 321/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.866, Loss: 1.609 Epoch 2 Batch 322/1077 - Train Accuracy: 0.829, Validation Accuracy: 0.869, Loss: 1.541 Epoch 2 Batch 323/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.860, Loss: 1.597 Epoch 2 Batch 324/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.849, Loss: 1.667 Epoch 2 Batch 325/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.856, Loss: 1.558 Epoch 2 Batch 326/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.839, Loss: 1.540 Epoch 2 Batch 327/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.840, Loss: 1.629 Epoch 2 Batch 328/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.848, Loss: 1.576 Epoch 2 Batch 329/1077 - Train Accuracy: 0.829, Validation Accuracy: 0.846, Loss: 1.682 Epoch 2 Batch 330/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.832, Loss: 1.597 Epoch 2 Batch 331/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.837, Loss: 1.644 Epoch 2 Batch 332/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.836, Loss: 1.536 Epoch 2 Batch 333/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.837, Loss: 1.588 Epoch 2 Batch 334/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.845, Loss: 1.660 Epoch 2 Batch 335/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.840, Loss: 1.562 Epoch 2 Batch 336/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.835, Loss: 1.587 Epoch 2 Batch 337/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.833, Loss: 1.611 Epoch 2 Batch 338/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.840, Loss: 1.632 Epoch 2 Batch 339/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.836, Loss: 1.524 Epoch 2 Batch 340/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.841, Loss: 1.575 Epoch 2 Batch 341/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.847, Loss: 1.683 Epoch 2 Batch 342/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.852, Loss: 1.601 Epoch 2 Batch 343/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.860, Loss: 1.588 Epoch 2 Batch 344/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.861, Loss: 1.552 Epoch 2 Batch 345/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.839, Loss: 1.617 Epoch 2 Batch 346/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.843, Loss: 1.570 Epoch 2 Batch 347/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.840, Loss: 1.606 Epoch 2 Batch 348/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.837, Loss: 1.505 Epoch 2 Batch 349/1077 - Train Accuracy: 0.854, Validation Accuracy: 0.840, Loss: 1.582 Epoch 2 Batch 350/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.837, Loss: 1.590 Epoch 2 Batch 351/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.835, Loss: 1.536 Epoch 2 Batch 352/1077 - Train Accuracy: 0.851, Validation Accuracy: 0.822, Loss: 1.587 Epoch 2 Batch 353/1077 - Train Accuracy: 0.810, Validation Accuracy: 0.826, Loss: 1.690 Epoch 2 Batch 354/1077 - Train Accuracy: 0.834, Validation Accuracy: 0.824, Loss: 1.662 Epoch 2 Batch 355/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.824, Loss: 1.586 Epoch 2 Batch 356/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.842, Loss: 1.608 Epoch 2 Batch 357/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.830, Loss: 1.646 Epoch 2 Batch 358/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.838, Loss: 1.635 Epoch 2 Batch 359/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.843, Loss: 1.635 Epoch 2 Batch 360/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.849, Loss: 1.551 Epoch 2 Batch 361/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.845, Loss: 1.536 Epoch 2 Batch 362/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.846, Loss: 1.627 Epoch 2 Batch 363/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.851, Loss: 1.485 Epoch 2 Batch 364/1077 - Train Accuracy: 0.876, Validation Accuracy: 0.847, Loss: 1.643 Epoch 2 Batch 365/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.846, Loss: 1.624 Epoch 2 Batch 366/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.859, Loss: 1.611 Epoch 2 Batch 367/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.864, Loss: 1.517 Epoch 2 Batch 368/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.864, Loss: 1.514 Epoch 2 Batch 369/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.859, Loss: 1.616 Epoch 2 Batch 370/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.837, Loss: 1.623 Epoch 2 Batch 371/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.843, Loss: 1.601 Epoch 2 Batch 372/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.832, Loss: 1.585 Epoch 2 Batch 373/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.833, Loss: 1.553 Epoch 2 Batch 374/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.830, Loss: 1.594 Epoch 2 Batch 375/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.829, Loss: 1.569 Epoch 2 Batch 376/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.838, Loss: 1.563 Epoch 2 Batch 377/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.852, Loss: 1.614 Epoch 2 Batch 378/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.848, Loss: 1.597 Epoch 2 Batch 379/1077 - Train Accuracy: 0.829, Validation Accuracy: 0.844, Loss: 1.622 Epoch 2 Batch 380/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.851, Loss: 1.597 Epoch 2 Batch 381/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.862, Loss: 1.601 Epoch 2 Batch 382/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.857, Loss: 1.644 Epoch 2 Batch 383/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.849, Loss: 1.560 Epoch 2 Batch 384/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.870, Loss: 1.568 Epoch 2 Batch 385/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.869, Loss: 1.513 Epoch 2 Batch 386/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.878, Loss: 1.561 Epoch 2 Batch 387/1077 - Train Accuracy: 0.858, Validation Accuracy: 0.870, Loss: 1.584 Epoch 2 Batch 388/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.863, Loss: 1.618 Epoch 2 Batch 389/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.856, Loss: 1.584 Epoch 2 Batch 390/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.846, Loss: 1.485 Epoch 2 Batch 391/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.847, Loss: 1.548 Epoch 2 Batch 392/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.841, Loss: 1.586 Epoch 2 Batch 393/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.832, Loss: 1.539 Epoch 2 Batch 394/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.825, Loss: 1.613 Epoch 2 Batch 395/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.821, Loss: 1.620 Epoch 2 Batch 396/1077 - Train Accuracy: 0.844, Validation Accuracy: 0.825, Loss: 1.556 Epoch 2 Batch 397/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.830, Loss: 1.611 Epoch 2 Batch 398/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.832, Loss: 1.589 Epoch 2 Batch 399/1077 - Train Accuracy: 0.826, Validation Accuracy: 0.840, Loss: 1.635 Epoch 2 Batch 400/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.845, Loss: 1.565 Epoch 2 Batch 401/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.843, Loss: 1.586 Epoch 2 Batch 402/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.847, Loss: 1.634 Epoch 2 Batch 403/1077 - Train Accuracy: 0.832, Validation Accuracy: 0.843, Loss: 1.578 Epoch 2 Batch 404/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.847, Loss: 1.497 Epoch 2 Batch 405/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.855, Loss: 1.657 Epoch 2 Batch 406/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.854, Loss: 1.497 Epoch 2 Batch 407/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.861, Loss: 1.588 Epoch 2 Batch 408/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.857, Loss: 1.630 Epoch 2 Batch 409/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.850, Loss: 1.637 Epoch 2 Batch 410/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.852, Loss: 1.512 Epoch 2 Batch 411/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.857, Loss: 1.647 Epoch 2 Batch 412/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.850, Loss: 1.598 Epoch 2 Batch 413/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.846, Loss: 1.593 Epoch 2 Batch 414/1077 - Train Accuracy: 0.837, Validation Accuracy: 0.864, Loss: 1.629 Epoch 2 Batch 415/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.867, Loss: 1.607 Epoch 2 Batch 416/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.859, Loss: 1.519 Epoch 2 Batch 417/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.854, Loss: 1.667 Epoch 2 Batch 418/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.849, Loss: 1.554 Epoch 2 Batch 419/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.851, Loss: 1.491 Epoch 2 Batch 420/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.841, Loss: 1.567 Epoch 2 Batch 421/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.840, Loss: 1.585 Epoch 2 Batch 422/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.830, Loss: 1.566 Epoch 2 Batch 423/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.836, Loss: 1.618 Epoch 2 Batch 424/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.824, Loss: 1.589 Epoch 2 Batch 425/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.840, Loss: 1.497 Epoch 2 Batch 426/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.848, Loss: 1.619 Epoch 2 Batch 427/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.847, Loss: 1.617 Epoch 2 Batch 428/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.848, Loss: 1.515 Epoch 2 Batch 429/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.852, Loss: 1.533 Epoch 2 Batch 430/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.852, Loss: 1.578 Epoch 2 Batch 431/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.850, Loss: 1.573 Epoch 2 Batch 432/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.852, Loss: 1.558 Epoch 2 Batch 433/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.848, Loss: 1.586 Epoch 2 Batch 434/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.849, Loss: 1.536 Epoch 2 Batch 435/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.849, Loss: 1.611 Epoch 2 Batch 436/1077 - Train Accuracy: 0.843, Validation Accuracy: 0.850, Loss: 1.612 Epoch 2 Batch 437/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.852, Loss: 1.603 Epoch 2 Batch 438/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.842, Loss: 1.540 Epoch 2 Batch 439/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.836, Loss: 1.628 Epoch 2 Batch 440/1077 - Train Accuracy: 0.832, Validation Accuracy: 0.837, Loss: 1.564 Epoch 2 Batch 441/1077 - Train Accuracy: 0.844, Validation Accuracy: 0.831, Loss: 1.639 Epoch 2 Batch 442/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.830, Loss: 1.578 Epoch 2 Batch 443/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.834, Loss: 1.607 Epoch 2 Batch 444/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.825, Loss: 1.564 Epoch 2 Batch 445/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.826, Loss: 1.610 Epoch 2 Batch 446/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.821, Loss: 1.526 Epoch 2 Batch 447/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.824, Loss: 1.600 Epoch 2 Batch 448/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.827, Loss: 1.527 Epoch 2 Batch 449/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.839, Loss: 1.614 Epoch 2 Batch 450/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.842, Loss: 1.628 Epoch 2 Batch 451/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.839, Loss: 1.580 Epoch 2 Batch 452/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.836, Loss: 1.621 Epoch 2 Batch 453/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.837, Loss: 1.649 Epoch 2 Batch 454/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.838, Loss: 1.575 Epoch 2 Batch 455/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.836, Loss: 1.594 Epoch 2 Batch 456/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.844, Loss: 1.607 Epoch 2 Batch 457/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.843, Loss: 1.549 Epoch 2 Batch 458/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.843, Loss: 1.703 Epoch 2 Batch 459/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.849, Loss: 1.550 Epoch 2 Batch 460/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.856, Loss: 1.604 Epoch 2 Batch 461/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.859, Loss: 1.555 Epoch 2 Batch 462/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.872, Loss: 1.623 Epoch 2 Batch 463/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.867, Loss: 1.577 Epoch 2 Batch 464/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.858, Loss: 1.593 Epoch 2 Batch 465/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.858, Loss: 1.551 Epoch 2 Batch 466/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.858, Loss: 1.548 Epoch 2 Batch 467/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.860, Loss: 1.557 Epoch 2 Batch 468/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.852, Loss: 1.582 Epoch 2 Batch 469/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.848, Loss: 1.591 Epoch 2 Batch 470/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.854, Loss: 1.553 Epoch 2 Batch 471/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.860, Loss: 1.576 Epoch 2 Batch 472/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.856, Loss: 1.524 Epoch 2 Batch 473/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.845, Loss: 1.534 Epoch 2 Batch 474/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.845, Loss: 1.554 Epoch 2 Batch 475/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.849, Loss: 1.531 Epoch 2 Batch 476/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.843, Loss: 1.516 Epoch 2 Batch 477/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.839, Loss: 1.609 Epoch 2 Batch 478/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.838, Loss: 1.643 Epoch 2 Batch 479/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.843, Loss: 1.575 Epoch 2 Batch 480/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.859, Loss: 1.626 Epoch 2 Batch 481/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.865, Loss: 1.597 Epoch 2 Batch 482/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.872, Loss: 1.658 Epoch 2 Batch 483/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.863, Loss: 1.576 Epoch 2 Batch 484/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.864, Loss: 1.509 Epoch 2 Batch 485/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.863, Loss: 1.490 Epoch 2 Batch 486/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.869, Loss: 1.570 Epoch 2 Batch 487/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.864, Loss: 1.475 Epoch 2 Batch 488/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.853, Loss: 1.636 Epoch 2 Batch 489/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.851, Loss: 1.506 Epoch 2 Batch 490/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.849, Loss: 1.596 Epoch 2 Batch 491/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.835, Loss: 1.641 Epoch 2 Batch 492/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.844, Loss: 1.565 Epoch 2 Batch 493/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.852, Loss: 1.616 Epoch 2 Batch 494/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.858, Loss: 1.591 Epoch 2 Batch 495/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.861, Loss: 1.556 Epoch 2 Batch 496/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.870, Loss: 1.494 Epoch 2 Batch 497/1077 - Train Accuracy: 0.854, Validation Accuracy: 0.863, Loss: 1.598 Epoch 2 Batch 498/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.857, Loss: 1.516 Epoch 2 Batch 499/1077 - Train Accuracy: 0.839, Validation Accuracy: 0.854, Loss: 1.501 Epoch 2 Batch 500/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.849, Loss: 1.634 Epoch 2 Batch 501/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.850, Loss: 1.631 Epoch 2 Batch 502/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.845, Loss: 1.493 Epoch 2 Batch 503/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.856, Loss: 1.525 Epoch 2 Batch 504/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.839, Loss: 1.561 Epoch 2 Batch 505/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.840, Loss: 1.516 Epoch 2 Batch 506/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.841, Loss: 1.546 Epoch 2 Batch 507/1077 - Train Accuracy: 0.820, Validation Accuracy: 0.840, Loss: 1.609 Epoch 2 Batch 508/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.844, Loss: 1.596 Epoch 2 Batch 509/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.848, Loss: 1.498 Epoch 2 Batch 510/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.850, Loss: 1.602 Epoch 2 Batch 511/1077 - Train Accuracy: 0.854, Validation Accuracy: 0.849, Loss: 1.607 Epoch 2 Batch 512/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.856, Loss: 1.560 Epoch 2 Batch 513/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.849, Loss: 1.568 Epoch 2 Batch 514/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.839, Loss: 1.657 Epoch 2 Batch 515/1077 - Train Accuracy: 0.843, Validation Accuracy: 0.843, Loss: 1.627 Epoch 2 Batch 516/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.841, Loss: 1.554 Epoch 2 Batch 517/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.847, Loss: 1.537 Epoch 2 Batch 518/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.852, Loss: 1.556 Epoch 2 Batch 519/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.857, Loss: 1.593 Epoch 2 Batch 520/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.856, Loss: 1.525 Epoch 2 Batch 521/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.850, Loss: 1.577 Epoch 2 Batch 522/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.853, Loss: 1.584 Epoch 2 Batch 523/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.858, Loss: 1.580 Epoch 2 Batch 524/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.856, Loss: 1.570 Epoch 2 Batch 525/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.858, Loss: 1.554 Epoch 2 Batch 526/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.864, Loss: 1.542 Epoch 2 Batch 527/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.854, Loss: 1.527 Epoch 2 Batch 528/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.850, Loss: 1.626 Epoch 2 Batch 529/1077 - Train Accuracy: 0.851, Validation Accuracy: 0.851, Loss: 1.580 Epoch 2 Batch 530/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.849, Loss: 1.606 Epoch 2 Batch 531/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.849, Loss: 1.543 Epoch 2 Batch 532/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.853, Loss: 1.595 Epoch 2 Batch 533/1077 - Train Accuracy: 0.839, Validation Accuracy: 0.858, Loss: 1.530 Epoch 2 Batch 534/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.864, Loss: 1.532 Epoch 2 Batch 535/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.859, Loss: 1.560 Epoch 2 Batch 536/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.856, Loss: 1.556 Epoch 2 Batch 537/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.861, Loss: 1.524 Epoch 2 Batch 538/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.842, Loss: 1.564 Epoch 2 Batch 539/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.854, Loss: 1.574 Epoch 2 Batch 540/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.854, Loss: 1.630 Epoch 2 Batch 541/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.848, Loss: 1.501 Epoch 2 Batch 542/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.847, Loss: 1.577 Epoch 2 Batch 543/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.842, Loss: 1.545 Epoch 2 Batch 544/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.848, Loss: 1.597 Epoch 2 Batch 545/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.853, Loss: 1.556 Epoch 2 Batch 546/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.850, Loss: 1.562 Epoch 2 Batch 547/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.850, Loss: 1.442 Epoch 2 Batch 548/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.864, Loss: 1.497 Epoch 2 Batch 549/1077 - Train Accuracy: 0.820, Validation Accuracy: 0.871, Loss: 1.574 Epoch 2 Batch 550/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.867, Loss: 1.544 Epoch 2 Batch 551/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.869, Loss: 1.443 Epoch 2 Batch 552/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.874, Loss: 1.568 Epoch 2 Batch 553/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.862, Loss: 1.515 Epoch 2 Batch 554/1077 - Train Accuracy: 0.851, Validation Accuracy: 0.866, Loss: 1.544 Epoch 2 Batch 555/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.868, Loss: 1.522 Epoch 2 Batch 556/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.868, Loss: 1.586 Epoch 2 Batch 557/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.871, Loss: 1.475 Epoch 2 Batch 558/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.864, Loss: 1.500 Epoch 2 Batch 559/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.849, Loss: 1.678 Epoch 2 Batch 560/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.846, Loss: 1.602 Epoch 2 Batch 561/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.847, Loss: 1.524 Epoch 2 Batch 562/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.844, Loss: 1.622 Epoch 2 Batch 563/1077 - Train Accuracy: 0.854, Validation Accuracy: 0.865, Loss: 1.590 Epoch 2 Batch 564/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.850, Loss: 1.586 Epoch 2 Batch 565/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.859, Loss: 1.602 Epoch 2 Batch 566/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.847, Loss: 1.553 Epoch 2 Batch 567/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.842, Loss: 1.533 Epoch 2 Batch 568/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.838, Loss: 1.631 Epoch 2 Batch 569/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.839, Loss: 1.533 Epoch 2 Batch 570/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.851, Loss: 1.566 Epoch 2 Batch 571/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.859, Loss: 1.547 Epoch 2 Batch 572/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.865, Loss: 1.499 Epoch 2 Batch 573/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.869, Loss: 1.538 Epoch 2 Batch 574/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.864, Loss: 1.595 Epoch 2 Batch 575/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.855, Loss: 1.613 Epoch 2 Batch 576/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.858, Loss: 1.566 Epoch 2 Batch 577/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.855, Loss: 1.525 Epoch 2 Batch 578/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.837, Loss: 1.551 Epoch 2 Batch 579/1077 - Train Accuracy: 0.851, Validation Accuracy: 0.837, Loss: 1.509 Epoch 2 Batch 580/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.835, Loss: 1.606 Epoch 2 Batch 581/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.828, Loss: 1.571 Epoch 2 Batch 582/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.843, Loss: 1.636 Epoch 2 Batch 583/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.855, Loss: 1.642 Epoch 2 Batch 584/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.851, Loss: 1.606 Epoch 2 Batch 585/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.845, Loss: 1.522 Epoch 2 Batch 586/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.839, Loss: 1.601 Epoch 2 Batch 587/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.841, Loss: 1.465 Epoch 2 Batch 588/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.847, Loss: 1.541 Epoch 2 Batch 589/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.847, Loss: 1.629 Epoch 2 Batch 590/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.851, Loss: 1.614 Epoch 2 Batch 591/1077 - Train Accuracy: 0.876, Validation Accuracy: 0.855, Loss: 1.539 Epoch 2 Batch 592/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.860, Loss: 1.518 Epoch 2 Batch 593/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.862, Loss: 1.577 Epoch 2 Batch 594/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.868, Loss: 1.551 Epoch 2 Batch 595/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.869, Loss: 1.570 Epoch 2 Batch 596/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.855, Loss: 1.484 Epoch 2 Batch 597/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.854, Loss: 1.558 Epoch 2 Batch 598/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.859, Loss: 1.618 Epoch 2 Batch 599/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.856, Loss: 1.600 Epoch 2 Batch 600/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.865, Loss: 1.515 Epoch 2 Batch 601/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.854, Loss: 1.457 Epoch 2 Batch 602/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.848, Loss: 1.524 Epoch 2 Batch 603/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.846, Loss: 1.600 Epoch 2 Batch 604/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.848, Loss: 1.511 Epoch 2 Batch 605/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.858, Loss: 1.627 Epoch 2 Batch 606/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.854, Loss: 1.596 Epoch 2 Batch 607/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.858, Loss: 1.616 Epoch 2 Batch 608/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.873, Loss: 1.619 Epoch 2 Batch 609/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.876, Loss: 1.552 Epoch 2 Batch 610/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.872, Loss: 1.563 Epoch 2 Batch 611/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.868, Loss: 1.555 Epoch 2 Batch 612/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.865, Loss: 1.558 Epoch 2 Batch 613/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.863, Loss: 1.495 Epoch 2 Batch 614/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.863, Loss: 1.500 Epoch 2 Batch 615/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.866, Loss: 1.581 Epoch 2 Batch 616/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.868, Loss: 1.571 Epoch 2 Batch 617/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.866, Loss: 1.540 Epoch 2 Batch 618/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.863, Loss: 1.553 Epoch 2 Batch 619/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.864, Loss: 1.612 Epoch 2 Batch 620/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.865, Loss: 1.600 Epoch 2 Batch 621/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.869, Loss: 1.545 Epoch 2 Batch 622/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.866, Loss: 1.613 Epoch 2 Batch 623/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.860, Loss: 1.488 Epoch 2 Batch 624/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.862, Loss: 1.525 Epoch 2 Batch 625/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.866, Loss: 1.641 Epoch 2 Batch 626/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.867, Loss: 1.530 Epoch 2 Batch 627/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.864, Loss: 1.575 Epoch 2 Batch 628/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.864, Loss: 1.587 Epoch 2 Batch 629/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.874, Loss: 1.548 Epoch 2 Batch 630/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.876, Loss: 1.596 Epoch 2 Batch 631/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.880, Loss: 1.574 Epoch 2 Batch 632/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.880, Loss: 1.516 Epoch 2 Batch 633/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.876, Loss: 1.585 Epoch 2 Batch 634/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.867, Loss: 1.569 Epoch 2 Batch 635/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.867, Loss: 1.550 Epoch 2 Batch 636/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.862, Loss: 1.570 Epoch 2 Batch 637/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.870, Loss: 1.534 Epoch 2 Batch 638/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.878, Loss: 1.554 Epoch 2 Batch 639/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.868, Loss: 1.577 Epoch 2 Batch 640/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.857, Loss: 1.558 Epoch 2 Batch 641/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.853, Loss: 1.588 Epoch 2 Batch 642/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.857, Loss: 1.572 Epoch 2 Batch 643/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.863, Loss: 1.507 Epoch 2 Batch 644/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.865, Loss: 1.469 Epoch 2 Batch 645/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.870, Loss: 1.526 Epoch 2 Batch 646/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.877, Loss: 1.583 Epoch 2 Batch 647/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.870, Loss: 1.612 Epoch 2 Batch 648/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.869, Loss: 1.557 Epoch 2 Batch 649/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.879, Loss: 1.495 Epoch 2 Batch 650/1077 - Train Accuracy: 0.854, Validation Accuracy: 0.882, Loss: 1.556 Epoch 2 Batch 651/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.873, Loss: 1.513 Epoch 2 Batch 652/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.874, Loss: 1.592 Epoch 2 Batch 653/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.866, Loss: 1.459 Epoch 2 Batch 654/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.870, Loss: 1.562 Epoch 2 Batch 655/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.876, Loss: 1.572 Epoch 2 Batch 656/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.874, Loss: 1.642 Epoch 2 Batch 657/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.868, Loss: 1.477 Epoch 2 Batch 658/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.866, Loss: 1.636 Epoch 2 Batch 659/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.869, Loss: 1.553 Epoch 2 Batch 660/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.865, Loss: 1.598 Epoch 2 Batch 661/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.871, Loss: 1.513 Epoch 2 Batch 662/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.873, Loss: 1.468 Epoch 2 Batch 663/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.879, Loss: 1.538 Epoch 2 Batch 664/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.876, Loss: 1.517 Epoch 2 Batch 665/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.868, Loss: 1.565 Epoch 2 Batch 666/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.866, Loss: 1.555 Epoch 2 Batch 667/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.867, Loss: 1.490 Epoch 2 Batch 668/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.869, Loss: 1.614 Epoch 2 Batch 669/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.863, Loss: 1.555 Epoch 2 Batch 670/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.856, Loss: 1.546 Epoch 2 Batch 671/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.855, Loss: 1.539 Epoch 2 Batch 672/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.867, Loss: 1.540 Epoch 2 Batch 673/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.875, Loss: 1.570 Epoch 2 Batch 674/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.867, Loss: 1.497 Epoch 2 Batch 675/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.869, Loss: 1.503 Epoch 2 Batch 676/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.867, Loss: 1.501 Epoch 2 Batch 677/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.861, Loss: 1.539 Epoch 2 Batch 678/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.867, Loss: 1.622 Epoch 2 Batch 679/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.875, Loss: 1.540 Epoch 2 Batch 680/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.873, Loss: 1.518 Epoch 2 Batch 681/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.875, Loss: 1.598 Epoch 2 Batch 682/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.875, Loss: 1.529 Epoch 2 Batch 683/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.874, Loss: 1.556 Epoch 2 Batch 684/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.873, Loss: 1.497 Epoch 2 Batch 685/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.871, Loss: 1.579 Epoch 2 Batch 686/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.868, Loss: 1.601 Epoch 2 Batch 687/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.856, Loss: 1.614 Epoch 2 Batch 688/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.862, Loss: 1.499 Epoch 2 Batch 689/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.857, Loss: 1.486 Epoch 2 Batch 690/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.857, Loss: 1.523 Epoch 2 Batch 691/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.860, Loss: 1.533 Epoch 2 Batch 692/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.869, Loss: 1.542 Epoch 2 Batch 693/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.868, Loss: 1.620 Epoch 2 Batch 694/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.868, Loss: 1.546 Epoch 2 Batch 695/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.863, Loss: 1.574 Epoch 2 Batch 696/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.865, Loss: 1.524 Epoch 2 Batch 697/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.865, Loss: 1.486 Epoch 2 Batch 698/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.862, Loss: 1.572 Epoch 2 Batch 699/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.861, Loss: 1.537 Epoch 2 Batch 700/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.860, Loss: 1.574 Epoch 2 Batch 701/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.875, Loss: 1.558 Epoch 2 Batch 702/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.865, Loss: 1.522 Epoch 2 Batch 703/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.864, Loss: 1.597 Epoch 2 Batch 704/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.863, Loss: 1.476 Epoch 2 Batch 705/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.859, Loss: 1.585 Epoch 2 Batch 706/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.863, Loss: 1.525 Epoch 2 Batch 707/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.865, Loss: 1.574 Epoch 2 Batch 708/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.864, Loss: 1.519 Epoch 2 Batch 709/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.853, Loss: 1.595 Epoch 2 Batch 710/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.867, Loss: 1.493 Epoch 2 Batch 711/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.863, Loss: 1.644 Epoch 2 Batch 712/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.871, Loss: 1.503 Epoch 2 Batch 713/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.857, Loss: 1.561 Epoch 2 Batch 714/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.849, Loss: 1.535 Epoch 2 Batch 715/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.851, Loss: 1.638 Epoch 2 Batch 716/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.860, Loss: 1.543 Epoch 2 Batch 717/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.863, Loss: 1.514 Epoch 2 Batch 718/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.864, Loss: 1.552 Epoch 2 Batch 719/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.873, Loss: 1.474 Epoch 2 Batch 720/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.865, Loss: 1.638 Epoch 2 Batch 721/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.875, Loss: 1.596 Epoch 2 Batch 722/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.881, Loss: 1.555 Epoch 2 Batch 723/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.880, Loss: 1.523 Epoch 2 Batch 724/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.870, Loss: 1.519 Epoch 2 Batch 725/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.866, Loss: 1.547 Epoch 2 Batch 726/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.869, Loss: 1.563 Epoch 2 Batch 727/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.874, Loss: 1.497 Epoch 2 Batch 728/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.865, Loss: 1.472 Epoch 2 Batch 729/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.871, Loss: 1.454 Epoch 2 Batch 730/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.870, Loss: 1.568 Epoch 2 Batch 731/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.868, Loss: 1.489 Epoch 2 Batch 732/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.866, Loss: 1.564 Epoch 2 Batch 733/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.860, Loss: 1.597 Epoch 2 Batch 734/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.856, Loss: 1.566 Epoch 2 Batch 735/1077 - Train Accuracy: 0.876, Validation Accuracy: 0.863, Loss: 1.577 Epoch 2 Batch 736/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.865, Loss: 1.549 Epoch 2 Batch 737/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.875, Loss: 1.496 Epoch 2 Batch 738/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.870, Loss: 1.521 Epoch 2 Batch 739/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.865, Loss: 1.514 Epoch 2 Batch 740/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.866, Loss: 1.578 Epoch 2 Batch 741/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.866, Loss: 1.571 Epoch 2 Batch 742/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.874, Loss: 1.577 Epoch 2 Batch 743/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.867, Loss: 1.575 Epoch 2 Batch 744/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.871, Loss: 1.481 Epoch 2 Batch 745/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.880, Loss: 1.574 Epoch 2 Batch 746/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.889, Loss: 1.585 Epoch 2 Batch 747/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.887, Loss: 1.496 Epoch 2 Batch 748/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.885, Loss: 1.514 Epoch 2 Batch 749/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.885, Loss: 1.582 Epoch 2 Batch 750/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.873, Loss: 1.569 Epoch 2 Batch 751/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.879, Loss: 1.594 Epoch 2 Batch 752/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.874, Loss: 1.532 Epoch 2 Batch 753/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.879, Loss: 1.555 Epoch 2 Batch 754/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.880, Loss: 1.567 Epoch 2 Batch 755/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.885, Loss: 1.516 Epoch 2 Batch 756/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.884, Loss: 1.526 Epoch 2 Batch 757/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.884, Loss: 1.511 Epoch 2 Batch 758/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.880, Loss: 1.503 Epoch 2 Batch 759/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.880, Loss: 1.549 Epoch 2 Batch 760/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.881, Loss: 1.573 Epoch 2 Batch 761/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.876, Loss: 1.542 Epoch 2 Batch 762/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.873, Loss: 1.614 Epoch 2 Batch 763/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.866, Loss: 1.525 Epoch 2 Batch 764/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.868, Loss: 1.524 Epoch 2 Batch 765/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.868, Loss: 1.616 Epoch 2 Batch 766/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.873, Loss: 1.571 Epoch 2 Batch 767/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.868, Loss: 1.496 Epoch 2 Batch 768/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.872, Loss: 1.593 Epoch 2 Batch 769/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.871, Loss: 1.489 Epoch 2 Batch 770/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.880, Loss: 1.480 Epoch 2 Batch 771/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.880, Loss: 1.519 Epoch 2 Batch 772/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.885, Loss: 1.549 Epoch 2 Batch 773/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.885, Loss: 1.539 Epoch 2 Batch 774/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.886, Loss: 1.524 Epoch 2 Batch 775/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.883, Loss: 1.561 Epoch 2 Batch 776/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.877, Loss: 1.525 Epoch 2 Batch 777/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.888, Loss: 1.520 Epoch 2 Batch 778/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.879, Loss: 1.538 Epoch 2 Batch 779/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.884, Loss: 1.485 Epoch 2 Batch 780/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.884, Loss: 1.643 Epoch 2 Batch 781/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.875, Loss: 1.524 Epoch 2 Batch 782/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.879, Loss: 1.515 Epoch 2 Batch 783/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.868, Loss: 1.510 Epoch 2 Batch 784/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.862, Loss: 1.516 Epoch 2 Batch 785/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.860, Loss: 1.540 Epoch 2 Batch 786/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.851, Loss: 1.552 Epoch 2 Batch 787/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.857, Loss: 1.471 Epoch 2 Batch 788/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.857, Loss: 1.537 Epoch 2 Batch 789/1077 - Train Accuracy: 0.858, Validation Accuracy: 0.862, Loss: 1.581 Epoch 2 Batch 790/1077 - Train Accuracy: 0.814, Validation Accuracy: 0.858, Loss: 1.501 Epoch 2 Batch 791/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.870, Loss: 1.562 Epoch 2 Batch 792/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.874, Loss: 1.507 Epoch 2 Batch 793/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.864, Loss: 1.571 Epoch 2 Batch 794/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.861, Loss: 1.508 Epoch 2 Batch 795/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.864, Loss: 1.545 Epoch 2 Batch 796/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.871, Loss: 1.478 Epoch 2 Batch 797/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.882, Loss: 1.534 Epoch 2 Batch 798/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.878, Loss: 1.559 Epoch 2 Batch 799/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.872, Loss: 1.535 Epoch 2 Batch 800/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.875, Loss: 1.528 Epoch 2 Batch 801/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.868, Loss: 1.506 Epoch 2 Batch 802/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.866, Loss: 1.546 Epoch 2 Batch 803/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.874, Loss: 1.512 Epoch 2 Batch 804/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.874, Loss: 1.558 Epoch 2 Batch 805/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.876, Loss: 1.543 Epoch 2 Batch 806/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.881, Loss: 1.498 Epoch 2 Batch 807/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.884, Loss: 1.532 Epoch 2 Batch 808/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.870, Loss: 1.505 Epoch 2 Batch 809/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.881, Loss: 1.556 Epoch 2 Batch 810/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.866, Loss: 1.500 Epoch 2 Batch 811/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.857, Loss: 1.531 Epoch 2 Batch 812/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.852, Loss: 1.543 Epoch 2 Batch 813/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.854, Loss: 1.494 Epoch 2 Batch 814/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.857, Loss: 1.509 Epoch 2 Batch 815/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.859, Loss: 1.521 Epoch 2 Batch 816/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.869, Loss: 1.539 Epoch 2 Batch 817/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.863, Loss: 1.630 Epoch 2 Batch 818/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.877, Loss: 1.549 Epoch 2 Batch 819/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.882, Loss: 1.573 Epoch 2 Batch 820/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.874, Loss: 1.549 Epoch 2 Batch 821/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.875, Loss: 1.567 Epoch 2 Batch 822/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.877, Loss: 1.445 Epoch 2 Batch 823/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.871, Loss: 1.495 Epoch 2 Batch 824/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.867, Loss: 1.541 Epoch 2 Batch 825/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.868, Loss: 1.566 Epoch 2 Batch 826/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.865, Loss: 1.493 Epoch 2 Batch 827/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.864, Loss: 1.579 Epoch 2 Batch 828/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.858, Loss: 1.424 Epoch 2 Batch 829/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.864, Loss: 1.495 Epoch 2 Batch 830/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.862, Loss: 1.518 Epoch 2 Batch 831/1077 - Train Accuracy: 0.844, Validation Accuracy: 0.866, Loss: 1.531 Epoch 2 Batch 832/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.870, Loss: 1.548 Epoch 2 Batch 833/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.872, Loss: 1.562 Epoch 2 Batch 834/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.868, Loss: 1.464 Epoch 2 Batch 835/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.869, Loss: 1.509 Epoch 2 Batch 836/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.872, Loss: 1.519 Epoch 2 Batch 837/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.876, Loss: 1.519 Epoch 2 Batch 838/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.876, Loss: 1.554 Epoch 2 Batch 839/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.881, Loss: 1.474 Epoch 2 Batch 840/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.878, Loss: 1.502 Epoch 2 Batch 841/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.878, Loss: 1.624 Epoch 2 Batch 842/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.876, Loss: 1.534 Epoch 2 Batch 843/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.879, Loss: 1.517 Epoch 2 Batch 844/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.880, Loss: 1.480 Epoch 2 Batch 845/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.873, Loss: 1.499 Epoch 2 Batch 846/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.870, Loss: 1.578 Epoch 2 Batch 847/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.872, Loss: 1.556 Epoch 2 Batch 848/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.877, Loss: 1.494 Epoch 2 Batch 849/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.870, Loss: 1.515 Epoch 2 Batch 850/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.871, Loss: 1.595 Epoch 2 Batch 851/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.870, Loss: 1.571 Epoch 2 Batch 852/1077 - Train Accuracy: 0.876, Validation Accuracy: 0.874, Loss: 1.524 Epoch 2 Batch 853/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.888, Loss: 1.468 Epoch 2 Batch 854/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.886, Loss: 1.539 Epoch 2 Batch 855/1077 - Train Accuracy: 0.858, Validation Accuracy: 0.886, Loss: 1.564 Epoch 2 Batch 856/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.890, Loss: 1.558 Epoch 2 Batch 857/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.884, Loss: 1.499 Epoch 2 Batch 858/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.890, Loss: 1.452 Epoch 2 Batch 859/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.893, Loss: 1.532 Epoch 2 Batch 860/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.886, Loss: 1.556 Epoch 2 Batch 861/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.887, Loss: 1.557 Epoch 2 Batch 862/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.884, Loss: 1.501 Epoch 2 Batch 863/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.883, Loss: 1.543 Epoch 2 Batch 864/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.882, Loss: 1.559 Epoch 2 Batch 865/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.883, Loss: 1.528 Epoch 2 Batch 866/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.881, Loss: 1.547 Epoch 2 Batch 867/1077 - Train Accuracy: 0.844, Validation Accuracy: 0.877, Loss: 1.607 Epoch 2 Batch 868/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.872, Loss: 1.558 Epoch 2 Batch 869/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.877, Loss: 1.594 Epoch 2 Batch 870/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.869, Loss: 1.549 Epoch 2 Batch 871/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.867, Loss: 1.480 Epoch 2 Batch 872/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.874, Loss: 1.492 Epoch 2 Batch 873/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.869, Loss: 1.514 Epoch 2 Batch 874/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.874, Loss: 1.483 Epoch 2 Batch 875/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.870, Loss: 1.552 Epoch 2 Batch 876/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.870, Loss: 1.568 Epoch 2 Batch 877/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.878, Loss: 1.529 Epoch 2 Batch 878/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.873, Loss: 1.505 Epoch 2 Batch 879/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.888, Loss: 1.579 Epoch 2 Batch 880/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.886, Loss: 1.492 Epoch 2 Batch 881/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.883, Loss: 1.613 Epoch 2 Batch 882/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.879, Loss: 1.488 Epoch 2 Batch 883/1077 - Train Accuracy: 0.851, Validation Accuracy: 0.874, Loss: 1.561 Epoch 2 Batch 884/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.875, Loss: 1.467 Epoch 2 Batch 885/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.873, Loss: 1.391 Epoch 2 Batch 886/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.875, Loss: 1.528 Epoch 2 Batch 887/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.878, Loss: 1.623 Epoch 2 Batch 888/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.873, Loss: 1.535 Epoch 2 Batch 889/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.852, Loss: 1.474 Epoch 2 Batch 890/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.853, Loss: 1.508 Epoch 2 Batch 891/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.851, Loss: 1.503 Epoch 2 Batch 892/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.861, Loss: 1.508 Epoch 2 Batch 893/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.860, Loss: 1.527 Epoch 2 Batch 894/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.865, Loss: 1.533 Epoch 2 Batch 895/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.868, Loss: 1.489 Epoch 2 Batch 896/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.866, Loss: 1.521 Epoch 2 Batch 897/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.872, Loss: 1.534 Epoch 2 Batch 898/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.879, Loss: 1.526 Epoch 2 Batch 899/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.880, Loss: 1.459 Epoch 2 Batch 900/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.876, Loss: 1.483 Epoch 2 Batch 901/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.874, Loss: 1.540 Epoch 2 Batch 902/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.875, Loss: 1.607 Epoch 2 Batch 903/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.873, Loss: 1.595 Epoch 2 Batch 904/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.879, Loss: 1.561 Epoch 2 Batch 905/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.876, Loss: 1.570 Epoch 2 Batch 906/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.881, Loss: 1.539 Epoch 2 Batch 907/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.886, Loss: 1.590 Epoch 2 Batch 908/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.890, Loss: 1.516 Epoch 2 Batch 909/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.887, Loss: 1.538 Epoch 2 Batch 910/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.883, Loss: 1.520 Epoch 2 Batch 911/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.882, Loss: 1.533 Epoch 2 Batch 912/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.886, Loss: 1.444 Epoch 2 Batch 913/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.866, Loss: 1.592 Epoch 2 Batch 914/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.860, Loss: 1.479 Epoch 2 Batch 915/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.860, Loss: 1.466 Epoch 2 Batch 916/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.852, Loss: 1.452 Epoch 2 Batch 917/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.852, Loss: 1.508 Epoch 2 Batch 918/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.857, Loss: 1.539 Epoch 2 Batch 919/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.856, Loss: 1.582 Epoch 2 Batch 920/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.858, Loss: 1.550 Epoch 2 Batch 921/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.853, Loss: 1.518 Epoch 2 Batch 922/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.854, Loss: 1.537 Epoch 2 Batch 923/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.864, Loss: 1.534 Epoch 2 Batch 924/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.877, Loss: 1.508 Epoch 2 Batch 925/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.882, Loss: 1.465 Epoch 2 Batch 926/1077 - Train Accuracy: 0.839, Validation Accuracy: 0.881, Loss: 1.564 Epoch 2 Batch 927/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.876, Loss: 1.598 Epoch 2 Batch 928/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.880, Loss: 1.501 Epoch 2 Batch 929/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.877, Loss: 1.566 Epoch 2 Batch 930/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.879, Loss: 1.515 Epoch 2 Batch 931/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.877, Loss: 1.571 Epoch 2 Batch 932/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.877, Loss: 1.536 Epoch 2 Batch 933/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.880, Loss: 1.446 Epoch 2 Batch 934/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.866, Loss: 1.523 Epoch 2 Batch 935/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.862, Loss: 1.526 Epoch 2 Batch 936/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.862, Loss: 1.575 Epoch 2 Batch 937/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.855, Loss: 1.586 Epoch 2 Batch 938/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.859, Loss: 1.578 Epoch 2 Batch 939/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.854, Loss: 1.511 Epoch 2 Batch 940/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.858, Loss: 1.452 Epoch 2 Batch 941/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.862, Loss: 1.530 Epoch 2 Batch 942/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.852, Loss: 1.528 Epoch 2 Batch 943/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.849, Loss: 1.509 Epoch 2 Batch 944/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.852, Loss: 1.508 Epoch 2 Batch 945/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.855, Loss: 1.509 Epoch 2 Batch 946/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.864, Loss: 1.500 Epoch 2 Batch 947/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.870, Loss: 1.529 Epoch 2 Batch 948/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.865, Loss: 1.583 Epoch 2 Batch 949/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.882, Loss: 1.595 Epoch 2 Batch 950/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.871, Loss: 1.597 Epoch 2 Batch 951/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.875, Loss: 1.489 Epoch 2 Batch 952/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.874, Loss: 1.587 Epoch 2 Batch 953/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.879, Loss: 1.588 Epoch 2 Batch 954/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.882, Loss: 1.519 Epoch 2 Batch 955/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.889, Loss: 1.557 Epoch 2 Batch 956/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.884, Loss: 1.483 Epoch 2 Batch 957/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.882, Loss: 1.519 Epoch 2 Batch 958/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.878, Loss: 1.422 Epoch 2 Batch 959/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.870, Loss: 1.476 Epoch 2 Batch 960/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.853, Loss: 1.526 Epoch 2 Batch 961/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.853, Loss: 1.508 Epoch 2 Batch 962/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.851, Loss: 1.517 Epoch 2 Batch 963/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.862, Loss: 1.547 Epoch 2 Batch 964/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.864, Loss: 1.452 Epoch 2 Batch 965/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.872, Loss: 1.483 Epoch 2 Batch 966/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.873, Loss: 1.537 Epoch 2 Batch 967/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.871, Loss: 1.646 Epoch 2 Batch 968/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.867, Loss: 1.536 Epoch 2 Batch 969/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.865, Loss: 1.568 Epoch 2 Batch 970/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.879, Loss: 1.574 Epoch 2 Batch 971/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.874, Loss: 1.566 Epoch 2 Batch 972/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.879, Loss: 1.507 Epoch 2 Batch 973/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.878, Loss: 1.517 Epoch 2 Batch 974/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.869, Loss: 1.550 Epoch 2 Batch 975/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.872, Loss: 1.474 Epoch 2 Batch 976/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.864, Loss: 1.524 Epoch 2 Batch 977/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.865, Loss: 1.544 Epoch 2 Batch 978/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.871, Loss: 1.604 Epoch 2 Batch 979/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.875, Loss: 1.570 Epoch 2 Batch 980/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.874, Loss: 1.498 Epoch 2 Batch 981/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.880, Loss: 1.469 Epoch 2 Batch 982/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.871, Loss: 1.591 Epoch 2 Batch 983/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.874, Loss: 1.545 Epoch 2 Batch 984/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.874, Loss: 1.593 Epoch 2 Batch 985/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.878, Loss: 1.522 Epoch 2 Batch 986/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.865, Loss: 1.541 Epoch 2 Batch 987/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.869, Loss: 1.472 Epoch 2 Batch 988/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.879, Loss: 1.498 Epoch 2 Batch 989/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.870, Loss: 1.559 Epoch 2 Batch 990/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.873, Loss: 1.554 Epoch 2 Batch 991/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.864, Loss: 1.498 Epoch 2 Batch 992/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.874, Loss: 1.489 Epoch 2 Batch 993/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.876, Loss: 1.515 Epoch 2 Batch 994/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.876, Loss: 1.528 Epoch 2 Batch 995/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.869, Loss: 1.526 Epoch 2 Batch 996/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.872, Loss: 1.437 Epoch 2 Batch 997/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.872, Loss: 1.484 Epoch 2 Batch 998/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.886, Loss: 1.606 Epoch 2 Batch 999/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.882, Loss: 1.529 Epoch 2 Batch 1000/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.888, Loss: 1.539 Epoch 2 Batch 1001/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.891, Loss: 1.499 Epoch 2 Batch 1002/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.893, Loss: 1.522 Epoch 2 Batch 1003/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.890, Loss: 1.568 Epoch 2 Batch 1004/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.891, Loss: 1.521 Epoch 2 Batch 1005/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.881, Loss: 1.434 Epoch 2 Batch 1006/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.886, Loss: 1.481 Epoch 2 Batch 1007/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.893, Loss: 1.522 Epoch 2 Batch 1008/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.894, Loss: 1.497 Epoch 2 Batch 1009/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.896, Loss: 1.525 Epoch 2 Batch 1010/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.891, Loss: 1.496 Epoch 2 Batch 1011/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.889, Loss: 1.514 Epoch 2 Batch 1012/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.888, Loss: 1.487 Epoch 2 Batch 1013/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.888, Loss: 1.490 Epoch 2 Batch 1014/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.887, Loss: 1.560 Epoch 2 Batch 1015/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.888, Loss: 1.576 Epoch 2 Batch 1016/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.884, Loss: 1.569 Epoch 2 Batch 1017/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.882, Loss: 1.469 Epoch 2 Batch 1018/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.877, Loss: 1.523 Epoch 2 Batch 1019/1077 - Train Accuracy: 0.858, Validation Accuracy: 0.878, Loss: 1.511 Epoch 2 Batch 1020/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.867, Loss: 1.508 Epoch 2 Batch 1021/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.878, Loss: 1.481 Epoch 2 Batch 1022/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.869, Loss: 1.537 Epoch 2 Batch 1023/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.874, Loss: 1.552 Epoch 2 Batch 1024/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.878, Loss: 1.520 Epoch 2 Batch 1025/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.875, Loss: 1.568 Epoch 2 Batch 1026/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.879, Loss: 1.541 Epoch 2 Batch 1027/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.879, Loss: 1.567 Epoch 2 Batch 1028/1077 - Train Accuracy: 0.844, Validation Accuracy: 0.897, Loss: 1.529 Epoch 2 Batch 1029/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.898, Loss: 1.596 Epoch 2 Batch 1030/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.905, Loss: 1.478 Epoch 2 Batch 1031/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.898, Loss: 1.488 Epoch 2 Batch 1032/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.884, Loss: 1.472 Epoch 2 Batch 1033/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.883, Loss: 1.562 Epoch 2 Batch 1034/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.879, Loss: 1.522 Epoch 2 Batch 1035/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.876, Loss: 1.501 Epoch 2 Batch 1036/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.871, Loss: 1.527 Epoch 2 Batch 1037/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.870, Loss: 1.509 Epoch 2 Batch 1038/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.870, Loss: 1.408 Epoch 2 Batch 1039/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.870, Loss: 1.499 Epoch 2 Batch 1040/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.863, Loss: 1.560 Epoch 2 Batch 1041/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.871, Loss: 1.604 Epoch 2 Batch 1042/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.871, Loss: 1.569 Epoch 2 Batch 1043/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.870, Loss: 1.508 Epoch 2 Batch 1044/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.870, Loss: 1.507 Epoch 2 Batch 1045/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.876, Loss: 1.577 Epoch 2 Batch 1046/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.874, Loss: 1.585 Epoch 2 Batch 1047/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.870, Loss: 1.548 Epoch 2 Batch 1048/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.881, Loss: 1.454 Epoch 2 Batch 1049/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.890, Loss: 1.510 Epoch 2 Batch 1050/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.885, Loss: 1.422 Epoch 2 Batch 1051/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.882, Loss: 1.478 Epoch 2 Batch 1052/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.881, Loss: 1.446 Epoch 2 Batch 1053/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.876, Loss: 1.488 Epoch 2 Batch 1054/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.863, Loss: 1.545 Epoch 2 Batch 1055/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.869, Loss: 1.567 Epoch 2 Batch 1056/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.873, Loss: 1.537 Epoch 2 Batch 1057/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.874, Loss: 1.473 Epoch 2 Batch 1058/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.885, Loss: 1.487 Epoch 2 Batch 1059/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.880, Loss: 1.503 Epoch 2 Batch 1060/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.885, Loss: 1.523 Epoch 2 Batch 1061/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.890, Loss: 1.505 Epoch 2 Batch 1062/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.891, Loss: 1.503 Epoch 2 Batch 1063/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.886, Loss: 1.454 Epoch 2 Batch 1064/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.885, Loss: 1.508 Epoch 2 Batch 1065/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.867, Loss: 1.471 Epoch 2 Batch 1066/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.863, Loss: 1.515 Epoch 2 Batch 1067/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.856, Loss: 1.487 Epoch 2 Batch 1068/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.849, Loss: 1.500 Epoch 2 Batch 1069/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.862, Loss: 1.571 Epoch 2 Batch 1070/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.865, Loss: 1.466 Epoch 2 Batch 1071/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.869, Loss: 1.465 Epoch 2 Batch 1072/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.869, Loss: 1.503 Epoch 2 Batch 1073/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.876, Loss: 1.493 Epoch 2 Batch 1074/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.890, Loss: 1.469 Epoch 2 Batch 1075/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.892, Loss: 1.479 Epoch 3 Batch 0/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.893, Loss: 1.454 Epoch 3 Batch 1/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.885, Loss: 1.450 Epoch 3 Batch 2/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.881, Loss: 1.522 Epoch 3 Batch 3/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.874, Loss: 1.487 Epoch 3 Batch 4/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.881, Loss: 1.482 Epoch 3 Batch 5/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.880, Loss: 1.554 Epoch 3 Batch 6/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.882, Loss: 1.507 Epoch 3 Batch 7/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.885, Loss: 1.516 Epoch 3 Batch 8/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.887, Loss: 1.477 Epoch 3 Batch 9/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.889, Loss: 1.498 Epoch 3 Batch 10/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.889, Loss: 1.608 Epoch 3 Batch 11/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.880, Loss: 1.486 Epoch 3 Batch 12/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.867, Loss: 1.545 Epoch 3 Batch 13/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.878, Loss: 1.569 Epoch 3 Batch 14/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.869, Loss: 1.555 Epoch 3 Batch 15/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.869, Loss: 1.589 Epoch 3 Batch 16/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.871, Loss: 1.524 Epoch 3 Batch 17/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.866, Loss: 1.522 Epoch 3 Batch 18/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.861, Loss: 1.509 Epoch 3 Batch 19/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.861, Loss: 1.520 Epoch 3 Batch 20/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.856, Loss: 1.546 Epoch 3 Batch 21/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.851, Loss: 1.549 Epoch 3 Batch 22/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.860, Loss: 1.486 Epoch 3 Batch 23/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.864, Loss: 1.447 Epoch 3 Batch 24/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.869, Loss: 1.477 Epoch 3 Batch 25/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.868, Loss: 1.490 Epoch 3 Batch 26/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.866, Loss: 1.575 Epoch 3 Batch 27/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.863, Loss: 1.583 Epoch 3 Batch 28/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.851, Loss: 1.541 Epoch 3 Batch 29/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.849, Loss: 1.494 Epoch 3 Batch 30/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.846, Loss: 1.432 Epoch 3 Batch 31/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.863, Loss: 1.535 Epoch 3 Batch 32/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.863, Loss: 1.536 Epoch 3 Batch 33/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.870, Loss: 1.401 Epoch 3 Batch 34/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.872, Loss: 1.534 Epoch 3 Batch 35/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.870, Loss: 1.579 Epoch 3 Batch 36/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.875, Loss: 1.457 Epoch 3 Batch 37/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.873, Loss: 1.545 Epoch 3 Batch 38/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.873, Loss: 1.559 Epoch 3 Batch 39/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.873, Loss: 1.499 Epoch 3 Batch 40/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.879, Loss: 1.487 Epoch 3 Batch 41/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.872, Loss: 1.504 Epoch 3 Batch 42/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.879, Loss: 1.498 Epoch 3 Batch 43/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.876, Loss: 1.537 Epoch 3 Batch 44/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.879, Loss: 1.529 Epoch 3 Batch 45/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.864, Loss: 1.602 Epoch 3 Batch 46/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.869, Loss: 1.487 Epoch 3 Batch 47/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.867, Loss: 1.510 Epoch 3 Batch 48/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.867, Loss: 1.584 Epoch 3 Batch 49/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.862, Loss: 1.473 Epoch 3 Batch 50/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.879, Loss: 1.495 Epoch 3 Batch 51/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.877, Loss: 1.489 Epoch 3 Batch 52/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.877, Loss: 1.472 Epoch 3 Batch 53/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.882, Loss: 1.572 Epoch 3 Batch 54/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.879, Loss: 1.584 Epoch 3 Batch 55/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.876, Loss: 1.464 Epoch 3 Batch 56/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.880, Loss: 1.434 Epoch 3 Batch 57/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.875, Loss: 1.488 Epoch 3 Batch 58/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.878, Loss: 1.512 Epoch 3 Batch 59/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.875, Loss: 1.525 Epoch 3 Batch 60/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.863, Loss: 1.596 Epoch 3 Batch 61/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.854, Loss: 1.556 Epoch 3 Batch 62/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.858, Loss: 1.538 Epoch 3 Batch 63/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.855, Loss: 1.530 Epoch 3 Batch 64/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.860, Loss: 1.450 Epoch 3 Batch 65/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.873, Loss: 1.459 Epoch 3 Batch 66/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.870, Loss: 1.453 Epoch 3 Batch 67/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.874, Loss: 1.565 Epoch 3 Batch 68/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.879, Loss: 1.498 Epoch 3 Batch 69/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.871, Loss: 1.617 Epoch 3 Batch 70/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.882, Loss: 1.472 Epoch 3 Batch 71/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.875, Loss: 1.493 Epoch 3 Batch 72/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.868, Loss: 1.530 Epoch 3 Batch 73/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.867, Loss: 1.486 Epoch 3 Batch 74/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.871, Loss: 1.514 Epoch 3 Batch 75/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.876, Loss: 1.528 Epoch 3 Batch 76/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.870, Loss: 1.540 Epoch 3 Batch 77/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.876, Loss: 1.466 Epoch 3 Batch 78/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.873, Loss: 1.506 Epoch 3 Batch 79/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.862, Loss: 1.478 Epoch 3 Batch 80/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.867, Loss: 1.471 Epoch 3 Batch 81/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.872, Loss: 1.508 Epoch 3 Batch 82/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.878, Loss: 1.502 Epoch 3 Batch 83/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.879, Loss: 1.498 Epoch 3 Batch 84/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.887, Loss: 1.491 Epoch 3 Batch 85/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.891, Loss: 1.447 Epoch 3 Batch 86/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.887, Loss: 1.431 Epoch 3 Batch 87/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.877, Loss: 1.531 Epoch 3 Batch 88/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.877, Loss: 1.475 Epoch 3 Batch 89/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.876, Loss: 1.553 Epoch 3 Batch 90/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.884, Loss: 1.467 Epoch 3 Batch 91/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.881, Loss: 1.517 Epoch 3 Batch 92/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.890, Loss: 1.554 Epoch 3 Batch 93/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.891, Loss: 1.499 Epoch 3 Batch 94/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.885, Loss: 1.521 Epoch 3 Batch 95/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.880, Loss: 1.546 Epoch 3 Batch 96/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.882, Loss: 1.466 Epoch 3 Batch 97/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.887, Loss: 1.513 Epoch 3 Batch 98/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.872, Loss: 1.442 Epoch 3 Batch 99/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.873, Loss: 1.503 Epoch 3 Batch 100/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.883, Loss: 1.419 Epoch 3 Batch 101/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.897, Loss: 1.546 Epoch 3 Batch 102/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.889, Loss: 1.490 Epoch 3 Batch 103/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.893, Loss: 1.495 Epoch 3 Batch 104/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.887, Loss: 1.536 Epoch 3 Batch 105/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.881, Loss: 1.454 Epoch 3 Batch 106/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.871, Loss: 1.543 Epoch 3 Batch 107/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.875, Loss: 1.492 Epoch 3 Batch 108/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.871, Loss: 1.495 Epoch 3 Batch 109/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.865, Loss: 1.455 Epoch 3 Batch 110/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.871, Loss: 1.486 Epoch 3 Batch 111/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.864, Loss: 1.517 Epoch 3 Batch 112/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.863, Loss: 1.526 Epoch 3 Batch 113/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.851, Loss: 1.478 Epoch 3 Batch 114/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.857, Loss: 1.418 Epoch 3 Batch 115/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.866, Loss: 1.551 Epoch 3 Batch 116/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.870, Loss: 1.476 Epoch 3 Batch 117/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.881, Loss: 1.546 Epoch 3 Batch 118/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.892, Loss: 1.531 Epoch 3 Batch 119/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.886, Loss: 1.507 Epoch 3 Batch 120/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.889, Loss: 1.407 Epoch 3 Batch 121/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.891, Loss: 1.460 Epoch 3 Batch 122/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.891, Loss: 1.485 Epoch 3 Batch 123/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.888, Loss: 1.455 Epoch 3 Batch 124/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.887, Loss: 1.552 Epoch 3 Batch 125/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.891, Loss: 1.525 Epoch 3 Batch 126/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.888, Loss: 1.480 Epoch 3 Batch 127/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.892, Loss: 1.502 Epoch 3 Batch 128/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.891, Loss: 1.509 Epoch 3 Batch 129/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.890, Loss: 1.608 Epoch 3 Batch 130/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.878, Loss: 1.558 Epoch 3 Batch 131/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.873, Loss: 1.561 Epoch 3 Batch 132/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.857, Loss: 1.467 Epoch 3 Batch 133/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.862, Loss: 1.503 Epoch 3 Batch 134/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.857, Loss: 1.529 Epoch 3 Batch 135/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.862, Loss: 1.443 Epoch 3 Batch 136/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.863, Loss: 1.522 Epoch 3 Batch 137/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.863, Loss: 1.458 Epoch 3 Batch 138/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.870, Loss: 1.475 Epoch 3 Batch 139/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.879, Loss: 1.527 Epoch 3 Batch 140/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.875, Loss: 1.517 Epoch 3 Batch 141/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.875, Loss: 1.518 Epoch 3 Batch 142/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.885, Loss: 1.559 Epoch 3 Batch 143/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.883, Loss: 1.524 Epoch 3 Batch 144/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.885, Loss: 1.576 Epoch 3 Batch 145/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.879, Loss: 1.536 Epoch 3 Batch 146/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.879, Loss: 1.471 Epoch 3 Batch 147/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.884, Loss: 1.472 Epoch 3 Batch 148/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.886, Loss: 1.509 Epoch 3 Batch 149/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.884, Loss: 1.466 Epoch 3 Batch 150/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.886, Loss: 1.458 Epoch 3 Batch 151/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.883, Loss: 1.546 Epoch 3 Batch 152/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.876, Loss: 1.580 Epoch 3 Batch 153/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.876, Loss: 1.464 Epoch 3 Batch 154/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.871, Loss: 1.538 Epoch 3 Batch 155/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.868, Loss: 1.493 Epoch 3 Batch 156/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.868, Loss: 1.436 Epoch 3 Batch 157/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.870, Loss: 1.452 Epoch 3 Batch 158/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.875, Loss: 1.503 Epoch 3 Batch 159/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.877, Loss: 1.473 Epoch 3 Batch 160/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.873, Loss: 1.517 Epoch 3 Batch 161/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.881, Loss: 1.491 Epoch 3 Batch 162/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.885, Loss: 1.492 Epoch 3 Batch 163/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.876, Loss: 1.580 Epoch 3 Batch 164/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.880, Loss: 1.516 Epoch 3 Batch 165/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.891, Loss: 1.419 Epoch 3 Batch 166/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.876, Loss: 1.521 Epoch 3 Batch 167/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.874, Loss: 1.480 Epoch 3 Batch 168/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.873, Loss: 1.498 Epoch 3 Batch 169/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.876, Loss: 1.489 Epoch 3 Batch 170/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.875, Loss: 1.435 Epoch 3 Batch 171/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.876, Loss: 1.429 Epoch 3 Batch 172/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.879, Loss: 1.450 Epoch 3 Batch 173/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.873, Loss: 1.558 Epoch 3 Batch 174/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.866, Loss: 1.469 Epoch 3 Batch 175/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.867, Loss: 1.574 Epoch 3 Batch 176/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.867, Loss: 1.522 Epoch 3 Batch 177/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.874, Loss: 1.573 Epoch 3 Batch 178/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.868, Loss: 1.505 Epoch 3 Batch 179/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.872, Loss: 1.518 Epoch 3 Batch 180/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.877, Loss: 1.528 Epoch 3 Batch 181/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.874, Loss: 1.498 Epoch 3 Batch 182/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.873, Loss: 1.553 Epoch 3 Batch 183/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.870, Loss: 1.442 Epoch 3 Batch 184/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.863, Loss: 1.504 Epoch 3 Batch 185/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.867, Loss: 1.506 Epoch 3 Batch 186/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.863, Loss: 1.464 Epoch 3 Batch 187/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.862, Loss: 1.505 Epoch 3 Batch 188/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.861, Loss: 1.551 Epoch 3 Batch 189/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.860, Loss: 1.449 Epoch 3 Batch 190/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.862, Loss: 1.422 Epoch 3 Batch 191/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.862, Loss: 1.476 Epoch 3 Batch 192/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.871, Loss: 1.558 Epoch 3 Batch 193/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.876, Loss: 1.475 Epoch 3 Batch 194/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.875, Loss: 1.530 Epoch 3 Batch 195/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.869, Loss: 1.585 Epoch 3 Batch 196/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.875, Loss: 1.520 Epoch 3 Batch 197/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.879, Loss: 1.450 Epoch 3 Batch 198/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.877, Loss: 1.558 Epoch 3 Batch 199/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.874, Loss: 1.469 Epoch 3 Batch 200/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.871, Loss: 1.482 Epoch 3 Batch 201/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.872, Loss: 1.640 Epoch 3 Batch 202/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.878, Loss: 1.427 Epoch 3 Batch 203/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.874, Loss: 1.517 Epoch 3 Batch 204/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.875, Loss: 1.554 Epoch 3 Batch 205/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.859, Loss: 1.553 Epoch 3 Batch 206/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.858, Loss: 1.494 Epoch 3 Batch 207/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.866, Loss: 1.497 Epoch 3 Batch 208/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.864, Loss: 1.540 Epoch 3 Batch 209/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.869, Loss: 1.467 Epoch 3 Batch 210/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.868, Loss: 1.450 Epoch 3 Batch 211/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.872, Loss: 1.484 Epoch 3 Batch 212/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.871, Loss: 1.452 Epoch 3 Batch 213/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.863, Loss: 1.484 Epoch 3 Batch 214/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.858, Loss: 1.511 Epoch 3 Batch 215/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.858, Loss: 1.481 Epoch 3 Batch 216/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.853, Loss: 1.423 Epoch 3 Batch 217/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.860, Loss: 1.485 Epoch 3 Batch 218/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.866, Loss: 1.580 Epoch 3 Batch 219/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.870, Loss: 1.470 Epoch 3 Batch 220/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.870, Loss: 1.511 Epoch 3 Batch 221/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.877, Loss: 1.452 Epoch 3 Batch 222/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.884, Loss: 1.412 Epoch 3 Batch 223/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.881, Loss: 1.519 Epoch 3 Batch 224/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.882, Loss: 1.541 Epoch 3 Batch 225/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.878, Loss: 1.469 Epoch 3 Batch 226/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.876, Loss: 1.538 Epoch 3 Batch 227/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.878, Loss: 1.549 Epoch 3 Batch 228/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.876, Loss: 1.571 Epoch 3 Batch 229/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.880, Loss: 1.513 Epoch 3 Batch 230/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.888, Loss: 1.446 Epoch 3 Batch 231/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.887, Loss: 1.512 Epoch 3 Batch 232/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.885, Loss: 1.490 Epoch 3 Batch 233/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.884, Loss: 1.540 Epoch 3 Batch 234/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.875, Loss: 1.603 Epoch 3 Batch 235/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.875, Loss: 1.506 Epoch 3 Batch 236/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.876, Loss: 1.483 Epoch 3 Batch 237/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.886, Loss: 1.529 Epoch 3 Batch 238/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.884, Loss: 1.433 Epoch 3 Batch 239/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.884, Loss: 1.467 Epoch 3 Batch 240/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.880, Loss: 1.460 Epoch 3 Batch 241/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.885, Loss: 1.457 Epoch 3 Batch 242/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.886, Loss: 1.533 Epoch 3 Batch 243/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.877, Loss: 1.491 Epoch 3 Batch 244/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.883, Loss: 1.503 Epoch 3 Batch 245/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.874, Loss: 1.523 Epoch 3 Batch 246/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.877, Loss: 1.466 Epoch 3 Batch 247/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.877, Loss: 1.500 Epoch 3 Batch 248/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.869, Loss: 1.480 Epoch 3 Batch 249/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.877, Loss: 1.446 Epoch 3 Batch 250/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.885, Loss: 1.514 Epoch 3 Batch 251/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.869, Loss: 1.428 Epoch 3 Batch 252/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.873, Loss: 1.469 Epoch 3 Batch 253/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.875, Loss: 1.500 Epoch 3 Batch 254/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.882, Loss: 1.548 Epoch 3 Batch 255/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.882, Loss: 1.488 Epoch 3 Batch 256/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.892, Loss: 1.470 Epoch 3 Batch 257/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.893, Loss: 1.503 Epoch 3 Batch 258/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.886, Loss: 1.429 Epoch 3 Batch 259/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.884, Loss: 1.460 Epoch 3 Batch 260/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.890, Loss: 1.527 Epoch 3 Batch 261/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.893, Loss: 1.497 Epoch 3 Batch 262/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.892, Loss: 1.533 Epoch 3 Batch 263/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.886, Loss: 1.469 Epoch 3 Batch 264/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.891, Loss: 1.534 Epoch 3 Batch 265/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.886, Loss: 1.520 Epoch 3 Batch 266/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.892, Loss: 1.546 Epoch 3 Batch 267/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.889, Loss: 1.521 Epoch 3 Batch 268/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.888, Loss: 1.536 Epoch 3 Batch 269/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.883, Loss: 1.538 Epoch 3 Batch 270/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.873, Loss: 1.502 Epoch 3 Batch 271/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.868, Loss: 1.552 Epoch 3 Batch 272/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.866, Loss: 1.520 Epoch 3 Batch 273/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.876, Loss: 1.500 Epoch 3 Batch 274/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.881, Loss: 1.469 Epoch 3 Batch 275/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.882, Loss: 1.455 Epoch 3 Batch 276/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.873, Loss: 1.458 Epoch 3 Batch 277/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.864, Loss: 1.458 Epoch 3 Batch 278/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.871, Loss: 1.600 Epoch 3 Batch 279/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.889, Loss: 1.500 Epoch 3 Batch 280/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.891, Loss: 1.532 Epoch 3 Batch 281/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.892, Loss: 1.554 Epoch 3 Batch 282/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.892, Loss: 1.476 Epoch 3 Batch 283/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.903, Loss: 1.530 Epoch 3 Batch 284/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.895, Loss: 1.524 Epoch 3 Batch 285/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.891, Loss: 1.522 Epoch 3 Batch 286/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.891, Loss: 1.537 Epoch 3 Batch 287/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.894, Loss: 1.522 Epoch 3 Batch 288/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.892, Loss: 1.456 Epoch 3 Batch 289/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.888, Loss: 1.507 Epoch 3 Batch 290/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.894, Loss: 1.531 Epoch 3 Batch 291/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.894, Loss: 1.527 Epoch 3 Batch 292/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.894, Loss: 1.552 Epoch 3 Batch 293/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.885, Loss: 1.509 Epoch 3 Batch 294/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.888, Loss: 1.522 Epoch 3 Batch 295/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.887, Loss: 1.524 Epoch 3 Batch 296/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.890, Loss: 1.493 Epoch 3 Batch 297/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.887, Loss: 1.500 Epoch 3 Batch 298/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.889, Loss: 1.602 Epoch 3 Batch 299/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.891, Loss: 1.476 Epoch 3 Batch 300/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.891, Loss: 1.501 Epoch 3 Batch 301/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.885, Loss: 1.540 Epoch 3 Batch 302/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.880, Loss: 1.453 Epoch 3 Batch 303/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.885, Loss: 1.494 Epoch 3 Batch 304/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.882, Loss: 1.479 Epoch 3 Batch 305/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.875, Loss: 1.439 Epoch 3 Batch 306/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.877, Loss: 1.523 Epoch 3 Batch 307/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.884, Loss: 1.522 Epoch 3 Batch 308/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.880, Loss: 1.515 Epoch 3 Batch 309/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.878, Loss: 1.495 Epoch 3 Batch 310/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.866, Loss: 1.513 Epoch 3 Batch 311/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.866, Loss: 1.445 Epoch 3 Batch 312/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.866, Loss: 1.543 Epoch 3 Batch 313/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.866, Loss: 1.499 Epoch 3 Batch 314/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.871, Loss: 1.513 Epoch 3 Batch 315/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.866, Loss: 1.538 Epoch 3 Batch 316/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.867, Loss: 1.451 Epoch 3 Batch 317/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.872, Loss: 1.458 Epoch 3 Batch 318/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.868, Loss: 1.560 Epoch 3 Batch 319/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.865, Loss: 1.599 Epoch 3 Batch 320/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.859, Loss: 1.428 Epoch 3 Batch 321/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.868, Loss: 1.487 Epoch 3 Batch 322/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.868, Loss: 1.482 Epoch 3 Batch 323/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.863, Loss: 1.510 Epoch 3 Batch 324/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.863, Loss: 1.472 Epoch 3 Batch 325/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.875, Loss: 1.524 Epoch 3 Batch 326/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.876, Loss: 1.379 Epoch 3 Batch 327/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.877, Loss: 1.475 Epoch 3 Batch 328/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.875, Loss: 1.501 Epoch 3 Batch 329/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.875, Loss: 1.399 Epoch 3 Batch 330/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.876, Loss: 1.427 Epoch 3 Batch 331/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.875, Loss: 1.565 Epoch 3 Batch 332/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.875, Loss: 1.462 Epoch 3 Batch 333/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.875, Loss: 1.431 Epoch 3 Batch 334/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.880, Loss: 1.514 Epoch 3 Batch 335/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.879, Loss: 1.433 Epoch 3 Batch 336/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.877, Loss: 1.533 Epoch 3 Batch 337/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.882, Loss: 1.505 Epoch 3 Batch 338/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.882, Loss: 1.521 Epoch 3 Batch 339/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.875, Loss: 1.452 Epoch 3 Batch 340/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.876, Loss: 1.507 Epoch 3 Batch 341/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.885, Loss: 1.427 Epoch 3 Batch 342/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.880, Loss: 1.559 Epoch 3 Batch 343/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.869, Loss: 1.537 Epoch 3 Batch 344/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.864, Loss: 1.501 Epoch 3 Batch 345/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.871, Loss: 1.478 Epoch 3 Batch 346/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.878, Loss: 1.453 Epoch 3 Batch 347/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.867, Loss: 1.493 Epoch 3 Batch 348/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.855, Loss: 1.388 Epoch 3 Batch 349/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.858, Loss: 1.451 Epoch 3 Batch 350/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.853, Loss: 1.468 Epoch 3 Batch 351/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.856, Loss: 1.496 Epoch 3 Batch 352/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.860, Loss: 1.451 Epoch 3 Batch 353/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.864, Loss: 1.480 Epoch 3 Batch 354/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.870, Loss: 1.474 Epoch 3 Batch 355/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.875, Loss: 1.502 Epoch 3 Batch 356/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.879, Loss: 1.432 Epoch 3 Batch 357/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.869, Loss: 1.449 Epoch 3 Batch 358/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.862, Loss: 1.398 Epoch 3 Batch 359/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.863, Loss: 1.529 Epoch 3 Batch 360/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.869, Loss: 1.446 Epoch 3 Batch 361/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.879, Loss: 1.513 Epoch 3 Batch 362/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.877, Loss: 1.455 Epoch 3 Batch 363/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.886, Loss: 1.434 Epoch 3 Batch 364/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.886, Loss: 1.534 Epoch 3 Batch 365/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.889, Loss: 1.446 Epoch 3 Batch 366/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.891, Loss: 1.451 Epoch 3 Batch 367/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.893, Loss: 1.515 Epoch 3 Batch 368/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.886, Loss: 1.471 Epoch 3 Batch 369/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.874, Loss: 1.421 Epoch 3 Batch 370/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.870, Loss: 1.484 Epoch 3 Batch 371/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.877, Loss: 1.447 Epoch 3 Batch 372/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.872, Loss: 1.452 Epoch 3 Batch 373/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.879, Loss: 1.466 Epoch 3 Batch 374/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.891, Loss: 1.548 Epoch 3 Batch 375/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.889, Loss: 1.506 Epoch 3 Batch 376/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.888, Loss: 1.502 Epoch 3 Batch 377/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.895, Loss: 1.586 Epoch 3 Batch 378/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.895, Loss: 1.593 Epoch 3 Batch 379/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.885, Loss: 1.463 Epoch 3 Batch 380/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.880, Loss: 1.460 Epoch 3 Batch 381/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.884, Loss: 1.572 Epoch 3 Batch 382/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.882, Loss: 1.505 Epoch 3 Batch 383/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.883, Loss: 1.410 Epoch 3 Batch 384/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.889, Loss: 1.520 Epoch 3 Batch 385/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.894, Loss: 1.390 Epoch 3 Batch 386/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.887, Loss: 1.457 Epoch 3 Batch 387/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.889, Loss: 1.480 Epoch 3 Batch 388/1077 - Train Accuracy: 0.876, Validation Accuracy: 0.890, Loss: 1.457 Epoch 3 Batch 389/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.891, Loss: 1.477 Epoch 3 Batch 390/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.896, Loss: 1.549 Epoch 3 Batch 391/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.891, Loss: 1.487 Epoch 3 Batch 392/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.891, Loss: 1.532 Epoch 3 Batch 393/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.890, Loss: 1.473 Epoch 3 Batch 394/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.901, Loss: 1.499 Epoch 3 Batch 395/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.897, Loss: 1.493 Epoch 3 Batch 396/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.890, Loss: 1.505 Epoch 3 Batch 397/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.891, Loss: 1.566 Epoch 3 Batch 398/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.876, Loss: 1.525 Epoch 3 Batch 399/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.867, Loss: 1.456 Epoch 3 Batch 400/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.875, Loss: 1.479 Epoch 3 Batch 401/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.886, Loss: 1.437 Epoch 3 Batch 402/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.887, Loss: 1.477 Epoch 3 Batch 403/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.884, Loss: 1.508 Epoch 3 Batch 404/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.886, Loss: 1.479 Epoch 3 Batch 405/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.901, Loss: 1.445 Epoch 3 Batch 406/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.902, Loss: 1.442 Epoch 3 Batch 407/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.895, Loss: 1.512 Epoch 3 Batch 408/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.897, Loss: 1.433 Epoch 3 Batch 409/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.897, Loss: 1.464 Epoch 3 Batch 410/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.885, Loss: 1.487 Epoch 3 Batch 411/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.890, Loss: 1.572 Epoch 3 Batch 412/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.897, Loss: 1.514 Epoch 3 Batch 413/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.903, Loss: 1.463 Epoch 3 Batch 414/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.893, Loss: 1.516 Epoch 3 Batch 415/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.893, Loss: 1.483 Epoch 3 Batch 416/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.894, Loss: 1.484 Epoch 3 Batch 417/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.893, Loss: 1.481 Epoch 3 Batch 418/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.891, Loss: 1.528 Epoch 3 Batch 419/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.882, Loss: 1.501 Epoch 3 Batch 420/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.877, Loss: 1.563 Epoch 3 Batch 421/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.877, Loss: 1.496 Epoch 3 Batch 422/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.877, Loss: 1.445 Epoch 3 Batch 423/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.878, Loss: 1.524 Epoch 3 Batch 424/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.888, Loss: 1.483 Epoch 3 Batch 425/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.900, Loss: 1.498 Epoch 3 Batch 426/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.902, Loss: 1.462 Epoch 3 Batch 427/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.904, Loss: 1.508 Epoch 3 Batch 428/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.908, Loss: 1.453 Epoch 3 Batch 429/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.902, Loss: 1.436 Epoch 3 Batch 430/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.901, Loss: 1.520 Epoch 3 Batch 431/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.901, Loss: 1.449 Epoch 3 Batch 432/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.907, Loss: 1.476 Epoch 3 Batch 433/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.893, Loss: 1.444 Epoch 3 Batch 434/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.897, Loss: 1.495 Epoch 3 Batch 435/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.899, Loss: 1.434 Epoch 3 Batch 436/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.893, Loss: 1.491 Epoch 3 Batch 437/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.898, Loss: 1.503 Epoch 3 Batch 438/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.893, Loss: 1.468 Epoch 3 Batch 439/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.884, Loss: 1.451 Epoch 3 Batch 440/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.879, Loss: 1.464 Epoch 3 Batch 441/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.870, Loss: 1.512 Epoch 3 Batch 442/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.866, Loss: 1.434 Epoch 3 Batch 443/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.858, Loss: 1.511 Epoch 3 Batch 444/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.870, Loss: 1.537 Epoch 3 Batch 445/1077 - Train Accuracy: 0.858, Validation Accuracy: 0.876, Loss: 1.490 Epoch 3 Batch 446/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.871, Loss: 1.503 Epoch 3 Batch 447/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.871, Loss: 1.416 Epoch 3 Batch 448/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.876, Loss: 1.437 Epoch 3 Batch 449/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.876, Loss: 1.546 Epoch 3 Batch 450/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.881, Loss: 1.438 Epoch 3 Batch 451/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.884, Loss: 1.459 Epoch 3 Batch 452/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.887, Loss: 1.561 Epoch 3 Batch 453/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.887, Loss: 1.535 Epoch 3 Batch 454/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.881, Loss: 1.445 Epoch 3 Batch 455/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.879, Loss: 1.489 Epoch 3 Batch 456/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.878, Loss: 1.494 Epoch 3 Batch 457/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.872, Loss: 1.457 Epoch 3 Batch 458/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.878, Loss: 1.504 Epoch 3 Batch 459/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.878, Loss: 1.427 Epoch 3 Batch 460/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.876, Loss: 1.500 Epoch 3 Batch 461/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.891, Loss: 1.515 Epoch 3 Batch 462/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.892, Loss: 1.549 Epoch 3 Batch 463/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.888, Loss: 1.523 Epoch 3 Batch 464/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.882, Loss: 1.505 Epoch 3 Batch 465/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.883, Loss: 1.488 Epoch 3 Batch 466/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.873, Loss: 1.580 Epoch 3 Batch 467/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.869, Loss: 1.421 Epoch 3 Batch 468/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.877, Loss: 1.416 Epoch 3 Batch 469/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.875, Loss: 1.487 Epoch 3 Batch 470/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.875, Loss: 1.407 Epoch 3 Batch 471/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.876, Loss: 1.474 Epoch 3 Batch 472/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.880, Loss: 1.485 Epoch 3 Batch 473/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.878, Loss: 1.527 Epoch 3 Batch 474/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.887, Loss: 1.428 Epoch 3 Batch 475/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.877, Loss: 1.499 Epoch 3 Batch 476/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.890, Loss: 1.478 Epoch 3 Batch 477/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.884, Loss: 1.551 Epoch 3 Batch 478/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.892, Loss: 1.473 Epoch 3 Batch 479/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.891, Loss: 1.479 Epoch 3 Batch 480/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.892, Loss: 1.477 Epoch 3 Batch 481/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.890, Loss: 1.564 Epoch 3 Batch 482/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.890, Loss: 1.529 Epoch 3 Batch 483/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.892, Loss: 1.517 Epoch 3 Batch 484/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.888, Loss: 1.511 Epoch 3 Batch 485/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.884, Loss: 1.468 Epoch 3 Batch 486/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.880, Loss: 1.436 Epoch 3 Batch 487/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.877, Loss: 1.507 Epoch 3 Batch 488/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.882, Loss: 1.529 Epoch 3 Batch 489/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.876, Loss: 1.488 Epoch 3 Batch 490/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.868, Loss: 1.577 Epoch 3 Batch 491/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.868, Loss: 1.578 Epoch 3 Batch 492/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.875, Loss: 1.497 Epoch 3 Batch 493/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.860, Loss: 1.401 Epoch 3 Batch 494/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.860, Loss: 1.461 Epoch 3 Batch 495/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.864, Loss: 1.416 Epoch 3 Batch 496/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.869, Loss: 1.579 Epoch 3 Batch 497/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.870, Loss: 1.533 Epoch 3 Batch 498/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.885, Loss: 1.477 Epoch 3 Batch 499/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.885, Loss: 1.479 Epoch 3 Batch 500/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.877, Loss: 1.490 Epoch 3 Batch 501/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.878, Loss: 1.481 Epoch 3 Batch 502/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.874, Loss: 1.511 Epoch 3 Batch 503/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.874, Loss: 1.528 Epoch 3 Batch 504/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.875, Loss: 1.408 Epoch 3 Batch 505/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.880, Loss: 1.513 Epoch 3 Batch 506/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.887, Loss: 1.443 Epoch 3 Batch 507/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.890, Loss: 1.487 Epoch 3 Batch 508/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.887, Loss: 1.557 Epoch 3 Batch 509/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.884, Loss: 1.483 Epoch 3 Batch 510/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.892, Loss: 1.512 Epoch 3 Batch 511/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.891, Loss: 1.497 Epoch 3 Batch 512/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.891, Loss: 1.511 Epoch 3 Batch 513/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.885, Loss: 1.514 Epoch 3 Batch 514/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.881, Loss: 1.502 Epoch 3 Batch 515/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.887, Loss: 1.436 Epoch 3 Batch 516/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.880, Loss: 1.463 Epoch 3 Batch 517/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.869, Loss: 1.473 Epoch 3 Batch 518/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.875, Loss: 1.423 Epoch 3 Batch 519/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.870, Loss: 1.418 Epoch 3 Batch 520/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.862, Loss: 1.453 Epoch 3 Batch 521/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.865, Loss: 1.495 Epoch 3 Batch 522/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.864, Loss: 1.465 Epoch 3 Batch 523/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.866, Loss: 1.443 Epoch 3 Batch 524/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.875, Loss: 1.515 Epoch 3 Batch 525/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.886, Loss: 1.565 Epoch 3 Batch 526/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.881, Loss: 1.471 Epoch 3 Batch 527/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.880, Loss: 1.492 Epoch 3 Batch 528/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.887, Loss: 1.451 Epoch 3 Batch 529/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.900, Loss: 1.443 Epoch 3 Batch 530/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.909, Loss: 1.490 Epoch 3 Batch 531/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.901, Loss: 1.446 Epoch 3 Batch 532/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.903, Loss: 1.478 Epoch 3 Batch 533/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.904, Loss: 1.505 Epoch 3 Batch 534/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.898, Loss: 1.472 Epoch 3 Batch 535/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.891, Loss: 1.535 Epoch 3 Batch 536/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.892, Loss: 1.477 Epoch 3 Batch 537/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.890, Loss: 1.451 Epoch 3 Batch 538/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.891, Loss: 1.544 Epoch 3 Batch 539/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.890, Loss: 1.430 Epoch 3 Batch 540/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.880, Loss: 1.482 Epoch 3 Batch 541/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.876, Loss: 1.497 Epoch 3 Batch 542/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.876, Loss: 1.462 Epoch 3 Batch 543/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.885, Loss: 1.466 Epoch 3 Batch 544/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.896, Loss: 1.540 Epoch 3 Batch 545/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.896, Loss: 1.535 Epoch 3 Batch 546/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.894, Loss: 1.446 Epoch 3 Batch 547/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.884, Loss: 1.429 Epoch 3 Batch 548/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.879, Loss: 1.509 Epoch 3 Batch 549/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.885, Loss: 1.516 Epoch 3 Batch 550/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.887, Loss: 1.522 Epoch 3 Batch 551/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.885, Loss: 1.413 Epoch 3 Batch 552/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.887, Loss: 1.486 Epoch 3 Batch 553/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.897, Loss: 1.510 Epoch 3 Batch 554/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.898, Loss: 1.379 Epoch 3 Batch 555/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.899, Loss: 1.471 Epoch 3 Batch 556/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.893, Loss: 1.447 Epoch 3 Batch 557/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.896, Loss: 1.470 Epoch 3 Batch 558/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.898, Loss: 1.473 Epoch 3 Batch 559/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.889, Loss: 1.467 Epoch 3 Batch 560/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.882, Loss: 1.503 Epoch 3 Batch 561/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.887, Loss: 1.471 Epoch 3 Batch 562/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.900, Loss: 1.402 Epoch 3 Batch 563/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.899, Loss: 1.534 Epoch 3 Batch 564/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.906, Loss: 1.476 Epoch 3 Batch 565/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.897, Loss: 1.570 Epoch 3 Batch 566/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.898, Loss: 1.451 Epoch 3 Batch 567/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.898, Loss: 1.469 Epoch 3 Batch 568/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.902, Loss: 1.504 Epoch 3 Batch 569/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.904, Loss: 1.474 Epoch 3 Batch 570/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.904, Loss: 1.466 Epoch 3 Batch 571/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.888, Loss: 1.579 Epoch 3 Batch 572/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.881, Loss: 1.507 Epoch 3 Batch 573/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.879, Loss: 1.432 Epoch 3 Batch 574/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.879, Loss: 1.502 Epoch 3 Batch 575/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.885, Loss: 1.576 Epoch 3 Batch 576/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.881, Loss: 1.435 Epoch 3 Batch 577/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.870, Loss: 1.416 Epoch 3 Batch 578/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.870, Loss: 1.441 Epoch 3 Batch 579/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.874, Loss: 1.456 Epoch 3 Batch 580/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.870, Loss: 1.453 Epoch 3 Batch 581/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.870, Loss: 1.466 Epoch 3 Batch 582/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.868, Loss: 1.437 Epoch 3 Batch 583/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.882, Loss: 1.505 Epoch 3 Batch 584/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.875, Loss: 1.482 Epoch 3 Batch 585/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.874, Loss: 1.475 Epoch 3 Batch 586/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.873, Loss: 1.491 Epoch 3 Batch 587/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.877, Loss: 1.502 Epoch 3 Batch 588/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.875, Loss: 1.497 Epoch 3 Batch 589/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.880, Loss: 1.471 Epoch 3 Batch 590/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.884, Loss: 1.554 Epoch 3 Batch 591/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.887, Loss: 1.520 Epoch 3 Batch 592/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.887, Loss: 1.507 Epoch 3 Batch 593/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.887, Loss: 1.433 Epoch 3 Batch 594/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.887, Loss: 1.438 Epoch 3 Batch 595/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.886, Loss: 1.420 Epoch 3 Batch 596/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.887, Loss: 1.500 Epoch 3 Batch 597/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.887, Loss: 1.446 Epoch 3 Batch 598/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.886, Loss: 1.473 Epoch 3 Batch 599/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.887, Loss: 1.496 Epoch 3 Batch 600/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.882, Loss: 1.486 Epoch 3 Batch 601/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.886, Loss: 1.477 Epoch 3 Batch 602/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.900, Loss: 1.450 Epoch 3 Batch 603/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.894, Loss: 1.541 Epoch 3 Batch 604/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.890, Loss: 1.554 Epoch 3 Batch 605/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.890, Loss: 1.502 Epoch 3 Batch 606/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.889, Loss: 1.517 Epoch 3 Batch 607/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.889, Loss: 1.499 Epoch 3 Batch 608/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.883, Loss: 1.479 Epoch 3 Batch 609/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.884, Loss: 1.508 Epoch 3 Batch 610/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.889, Loss: 1.445 Epoch 3 Batch 611/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.889, Loss: 1.488 Epoch 3 Batch 612/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.889, Loss: 1.435 Epoch 3 Batch 613/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.895, Loss: 1.425 Epoch 3 Batch 614/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.887, Loss: 1.504 Epoch 3 Batch 615/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.891, Loss: 1.475 Epoch 3 Batch 616/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.888, Loss: 1.515 Epoch 3 Batch 617/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.888, Loss: 1.398 Epoch 3 Batch 618/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.889, Loss: 1.531 Epoch 3 Batch 619/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.890, Loss: 1.476 Epoch 3 Batch 620/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.898, Loss: 1.469 Epoch 3 Batch 621/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.908, Loss: 1.418 Epoch 3 Batch 622/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.901, Loss: 1.441 Epoch 3 Batch 623/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.898, Loss: 1.581 Epoch 3 Batch 624/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.900, Loss: 1.485 Epoch 3 Batch 625/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.895, Loss: 1.350 Epoch 3 Batch 626/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.896, Loss: 1.513 Epoch 3 Batch 627/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.898, Loss: 1.452 Epoch 3 Batch 628/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.903, Loss: 1.439 Epoch 3 Batch 629/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.893, Loss: 1.512 Epoch 3 Batch 630/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.893, Loss: 1.480 Epoch 3 Batch 631/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.890, Loss: 1.485 Epoch 3 Batch 632/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.885, Loss: 1.413 Epoch 3 Batch 633/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.893, Loss: 1.544 Epoch 3 Batch 634/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.893, Loss: 1.454 Epoch 3 Batch 635/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.897, Loss: 1.546 Epoch 3 Batch 636/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.877, Loss: 1.465 Epoch 3 Batch 637/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.881, Loss: 1.444 Epoch 3 Batch 638/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.888, Loss: 1.481 Epoch 3 Batch 639/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.878, Loss: 1.561 Epoch 3 Batch 640/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.874, Loss: 1.497 Epoch 3 Batch 641/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.876, Loss: 1.438 Epoch 3 Batch 642/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.876, Loss: 1.498 Epoch 3 Batch 643/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.881, Loss: 1.476 Epoch 3 Batch 644/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.890, Loss: 1.459 Epoch 3 Batch 645/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.893, Loss: 1.424 Epoch 3 Batch 646/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.895, Loss: 1.440 Epoch 3 Batch 647/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.888, Loss: 1.536 Epoch 3 Batch 648/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.891, Loss: 1.526 Epoch 3 Batch 649/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.886, Loss: 1.512 Epoch 3 Batch 650/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.881, Loss: 1.530 Epoch 3 Batch 651/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.883, Loss: 1.434 Epoch 3 Batch 652/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.877, Loss: 1.570 Epoch 3 Batch 653/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.887, Loss: 1.485 Epoch 3 Batch 654/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.895, Loss: 1.516 Epoch 3 Batch 655/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.888, Loss: 1.543 Epoch 3 Batch 656/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.895, Loss: 1.446 Epoch 3 Batch 657/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.900, Loss: 1.410 Epoch 3 Batch 658/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.904, Loss: 1.499 Epoch 3 Batch 659/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.900, Loss: 1.466 Epoch 3 Batch 660/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.900, Loss: 1.462 Epoch 3 Batch 661/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.891, Loss: 1.490 Epoch 3 Batch 662/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.888, Loss: 1.465 Epoch 3 Batch 663/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.885, Loss: 1.452 Epoch 3 Batch 664/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.878, Loss: 1.488 Epoch 3 Batch 665/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.882, Loss: 1.477 Epoch 3 Batch 666/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.891, Loss: 1.467 Epoch 3 Batch 667/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.895, Loss: 1.557 Epoch 3 Batch 668/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.895, Loss: 1.542 Epoch 3 Batch 669/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.891, Loss: 1.460 Epoch 3 Batch 670/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.890, Loss: 1.529 Epoch 3 Batch 671/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.902, Loss: 1.450 Epoch 3 Batch 672/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.891, Loss: 1.428 Epoch 3 Batch 673/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.890, Loss: 1.428 Epoch 3 Batch 674/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.884, Loss: 1.518 Epoch 3 Batch 675/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.893, Loss: 1.456 Epoch 3 Batch 676/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.895, Loss: 1.511 Epoch 3 Batch 677/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.895, Loss: 1.544 Epoch 3 Batch 678/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.895, Loss: 1.474 Epoch 3 Batch 679/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.895, Loss: 1.443 Epoch 3 Batch 680/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.895, Loss: 1.445 Epoch 3 Batch 681/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.901, Loss: 1.484 Epoch 3 Batch 682/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.901, Loss: 1.438 Epoch 3 Batch 683/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.899, Loss: 1.484 Epoch 3 Batch 684/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.900, Loss: 1.442 Epoch 3 Batch 685/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.896, Loss: 1.508 Epoch 3 Batch 686/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.909, Loss: 1.416 Epoch 3 Batch 687/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.909, Loss: 1.536 Epoch 3 Batch 688/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.907, Loss: 1.442 Epoch 3 Batch 689/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.901, Loss: 1.486 Epoch 3 Batch 690/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.901, Loss: 1.479 Epoch 3 Batch 691/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.898, Loss: 1.434 Epoch 3 Batch 692/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.896, Loss: 1.487 Epoch 3 Batch 693/1077 - Train Accuracy: 0.858, Validation Accuracy: 0.893, Loss: 1.509 Epoch 3 Batch 694/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.886, Loss: 1.526 Epoch 3 Batch 695/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.885, Loss: 1.554 Epoch 3 Batch 696/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.877, Loss: 1.477 Epoch 3 Batch 697/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.877, Loss: 1.443 Epoch 3 Batch 698/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.876, Loss: 1.426 Epoch 3 Batch 699/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.874, Loss: 1.455 Epoch 3 Batch 700/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.871, Loss: 1.480 Epoch 3 Batch 701/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.873, Loss: 1.500 Epoch 3 Batch 702/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.878, Loss: 1.519 Epoch 3 Batch 703/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.895, Loss: 1.593 Epoch 3 Batch 704/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.899, Loss: 1.501 Epoch 3 Batch 705/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.900, Loss: 1.488 Epoch 3 Batch 706/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.906, Loss: 1.464 Epoch 3 Batch 707/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.893, Loss: 1.482 Epoch 3 Batch 708/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.897, Loss: 1.408 Epoch 3 Batch 709/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.893, Loss: 1.539 Epoch 3 Batch 710/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.907, Loss: 1.410 Epoch 3 Batch 711/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.904, Loss: 1.542 Epoch 3 Batch 712/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.901, Loss: 1.475 Epoch 3 Batch 713/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.893, Loss: 1.433 Epoch 3 Batch 714/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.887, Loss: 1.530 Epoch 3 Batch 715/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.896, Loss: 1.510 Epoch 3 Batch 716/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.891, Loss: 1.409 Epoch 3 Batch 717/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.898, Loss: 1.504 Epoch 3 Batch 718/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.897, Loss: 1.543 Epoch 3 Batch 719/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.890, Loss: 1.485 Epoch 3 Batch 720/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.874, Loss: 1.548 Epoch 3 Batch 721/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.884, Loss: 1.575 Epoch 3 Batch 722/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.886, Loss: 1.490 Epoch 3 Batch 723/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.894, Loss: 1.438 Epoch 3 Batch 724/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.891, Loss: 1.486 Epoch 3 Batch 725/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.890, Loss: 1.452 Epoch 3 Batch 726/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.902, Loss: 1.407 Epoch 3 Batch 727/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.908, Loss: 1.514 Epoch 3 Batch 728/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.914, Loss: 1.461 Epoch 3 Batch 729/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.904, Loss: 1.533 Epoch 3 Batch 730/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.901, Loss: 1.487 Epoch 3 Batch 731/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.902, Loss: 1.565 Epoch 3 Batch 732/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.894, Loss: 1.468 Epoch 3 Batch 733/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.889, Loss: 1.528 Epoch 3 Batch 734/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.882, Loss: 1.469 Epoch 3 Batch 735/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.880, Loss: 1.449 Epoch 3 Batch 736/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.880, Loss: 1.459 Epoch 3 Batch 737/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.880, Loss: 1.405 Epoch 3 Batch 738/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.884, Loss: 1.517 Epoch 3 Batch 739/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.882, Loss: 1.414 Epoch 3 Batch 740/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.897, Loss: 1.529 Epoch 3 Batch 741/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.907, Loss: 1.536 Epoch 3 Batch 742/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.901, Loss: 1.394 Epoch 3 Batch 743/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.906, Loss: 1.461 Epoch 3 Batch 744/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.903, Loss: 1.528 Epoch 3 Batch 745/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.898, Loss: 1.548 Epoch 3 Batch 746/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.900, Loss: 1.486 Epoch 3 Batch 747/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.909, Loss: 1.463 Epoch 3 Batch 748/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.909, Loss: 1.500 Epoch 3 Batch 749/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.902, Loss: 1.450 Epoch 3 Batch 750/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.892, Loss: 1.411 Epoch 3 Batch 751/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.892, Loss: 1.469 Epoch 3 Batch 752/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.910, Loss: 1.496 Epoch 3 Batch 753/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.914, Loss: 1.486 Epoch 3 Batch 754/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.930, Loss: 1.528 Epoch 3 Batch 755/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.942, Loss: 1.428 Epoch 3 Batch 756/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.923, Loss: 1.435 Epoch 3 Batch 757/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.915, Loss: 1.433 Epoch 3 Batch 758/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.913, Loss: 1.456 Epoch 3 Batch 759/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.914, Loss: 1.552 Epoch 3 Batch 760/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.911, Loss: 1.468 Epoch 3 Batch 761/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.909, Loss: 1.427 Epoch 3 Batch 762/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.904, Loss: 1.421 Epoch 3 Batch 763/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.908, Loss: 1.466 Epoch 3 Batch 764/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.907, Loss: 1.456 Epoch 3 Batch 765/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.906, Loss: 1.514 Epoch 3 Batch 766/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.916, Loss: 1.441 Epoch 3 Batch 767/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.917, Loss: 1.470 Epoch 3 Batch 768/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.906, Loss: 1.466 Epoch 3 Batch 769/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.900, Loss: 1.437 Epoch 3 Batch 770/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.908, Loss: 1.500 Epoch 3 Batch 771/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.905, Loss: 1.485 Epoch 3 Batch 772/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.905, Loss: 1.470 Epoch 3 Batch 773/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.907, Loss: 1.464 Epoch 3 Batch 774/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.908, Loss: 1.450 Epoch 3 Batch 775/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.907, Loss: 1.421 Epoch 3 Batch 776/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.904, Loss: 1.528 Epoch 3 Batch 777/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.912, Loss: 1.516 Epoch 3 Batch 778/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.908, Loss: 1.464 Epoch 3 Batch 779/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.901, Loss: 1.488 Epoch 3 Batch 780/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.900, Loss: 1.507 Epoch 3 Batch 781/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.904, Loss: 1.372 Epoch 3 Batch 782/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.904, Loss: 1.459 Epoch 3 Batch 783/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.909, Loss: 1.535 Epoch 3 Batch 784/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.903, Loss: 1.423 Epoch 3 Batch 785/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.897, Loss: 1.437 Epoch 3 Batch 786/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.893, Loss: 1.517 Epoch 3 Batch 787/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.885, Loss: 1.532 Epoch 3 Batch 788/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.879, Loss: 1.405 Epoch 3 Batch 789/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.880, Loss: 1.441 Epoch 3 Batch 790/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.885, Loss: 1.471 Epoch 3 Batch 791/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.896, Loss: 1.459 Epoch 3 Batch 792/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.895, Loss: 1.419 Epoch 3 Batch 793/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.894, Loss: 1.433 Epoch 3 Batch 794/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.886, Loss: 1.448 Epoch 3 Batch 795/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.891, Loss: 1.436 Epoch 3 Batch 796/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.903, Loss: 1.500 Epoch 3 Batch 797/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.907, Loss: 1.403 Epoch 3 Batch 798/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.901, Loss: 1.488 Epoch 3 Batch 799/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.911, Loss: 1.516 Epoch 3 Batch 800/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.921, Loss: 1.476 Epoch 3 Batch 801/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.917, Loss: 1.440 Epoch 3 Batch 802/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.913, Loss: 1.473 Epoch 3 Batch 803/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.912, Loss: 1.499 Epoch 3 Batch 804/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.896, Loss: 1.425 Epoch 3 Batch 805/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.900, Loss: 1.370 Epoch 3 Batch 806/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.886, Loss: 1.476 Epoch 3 Batch 807/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.891, Loss: 1.523 Epoch 3 Batch 808/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.885, Loss: 1.456 Epoch 3 Batch 809/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.890, Loss: 1.472 Epoch 3 Batch 810/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.899, Loss: 1.552 Epoch 3 Batch 811/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.893, Loss: 1.508 Epoch 3 Batch 812/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.906, Loss: 1.474 Epoch 3 Batch 813/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.911, Loss: 1.477 Epoch 3 Batch 814/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.910, Loss: 1.518 Epoch 3 Batch 815/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.904, Loss: 1.514 Epoch 3 Batch 816/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.901, Loss: 1.442 Epoch 3 Batch 817/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.898, Loss: 1.514 Epoch 3 Batch 818/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.901, Loss: 1.485 Epoch 3 Batch 819/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.900, Loss: 1.587 Epoch 3 Batch 820/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.900, Loss: 1.442 Epoch 3 Batch 821/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.901, Loss: 1.448 Epoch 3 Batch 822/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.906, Loss: 1.434 Epoch 3 Batch 823/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.901, Loss: 1.479 Epoch 3 Batch 824/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.903, Loss: 1.451 Epoch 3 Batch 825/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.899, Loss: 1.410 Epoch 3 Batch 826/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.899, Loss: 1.483 Epoch 3 Batch 827/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.896, Loss: 1.482 Epoch 3 Batch 828/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.900, Loss: 1.449 Epoch 3 Batch 829/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.896, Loss: 1.474 Epoch 3 Batch 830/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.900, Loss: 1.529 Epoch 3 Batch 831/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.891, Loss: 1.401 Epoch 3 Batch 832/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.884, Loss: 1.472 Epoch 3 Batch 833/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.883, Loss: 1.541 Epoch 3 Batch 834/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.887, Loss: 1.443 Epoch 3 Batch 835/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.882, Loss: 1.397 Epoch 3 Batch 836/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.887, Loss: 1.552 Epoch 3 Batch 837/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.890, Loss: 1.444 Epoch 3 Batch 838/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.898, Loss: 1.485 Epoch 3 Batch 839/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.899, Loss: 1.445 Epoch 3 Batch 840/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.900, Loss: 1.442 Epoch 3 Batch 841/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.900, Loss: 1.538 Epoch 3 Batch 842/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.897, Loss: 1.489 Epoch 3 Batch 843/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.899, Loss: 1.411 Epoch 3 Batch 844/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.901, Loss: 1.492 Epoch 3 Batch 845/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.905, Loss: 1.452 Epoch 3 Batch 846/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.910, Loss: 1.478 Epoch 3 Batch 847/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.910, Loss: 1.508 Epoch 3 Batch 848/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.914, Loss: 1.386 Epoch 3 Batch 849/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.908, Loss: 1.445 Epoch 3 Batch 850/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.897, Loss: 1.490 Epoch 3 Batch 851/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.893, Loss: 1.506 Epoch 3 Batch 852/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.899, Loss: 1.423 Epoch 3 Batch 853/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.890, Loss: 1.486 Epoch 3 Batch 854/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.898, Loss: 1.496 Epoch 3 Batch 855/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.893, Loss: 1.497 Epoch 3 Batch 856/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.901, Loss: 1.464 Epoch 3 Batch 857/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.896, Loss: 1.443 Epoch 3 Batch 858/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.891, Loss: 1.440 Epoch 3 Batch 859/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.884, Loss: 1.525 Epoch 3 Batch 860/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.883, Loss: 1.483 Epoch 3 Batch 861/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.883, Loss: 1.438 Epoch 3 Batch 862/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.886, Loss: 1.519 Epoch 3 Batch 863/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.890, Loss: 1.411 Epoch 3 Batch 864/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.883, Loss: 1.443 Epoch 3 Batch 865/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.881, Loss: 1.466 Epoch 3 Batch 866/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.881, Loss: 1.458 Epoch 3 Batch 867/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.888, Loss: 1.580 Epoch 3 Batch 868/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.888, Loss: 1.456 Epoch 3 Batch 869/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.887, Loss: 1.502 Epoch 3 Batch 870/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.883, Loss: 1.552 Epoch 3 Batch 871/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.892, Loss: 1.441 Epoch 3 Batch 872/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.897, Loss: 1.493 Epoch 3 Batch 873/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.899, Loss: 1.442 Epoch 3 Batch 874/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.904, Loss: 1.511 Epoch 3 Batch 875/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.907, Loss: 1.487 Epoch 3 Batch 876/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.911, Loss: 1.468 Epoch 3 Batch 877/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.905, Loss: 1.459 Epoch 3 Batch 878/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.899, Loss: 1.455 Epoch 3 Batch 879/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.901, Loss: 1.454 Epoch 3 Batch 880/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.906, Loss: 1.484 Epoch 3 Batch 881/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.904, Loss: 1.484 Epoch 3 Batch 882/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.907, Loss: 1.492 Epoch 3 Batch 883/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.902, Loss: 1.449 Epoch 3 Batch 884/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.902, Loss: 1.383 Epoch 3 Batch 885/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.900, Loss: 1.468 Epoch 3 Batch 886/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.897, Loss: 1.382 Epoch 3 Batch 887/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.892, Loss: 1.523 Epoch 3 Batch 888/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.885, Loss: 1.492 Epoch 3 Batch 889/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.889, Loss: 1.480 Epoch 3 Batch 890/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.891, Loss: 1.519 Epoch 3 Batch 891/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.889, Loss: 1.466 Epoch 3 Batch 892/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.897, Loss: 1.590 Epoch 3 Batch 893/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.897, Loss: 1.491 Epoch 3 Batch 894/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.885, Loss: 1.382 Epoch 3 Batch 895/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.879, Loss: 1.563 Epoch 3 Batch 896/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.883, Loss: 1.489 Epoch 3 Batch 897/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.885, Loss: 1.451 Epoch 3 Batch 898/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.893, Loss: 1.450 Epoch 3 Batch 899/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.903, Loss: 1.481 Epoch 3 Batch 900/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.910, Loss: 1.497 Epoch 3 Batch 901/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.912, Loss: 1.415 Epoch 3 Batch 902/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.913, Loss: 1.489 Epoch 3 Batch 903/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.915, Loss: 1.447 Epoch 3 Batch 904/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.914, Loss: 1.519 Epoch 3 Batch 905/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.918, Loss: 1.488 Epoch 3 Batch 906/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.918, Loss: 1.403 Epoch 3 Batch 907/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.904, Loss: 1.522 Epoch 3 Batch 908/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.905, Loss: 1.412 Epoch 3 Batch 909/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.904, Loss: 1.443 Epoch 3 Batch 910/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.893, Loss: 1.470 Epoch 3 Batch 911/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.879, Loss: 1.471 Epoch 3 Batch 912/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.867, Loss: 1.443 Epoch 3 Batch 913/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.864, Loss: 1.516 Epoch 3 Batch 914/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.876, Loss: 1.440 Epoch 3 Batch 915/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.901, Loss: 1.425 Epoch 3 Batch 916/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.896, Loss: 1.431 Epoch 3 Batch 917/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.896, Loss: 1.501 Epoch 3 Batch 918/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.900, Loss: 1.450 Epoch 3 Batch 919/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.897, Loss: 1.428 Epoch 3 Batch 920/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.886, Loss: 1.501 Epoch 3 Batch 921/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.886, Loss: 1.476 Epoch 3 Batch 922/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.893, Loss: 1.437 Epoch 3 Batch 923/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.888, Loss: 1.447 Epoch 3 Batch 924/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.891, Loss: 1.485 Epoch 3 Batch 925/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.896, Loss: 1.442 Epoch 3 Batch 926/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.906, Loss: 1.525 Epoch 3 Batch 927/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.916, Loss: 1.434 Epoch 3 Batch 928/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.911, Loss: 1.454 Epoch 3 Batch 929/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.918, Loss: 1.517 Epoch 3 Batch 930/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.911, Loss: 1.459 Epoch 3 Batch 931/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.916, Loss: 1.493 Epoch 3 Batch 932/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.907, Loss: 1.473 Epoch 3 Batch 933/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.908, Loss: 1.397 Epoch 3 Batch 934/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.898, Loss: 1.562 Epoch 3 Batch 935/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.901, Loss: 1.427 Epoch 3 Batch 936/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.894, Loss: 1.552 Epoch 3 Batch 937/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.885, Loss: 1.476 Epoch 3 Batch 938/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.881, Loss: 1.516 Epoch 3 Batch 939/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.885, Loss: 1.556 Epoch 3 Batch 940/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.890, Loss: 1.472 Epoch 3 Batch 941/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.885, Loss: 1.449 Epoch 3 Batch 942/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.882, Loss: 1.367 Epoch 3 Batch 943/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.878, Loss: 1.472 Epoch 3 Batch 944/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.885, Loss: 1.495 Epoch 3 Batch 945/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.887, Loss: 1.517 Epoch 3 Batch 946/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.891, Loss: 1.541 Epoch 3 Batch 947/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.891, Loss: 1.566 Epoch 3 Batch 948/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.893, Loss: 1.520 Epoch 3 Batch 949/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.895, Loss: 1.433 Epoch 3 Batch 950/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.906, Loss: 1.464 Epoch 3 Batch 951/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.905, Loss: 1.511 Epoch 3 Batch 952/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.908, Loss: 1.546 Epoch 3 Batch 953/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.905, Loss: 1.492 Epoch 3 Batch 954/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.909, Loss: 1.484 Epoch 3 Batch 955/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.914, Loss: 1.395 Epoch 3 Batch 956/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.910, Loss: 1.432 Epoch 3 Batch 957/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.901, Loss: 1.364 Epoch 3 Batch 958/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.898, Loss: 1.473 Epoch 3 Batch 959/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.900, Loss: 1.464 Epoch 3 Batch 960/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.898, Loss: 1.523 Epoch 3 Batch 961/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.907, Loss: 1.505 Epoch 3 Batch 962/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.914, Loss: 1.458 Epoch 3 Batch 963/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.899, Loss: 1.405 Epoch 3 Batch 964/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.898, Loss: 1.538 Epoch 3 Batch 965/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.904, Loss: 1.524 Epoch 3 Batch 966/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.908, Loss: 1.434 Epoch 3 Batch 967/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.902, Loss: 1.391 Epoch 3 Batch 968/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.893, Loss: 1.566 Epoch 3 Batch 969/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.897, Loss: 1.480 Epoch 3 Batch 970/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.898, Loss: 1.431 Epoch 3 Batch 971/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.897, Loss: 1.452 Epoch 3 Batch 972/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.897, Loss: 1.474 Epoch 3 Batch 973/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.899, Loss: 1.479 Epoch 3 Batch 974/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.902, Loss: 1.461 Epoch 3 Batch 975/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.898, Loss: 1.451 Epoch 3 Batch 976/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.898, Loss: 1.478 Epoch 3 Batch 977/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.902, Loss: 1.440 Epoch 3 Batch 978/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.900, Loss: 1.527 Epoch 3 Batch 979/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.899, Loss: 1.431 Epoch 3 Batch 980/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.903, Loss: 1.428 Epoch 3 Batch 981/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.914, Loss: 1.469 Epoch 3 Batch 982/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.914, Loss: 1.495 Epoch 3 Batch 983/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.911, Loss: 1.521 Epoch 3 Batch 984/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.902, Loss: 1.462 Epoch 3 Batch 985/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.902, Loss: 1.519 Epoch 3 Batch 986/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.906, Loss: 1.423 Epoch 3 Batch 987/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.901, Loss: 1.470 Epoch 3 Batch 988/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.904, Loss: 1.414 Epoch 3 Batch 989/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.899, Loss: 1.488 Epoch 3 Batch 990/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.897, Loss: 1.456 Epoch 3 Batch 991/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.906, Loss: 1.432 Epoch 3 Batch 992/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.901, Loss: 1.521 Epoch 3 Batch 993/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.903, Loss: 1.547 Epoch 3 Batch 994/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.895, Loss: 1.509 Epoch 3 Batch 995/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.890, Loss: 1.498 Epoch 3 Batch 996/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.890, Loss: 1.472 Epoch 3 Batch 997/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.901, Loss: 1.412 Epoch 3 Batch 998/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.901, Loss: 1.459 Epoch 3 Batch 999/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.904, Loss: 1.564 Epoch 3 Batch 1000/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.909, Loss: 1.474 Epoch 3 Batch 1001/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.905, Loss: 1.423 Epoch 3 Batch 1002/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.902, Loss: 1.526 Epoch 3 Batch 1003/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.895, Loss: 1.505 Epoch 3 Batch 1004/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.896, Loss: 1.497 Epoch 3 Batch 1005/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.905, Loss: 1.459 Epoch 3 Batch 1006/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.906, Loss: 1.475 Epoch 3 Batch 1007/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.915, Loss: 1.546 Epoch 3 Batch 1008/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.915, Loss: 1.456 Epoch 3 Batch 1009/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.919, Loss: 1.474 Epoch 3 Batch 1010/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.919, Loss: 1.423 Epoch 3 Batch 1011/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.905, Loss: 1.454 Epoch 3 Batch 1012/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.904, Loss: 1.444 Epoch 3 Batch 1013/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.905, Loss: 1.357 Epoch 3 Batch 1014/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.894, Loss: 1.499 Epoch 3 Batch 1015/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.909, Loss: 1.469 Epoch 3 Batch 1016/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.909, Loss: 1.464 Epoch 3 Batch 1017/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.911, Loss: 1.485 Epoch 3 Batch 1018/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.911, Loss: 1.417 Epoch 3 Batch 1019/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.904, Loss: 1.522 Epoch 3 Batch 1020/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.904, Loss: 1.466 Epoch 3 Batch 1021/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.897, Loss: 1.486 Epoch 3 Batch 1022/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.901, Loss: 1.456 Epoch 3 Batch 1023/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.898, Loss: 1.458 Epoch 3 Batch 1024/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.889, Loss: 1.427 Epoch 3 Batch 1025/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.893, Loss: 1.455 Epoch 3 Batch 1026/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.898, Loss: 1.519 Epoch 3 Batch 1027/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.895, Loss: 1.432 Epoch 3 Batch 1028/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.890, Loss: 1.469 Epoch 3 Batch 1029/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.890, Loss: 1.412 Epoch 3 Batch 1030/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.895, Loss: 1.455 Epoch 3 Batch 1031/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.900, Loss: 1.462 Epoch 3 Batch 1032/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.907, Loss: 1.517 Epoch 3 Batch 1033/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.910, Loss: 1.503 Epoch 3 Batch 1034/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.910, Loss: 1.463 Epoch 3 Batch 1035/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.909, Loss: 1.446 Epoch 3 Batch 1036/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.906, Loss: 1.488 Epoch 3 Batch 1037/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.900, Loss: 1.413 Epoch 3 Batch 1038/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.900, Loss: 1.474 Epoch 3 Batch 1039/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.905, Loss: 1.481 Epoch 3 Batch 1040/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.898, Loss: 1.444 Epoch 3 Batch 1041/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.916, Loss: 1.482 Epoch 3 Batch 1042/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.910, Loss: 1.459 Epoch 3 Batch 1043/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.916, Loss: 1.534 Epoch 3 Batch 1044/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.914, Loss: 1.533 Epoch 3 Batch 1045/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.919, Loss: 1.467 Epoch 3 Batch 1046/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.914, Loss: 1.526 Epoch 3 Batch 1047/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.915, Loss: 1.497 Epoch 3 Batch 1048/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.920, Loss: 1.514 Epoch 3 Batch 1049/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.909, Loss: 1.481 Epoch 3 Batch 1050/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.909, Loss: 1.442 Epoch 3 Batch 1051/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.911, Loss: 1.457 Epoch 3 Batch 1052/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.907, Loss: 1.505 Epoch 3 Batch 1053/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.911, Loss: 1.454 Epoch 3 Batch 1054/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.924, Loss: 1.516 Epoch 3 Batch 1055/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.924, Loss: 1.489 Epoch 3 Batch 1056/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.924, Loss: 1.433 Epoch 3 Batch 1057/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.925, Loss: 1.436 Epoch 3 Batch 1058/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.916, Loss: 1.494 Epoch 3 Batch 1059/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.915, Loss: 1.549 Epoch 3 Batch 1060/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.911, Loss: 1.463 Epoch 3 Batch 1061/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.906, Loss: 1.394 Epoch 3 Batch 1062/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.899, Loss: 1.518 Epoch 3 Batch 1063/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.903, Loss: 1.459 Epoch 3 Batch 1064/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.903, Loss: 1.483 Epoch 3 Batch 1065/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.907, Loss: 1.463 Epoch 3 Batch 1066/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.898, Loss: 1.468 Epoch 3 Batch 1067/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.891, Loss: 1.527 Epoch 3 Batch 1068/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.892, Loss: 1.492 Epoch 3 Batch 1069/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.893, Loss: 1.484 Epoch 3 Batch 1070/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.897, Loss: 1.478 Epoch 3 Batch 1071/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.902, Loss: 1.441 Epoch 3 Batch 1072/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.895, Loss: 1.431 Epoch 3 Batch 1073/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.895, Loss: 1.556 Epoch 3 Batch 1074/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.896, Loss: 1.514 Epoch 3 Batch 1075/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.891, Loss: 1.465 Epoch 4 Batch 0/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.891, Loss: 1.394 Epoch 4 Batch 1/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.907, Loss: 1.443 Epoch 4 Batch 2/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.903, Loss: 1.495 Epoch 4 Batch 3/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.896, Loss: 1.458 Epoch 4 Batch 4/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.905, Loss: 1.433 Epoch 4 Batch 5/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.909, Loss: 1.527 Epoch 4 Batch 6/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.919, Loss: 1.508 Epoch 4 Batch 7/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.927, Loss: 1.482 Epoch 4 Batch 8/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.929, Loss: 1.426 Epoch 4 Batch 9/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.938, Loss: 1.431 Epoch 4 Batch 10/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.938, Loss: 1.378 Epoch 4 Batch 11/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.932, Loss: 1.449 Epoch 4 Batch 12/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.935, Loss: 1.456 Epoch 4 Batch 13/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.919, Loss: 1.457 Epoch 4 Batch 14/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.924, Loss: 1.461 Epoch 4 Batch 15/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.928, Loss: 1.461 Epoch 4 Batch 16/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.930, Loss: 1.438 Epoch 4 Batch 17/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.926, Loss: 1.528 Epoch 4 Batch 18/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.921, Loss: 1.467 Epoch 4 Batch 19/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.907, Loss: 1.446 Epoch 4 Batch 20/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.921, Loss: 1.329 Epoch 4 Batch 21/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.917, Loss: 1.463 Epoch 4 Batch 22/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.911, Loss: 1.496 Epoch 4 Batch 23/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.907, Loss: 1.446 Epoch 4 Batch 24/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.903, Loss: 1.426 Epoch 4 Batch 25/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.898, Loss: 1.367 Epoch 4 Batch 26/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.903, Loss: 1.559 Epoch 4 Batch 27/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.898, Loss: 1.453 Epoch 4 Batch 28/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.891, Loss: 1.398 Epoch 4 Batch 29/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.904, Loss: 1.465 Epoch 4 Batch 30/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.908, Loss: 1.435 Epoch 4 Batch 31/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.908, Loss: 1.464 Epoch 4 Batch 32/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.903, Loss: 1.469 Epoch 4 Batch 33/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.886, Loss: 1.448 Epoch 4 Batch 34/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.892, Loss: 1.446 Epoch 4 Batch 35/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.898, Loss: 1.393 Epoch 4 Batch 36/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.920, Loss: 1.429 Epoch 4 Batch 37/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.920, Loss: 1.475 Epoch 4 Batch 38/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.926, Loss: 1.552 Epoch 4 Batch 39/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.928, Loss: 1.521 Epoch 4 Batch 40/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.923, Loss: 1.501 Epoch 4 Batch 41/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.917, Loss: 1.410 Epoch 4 Batch 42/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.922, Loss: 1.461 Epoch 4 Batch 43/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.914, Loss: 1.352 Epoch 4 Batch 44/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.916, Loss: 1.413 Epoch 4 Batch 45/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.920, Loss: 1.453 Epoch 4 Batch 46/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.928, Loss: 1.491 Epoch 4 Batch 47/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.924, Loss: 1.410 Epoch 4 Batch 48/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.912, Loss: 1.492 Epoch 4 Batch 49/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.912, Loss: 1.512 Epoch 4 Batch 50/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.912, Loss: 1.459 Epoch 4 Batch 51/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.912, Loss: 1.471 Epoch 4 Batch 52/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.914, Loss: 1.487 Epoch 4 Batch 53/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.909, Loss: 1.487 Epoch 4 Batch 54/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.908, Loss: 1.468 Epoch 4 Batch 55/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.908, Loss: 1.423 Epoch 4 Batch 56/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.908, Loss: 1.442 Epoch 4 Batch 57/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.902, Loss: 1.438 Epoch 4 Batch 58/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.898, Loss: 1.475 Epoch 4 Batch 59/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.907, Loss: 1.461 Epoch 4 Batch 60/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.907, Loss: 1.417 Epoch 4 Batch 61/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.907, Loss: 1.489 Epoch 4 Batch 62/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.905, Loss: 1.411 Epoch 4 Batch 63/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.898, Loss: 1.503 Epoch 4 Batch 64/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.892, Loss: 1.354 Epoch 4 Batch 65/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.892, Loss: 1.468 Epoch 4 Batch 66/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.887, Loss: 1.487 Epoch 4 Batch 67/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.886, Loss: 1.466 Epoch 4 Batch 68/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.887, Loss: 1.377 Epoch 4 Batch 69/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.891, Loss: 1.475 Epoch 4 Batch 70/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.893, Loss: 1.385 Epoch 4 Batch 71/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.897, Loss: 1.420 Epoch 4 Batch 72/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.897, Loss: 1.492 Epoch 4 Batch 73/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.902, Loss: 1.467 Epoch 4 Batch 74/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.906, Loss: 1.439 Epoch 4 Batch 75/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.916, Loss: 1.470 Epoch 4 Batch 76/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.912, Loss: 1.536 Epoch 4 Batch 77/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.918, Loss: 1.505 Epoch 4 Batch 78/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.914, Loss: 1.475 Epoch 4 Batch 79/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.918, Loss: 1.509 Epoch 4 Batch 80/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.915, Loss: 1.437 Epoch 4 Batch 81/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.915, Loss: 1.450 Epoch 4 Batch 82/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.917, Loss: 1.467 Epoch 4 Batch 83/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.923, Loss: 1.453 Epoch 4 Batch 84/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.927, Loss: 1.532 Epoch 4 Batch 85/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.928, Loss: 1.521 Epoch 4 Batch 86/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.928, Loss: 1.477 Epoch 4 Batch 87/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.928, Loss: 1.449 Epoch 4 Batch 88/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.928, Loss: 1.456 Epoch 4 Batch 89/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.913, Loss: 1.440 Epoch 4 Batch 90/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.901, Loss: 1.379 Epoch 4 Batch 91/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.904, Loss: 1.457 Epoch 4 Batch 92/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.902, Loss: 1.429 Epoch 4 Batch 93/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.900, Loss: 1.426 Epoch 4 Batch 94/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.904, Loss: 1.433 Epoch 4 Batch 95/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.904, Loss: 1.453 Epoch 4 Batch 96/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.898, Loss: 1.454 Epoch 4 Batch 97/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.903, Loss: 1.527 Epoch 4 Batch 98/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.915, Loss: 1.474 Epoch 4 Batch 99/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.915, Loss: 1.458 Epoch 4 Batch 100/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.918, Loss: 1.522 Epoch 4 Batch 101/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.922, Loss: 1.404 Epoch 4 Batch 102/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.912, Loss: 1.525 Epoch 4 Batch 103/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.907, Loss: 1.536 Epoch 4 Batch 104/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.907, Loss: 1.474 Epoch 4 Batch 105/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.906, Loss: 1.463 Epoch 4 Batch 106/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.907, Loss: 1.484 Epoch 4 Batch 107/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.911, Loss: 1.430 Epoch 4 Batch 108/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.914, Loss: 1.430 Epoch 4 Batch 109/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.915, Loss: 1.472 Epoch 4 Batch 110/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.910, Loss: 1.482 Epoch 4 Batch 111/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.906, Loss: 1.416 Epoch 4 Batch 112/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.906, Loss: 1.389 Epoch 4 Batch 113/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.913, Loss: 1.530 Epoch 4 Batch 114/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.912, Loss: 1.414 Epoch 4 Batch 115/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.909, Loss: 1.422 Epoch 4 Batch 116/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.911, Loss: 1.453 Epoch 4 Batch 117/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.913, Loss: 1.522 Epoch 4 Batch 118/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.916, Loss: 1.557 Epoch 4 Batch 119/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.907, Loss: 1.463 Epoch 4 Batch 120/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.912, Loss: 1.477 Epoch 4 Batch 121/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.908, Loss: 1.464 Epoch 4 Batch 122/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.909, Loss: 1.475 Epoch 4 Batch 123/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.909, Loss: 1.470 Epoch 4 Batch 124/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.915, Loss: 1.428 Epoch 4 Batch 125/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.917, Loss: 1.503 Epoch 4 Batch 126/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.916, Loss: 1.425 Epoch 4 Batch 127/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.920, Loss: 1.434 Epoch 4 Batch 128/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.922, Loss: 1.398 Epoch 4 Batch 129/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.910, Loss: 1.572 Epoch 4 Batch 130/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.910, Loss: 1.440 Epoch 4 Batch 131/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.897, Loss: 1.485 Epoch 4 Batch 132/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.898, Loss: 1.481 Epoch 4 Batch 133/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.894, Loss: 1.466 Epoch 4 Batch 134/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.890, Loss: 1.445 Epoch 4 Batch 135/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.902, Loss: 1.480 Epoch 4 Batch 136/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.902, Loss: 1.514 Epoch 4 Batch 137/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.902, Loss: 1.437 Epoch 4 Batch 138/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.910, Loss: 1.478 Epoch 4 Batch 139/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.910, Loss: 1.455 Epoch 4 Batch 140/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.922, Loss: 1.476 Epoch 4 Batch 141/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.928, Loss: 1.394 Epoch 4 Batch 142/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.925, Loss: 1.458 Epoch 4 Batch 143/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.917, Loss: 1.394 Epoch 4 Batch 144/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.912, Loss: 1.466 Epoch 4 Batch 145/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.912, Loss: 1.558 Epoch 4 Batch 146/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.912, Loss: 1.500 Epoch 4 Batch 147/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.907, Loss: 1.479 Epoch 4 Batch 148/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.908, Loss: 1.471 Epoch 4 Batch 149/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.915, Loss: 1.518 Epoch 4 Batch 150/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.915, Loss: 1.474 Epoch 4 Batch 151/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.925, Loss: 1.508 Epoch 4 Batch 152/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.919, Loss: 1.461 Epoch 4 Batch 153/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.919, Loss: 1.532 Epoch 4 Batch 154/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.919, Loss: 1.422 Epoch 4 Batch 155/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.914, Loss: 1.436 Epoch 4 Batch 156/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.918, Loss: 1.407 Epoch 4 Batch 157/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.918, Loss: 1.460 Epoch 4 Batch 158/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.924, Loss: 1.440 Epoch 4 Batch 159/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.913, Loss: 1.453 Epoch 4 Batch 160/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.923, Loss: 1.479 Epoch 4 Batch 161/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.927, Loss: 1.550 Epoch 4 Batch 162/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.923, Loss: 1.565 Epoch 4 Batch 163/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.923, Loss: 1.514 Epoch 4 Batch 164/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.923, Loss: 1.428 Epoch 4 Batch 165/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.923, Loss: 1.496 Epoch 4 Batch 166/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.919, Loss: 1.503 Epoch 4 Batch 167/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.919, Loss: 1.428 Epoch 4 Batch 168/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.913, Loss: 1.344 Epoch 4 Batch 169/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.909, Loss: 1.524 Epoch 4 Batch 170/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.902, Loss: 1.367 Epoch 4 Batch 171/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.903, Loss: 1.409 Epoch 4 Batch 172/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.903, Loss: 1.424 Epoch 4 Batch 173/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.901, Loss: 1.530 Epoch 4 Batch 174/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.897, Loss: 1.431 Epoch 4 Batch 175/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.897, Loss: 1.527 Epoch 4 Batch 176/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.893, Loss: 1.433 Epoch 4 Batch 177/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.900, Loss: 1.484 Epoch 4 Batch 178/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.900, Loss: 1.526 Epoch 4 Batch 179/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.906, Loss: 1.462 Epoch 4 Batch 180/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.906, Loss: 1.473 Epoch 4 Batch 181/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.905, Loss: 1.471 Epoch 4 Batch 182/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.900, Loss: 1.539 Epoch 4 Batch 183/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.894, Loss: 1.452 Epoch 4 Batch 184/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.899, Loss: 1.436 Epoch 4 Batch 185/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.904, Loss: 1.486 Epoch 4 Batch 186/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.909, Loss: 1.476 Epoch 4 Batch 187/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.909, Loss: 1.393 Epoch 4 Batch 188/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.909, Loss: 1.447 Epoch 4 Batch 189/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.904, Loss: 1.480 Epoch 4 Batch 190/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.907, Loss: 1.499 Epoch 4 Batch 191/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.907, Loss: 1.472 Epoch 4 Batch 192/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.905, Loss: 1.516 Epoch 4 Batch 193/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.910, Loss: 1.465 Epoch 4 Batch 194/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.914, Loss: 1.448 Epoch 4 Batch 195/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.914, Loss: 1.452 Epoch 4 Batch 196/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.914, Loss: 1.399 Epoch 4 Batch 197/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.913, Loss: 1.476 Epoch 4 Batch 198/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.915, Loss: 1.577 Epoch 4 Batch 199/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.910, Loss: 1.427 Epoch 4 Batch 200/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.916, Loss: 1.491 Epoch 4 Batch 201/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.910, Loss: 1.500 Epoch 4 Batch 202/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.914, Loss: 1.453 Epoch 4 Batch 203/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.917, Loss: 1.451 Epoch 4 Batch 204/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.912, Loss: 1.494 Epoch 4 Batch 205/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.913, Loss: 1.486 Epoch 4 Batch 206/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.915, Loss: 1.461 Epoch 4 Batch 207/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.910, Loss: 1.550 Epoch 4 Batch 208/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.917, Loss: 1.364 Epoch 4 Batch 209/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.921, Loss: 1.426 Epoch 4 Batch 210/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.921, Loss: 1.438 Epoch 4 Batch 211/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.934, Loss: 1.389 Epoch 4 Batch 212/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.929, Loss: 1.425 Epoch 4 Batch 213/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.908, Loss: 1.520 Epoch 4 Batch 214/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.901, Loss: 1.499 Epoch 4 Batch 215/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.903, Loss: 1.577 Epoch 4 Batch 216/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.900, Loss: 1.454 Epoch 4 Batch 217/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.895, Loss: 1.501 Epoch 4 Batch 218/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.901, Loss: 1.505 Epoch 4 Batch 219/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.896, Loss: 1.465 Epoch 4 Batch 220/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.902, Loss: 1.460 Epoch 4 Batch 221/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.896, Loss: 1.550 Epoch 4 Batch 222/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.895, Loss: 1.535 Epoch 4 Batch 223/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.910, Loss: 1.404 Epoch 4 Batch 224/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.911, Loss: 1.478 Epoch 4 Batch 225/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.904, Loss: 1.572 Epoch 4 Batch 226/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.918, Loss: 1.400 Epoch 4 Batch 227/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.917, Loss: 1.487 Epoch 4 Batch 228/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.911, Loss: 1.455 Epoch 4 Batch 229/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.911, Loss: 1.335 Epoch 4 Batch 230/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.907, Loss: 1.417 Epoch 4 Batch 231/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.910, Loss: 1.434 Epoch 4 Batch 232/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.909, Loss: 1.426 Epoch 4 Batch 233/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.901, Loss: 1.540 Epoch 4 Batch 234/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.895, Loss: 1.522 Epoch 4 Batch 235/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.904, Loss: 1.476 Epoch 4 Batch 236/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.906, Loss: 1.526 Epoch 4 Batch 237/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.913, Loss: 1.452 Epoch 4 Batch 238/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.909, Loss: 1.451 Epoch 4 Batch 239/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.904, Loss: 1.502 Epoch 4 Batch 240/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.913, Loss: 1.461 Epoch 4 Batch 241/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.908, Loss: 1.438 Epoch 4 Batch 242/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.899, Loss: 1.498 Epoch 4 Batch 243/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.904, Loss: 1.427 Epoch 4 Batch 244/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.913, Loss: 1.509 Epoch 4 Batch 245/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.911, Loss: 1.461 Epoch 4 Batch 246/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.911, Loss: 1.485 Epoch 4 Batch 247/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.914, Loss: 1.474 Epoch 4 Batch 248/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.914, Loss: 1.424 Epoch 4 Batch 249/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.914, Loss: 1.508 Epoch 4 Batch 250/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.906, Loss: 1.500 Epoch 4 Batch 251/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.906, Loss: 1.430 Epoch 4 Batch 252/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.906, Loss: 1.480 Epoch 4 Batch 253/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.901, Loss: 1.451 Epoch 4 Batch 254/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.911, Loss: 1.505 Epoch 4 Batch 255/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.910, Loss: 1.471 Epoch 4 Batch 256/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.910, Loss: 1.428 Epoch 4 Batch 257/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.905, Loss: 1.500 Epoch 4 Batch 258/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.918, Loss: 1.390 Epoch 4 Batch 259/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.918, Loss: 1.510 Epoch 4 Batch 260/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.919, Loss: 1.404 Epoch 4 Batch 261/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.912, Loss: 1.522 Epoch 4 Batch 262/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.914, Loss: 1.452 Epoch 4 Batch 263/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.907, Loss: 1.379 Epoch 4 Batch 264/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.916, Loss: 1.480 Epoch 4 Batch 265/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.921, Loss: 1.419 Epoch 4 Batch 266/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.914, Loss: 1.503 Epoch 4 Batch 267/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.905, Loss: 1.468 Epoch 4 Batch 268/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.906, Loss: 1.460 Epoch 4 Batch 269/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.916, Loss: 1.492 Epoch 4 Batch 270/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.917, Loss: 1.539 Epoch 4 Batch 271/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.907, Loss: 1.482 Epoch 4 Batch 272/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.911, Loss: 1.459 Epoch 4 Batch 273/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.911, Loss: 1.462 Epoch 4 Batch 274/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.906, Loss: 1.449 Epoch 4 Batch 275/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.906, Loss: 1.510 Epoch 4 Batch 276/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.901, Loss: 1.474 Epoch 4 Batch 277/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.898, Loss: 1.436 Epoch 4 Batch 278/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.897, Loss: 1.446 Epoch 4 Batch 279/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.902, Loss: 1.454 Epoch 4 Batch 280/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.904, Loss: 1.473 Epoch 4 Batch 281/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.901, Loss: 1.517 Epoch 4 Batch 282/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.895, Loss: 1.415 Epoch 4 Batch 283/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.900, Loss: 1.507 Epoch 4 Batch 284/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.902, Loss: 1.450 Epoch 4 Batch 285/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.901, Loss: 1.394 Epoch 4 Batch 286/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.903, Loss: 1.463 Epoch 4 Batch 287/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.908, Loss: 1.411 Epoch 4 Batch 288/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.913, Loss: 1.494 Epoch 4 Batch 289/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.919, Loss: 1.524 Epoch 4 Batch 290/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.919, Loss: 1.443 Epoch 4 Batch 291/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.920, Loss: 1.459 Epoch 4 Batch 292/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.918, Loss: 1.432 Epoch 4 Batch 293/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.919, Loss: 1.446 Epoch 4 Batch 294/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.914, Loss: 1.428 Epoch 4 Batch 295/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.920, Loss: 1.427 Epoch 4 Batch 296/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.905, Loss: 1.548 Epoch 4 Batch 297/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.898, Loss: 1.457 Epoch 4 Batch 298/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.902, Loss: 1.510 Epoch 4 Batch 299/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.905, Loss: 1.424 Epoch 4 Batch 300/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.900, Loss: 1.450 Epoch 4 Batch 301/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.900, Loss: 1.461 Epoch 4 Batch 302/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.922, Loss: 1.463 Epoch 4 Batch 303/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.913, Loss: 1.487 Epoch 4 Batch 304/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.916, Loss: 1.437 Epoch 4 Batch 305/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.919, Loss: 1.456 Epoch 4 Batch 306/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.924, Loss: 1.427 Epoch 4 Batch 307/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.920, Loss: 1.476 Epoch 4 Batch 308/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.924, Loss: 1.501 Epoch 4 Batch 309/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.923, Loss: 1.441 Epoch 4 Batch 310/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.910, Loss: 1.364 Epoch 4 Batch 311/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.907, Loss: 1.369 Epoch 4 Batch 312/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.906, Loss: 1.544 Epoch 4 Batch 313/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.906, Loss: 1.506 Epoch 4 Batch 314/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.913, Loss: 1.391 Epoch 4 Batch 315/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.908, Loss: 1.500 Epoch 4 Batch 316/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.903, Loss: 1.444 Epoch 4 Batch 317/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.903, Loss: 1.499 Epoch 4 Batch 318/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.900, Loss: 1.488 Epoch 4 Batch 319/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.893, Loss: 1.460 Epoch 4 Batch 320/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.891, Loss: 1.531 Epoch 4 Batch 321/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.896, Loss: 1.489 Epoch 4 Batch 322/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.909, Loss: 1.556 Epoch 4 Batch 323/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.895, Loss: 1.449 Epoch 4 Batch 324/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.892, Loss: 1.517 Epoch 4 Batch 325/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.892, Loss: 1.508 Epoch 4 Batch 326/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.897, Loss: 1.426 Epoch 4 Batch 327/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.898, Loss: 1.465 Epoch 4 Batch 328/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.905, Loss: 1.459 Epoch 4 Batch 329/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.911, Loss: 1.489 Epoch 4 Batch 330/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.911, Loss: 1.459 Epoch 4 Batch 331/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.906, Loss: 1.611 Epoch 4 Batch 332/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.909, Loss: 1.452 Epoch 4 Batch 333/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.910, Loss: 1.483 Epoch 4 Batch 334/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.905, Loss: 1.402 Epoch 4 Batch 335/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.907, Loss: 1.451 Epoch 4 Batch 336/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.897, Loss: 1.446 Epoch 4 Batch 337/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.892, Loss: 1.435 Epoch 4 Batch 338/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.902, Loss: 1.524 Epoch 4 Batch 339/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.899, Loss: 1.413 Epoch 4 Batch 340/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.897, Loss: 1.449 Epoch 4 Batch 341/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.900, Loss: 1.487 Epoch 4 Batch 342/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.904, Loss: 1.430 Epoch 4 Batch 343/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.898, Loss: 1.489 Epoch 4 Batch 344/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.904, Loss: 1.416 Epoch 4 Batch 345/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.904, Loss: 1.496 Epoch 4 Batch 346/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.906, Loss: 1.401 Epoch 4 Batch 347/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.911, Loss: 1.438 Epoch 4 Batch 348/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.907, Loss: 1.414 Epoch 4 Batch 349/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.901, Loss: 1.463 Epoch 4 Batch 350/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.900, Loss: 1.495 Epoch 4 Batch 351/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.899, Loss: 1.471 Epoch 4 Batch 352/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.889, Loss: 1.494 Epoch 4 Batch 353/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.887, Loss: 1.453 Epoch 4 Batch 354/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.889, Loss: 1.553 Epoch 4 Batch 355/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.896, Loss: 1.426 Epoch 4 Batch 356/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.892, Loss: 1.493 Epoch 4 Batch 357/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.892, Loss: 1.575 Epoch 4 Batch 358/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.897, Loss: 1.455 Epoch 4 Batch 359/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.898, Loss: 1.468 Epoch 4 Batch 360/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.902, Loss: 1.439 Epoch 4 Batch 361/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.913, Loss: 1.449 Epoch 4 Batch 362/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.914, Loss: 1.458 Epoch 4 Batch 363/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.913, Loss: 1.502 Epoch 4 Batch 364/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.904, Loss: 1.447 Epoch 4 Batch 365/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.909, Loss: 1.513 Epoch 4 Batch 366/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.915, Loss: 1.420 Epoch 4 Batch 367/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.916, Loss: 1.436 Epoch 4 Batch 368/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.922, Loss: 1.457 Epoch 4 Batch 369/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.918, Loss: 1.479 Epoch 4 Batch 370/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.917, Loss: 1.391 Epoch 4 Batch 371/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.905, Loss: 1.443 Epoch 4 Batch 372/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.910, Loss: 1.481 Epoch 4 Batch 373/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.909, Loss: 1.367 Epoch 4 Batch 374/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.912, Loss: 1.451 Epoch 4 Batch 375/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.918, Loss: 1.508 Epoch 4 Batch 376/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.917, Loss: 1.481 Epoch 4 Batch 377/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.912, Loss: 1.433 Epoch 4 Batch 378/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.903, Loss: 1.461 Epoch 4 Batch 379/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.902, Loss: 1.530 Epoch 4 Batch 380/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.906, Loss: 1.367 Epoch 4 Batch 381/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.903, Loss: 1.467 Epoch 4 Batch 382/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.912, Loss: 1.457 Epoch 4 Batch 383/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.907, Loss: 1.476 Epoch 4 Batch 384/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.916, Loss: 1.417 Epoch 4 Batch 385/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.916, Loss: 1.418 Epoch 4 Batch 386/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.922, Loss: 1.455 Epoch 4 Batch 387/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.918, Loss: 1.448 Epoch 4 Batch 388/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.919, Loss: 1.500 Epoch 4 Batch 389/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.926, Loss: 1.476 Epoch 4 Batch 390/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.922, Loss: 1.468 Epoch 4 Batch 391/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.920, Loss: 1.458 Epoch 4 Batch 392/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.919, Loss: 1.433 Epoch 4 Batch 393/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.913, Loss: 1.429 Epoch 4 Batch 394/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.915, Loss: 1.430 Epoch 4 Batch 395/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.908, Loss: 1.426 Epoch 4 Batch 396/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.913, Loss: 1.489 Epoch 4 Batch 397/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.908, Loss: 1.388 Epoch 4 Batch 398/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.903, Loss: 1.433 Epoch 4 Batch 399/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.897, Loss: 1.468 Epoch 4 Batch 400/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.897, Loss: 1.449 Epoch 4 Batch 401/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.897, Loss: 1.489 Epoch 4 Batch 402/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.897, Loss: 1.426 Epoch 4 Batch 403/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.906, Loss: 1.522 Epoch 4 Batch 404/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.897, Loss: 1.427 Epoch 4 Batch 405/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.907, Loss: 1.431 Epoch 4 Batch 406/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.908, Loss: 1.416 Epoch 4 Batch 407/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.905, Loss: 1.503 Epoch 4 Batch 408/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.915, Loss: 1.507 Epoch 4 Batch 409/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.915, Loss: 1.480 Epoch 4 Batch 410/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.915, Loss: 1.507 Epoch 4 Batch 411/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.920, Loss: 1.465 Epoch 4 Batch 412/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.920, Loss: 1.419 Epoch 4 Batch 413/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.925, Loss: 1.405 Epoch 4 Batch 414/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.926, Loss: 1.449 Epoch 4 Batch 415/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.924, Loss: 1.422 Epoch 4 Batch 416/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.924, Loss: 1.447 Epoch 4 Batch 417/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.930, Loss: 1.453 Epoch 4 Batch 418/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.930, Loss: 1.494 Epoch 4 Batch 419/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.926, Loss: 1.386 Epoch 4 Batch 420/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.926, Loss: 1.467 Epoch 4 Batch 421/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.926, Loss: 1.451 Epoch 4 Batch 422/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.920, Loss: 1.468 Epoch 4 Batch 423/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.922, Loss: 1.489 Epoch 4 Batch 424/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.920, Loss: 1.447 Epoch 4 Batch 425/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.919, Loss: 1.409 Epoch 4 Batch 426/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.919, Loss: 1.490 Epoch 4 Batch 427/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.924, Loss: 1.446 Epoch 4 Batch 428/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.929, Loss: 1.417 Epoch 4 Batch 429/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.925, Loss: 1.447 Epoch 4 Batch 430/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.920, Loss: 1.466 Epoch 4 Batch 431/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.915, Loss: 1.394 Epoch 4 Batch 432/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.920, Loss: 1.427 Epoch 4 Batch 433/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.922, Loss: 1.503 Epoch 4 Batch 434/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.917, Loss: 1.428 Epoch 4 Batch 435/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.917, Loss: 1.572 Epoch 4 Batch 436/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.917, Loss: 1.525 Epoch 4 Batch 437/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.912, Loss: 1.360 Epoch 4 Batch 438/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.912, Loss: 1.439 Epoch 4 Batch 439/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.914, Loss: 1.467 Epoch 4 Batch 440/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.913, Loss: 1.427 Epoch 4 Batch 441/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.911, Loss: 1.428 Epoch 4 Batch 442/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.911, Loss: 1.453 Epoch 4 Batch 443/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.905, Loss: 1.496 Epoch 4 Batch 444/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.900, Loss: 1.400 Epoch 4 Batch 445/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.900, Loss: 1.475 Epoch 4 Batch 446/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.889, Loss: 1.439 Epoch 4 Batch 447/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.883, Loss: 1.467 Epoch 4 Batch 448/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.890, Loss: 1.500 Epoch 4 Batch 449/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.895, Loss: 1.482 Epoch 4 Batch 450/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.894, Loss: 1.436 Epoch 4 Batch 451/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.914, Loss: 1.480 Epoch 4 Batch 452/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.907, Loss: 1.553 Epoch 4 Batch 453/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.903, Loss: 1.410 Epoch 4 Batch 454/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.892, Loss: 1.439 Epoch 4 Batch 455/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.871, Loss: 1.533 Epoch 4 Batch 456/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.877, Loss: 1.400 Epoch 4 Batch 457/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.880, Loss: 1.516 Epoch 4 Batch 458/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.904, Loss: 1.505 Epoch 4 Batch 459/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.899, Loss: 1.479 Epoch 4 Batch 460/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.892, Loss: 1.558 Epoch 4 Batch 461/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.887, Loss: 1.535 Epoch 4 Batch 462/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.898, Loss: 1.471 Epoch 4 Batch 463/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.900, Loss: 1.501 Epoch 4 Batch 464/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.904, Loss: 1.419 Epoch 4 Batch 465/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.902, Loss: 1.502 Epoch 4 Batch 466/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.915, Loss: 1.468 Epoch 4 Batch 467/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.911, Loss: 1.484 Epoch 4 Batch 468/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.904, Loss: 1.533 Epoch 4 Batch 469/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.910, Loss: 1.362 Epoch 4 Batch 470/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.910, Loss: 1.560 Epoch 4 Batch 471/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.919, Loss: 1.501 Epoch 4 Batch 472/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.914, Loss: 1.520 Epoch 4 Batch 473/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.917, Loss: 1.471 Epoch 4 Batch 474/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.917, Loss: 1.494 Epoch 4 Batch 475/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.918, Loss: 1.475 Epoch 4 Batch 476/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.908, Loss: 1.436 Epoch 4 Batch 477/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.924, Loss: 1.491 Epoch 4 Batch 478/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.923, Loss: 1.451 Epoch 4 Batch 479/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.917, Loss: 1.386 Epoch 4 Batch 480/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.922, Loss: 1.450 Epoch 4 Batch 481/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.918, Loss: 1.431 Epoch 4 Batch 482/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.921, Loss: 1.495 Epoch 4 Batch 483/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.899, Loss: 1.429 Epoch 4 Batch 484/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.900, Loss: 1.381 Epoch 4 Batch 485/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.903, Loss: 1.496 Epoch 4 Batch 486/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.912, Loss: 1.371 Epoch 4 Batch 487/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.917, Loss: 1.432 Epoch 4 Batch 488/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.916, Loss: 1.541 Epoch 4 Batch 489/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.915, Loss: 1.449 Epoch 4 Batch 490/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.911, Loss: 1.553 Epoch 4 Batch 491/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.894, Loss: 1.434 Epoch 4 Batch 492/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.884, Loss: 1.554 Epoch 4 Batch 493/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.885, Loss: 1.476 Epoch 4 Batch 494/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.883, Loss: 1.455 Epoch 4 Batch 495/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.887, Loss: 1.447 Epoch 4 Batch 496/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.893, Loss: 1.485 Epoch 4 Batch 497/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.897, Loss: 1.416 Epoch 4 Batch 498/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.894, Loss: 1.500 Epoch 4 Batch 499/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.891, Loss: 1.418 Epoch 4 Batch 500/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.892, Loss: 1.432 Epoch 4 Batch 501/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.893, Loss: 1.507 Epoch 4 Batch 502/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.898, Loss: 1.444 Epoch 4 Batch 503/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.907, Loss: 1.473 Epoch 4 Batch 504/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.911, Loss: 1.401 Epoch 4 Batch 505/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.914, Loss: 1.419 Epoch 4 Batch 506/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.914, Loss: 1.554 Epoch 4 Batch 507/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.919, Loss: 1.444 Epoch 4 Batch 508/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.925, Loss: 1.507 Epoch 4 Batch 509/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.918, Loss: 1.465 Epoch 4 Batch 510/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.915, Loss: 1.474 Epoch 4 Batch 511/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.919, Loss: 1.466 Epoch 4 Batch 512/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.898, Loss: 1.555 Epoch 4 Batch 513/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.898, Loss: 1.413 Epoch 4 Batch 514/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.901, Loss: 1.492 Epoch 4 Batch 515/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.904, Loss: 1.468 Epoch 4 Batch 516/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.898, Loss: 1.432 Epoch 4 Batch 517/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.895, Loss: 1.427 Epoch 4 Batch 518/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.897, Loss: 1.402 Epoch 4 Batch 519/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.909, Loss: 1.478 Epoch 4 Batch 520/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.900, Loss: 1.431 Epoch 4 Batch 521/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.907, Loss: 1.423 Epoch 4 Batch 522/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.912, Loss: 1.498 Epoch 4 Batch 523/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.906, Loss: 1.422 Epoch 4 Batch 524/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.906, Loss: 1.379 Epoch 4 Batch 525/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.910, Loss: 1.502 Epoch 4 Batch 526/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.917, Loss: 1.446 Epoch 4 Batch 527/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.924, Loss: 1.487 Epoch 4 Batch 528/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.925, Loss: 1.551 Epoch 4 Batch 529/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.917, Loss: 1.411 Epoch 4 Batch 530/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.912, Loss: 1.456 Epoch 4 Batch 531/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.914, Loss: 1.441 Epoch 4 Batch 532/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.911, Loss: 1.502 Epoch 4 Batch 533/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.911, Loss: 1.524 Epoch 4 Batch 534/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.906, Loss: 1.412 Epoch 4 Batch 535/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.905, Loss: 1.433 Epoch 4 Batch 536/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.902, Loss: 1.462 Epoch 4 Batch 537/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.906, Loss: 1.417 Epoch 4 Batch 538/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.907, Loss: 1.428 Epoch 4 Batch 539/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.907, Loss: 1.435 Epoch 4 Batch 540/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.907, Loss: 1.388 Epoch 4 Batch 541/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.905, Loss: 1.439 Epoch 4 Batch 542/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.907, Loss: 1.425 Epoch 4 Batch 543/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.916, Loss: 1.501 Epoch 4 Batch 544/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.917, Loss: 1.403 Epoch 4 Batch 545/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.925, Loss: 1.486 Epoch 4 Batch 546/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.924, Loss: 1.483 Epoch 4 Batch 547/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.925, Loss: 1.410 Epoch 4 Batch 548/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.918, Loss: 1.474 Epoch 4 Batch 549/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.914, Loss: 1.426 Epoch 4 Batch 550/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.896, Loss: 1.441 Epoch 4 Batch 551/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.896, Loss: 1.493 Epoch 4 Batch 552/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.898, Loss: 1.487 Epoch 4 Batch 553/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.901, Loss: 1.429 Epoch 4 Batch 554/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.906, Loss: 1.473 Epoch 4 Batch 555/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.922, Loss: 1.456 Epoch 4 Batch 556/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.928, Loss: 1.464 Epoch 4 Batch 557/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.935, Loss: 1.514 Epoch 4 Batch 558/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.931, Loss: 1.471 Epoch 4 Batch 559/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.932, Loss: 1.531 Epoch 4 Batch 560/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.930, Loss: 1.385 Epoch 4 Batch 561/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.930, Loss: 1.521 Epoch 4 Batch 562/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.924, Loss: 1.480 Epoch 4 Batch 563/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.920, Loss: 1.485 Epoch 4 Batch 564/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.908, Loss: 1.453 Epoch 4 Batch 565/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.907, Loss: 1.480 Epoch 4 Batch 566/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.902, Loss: 1.463 Epoch 4 Batch 567/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.902, Loss: 1.475 Epoch 4 Batch 568/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.916, Loss: 1.436 Epoch 4 Batch 569/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.911, Loss: 1.472 Epoch 4 Batch 570/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.922, Loss: 1.417 Epoch 4 Batch 571/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.918, Loss: 1.496 Epoch 4 Batch 572/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.911, Loss: 1.495 Epoch 4 Batch 573/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.900, Loss: 1.388 Epoch 4 Batch 574/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.910, Loss: 1.459 Epoch 4 Batch 575/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.911, Loss: 1.456 Epoch 4 Batch 576/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.906, Loss: 1.453 Epoch 4 Batch 577/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.905, Loss: 1.519 Epoch 4 Batch 578/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.902, Loss: 1.463 Epoch 4 Batch 579/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.900, Loss: 1.363 Epoch 4 Batch 580/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.902, Loss: 1.450 Epoch 4 Batch 581/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.902, Loss: 1.430 Epoch 4 Batch 582/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.897, Loss: 1.372 Epoch 4 Batch 583/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.902, Loss: 1.419 Epoch 4 Batch 584/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.897, Loss: 1.506 Epoch 4 Batch 585/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.897, Loss: 1.352 Epoch 4 Batch 586/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.902, Loss: 1.437 Epoch 4 Batch 587/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.899, Loss: 1.494 Epoch 4 Batch 588/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.913, Loss: 1.419 Epoch 4 Batch 589/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.908, Loss: 1.450 Epoch 4 Batch 590/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.907, Loss: 1.386 Epoch 4 Batch 591/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.913, Loss: 1.496 Epoch 4 Batch 592/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.913, Loss: 1.436 Epoch 4 Batch 593/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.922, Loss: 1.499 Epoch 4 Batch 594/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.903, Loss: 1.509 Epoch 4 Batch 595/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.903, Loss: 1.469 Epoch 4 Batch 596/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.914, Loss: 1.544 Epoch 4 Batch 597/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.926, Loss: 1.485 Epoch 4 Batch 598/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.921, Loss: 1.478 Epoch 4 Batch 599/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.922, Loss: 1.556 Epoch 4 Batch 600/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.913, Loss: 1.468 Epoch 4 Batch 601/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.919, Loss: 1.442 Epoch 4 Batch 602/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.925, Loss: 1.548 Epoch 4 Batch 603/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.932, Loss: 1.484 Epoch 4 Batch 604/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.922, Loss: 1.509 Epoch 4 Batch 605/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.918, Loss: 1.544 Epoch 4 Batch 606/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.916, Loss: 1.434 Epoch 4 Batch 607/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.915, Loss: 1.456 Epoch 4 Batch 608/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.911, Loss: 1.453 Epoch 4 Batch 609/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.918, Loss: 1.558 Epoch 4 Batch 610/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.923, Loss: 1.527 Epoch 4 Batch 611/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.910, Loss: 1.437 Epoch 4 Batch 612/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.912, Loss: 1.414 Epoch 4 Batch 613/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.911, Loss: 1.476 Epoch 4 Batch 614/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.912, Loss: 1.471 Epoch 4 Batch 615/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.917, Loss: 1.462 Epoch 4 Batch 616/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.909, Loss: 1.422 Epoch 4 Batch 617/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.907, Loss: 1.483 Epoch 4 Batch 618/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.913, Loss: 1.490 Epoch 4 Batch 619/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.918, Loss: 1.477 Epoch 4 Batch 620/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.918, Loss: 1.464 Epoch 4 Batch 621/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.929, Loss: 1.442 Epoch 4 Batch 622/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.929, Loss: 1.452 Epoch 4 Batch 623/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.935, Loss: 1.436 Epoch 4 Batch 624/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.934, Loss: 1.453 Epoch 4 Batch 625/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.940, Loss: 1.473 Epoch 4 Batch 626/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.938, Loss: 1.470 Epoch 4 Batch 627/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.922, Loss: 1.450 Epoch 4 Batch 628/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.920, Loss: 1.484 Epoch 4 Batch 629/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.924, Loss: 1.520 Epoch 4 Batch 630/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.924, Loss: 1.517 Epoch 4 Batch 631/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.924, Loss: 1.517 Epoch 4 Batch 632/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.925, Loss: 1.471 Epoch 4 Batch 633/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.935, Loss: 1.440 Epoch 4 Batch 634/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.930, Loss: 1.433 Epoch 4 Batch 635/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.927, Loss: 1.499 Epoch 4 Batch 636/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.921, Loss: 1.431 Epoch 4 Batch 637/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.920, Loss: 1.472 Epoch 4 Batch 638/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.912, Loss: 1.378 Epoch 4 Batch 639/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.912, Loss: 1.405 Epoch 4 Batch 640/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.912, Loss: 1.399 Epoch 4 Batch 641/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.903, Loss: 1.414 Epoch 4 Batch 642/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.894, Loss: 1.483 Epoch 4 Batch 643/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.899, Loss: 1.480 Epoch 4 Batch 644/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.907, Loss: 1.459 Epoch 4 Batch 645/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.907, Loss: 1.473 Epoch 4 Batch 646/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.895, Loss: 1.419 Epoch 4 Batch 647/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.895, Loss: 1.497 Epoch 4 Batch 648/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.894, Loss: 1.426 Epoch 4 Batch 649/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.889, Loss: 1.438 Epoch 4 Batch 650/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.902, Loss: 1.515 Epoch 4 Batch 651/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.906, Loss: 1.456 Epoch 4 Batch 652/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.906, Loss: 1.431 Epoch 4 Batch 653/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.913, Loss: 1.383 Epoch 4 Batch 654/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.913, Loss: 1.445 Epoch 4 Batch 655/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.923, Loss: 1.417 Epoch 4 Batch 656/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.935, Loss: 1.489 Epoch 4 Batch 657/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.930, Loss: 1.461 Epoch 4 Batch 658/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.929, Loss: 1.449 Epoch 4 Batch 659/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.929, Loss: 1.482 Epoch 4 Batch 660/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.928, Loss: 1.503 Epoch 4 Batch 661/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.924, Loss: 1.498 Epoch 4 Batch 662/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.923, Loss: 1.430 Epoch 4 Batch 663/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.923, Loss: 1.462 Epoch 4 Batch 664/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.917, Loss: 1.469 Epoch 4 Batch 665/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.913, Loss: 1.431 Epoch 4 Batch 666/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.912, Loss: 1.499 Epoch 4 Batch 667/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.917, Loss: 1.398 Epoch 4 Batch 668/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.914, Loss: 1.360 Epoch 4 Batch 669/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.916, Loss: 1.442 Epoch 4 Batch 670/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.919, Loss: 1.417 Epoch 4 Batch 671/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.924, Loss: 1.530 Epoch 4 Batch 672/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.917, Loss: 1.369 Epoch 4 Batch 673/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.922, Loss: 1.398 Epoch 4 Batch 674/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.921, Loss: 1.549 Epoch 4 Batch 675/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.935, Loss: 1.503 Epoch 4 Batch 676/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.935, Loss: 1.458 Epoch 4 Batch 677/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.936, Loss: 1.534 Epoch 4 Batch 678/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.935, Loss: 1.448 Epoch 4 Batch 679/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.935, Loss: 1.447 Epoch 4 Batch 680/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.931, Loss: 1.490 Epoch 4 Batch 681/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.926, Loss: 1.401 Epoch 4 Batch 682/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.937, Loss: 1.484 Epoch 4 Batch 683/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.942, Loss: 1.448 Epoch 4 Batch 684/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.941, Loss: 1.429 Epoch 4 Batch 685/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.941, Loss: 1.484 Epoch 4 Batch 686/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.941, Loss: 1.450 Epoch 4 Batch 687/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.935, Loss: 1.426 Epoch 4 Batch 688/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.931, Loss: 1.470 Epoch 4 Batch 689/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.925, Loss: 1.477 Epoch 4 Batch 690/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.930, Loss: 1.485 Epoch 4 Batch 691/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.924, Loss: 1.431 Epoch 4 Batch 692/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.924, Loss: 1.412 Epoch 4 Batch 693/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.918, Loss: 1.448 Epoch 4 Batch 694/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.912, Loss: 1.480 Epoch 4 Batch 695/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.904, Loss: 1.438 Epoch 4 Batch 696/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.902, Loss: 1.499 Epoch 4 Batch 697/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.902, Loss: 1.477 Epoch 4 Batch 698/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.904, Loss: 1.462 Epoch 4 Batch 699/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.899, Loss: 1.407 Epoch 4 Batch 700/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.895, Loss: 1.445 Epoch 4 Batch 701/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.897, Loss: 1.453 Epoch 4 Batch 702/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.903, Loss: 1.420 Epoch 4 Batch 703/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.912, Loss: 1.487 Epoch 4 Batch 704/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.906, Loss: 1.468 Epoch 4 Batch 705/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.912, Loss: 1.483 Epoch 4 Batch 706/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.923, Loss: 1.523 Epoch 4 Batch 707/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.919, Loss: 1.414 Epoch 4 Batch 708/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.930, Loss: 1.392 Epoch 4 Batch 709/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.934, Loss: 1.536 Epoch 4 Batch 710/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.926, Loss: 1.415 Epoch 4 Batch 711/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.923, Loss: 1.432 Epoch 4 Batch 712/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.919, Loss: 1.498 Epoch 4 Batch 713/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.923, Loss: 1.415 Epoch 4 Batch 714/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.912, Loss: 1.489 Epoch 4 Batch 715/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.912, Loss: 1.445 Epoch 4 Batch 716/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.911, Loss: 1.475 Epoch 4 Batch 717/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.911, Loss: 1.424 Epoch 4 Batch 718/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.915, Loss: 1.427 Epoch 4 Batch 719/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.915, Loss: 1.423 Epoch 4 Batch 720/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.920, Loss: 1.453 Epoch 4 Batch 721/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.925, Loss: 1.423 Epoch 4 Batch 722/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.934, Loss: 1.529 Epoch 4 Batch 723/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.935, Loss: 1.462 Epoch 4 Batch 724/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.930, Loss: 1.396 Epoch 4 Batch 725/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.931, Loss: 1.491 Epoch 4 Batch 726/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.942, Loss: 1.428 Epoch 4 Batch 727/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.938, Loss: 1.411 Epoch 4 Batch 728/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.929, Loss: 1.494 Epoch 4 Batch 729/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.929, Loss: 1.421 Epoch 4 Batch 730/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.930, Loss: 1.476 Epoch 4 Batch 731/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.925, Loss: 1.456 Epoch 4 Batch 732/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.926, Loss: 1.418 Epoch 4 Batch 733/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.926, Loss: 1.437 Epoch 4 Batch 734/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.934, Loss: 1.462 Epoch 4 Batch 735/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.933, Loss: 1.443 Epoch 4 Batch 736/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.928, Loss: 1.479 Epoch 4 Batch 737/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.927, Loss: 1.475 Epoch 4 Batch 738/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.922, Loss: 1.412 Epoch 4 Batch 739/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.922, Loss: 1.503 Epoch 4 Batch 740/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.922, Loss: 1.512 Epoch 4 Batch 741/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.927, Loss: 1.484 Epoch 4 Batch 742/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.920, Loss: 1.439 Epoch 4 Batch 743/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.920, Loss: 1.505 Epoch 4 Batch 744/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.921, Loss: 1.521 Epoch 4 Batch 745/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.926, Loss: 1.458 Epoch 4 Batch 746/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.922, Loss: 1.446 Epoch 4 Batch 747/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.915, Loss: 1.481 Epoch 4 Batch 748/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.915, Loss: 1.454 Epoch 4 Batch 749/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.921, Loss: 1.517 Epoch 4 Batch 750/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.928, Loss: 1.480 Epoch 4 Batch 751/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.932, Loss: 1.500 Epoch 4 Batch 752/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.937, Loss: 1.450 Epoch 4 Batch 753/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.936, Loss: 1.384 Epoch 4 Batch 754/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.932, Loss: 1.382 Epoch 4 Batch 755/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.935, Loss: 1.526 Epoch 4 Batch 756/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.935, Loss: 1.386 Epoch 4 Batch 757/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.922, Loss: 1.442 Epoch 4 Batch 758/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.926, Loss: 1.430 Epoch 4 Batch 759/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.926, Loss: 1.431 Epoch 4 Batch 760/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.926, Loss: 1.475 Epoch 4 Batch 761/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.930, Loss: 1.480 Epoch 4 Batch 762/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.933, Loss: 1.438 Epoch 4 Batch 763/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.927, Loss: 1.453 Epoch 4 Batch 764/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.929, Loss: 1.516 Epoch 4 Batch 765/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.933, Loss: 1.384 Epoch 4 Batch 766/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.933, Loss: 1.457 Epoch 4 Batch 767/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.933, Loss: 1.420 Epoch 4 Batch 768/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.938, Loss: 1.451 Epoch 4 Batch 769/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.939, Loss: 1.519 Epoch 4 Batch 770/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.948, Loss: 1.435 Epoch 4 Batch 771/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.947, Loss: 1.484 Epoch 4 Batch 772/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.945, Loss: 1.453 Epoch 4 Batch 773/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.946, Loss: 1.442 Epoch 4 Batch 774/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.947, Loss: 1.413 Epoch 4 Batch 775/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.941, Loss: 1.438 Epoch 4 Batch 776/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.945, Loss: 1.500 Epoch 4 Batch 777/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.946, Loss: 1.412 Epoch 4 Batch 778/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.932, Loss: 1.434 Epoch 4 Batch 779/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.934, Loss: 1.412 Epoch 4 Batch 780/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.927, Loss: 1.389 Epoch 4 Batch 781/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.931, Loss: 1.441 Epoch 4 Batch 782/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.931, Loss: 1.452 Epoch 4 Batch 783/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.924, Loss: 1.517 Epoch 4 Batch 784/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.924, Loss: 1.441 Epoch 4 Batch 785/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.923, Loss: 1.463 Epoch 4 Batch 786/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.923, Loss: 1.438 Epoch 4 Batch 787/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.929, Loss: 1.394 Epoch 4 Batch 788/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.931, Loss: 1.463 Epoch 4 Batch 789/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.926, Loss: 1.411 Epoch 4 Batch 790/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.920, Loss: 1.450 Epoch 4 Batch 791/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.918, Loss: 1.481 Epoch 4 Batch 792/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.933, Loss: 1.402 Epoch 4 Batch 793/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.916, Loss: 1.447 Epoch 4 Batch 794/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.921, Loss: 1.423 Epoch 4 Batch 795/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.914, Loss: 1.402 Epoch 4 Batch 796/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.921, Loss: 1.384 Epoch 4 Batch 797/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.927, Loss: 1.509 Epoch 4 Batch 798/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.930, Loss: 1.384 Epoch 4 Batch 799/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.924, Loss: 1.460 Epoch 4 Batch 800/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.920, Loss: 1.446 Epoch 4 Batch 801/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.924, Loss: 1.481 Epoch 4 Batch 802/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.928, Loss: 1.400 Epoch 4 Batch 803/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.933, Loss: 1.476 Epoch 4 Batch 804/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.935, Loss: 1.416 Epoch 4 Batch 805/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.935, Loss: 1.468 Epoch 4 Batch 806/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.923, Loss: 1.415 Epoch 4 Batch 807/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.918, Loss: 1.420 Epoch 4 Batch 808/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.921, Loss: 1.499 Epoch 4 Batch 809/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.931, Loss: 1.517 Epoch 4 Batch 810/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.937, Loss: 1.418 Epoch 4 Batch 811/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.930, Loss: 1.500 Epoch 4 Batch 812/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.930, Loss: 1.517 Epoch 4 Batch 813/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.930, Loss: 1.476 Epoch 4 Batch 814/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.931, Loss: 1.447 Epoch 4 Batch 815/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.933, Loss: 1.429 Epoch 4 Batch 816/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.937, Loss: 1.470 Epoch 4 Batch 817/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.936, Loss: 1.442 Epoch 4 Batch 818/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.934, Loss: 1.411 Epoch 4 Batch 819/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.929, Loss: 1.470 Epoch 4 Batch 820/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.934, Loss: 1.463 Epoch 4 Batch 821/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.929, Loss: 1.442 Epoch 4 Batch 822/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.922, Loss: 1.420 Epoch 4 Batch 823/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.925, Loss: 1.448 Epoch 4 Batch 824/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.924, Loss: 1.429 Epoch 4 Batch 825/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.919, Loss: 1.352 Epoch 4 Batch 826/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.920, Loss: 1.447 Epoch 4 Batch 827/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.920, Loss: 1.460 Epoch 4 Batch 828/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.920, Loss: 1.407 Epoch 4 Batch 829/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.920, Loss: 1.484 Epoch 4 Batch 830/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.915, Loss: 1.476 Epoch 4 Batch 831/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.920, Loss: 1.462 Epoch 4 Batch 832/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.919, Loss: 1.484 Epoch 4 Batch 833/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.922, Loss: 1.488 Epoch 4 Batch 834/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.918, Loss: 1.565 Epoch 4 Batch 835/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.924, Loss: 1.405 Epoch 4 Batch 836/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.913, Loss: 1.493 Epoch 4 Batch 837/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.919, Loss: 1.479 Epoch 4 Batch 838/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.919, Loss: 1.460 Epoch 4 Batch 839/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.925, Loss: 1.465 Epoch 4 Batch 840/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.931, Loss: 1.420 Epoch 4 Batch 841/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.927, Loss: 1.395 Epoch 4 Batch 842/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.927, Loss: 1.420 Epoch 4 Batch 843/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.931, Loss: 1.461 Epoch 4 Batch 844/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.936, Loss: 1.454 Epoch 4 Batch 845/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.936, Loss: 1.443 Epoch 4 Batch 846/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.939, Loss: 1.522 Epoch 4 Batch 847/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.939, Loss: 1.452 Epoch 4 Batch 848/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.945, Loss: 1.402 Epoch 4 Batch 849/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.949, Loss: 1.437 Epoch 4 Batch 850/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.949, Loss: 1.420 Epoch 4 Batch 851/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.949, Loss: 1.427 Epoch 4 Batch 852/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.949, Loss: 1.421 Epoch 4 Batch 853/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.946, Loss: 1.514 Epoch 4 Batch 854/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.941, Loss: 1.423 Epoch 4 Batch 855/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.945, Loss: 1.471 Epoch 4 Batch 856/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.946, Loss: 1.443 Epoch 4 Batch 857/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.943, Loss: 1.477 Epoch 4 Batch 858/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.932, Loss: 1.435 Epoch 4 Batch 859/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.944, Loss: 1.415 Epoch 4 Batch 860/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.940, Loss: 1.419 Epoch 4 Batch 861/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.936, Loss: 1.414 Epoch 4 Batch 862/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.940, Loss: 1.452 Epoch 4 Batch 863/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.933, Loss: 1.401 Epoch 4 Batch 864/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.933, Loss: 1.461 Epoch 4 Batch 865/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.923, Loss: 1.377 Epoch 4 Batch 866/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.923, Loss: 1.438 Epoch 4 Batch 867/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.923, Loss: 1.520 Epoch 4 Batch 868/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.931, Loss: 1.471 Epoch 4 Batch 869/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.927, Loss: 1.474 Epoch 4 Batch 870/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.927, Loss: 1.421 Epoch 4 Batch 871/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.933, Loss: 1.446 Epoch 4 Batch 872/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.937, Loss: 1.379 Epoch 4 Batch 873/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.937, Loss: 1.422 Epoch 4 Batch 874/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.939, Loss: 1.515 Epoch 4 Batch 875/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.943, Loss: 1.480 Epoch 4 Batch 876/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.947, Loss: 1.510 Epoch 4 Batch 877/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.948, Loss: 1.469 Epoch 4 Batch 878/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.948, Loss: 1.359 Epoch 4 Batch 879/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.948, Loss: 1.475 Epoch 4 Batch 880/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.944, Loss: 1.472 Epoch 4 Batch 881/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.939, Loss: 1.452 Epoch 4 Batch 882/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.936, Loss: 1.432 Epoch 4 Batch 883/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.937, Loss: 1.441 Epoch 4 Batch 884/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.936, Loss: 1.481 Epoch 4 Batch 885/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.941, Loss: 1.442 Epoch 4 Batch 886/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.947, Loss: 1.410 Epoch 4 Batch 887/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.943, Loss: 1.474 Epoch 4 Batch 888/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.936, Loss: 1.401 Epoch 4 Batch 889/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.936, Loss: 1.390 Epoch 4 Batch 890/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.936, Loss: 1.452 Epoch 4 Batch 891/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.931, Loss: 1.503 Epoch 4 Batch 892/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.931, Loss: 1.431 Epoch 4 Batch 893/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.943, Loss: 1.374 Epoch 4 Batch 894/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.942, Loss: 1.515 Epoch 4 Batch 895/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.937, Loss: 1.361 Epoch 4 Batch 896/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.934, Loss: 1.442 Epoch 4 Batch 897/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.934, Loss: 1.477 Epoch 4 Batch 898/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.934, Loss: 1.442 Epoch 4 Batch 899/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.930, Loss: 1.503 Epoch 4 Batch 900/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.924, Loss: 1.411 Epoch 4 Batch 901/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.929, Loss: 1.426 Epoch 4 Batch 902/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.930, Loss: 1.501 Epoch 4 Batch 903/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.933, Loss: 1.426 Epoch 4 Batch 904/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.931, Loss: 1.406 Epoch 4 Batch 905/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.931, Loss: 1.491 Epoch 4 Batch 906/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.926, Loss: 1.440 Epoch 4 Batch 907/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.931, Loss: 1.461 Epoch 4 Batch 908/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.936, Loss: 1.457 Epoch 4 Batch 909/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.926, Loss: 1.529 Epoch 4 Batch 910/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.933, Loss: 1.481 Epoch 4 Batch 911/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.938, Loss: 1.386 Epoch 4 Batch 912/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.939, Loss: 1.495 Epoch 4 Batch 913/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.939, Loss: 1.455 Epoch 4 Batch 914/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.930, Loss: 1.526 Epoch 4 Batch 915/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.934, Loss: 1.445 Epoch 4 Batch 916/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.927, Loss: 1.443 Epoch 4 Batch 917/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.938, Loss: 1.442 Epoch 4 Batch 918/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.942, Loss: 1.455 Epoch 4 Batch 919/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.942, Loss: 1.438 Epoch 4 Batch 920/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.942, Loss: 1.398 Epoch 4 Batch 921/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.946, Loss: 1.495 Epoch 4 Batch 922/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.940, Loss: 1.459 Epoch 4 Batch 923/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.948, Loss: 1.440 Epoch 4 Batch 924/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.945, Loss: 1.480 Epoch 4 Batch 925/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.945, Loss: 1.457 Epoch 4 Batch 926/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.946, Loss: 1.467 Epoch 4 Batch 927/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.942, Loss: 1.398 Epoch 4 Batch 928/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.941, Loss: 1.433 Epoch 4 Batch 929/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.933, Loss: 1.465 Epoch 4 Batch 930/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.927, Loss: 1.519 Epoch 4 Batch 931/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.919, Loss: 1.465 Epoch 4 Batch 932/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.919, Loss: 1.452 Epoch 4 Batch 933/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.914, Loss: 1.461 Epoch 4 Batch 934/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.919, Loss: 1.360 Epoch 4 Batch 935/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.924, Loss: 1.445 Epoch 4 Batch 936/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.930, Loss: 1.452 Epoch 4 Batch 937/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.930, Loss: 1.378 Epoch 4 Batch 938/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.931, Loss: 1.493 Epoch 4 Batch 939/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.924, Loss: 1.559 Epoch 4 Batch 940/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.926, Loss: 1.379 Epoch 4 Batch 941/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.927, Loss: 1.417 Epoch 4 Batch 942/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.927, Loss: 1.455 Epoch 4 Batch 943/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.932, Loss: 1.454 Epoch 4 Batch 944/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.923, Loss: 1.442 Epoch 4 Batch 945/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.923, Loss: 1.417 Epoch 4 Batch 946/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.923, Loss: 1.483 Epoch 4 Batch 947/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.921, Loss: 1.460 Epoch 4 Batch 948/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.924, Loss: 1.431 Epoch 4 Batch 949/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.918, Loss: 1.419 Epoch 4 Batch 950/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.914, Loss: 1.446 Epoch 4 Batch 951/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.914, Loss: 1.463 Epoch 4 Batch 952/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.914, Loss: 1.463 Epoch 4 Batch 953/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.919, Loss: 1.494 Epoch 4 Batch 954/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.932, Loss: 1.489 Epoch 4 Batch 955/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.939, Loss: 1.405 Epoch 4 Batch 956/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.933, Loss: 1.480 Epoch 4 Batch 957/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.933, Loss: 1.452 Epoch 4 Batch 958/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.941, Loss: 1.441 Epoch 4 Batch 959/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.942, Loss: 1.420 Epoch 4 Batch 960/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.935, Loss: 1.493 Epoch 4 Batch 961/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.930, Loss: 1.470 Epoch 4 Batch 962/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.930, Loss: 1.427 Epoch 4 Batch 963/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.934, Loss: 1.472 Epoch 4 Batch 964/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.933, Loss: 1.425 Epoch 4 Batch 965/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.930, Loss: 1.374 Epoch 4 Batch 966/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.930, Loss: 1.382 Epoch 4 Batch 967/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.925, Loss: 1.588 Epoch 4 Batch 968/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.919, Loss: 1.478 Epoch 4 Batch 969/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.919, Loss: 1.464 Epoch 4 Batch 970/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.912, Loss: 1.394 Epoch 4 Batch 971/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.906, Loss: 1.437 Epoch 4 Batch 972/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.908, Loss: 1.372 Epoch 4 Batch 973/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.908, Loss: 1.483 Epoch 4 Batch 974/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.919, Loss: 1.384 Epoch 4 Batch 975/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.912, Loss: 1.406 Epoch 4 Batch 976/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.920, Loss: 1.464 Epoch 4 Batch 977/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.928, Loss: 1.467 Epoch 4 Batch 978/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.923, Loss: 1.393 Epoch 4 Batch 979/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.923, Loss: 1.482 Epoch 4 Batch 980/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.928, Loss: 1.396 Epoch 4 Batch 981/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.928, Loss: 1.479 Epoch 4 Batch 982/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.923, Loss: 1.467 Epoch 4 Batch 983/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.927, Loss: 1.488 Epoch 4 Batch 984/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.936, Loss: 1.477 Epoch 4 Batch 985/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.932, Loss: 1.364 Epoch 4 Batch 986/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.928, Loss: 1.419 Epoch 4 Batch 987/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.918, Loss: 1.420 Epoch 4 Batch 988/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.908, Loss: 1.474 Epoch 4 Batch 989/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.908, Loss: 1.465 Epoch 4 Batch 990/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.912, Loss: 1.461 Epoch 4 Batch 991/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.912, Loss: 1.518 Epoch 4 Batch 992/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.911, Loss: 1.473 Epoch 4 Batch 993/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.922, Loss: 1.447 Epoch 4 Batch 994/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.922, Loss: 1.498 Epoch 4 Batch 995/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.925, Loss: 1.332 Epoch 4 Batch 996/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.930, Loss: 1.371 Epoch 4 Batch 997/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.930, Loss: 1.542 Epoch 4 Batch 998/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.924, Loss: 1.390 Epoch 4 Batch 999/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.929, Loss: 1.439 Epoch 4 Batch 1000/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.920, Loss: 1.462 Epoch 4 Batch 1001/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.929, Loss: 1.421 Epoch 4 Batch 1002/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.924, Loss: 1.429 Epoch 4 Batch 1003/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.929, Loss: 1.512 Epoch 4 Batch 1004/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.924, Loss: 1.411 Epoch 4 Batch 1005/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.923, Loss: 1.456 Epoch 4 Batch 1006/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.913, Loss: 1.460 Epoch 4 Batch 1007/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.923, Loss: 1.476 Epoch 4 Batch 1008/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.916, Loss: 1.495 Epoch 4 Batch 1009/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.917, Loss: 1.415 Epoch 4 Batch 1010/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.914, Loss: 1.446 Epoch 4 Batch 1011/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.914, Loss: 1.432 Epoch 4 Batch 1012/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.919, Loss: 1.441 Epoch 4 Batch 1013/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.918, Loss: 1.437 Epoch 4 Batch 1014/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.913, Loss: 1.399 Epoch 4 Batch 1015/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.911, Loss: 1.502 Epoch 4 Batch 1016/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.915, Loss: 1.475 Epoch 4 Batch 1017/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.907, Loss: 1.458 Epoch 4 Batch 1018/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.915, Loss: 1.440 Epoch 4 Batch 1019/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.907, Loss: 1.493 Epoch 4 Batch 1020/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.921, Loss: 1.453 Epoch 4 Batch 1021/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.925, Loss: 1.460 Epoch 4 Batch 1022/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.924, Loss: 1.497 Epoch 4 Batch 1023/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.924, Loss: 1.488 Epoch 4 Batch 1024/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.923, Loss: 1.377 Epoch 4 Batch 1025/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.915, Loss: 1.459 Epoch 4 Batch 1026/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.915, Loss: 1.378 Epoch 4 Batch 1027/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.917, Loss: 1.458 Epoch 4 Batch 1028/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.922, Loss: 1.479 Epoch 4 Batch 1029/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.921, Loss: 1.330 Epoch 4 Batch 1030/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.926, Loss: 1.423 Epoch 4 Batch 1031/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.928, Loss: 1.421 Epoch 4 Batch 1032/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.928, Loss: 1.442 Epoch 4 Batch 1033/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.923, Loss: 1.494 Epoch 4 Batch 1034/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.928, Loss: 1.475 Epoch 4 Batch 1035/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.911, Loss: 1.486 Epoch 4 Batch 1036/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.911, Loss: 1.513 Epoch 4 Batch 1037/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.910, Loss: 1.539 Epoch 4 Batch 1038/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.910, Loss: 1.476 Epoch 4 Batch 1039/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.923, Loss: 1.516 Epoch 4 Batch 1040/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.916, Loss: 1.423 Epoch 4 Batch 1041/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.916, Loss: 1.427 Epoch 4 Batch 1042/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.920, Loss: 1.418 Epoch 4 Batch 1043/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.921, Loss: 1.439 Epoch 4 Batch 1044/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.934, Loss: 1.475 Epoch 4 Batch 1045/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.939, Loss: 1.424 Epoch 4 Batch 1046/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.942, Loss: 1.490 Epoch 4 Batch 1047/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.941, Loss: 1.398 Epoch 4 Batch 1048/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.941, Loss: 1.488 Epoch 4 Batch 1049/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.922, Loss: 1.440 Epoch 4 Batch 1050/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.925, Loss: 1.440 Epoch 4 Batch 1051/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.925, Loss: 1.499 Epoch 4 Batch 1052/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.925, Loss: 1.480 Epoch 4 Batch 1053/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.930, Loss: 1.371 Epoch 4 Batch 1054/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.926, Loss: 1.392 Epoch 4 Batch 1055/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.933, Loss: 1.410 Epoch 4 Batch 1056/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.938, Loss: 1.456 Epoch 4 Batch 1057/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.940, Loss: 1.469 Epoch 4 Batch 1058/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.936, Loss: 1.486 Epoch 4 Batch 1059/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.935, Loss: 1.433 Epoch 4 Batch 1060/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.929, Loss: 1.450 Epoch 4 Batch 1061/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.931, Loss: 1.545 Epoch 4 Batch 1062/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.931, Loss: 1.507 Epoch 4 Batch 1063/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.933, Loss: 1.421 Epoch 4 Batch 1064/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.932, Loss: 1.439 Epoch 4 Batch 1065/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.932, Loss: 1.436 Epoch 4 Batch 1066/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.931, Loss: 1.435 Epoch 4 Batch 1067/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.934, Loss: 1.462 Epoch 4 Batch 1068/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.941, Loss: 1.409 Epoch 4 Batch 1069/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.941, Loss: 1.533 Epoch 4 Batch 1070/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.941, Loss: 1.459 Epoch 4 Batch 1071/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.948, Loss: 1.470 Epoch 4 Batch 1072/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.947, Loss: 1.479 Epoch 4 Batch 1073/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.945, Loss: 1.423 Epoch 4 Batch 1074/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.484 Epoch 4 Batch 1075/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.944, Loss: 1.446 Epoch 5 Batch 0/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.944, Loss: 1.428 Epoch 5 Batch 1/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.950, Loss: 1.493 Epoch 5 Batch 2/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.945, Loss: 1.507 Epoch 5 Batch 3/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.945, Loss: 1.467 Epoch 5 Batch 4/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.945, Loss: 1.444 Epoch 5 Batch 5/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.945, Loss: 1.480 Epoch 5 Batch 6/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.938, Loss: 1.363 Epoch 5 Batch 7/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.943, Loss: 1.510 Epoch 5 Batch 8/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.948, Loss: 1.443 Epoch 5 Batch 9/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.947, Loss: 1.392 Epoch 5 Batch 10/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.948, Loss: 1.477 Epoch 5 Batch 11/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.948, Loss: 1.439 Epoch 5 Batch 12/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.944, Loss: 1.472 Epoch 5 Batch 13/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.938, Loss: 1.433 Epoch 5 Batch 14/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.937, Loss: 1.481 Epoch 5 Batch 15/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.926, Loss: 1.494 Epoch 5 Batch 16/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.933, Loss: 1.510 Epoch 5 Batch 17/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.938, Loss: 1.389 Epoch 5 Batch 18/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.948, Loss: 1.442 Epoch 5 Batch 19/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.948, Loss: 1.423 Epoch 5 Batch 20/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.946, Loss: 1.509 Epoch 5 Batch 21/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.946, Loss: 1.496 Epoch 5 Batch 22/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.946, Loss: 1.374 Epoch 5 Batch 23/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.946, Loss: 1.449 Epoch 5 Batch 24/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.941, Loss: 1.443 Epoch 5 Batch 25/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.940, Loss: 1.452 Epoch 5 Batch 26/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.950, Loss: 1.479 Epoch 5 Batch 27/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.947, Loss: 1.458 Epoch 5 Batch 28/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.939, Loss: 1.434 Epoch 5 Batch 29/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.933, Loss: 1.360 Epoch 5 Batch 30/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.919, Loss: 1.422 Epoch 5 Batch 31/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.919, Loss: 1.481 Epoch 5 Batch 32/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.919, Loss: 1.460 Epoch 5 Batch 33/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.915, Loss: 1.405 Epoch 5 Batch 34/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.915, Loss: 1.501 Epoch 5 Batch 35/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.915, Loss: 1.485 Epoch 5 Batch 36/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.917, Loss: 1.418 Epoch 5 Batch 37/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.913, Loss: 1.555 Epoch 5 Batch 38/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.913, Loss: 1.469 Epoch 5 Batch 39/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.913, Loss: 1.440 Epoch 5 Batch 40/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.908, Loss: 1.408 Epoch 5 Batch 41/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.913, Loss: 1.393 Epoch 5 Batch 42/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.919, Loss: 1.495 Epoch 5 Batch 43/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.919, Loss: 1.423 Epoch 5 Batch 44/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.930, Loss: 1.408 Epoch 5 Batch 45/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.950, Loss: 1.391 Epoch 5 Batch 46/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.950, Loss: 1.519 Epoch 5 Batch 47/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.950, Loss: 1.563 Epoch 5 Batch 48/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.951, Loss: 1.426 Epoch 5 Batch 49/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.947, Loss: 1.464 Epoch 5 Batch 50/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.949, Loss: 1.484 Epoch 5 Batch 51/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.949, Loss: 1.453 Epoch 5 Batch 52/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.952, Loss: 1.374 Epoch 5 Batch 53/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.952, Loss: 1.431 Epoch 5 Batch 54/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.951, Loss: 1.442 Epoch 5 Batch 55/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.941, Loss: 1.412 Epoch 5 Batch 56/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.929, Loss: 1.401 Epoch 5 Batch 57/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.928, Loss: 1.475 Epoch 5 Batch 58/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.927, Loss: 1.374 Epoch 5 Batch 59/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.915, Loss: 1.415 Epoch 5 Batch 60/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.916, Loss: 1.454 Epoch 5 Batch 61/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.911, Loss: 1.400 Epoch 5 Batch 62/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.917, Loss: 1.386 Epoch 5 Batch 63/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.918, Loss: 1.476 Epoch 5 Batch 64/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.925, Loss: 1.493 Epoch 5 Batch 65/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.941, Loss: 1.451 Epoch 5 Batch 66/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.946, Loss: 1.433 Epoch 5 Batch 67/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.942, Loss: 1.532 Epoch 5 Batch 68/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.929, Loss: 1.406 Epoch 5 Batch 69/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.934, Loss: 1.469 Epoch 5 Batch 70/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.931, Loss: 1.478 Epoch 5 Batch 71/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.926, Loss: 1.406 Epoch 5 Batch 72/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.928, Loss: 1.442 Epoch 5 Batch 73/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.928, Loss: 1.455 Epoch 5 Batch 74/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.936, Loss: 1.417 Epoch 5 Batch 75/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.938, Loss: 1.418 Epoch 5 Batch 76/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.939, Loss: 1.446 Epoch 5 Batch 77/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.939, Loss: 1.447 Epoch 5 Batch 78/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.945, Loss: 1.420 Epoch 5 Batch 79/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.948, Loss: 1.414 Epoch 5 Batch 80/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.943, Loss: 1.484 Epoch 5 Batch 81/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.939, Loss: 1.458 Epoch 5 Batch 82/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.945, Loss: 1.459 Epoch 5 Batch 83/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.940, Loss: 1.447 Epoch 5 Batch 84/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.937, Loss: 1.498 Epoch 5 Batch 85/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.935, Loss: 1.368 Epoch 5 Batch 86/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.461 Epoch 5 Batch 87/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.934, Loss: 1.455 Epoch 5 Batch 88/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.935, Loss: 1.419 Epoch 5 Batch 89/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.935, Loss: 1.470 Epoch 5 Batch 90/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.932, Loss: 1.413 Epoch 5 Batch 91/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.926, Loss: 1.435 Epoch 5 Batch 92/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.924, Loss: 1.418 Epoch 5 Batch 93/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.926, Loss: 1.470 Epoch 5 Batch 94/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.927, Loss: 1.491 Epoch 5 Batch 95/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.913, Loss: 1.482 Epoch 5 Batch 96/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.919, Loss: 1.504 Epoch 5 Batch 97/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.922, Loss: 1.429 Epoch 5 Batch 98/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.914, Loss: 1.359 Epoch 5 Batch 99/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.922, Loss: 1.400 Epoch 5 Batch 100/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.918, Loss: 1.387 Epoch 5 Batch 101/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.913, Loss: 1.495 Epoch 5 Batch 102/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.913, Loss: 1.489 Epoch 5 Batch 103/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.920, Loss: 1.527 Epoch 5 Batch 104/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.918, Loss: 1.443 Epoch 5 Batch 105/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.919, Loss: 1.502 Epoch 5 Batch 106/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.928, Loss: 1.449 Epoch 5 Batch 107/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.926, Loss: 1.432 Epoch 5 Batch 108/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.939, Loss: 1.401 Epoch 5 Batch 109/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.939, Loss: 1.452 Epoch 5 Batch 110/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.940, Loss: 1.447 Epoch 5 Batch 111/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.935, Loss: 1.467 Epoch 5 Batch 112/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.935, Loss: 1.453 Epoch 5 Batch 113/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.935, Loss: 1.354 Epoch 5 Batch 114/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.930, Loss: 1.396 Epoch 5 Batch 115/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.503 Epoch 5 Batch 116/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.932, Loss: 1.457 Epoch 5 Batch 117/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.925, Loss: 1.473 Epoch 5 Batch 118/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.926, Loss: 1.461 Epoch 5 Batch 119/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.921, Loss: 1.451 Epoch 5 Batch 120/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.925, Loss: 1.495 Epoch 5 Batch 121/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.944, Loss: 1.423 Epoch 5 Batch 122/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.941, Loss: 1.404 Epoch 5 Batch 123/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.941, Loss: 1.396 Epoch 5 Batch 124/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.936, Loss: 1.442 Epoch 5 Batch 125/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.940, Loss: 1.416 Epoch 5 Batch 126/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.938, Loss: 1.367 Epoch 5 Batch 127/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.939, Loss: 1.449 Epoch 5 Batch 128/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.934, Loss: 1.433 Epoch 5 Batch 129/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.941, Loss: 1.410 Epoch 5 Batch 130/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.935, Loss: 1.436 Epoch 5 Batch 131/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.925, Loss: 1.470 Epoch 5 Batch 132/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.930, Loss: 1.477 Epoch 5 Batch 133/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.922, Loss: 1.369 Epoch 5 Batch 134/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.915, Loss: 1.441 Epoch 5 Batch 135/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.898, Loss: 1.431 Epoch 5 Batch 136/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.902, Loss: 1.418 Epoch 5 Batch 137/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.902, Loss: 1.382 Epoch 5 Batch 138/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.896, Loss: 1.438 Epoch 5 Batch 139/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.892, Loss: 1.530 Epoch 5 Batch 140/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.901, Loss: 1.415 Epoch 5 Batch 141/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.902, Loss: 1.475 Epoch 5 Batch 142/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.925, Loss: 1.473 Epoch 5 Batch 143/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.928, Loss: 1.459 Epoch 5 Batch 144/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.928, Loss: 1.470 Epoch 5 Batch 145/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.928, Loss: 1.427 Epoch 5 Batch 146/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.925, Loss: 1.439 Epoch 5 Batch 147/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.925, Loss: 1.499 Epoch 5 Batch 148/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.925, Loss: 1.506 Epoch 5 Batch 149/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.926, Loss: 1.522 Epoch 5 Batch 150/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.919, Loss: 1.396 Epoch 5 Batch 151/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.920, Loss: 1.450 Epoch 5 Batch 152/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.925, Loss: 1.429 Epoch 5 Batch 153/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.925, Loss: 1.498 Epoch 5 Batch 154/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.925, Loss: 1.354 Epoch 5 Batch 155/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.925, Loss: 1.383 Epoch 5 Batch 156/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.925, Loss: 1.390 Epoch 5 Batch 157/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.925, Loss: 1.459 Epoch 5 Batch 158/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.936, Loss: 1.336 Epoch 5 Batch 159/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.923, Loss: 1.440 Epoch 5 Batch 160/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.923, Loss: 1.423 Epoch 5 Batch 161/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.928, Loss: 1.440 Epoch 5 Batch 162/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.928, Loss: 1.503 Epoch 5 Batch 163/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.919, Loss: 1.447 Epoch 5 Batch 164/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.919, Loss: 1.415 Epoch 5 Batch 165/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.924, Loss: 1.418 Epoch 5 Batch 166/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.926, Loss: 1.440 Epoch 5 Batch 167/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.931, Loss: 1.462 Epoch 5 Batch 168/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.926, Loss: 1.417 Epoch 5 Batch 169/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.926, Loss: 1.361 Epoch 5 Batch 170/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.926, Loss: 1.441 Epoch 5 Batch 171/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.921, Loss: 1.397 Epoch 5 Batch 172/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.930, Loss: 1.426 Epoch 5 Batch 173/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.923, Loss: 1.423 Epoch 5 Batch 174/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.913, Loss: 1.416 Epoch 5 Batch 175/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.923, Loss: 1.486 Epoch 5 Batch 176/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.923, Loss: 1.486 Epoch 5 Batch 177/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.930, Loss: 1.457 Epoch 5 Batch 178/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.936, Loss: 1.439 Epoch 5 Batch 179/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.935, Loss: 1.392 Epoch 5 Batch 180/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.935, Loss: 1.459 Epoch 5 Batch 181/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.935, Loss: 1.390 Epoch 5 Batch 182/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.936, Loss: 1.476 Epoch 5 Batch 183/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.940, Loss: 1.480 Epoch 5 Batch 184/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.940, Loss: 1.518 Epoch 5 Batch 185/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.944, Loss: 1.393 Epoch 5 Batch 186/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.945, Loss: 1.400 Epoch 5 Batch 187/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.945, Loss: 1.479 Epoch 5 Batch 188/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.945, Loss: 1.489 Epoch 5 Batch 189/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.945, Loss: 1.460 Epoch 5 Batch 190/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.945, Loss: 1.472 Epoch 5 Batch 191/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.943, Loss: 1.462 Epoch 5 Batch 192/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.939, Loss: 1.385 Epoch 5 Batch 193/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.943, Loss: 1.487 Epoch 5 Batch 194/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.943, Loss: 1.469 Epoch 5 Batch 195/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.943, Loss: 1.443 Epoch 5 Batch 196/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.945, Loss: 1.399 Epoch 5 Batch 197/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.945, Loss: 1.446 Epoch 5 Batch 198/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.946, Loss: 1.427 Epoch 5 Batch 199/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.946, Loss: 1.475 Epoch 5 Batch 200/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.946, Loss: 1.455 Epoch 5 Batch 201/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.942, Loss: 1.462 Epoch 5 Batch 202/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.935, Loss: 1.442 Epoch 5 Batch 203/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.931, Loss: 1.438 Epoch 5 Batch 204/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.925, Loss: 1.574 Epoch 5 Batch 205/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.938, Loss: 1.506 Epoch 5 Batch 206/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.933, Loss: 1.443 Epoch 5 Batch 207/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.929, Loss: 1.450 Epoch 5 Batch 208/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.935, Loss: 1.455 Epoch 5 Batch 209/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.935, Loss: 1.406 Epoch 5 Batch 210/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.939, Loss: 1.493 Epoch 5 Batch 211/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.421 Epoch 5 Batch 212/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.930, Loss: 1.464 Epoch 5 Batch 213/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.932, Loss: 1.460 Epoch 5 Batch 214/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.945, Loss: 1.425 Epoch 5 Batch 215/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.931, Loss: 1.474 Epoch 5 Batch 216/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.931, Loss: 1.451 Epoch 5 Batch 217/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.931, Loss: 1.488 Epoch 5 Batch 218/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.920, Loss: 1.470 Epoch 5 Batch 219/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.920, Loss: 1.392 Epoch 5 Batch 220/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.919, Loss: 1.477 Epoch 5 Batch 221/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.919, Loss: 1.494 Epoch 5 Batch 222/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.930, Loss: 1.424 Epoch 5 Batch 223/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.940, Loss: 1.436 Epoch 5 Batch 224/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.945, Loss: 1.414 Epoch 5 Batch 225/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.944, Loss: 1.511 Epoch 5 Batch 226/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.945, Loss: 1.399 Epoch 5 Batch 227/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.936, Loss: 1.475 Epoch 5 Batch 228/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.931, Loss: 1.474 Epoch 5 Batch 229/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.931, Loss: 1.430 Epoch 5 Batch 230/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.931, Loss: 1.446 Epoch 5 Batch 231/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.927, Loss: 1.516 Epoch 5 Batch 232/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.928, Loss: 1.412 Epoch 5 Batch 233/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.931, Loss: 1.395 Epoch 5 Batch 234/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.935, Loss: 1.477 Epoch 5 Batch 235/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.932, Loss: 1.443 Epoch 5 Batch 236/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.933, Loss: 1.521 Epoch 5 Batch 237/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.938, Loss: 1.525 Epoch 5 Batch 238/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.936, Loss: 1.483 Epoch 5 Batch 239/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.931, Loss: 1.472 Epoch 5 Batch 240/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.938, Loss: 1.458 Epoch 5 Batch 241/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.938, Loss: 1.473 Epoch 5 Batch 242/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.942, Loss: 1.480 Epoch 5 Batch 243/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.942, Loss: 1.414 Epoch 5 Batch 244/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.935, Loss: 1.454 Epoch 5 Batch 245/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.936, Loss: 1.389 Epoch 5 Batch 246/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.935, Loss: 1.422 Epoch 5 Batch 247/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.935, Loss: 1.491 Epoch 5 Batch 248/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.931, Loss: 1.448 Epoch 5 Batch 249/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.927, Loss: 1.453 Epoch 5 Batch 250/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.932, Loss: 1.458 Epoch 5 Batch 251/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.936, Loss: 1.475 Epoch 5 Batch 252/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.935, Loss: 1.448 Epoch 5 Batch 253/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.927, Loss: 1.510 Epoch 5 Batch 254/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.928, Loss: 1.498 Epoch 5 Batch 255/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.926, Loss: 1.427 Epoch 5 Batch 256/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.935, Loss: 1.414 Epoch 5 Batch 257/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.932, Loss: 1.421 Epoch 5 Batch 258/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.932, Loss: 1.391 Epoch 5 Batch 259/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.933, Loss: 1.409 Epoch 5 Batch 260/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.944, Loss: 1.497 Epoch 5 Batch 261/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.944, Loss: 1.489 Epoch 5 Batch 262/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.939, Loss: 1.459 Epoch 5 Batch 263/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.937, Loss: 1.485 Epoch 5 Batch 264/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.941, Loss: 1.456 Epoch 5 Batch 265/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.941, Loss: 1.448 Epoch 5 Batch 266/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.941, Loss: 1.387 Epoch 5 Batch 267/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.946, Loss: 1.420 Epoch 5 Batch 268/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.940, Loss: 1.512 Epoch 5 Batch 269/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.945, Loss: 1.576 Epoch 5 Batch 270/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.945, Loss: 1.513 Epoch 5 Batch 271/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.940, Loss: 1.489 Epoch 5 Batch 272/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.937, Loss: 1.453 Epoch 5 Batch 273/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.941, Loss: 1.469 Epoch 5 Batch 274/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.941, Loss: 1.435 Epoch 5 Batch 275/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.930, Loss: 1.421 Epoch 5 Batch 276/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.930, Loss: 1.474 Epoch 5 Batch 277/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.932, Loss: 1.476 Epoch 5 Batch 278/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.929, Loss: 1.402 Epoch 5 Batch 279/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.937, Loss: 1.458 Epoch 5 Batch 280/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.940, Loss: 1.486 Epoch 5 Batch 281/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.940, Loss: 1.461 Epoch 5 Batch 282/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.938, Loss: 1.484 Epoch 5 Batch 283/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.926, Loss: 1.462 Epoch 5 Batch 284/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.913, Loss: 1.453 Epoch 5 Batch 285/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.912, Loss: 1.464 Epoch 5 Batch 286/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.912, Loss: 1.475 Epoch 5 Batch 287/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.925, Loss: 1.386 Epoch 5 Batch 288/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.937, Loss: 1.453 Epoch 5 Batch 289/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.936, Loss: 1.476 Epoch 5 Batch 290/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.932, Loss: 1.406 Epoch 5 Batch 291/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.932, Loss: 1.395 Epoch 5 Batch 292/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.930, Loss: 1.421 Epoch 5 Batch 293/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.924, Loss: 1.440 Epoch 5 Batch 294/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.929, Loss: 1.436 Epoch 5 Batch 295/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.933, Loss: 1.440 Epoch 5 Batch 296/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.933, Loss: 1.411 Epoch 5 Batch 297/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.931, Loss: 1.439 Epoch 5 Batch 298/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.936, Loss: 1.468 Epoch 5 Batch 299/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.936, Loss: 1.488 Epoch 5 Batch 300/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.942, Loss: 1.445 Epoch 5 Batch 301/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.938, Loss: 1.453 Epoch 5 Batch 302/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.941, Loss: 1.447 Epoch 5 Batch 303/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.935, Loss: 1.441 Epoch 5 Batch 304/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.929, Loss: 1.437 Epoch 5 Batch 305/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.929, Loss: 1.448 Epoch 5 Batch 306/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.929, Loss: 1.485 Epoch 5 Batch 307/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.932, Loss: 1.359 Epoch 5 Batch 308/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.932, Loss: 1.458 Epoch 5 Batch 309/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.936, Loss: 1.435 Epoch 5 Batch 310/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.936, Loss: 1.404 Epoch 5 Batch 311/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.937, Loss: 1.410 Epoch 5 Batch 312/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.936, Loss: 1.471 Epoch 5 Batch 313/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.936, Loss: 1.449 Epoch 5 Batch 314/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.942, Loss: 1.451 Epoch 5 Batch 315/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.937, Loss: 1.481 Epoch 5 Batch 316/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.940, Loss: 1.470 Epoch 5 Batch 317/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.940, Loss: 1.442 Epoch 5 Batch 318/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.933, Loss: 1.505 Epoch 5 Batch 319/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.928, Loss: 1.487 Epoch 5 Batch 320/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.925, Loss: 1.537 Epoch 5 Batch 321/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.909, Loss: 1.393 Epoch 5 Batch 322/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.915, Loss: 1.501 Epoch 5 Batch 323/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.918, Loss: 1.465 Epoch 5 Batch 324/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.918, Loss: 1.442 Epoch 5 Batch 325/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.911, Loss: 1.498 Epoch 5 Batch 326/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.934, Loss: 1.497 Epoch 5 Batch 327/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.945, Loss: 1.502 Epoch 5 Batch 328/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.945, Loss: 1.422 Epoch 5 Batch 329/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.944, Loss: 1.461 Epoch 5 Batch 330/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.949, Loss: 1.471 Epoch 5 Batch 331/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.949, Loss: 1.478 Epoch 5 Batch 332/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.950, Loss: 1.437 Epoch 5 Batch 333/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.946, Loss: 1.465 Epoch 5 Batch 334/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.929, Loss: 1.374 Epoch 5 Batch 335/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.935, Loss: 1.471 Epoch 5 Batch 336/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.930, Loss: 1.444 Epoch 5 Batch 337/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.938, Loss: 1.478 Epoch 5 Batch 338/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.925, Loss: 1.430 Epoch 5 Batch 339/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.926, Loss: 1.473 Epoch 5 Batch 340/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.913, Loss: 1.459 Epoch 5 Batch 341/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.914, Loss: 1.556 Epoch 5 Batch 342/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.919, Loss: 1.496 Epoch 5 Batch 343/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.913, Loss: 1.414 Epoch 5 Batch 344/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.923, Loss: 1.509 Epoch 5 Batch 345/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.921, Loss: 1.427 Epoch 5 Batch 346/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.926, Loss: 1.459 Epoch 5 Batch 347/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.917, Loss: 1.446 Epoch 5 Batch 348/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.912, Loss: 1.456 Epoch 5 Batch 349/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.916, Loss: 1.384 Epoch 5 Batch 350/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.924, Loss: 1.455 Epoch 5 Batch 351/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.918, Loss: 1.422 Epoch 5 Batch 352/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.911, Loss: 1.424 Epoch 5 Batch 353/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.921, Loss: 1.429 Epoch 5 Batch 354/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.926, Loss: 1.436 Epoch 5 Batch 355/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.934, Loss: 1.488 Epoch 5 Batch 356/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.939, Loss: 1.460 Epoch 5 Batch 357/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.407 Epoch 5 Batch 358/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.949, Loss: 1.449 Epoch 5 Batch 359/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.949, Loss: 1.502 Epoch 5 Batch 360/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.942, Loss: 1.474 Epoch 5 Batch 361/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.942, Loss: 1.389 Epoch 5 Batch 362/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.935, Loss: 1.474 Epoch 5 Batch 363/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.928, Loss: 1.463 Epoch 5 Batch 364/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.926, Loss: 1.428 Epoch 5 Batch 365/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.942, Loss: 1.452 Epoch 5 Batch 366/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.927, Loss: 1.425 Epoch 5 Batch 367/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.929, Loss: 1.456 Epoch 5 Batch 368/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.940, Loss: 1.454 Epoch 5 Batch 369/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.940, Loss: 1.476 Epoch 5 Batch 370/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.940, Loss: 1.377 Epoch 5 Batch 371/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.936, Loss: 1.497 Epoch 5 Batch 372/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.938, Loss: 1.385 Epoch 5 Batch 373/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.939, Loss: 1.411 Epoch 5 Batch 374/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.941, Loss: 1.450 Epoch 5 Batch 375/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.941, Loss: 1.436 Epoch 5 Batch 376/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.941, Loss: 1.505 Epoch 5 Batch 377/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.946, Loss: 1.453 Epoch 5 Batch 378/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.942, Loss: 1.442 Epoch 5 Batch 379/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.936, Loss: 1.377 Epoch 5 Batch 380/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.936, Loss: 1.436 Epoch 5 Batch 381/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.942, Loss: 1.522 Epoch 5 Batch 382/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.949, Loss: 1.436 Epoch 5 Batch 383/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.942, Loss: 1.481 Epoch 5 Batch 384/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.942, Loss: 1.526 Epoch 5 Batch 385/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.943, Loss: 1.458 Epoch 5 Batch 386/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.948, Loss: 1.450 Epoch 5 Batch 387/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.943, Loss: 1.409 Epoch 5 Batch 388/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.940, Loss: 1.426 Epoch 5 Batch 389/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.953, Loss: 1.440 Epoch 5 Batch 390/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.951, Loss: 1.514 Epoch 5 Batch 391/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.949, Loss: 1.430 Epoch 5 Batch 392/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.942, Loss: 1.449 Epoch 5 Batch 393/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.939, Loss: 1.431 Epoch 5 Batch 394/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.938, Loss: 1.448 Epoch 5 Batch 395/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.939, Loss: 1.487 Epoch 5 Batch 396/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.939, Loss: 1.478 Epoch 5 Batch 397/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.939, Loss: 1.430 Epoch 5 Batch 398/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.939, Loss: 1.364 Epoch 5 Batch 399/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.938, Loss: 1.411 Epoch 5 Batch 400/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.939, Loss: 1.470 Epoch 5 Batch 401/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.941, Loss: 1.391 Epoch 5 Batch 402/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.936, Loss: 1.486 Epoch 5 Batch 403/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.935, Loss: 1.502 Epoch 5 Batch 404/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.936, Loss: 1.497 Epoch 5 Batch 405/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.936, Loss: 1.377 Epoch 5 Batch 406/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.934, Loss: 1.442 Epoch 5 Batch 407/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.936, Loss: 1.476 Epoch 5 Batch 408/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.932, Loss: 1.423 Epoch 5 Batch 409/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.935, Loss: 1.497 Epoch 5 Batch 410/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.938, Loss: 1.463 Epoch 5 Batch 411/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.935, Loss: 1.401 Epoch 5 Batch 412/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.931, Loss: 1.461 Epoch 5 Batch 413/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.941, Loss: 1.511 Epoch 5 Batch 414/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.947, Loss: 1.413 Epoch 5 Batch 415/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.942, Loss: 1.424 Epoch 5 Batch 416/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.940, Loss: 1.445 Epoch 5 Batch 417/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.938, Loss: 1.460 Epoch 5 Batch 418/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.942, Loss: 1.473 Epoch 5 Batch 419/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.946, Loss: 1.475 Epoch 5 Batch 420/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.946, Loss: 1.432 Epoch 5 Batch 421/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.936, Loss: 1.374 Epoch 5 Batch 422/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.936, Loss: 1.463 Epoch 5 Batch 423/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.925, Loss: 1.461 Epoch 5 Batch 424/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.919, Loss: 1.458 Epoch 5 Batch 425/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.925, Loss: 1.430 Epoch 5 Batch 426/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.935, Loss: 1.516 Epoch 5 Batch 427/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.935, Loss: 1.490 Epoch 5 Batch 428/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.943, Loss: 1.483 Epoch 5 Batch 429/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.939, Loss: 1.519 Epoch 5 Batch 430/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.939, Loss: 1.371 Epoch 5 Batch 431/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.939, Loss: 1.455 Epoch 5 Batch 432/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.941, Loss: 1.453 Epoch 5 Batch 433/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.951, Loss: 1.401 Epoch 5 Batch 434/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.942, Loss: 1.425 Epoch 5 Batch 435/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.928, Loss: 1.400 Epoch 5 Batch 436/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.928, Loss: 1.493 Epoch 5 Batch 437/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.926, Loss: 1.489 Epoch 5 Batch 438/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.926, Loss: 1.395 Epoch 5 Batch 439/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.924, Loss: 1.437 Epoch 5 Batch 440/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.926, Loss: 1.446 Epoch 5 Batch 441/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.927, Loss: 1.429 Epoch 5 Batch 442/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.927, Loss: 1.415 Epoch 5 Batch 443/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.937, Loss: 1.431 Epoch 5 Batch 444/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.935, Loss: 1.463 Epoch 5 Batch 445/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.935, Loss: 1.439 Epoch 5 Batch 446/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.935, Loss: 1.382 Epoch 5 Batch 447/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.937, Loss: 1.477 Epoch 5 Batch 448/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.932, Loss: 1.438 Epoch 5 Batch 449/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.932, Loss: 1.445 Epoch 5 Batch 450/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.931, Loss: 1.523 Epoch 5 Batch 451/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.927, Loss: 1.428 Epoch 5 Batch 452/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.927, Loss: 1.358 Epoch 5 Batch 453/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.928, Loss: 1.424 Epoch 5 Batch 454/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.929, Loss: 1.475 Epoch 5 Batch 455/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.929, Loss: 1.487 Epoch 5 Batch 456/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.934, Loss: 1.465 Epoch 5 Batch 457/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.934, Loss: 1.432 Epoch 5 Batch 458/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.934, Loss: 1.419 Epoch 5 Batch 459/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.938, Loss: 1.375 Epoch 5 Batch 460/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.938, Loss: 1.495 Epoch 5 Batch 461/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.943, Loss: 1.488 Epoch 5 Batch 462/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.944, Loss: 1.501 Epoch 5 Batch 463/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.944, Loss: 1.485 Epoch 5 Batch 464/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.939, Loss: 1.470 Epoch 5 Batch 465/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.471 Epoch 5 Batch 466/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.935, Loss: 1.477 Epoch 5 Batch 467/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.933, Loss: 1.451 Epoch 5 Batch 468/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.926, Loss: 1.459 Epoch 5 Batch 469/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.931, Loss: 1.464 Epoch 5 Batch 470/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.934, Loss: 1.382 Epoch 5 Batch 471/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.927, Loss: 1.445 Epoch 5 Batch 472/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.913, Loss: 1.461 Epoch 5 Batch 473/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.924, Loss: 1.457 Epoch 5 Batch 474/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.924, Loss: 1.446 Epoch 5 Batch 475/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.924, Loss: 1.492 Epoch 5 Batch 476/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.923, Loss: 1.370 Epoch 5 Batch 477/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.933, Loss: 1.398 Epoch 5 Batch 478/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.934, Loss: 1.459 Epoch 5 Batch 479/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.939, Loss: 1.436 Epoch 5 Batch 480/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.933, Loss: 1.485 Epoch 5 Batch 481/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.937, Loss: 1.354 Epoch 5 Batch 482/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.933, Loss: 1.472 Epoch 5 Batch 483/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.939, Loss: 1.413 Epoch 5 Batch 484/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.935, Loss: 1.444 Epoch 5 Batch 485/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.935, Loss: 1.437 Epoch 5 Batch 486/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.936, Loss: 1.455 Epoch 5 Batch 487/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.938, Loss: 1.445 Epoch 5 Batch 488/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.945, Loss: 1.429 Epoch 5 Batch 489/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.939, Loss: 1.420 Epoch 5 Batch 490/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.933, Loss: 1.464 Epoch 5 Batch 491/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.931, Loss: 1.458 Epoch 5 Batch 492/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.931, Loss: 1.436 Epoch 5 Batch 493/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.919, Loss: 1.447 Epoch 5 Batch 494/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.918, Loss: 1.409 Epoch 5 Batch 495/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.920, Loss: 1.430 Epoch 5 Batch 496/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.924, Loss: 1.482 Epoch 5 Batch 497/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.928, Loss: 1.435 Epoch 5 Batch 498/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.938, Loss: 1.393 Epoch 5 Batch 499/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.944, Loss: 1.429 Epoch 5 Batch 500/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.944, Loss: 1.412 Epoch 5 Batch 501/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.944, Loss: 1.453 Epoch 5 Batch 502/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.944, Loss: 1.463 Epoch 5 Batch 503/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.939, Loss: 1.401 Epoch 5 Batch 504/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.512 Epoch 5 Batch 505/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.939, Loss: 1.489 Epoch 5 Batch 506/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.944, Loss: 1.506 Epoch 5 Batch 507/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.952, Loss: 1.455 Epoch 5 Batch 508/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.952, Loss: 1.453 Epoch 5 Batch 509/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.953, Loss: 1.403 Epoch 5 Batch 510/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.946, Loss: 1.459 Epoch 5 Batch 511/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.941, Loss: 1.442 Epoch 5 Batch 512/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.944, Loss: 1.375 Epoch 5 Batch 513/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.945, Loss: 1.430 Epoch 5 Batch 514/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.944, Loss: 1.430 Epoch 5 Batch 515/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.944, Loss: 1.437 Epoch 5 Batch 516/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.944, Loss: 1.485 Epoch 5 Batch 517/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.944, Loss: 1.483 Epoch 5 Batch 518/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.933, Loss: 1.420 Epoch 5 Batch 519/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.933, Loss: 1.506 Epoch 5 Batch 520/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.933, Loss: 1.443 Epoch 5 Batch 521/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.933, Loss: 1.362 Epoch 5 Batch 522/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.931, Loss: 1.421 Epoch 5 Batch 523/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.941, Loss: 1.441 Epoch 5 Batch 524/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.948, Loss: 1.575 Epoch 5 Batch 525/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.948, Loss: 1.470 Epoch 5 Batch 526/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.948, Loss: 1.510 Epoch 5 Batch 527/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.948, Loss: 1.460 Epoch 5 Batch 528/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.943, Loss: 1.503 Epoch 5 Batch 529/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.932, Loss: 1.416 Epoch 5 Batch 530/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.932, Loss: 1.473 Epoch 5 Batch 531/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.941, Loss: 1.516 Epoch 5 Batch 532/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.943, Loss: 1.498 Epoch 5 Batch 533/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.939, Loss: 1.460 Epoch 5 Batch 534/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.935, Loss: 1.431 Epoch 5 Batch 535/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.927, Loss: 1.472 Epoch 5 Batch 536/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.918, Loss: 1.449 Epoch 5 Batch 537/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.912, Loss: 1.504 Epoch 5 Batch 538/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.912, Loss: 1.479 Epoch 5 Batch 539/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.907, Loss: 1.460 Epoch 5 Batch 540/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.921, Loss: 1.441 Epoch 5 Batch 541/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.921, Loss: 1.482 Epoch 5 Batch 542/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.934, Loss: 1.425 Epoch 5 Batch 543/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.940, Loss: 1.431 Epoch 5 Batch 544/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.940, Loss: 1.493 Epoch 5 Batch 545/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.930, Loss: 1.469 Epoch 5 Batch 546/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.925, Loss: 1.423 Epoch 5 Batch 547/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.930, Loss: 1.445 Epoch 5 Batch 548/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.930, Loss: 1.451 Epoch 5 Batch 549/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.931, Loss: 1.467 Epoch 5 Batch 550/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.930, Loss: 1.479 Epoch 5 Batch 551/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.926, Loss: 1.413 Epoch 5 Batch 552/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.931, Loss: 1.519 Epoch 5 Batch 553/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.929, Loss: 1.523 Epoch 5 Batch 554/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.925, Loss: 1.389 Epoch 5 Batch 555/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.925, Loss: 1.444 Epoch 5 Batch 556/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.925, Loss: 1.441 Epoch 5 Batch 557/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.918, Loss: 1.484 Epoch 5 Batch 558/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.913, Loss: 1.453 Epoch 5 Batch 559/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.913, Loss: 1.506 Epoch 5 Batch 560/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.924, Loss: 1.421 Epoch 5 Batch 561/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.924, Loss: 1.442 Epoch 5 Batch 562/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.929, Loss: 1.505 Epoch 5 Batch 563/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.922, Loss: 1.399 Epoch 5 Batch 564/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.922, Loss: 1.356 Epoch 5 Batch 565/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.922, Loss: 1.389 Epoch 5 Batch 566/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.918, Loss: 1.446 Epoch 5 Batch 567/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.922, Loss: 1.437 Epoch 5 Batch 568/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.922, Loss: 1.483 Epoch 5 Batch 569/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.927, Loss: 1.411 Epoch 5 Batch 570/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.926, Loss: 1.476 Epoch 5 Batch 571/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.923, Loss: 1.422 Epoch 5 Batch 572/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.928, Loss: 1.463 Epoch 5 Batch 573/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.927, Loss: 1.363 Epoch 5 Batch 574/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.929, Loss: 1.413 Epoch 5 Batch 575/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.934, Loss: 1.451 Epoch 5 Batch 576/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.925, Loss: 1.506 Epoch 5 Batch 577/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.921, Loss: 1.481 Epoch 5 Batch 578/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.920, Loss: 1.504 Epoch 5 Batch 579/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.925, Loss: 1.484 Epoch 5 Batch 580/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.929, Loss: 1.424 Epoch 5 Batch 581/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.930, Loss: 1.434 Epoch 5 Batch 582/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.931, Loss: 1.402 Epoch 5 Batch 583/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.942, Loss: 1.412 Epoch 5 Batch 584/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.938, Loss: 1.418 Epoch 5 Batch 585/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.940, Loss: 1.361 Epoch 5 Batch 586/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.939, Loss: 1.471 Epoch 5 Batch 587/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.939, Loss: 1.504 Epoch 5 Batch 588/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.390 Epoch 5 Batch 589/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.939, Loss: 1.450 Epoch 5 Batch 590/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.939, Loss: 1.417 Epoch 5 Batch 591/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.939, Loss: 1.379 Epoch 5 Batch 592/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.937, Loss: 1.338 Epoch 5 Batch 593/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.937, Loss: 1.393 Epoch 5 Batch 594/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.942, Loss: 1.432 Epoch 5 Batch 595/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.932, Loss: 1.456 Epoch 5 Batch 596/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.927, Loss: 1.516 Epoch 5 Batch 597/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.927, Loss: 1.492 Epoch 5 Batch 598/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.932, Loss: 1.455 Epoch 5 Batch 599/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.932, Loss: 1.460 Epoch 5 Batch 600/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.936, Loss: 1.481 Epoch 5 Batch 601/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.938, Loss: 1.364 Epoch 5 Batch 602/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.944, Loss: 1.386 Epoch 5 Batch 603/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.949, Loss: 1.411 Epoch 5 Batch 604/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.938, Loss: 1.439 Epoch 5 Batch 605/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.932, Loss: 1.397 Epoch 5 Batch 606/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.932, Loss: 1.431 Epoch 5 Batch 607/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.933, Loss: 1.408 Epoch 5 Batch 608/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.935, Loss: 1.418 Epoch 5 Batch 609/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.941, Loss: 1.429 Epoch 5 Batch 610/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.939, Loss: 1.435 Epoch 5 Batch 611/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.939, Loss: 1.325 Epoch 5 Batch 612/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.935, Loss: 1.458 Epoch 5 Batch 613/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.943, Loss: 1.448 Epoch 5 Batch 614/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.943, Loss: 1.443 Epoch 5 Batch 615/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.935, Loss: 1.507 Epoch 5 Batch 616/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.935, Loss: 1.477 Epoch 5 Batch 617/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.935, Loss: 1.347 Epoch 5 Batch 618/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.930, Loss: 1.526 Epoch 5 Batch 619/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.935, Loss: 1.366 Epoch 5 Batch 620/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.931, Loss: 1.384 Epoch 5 Batch 621/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.936, Loss: 1.469 Epoch 5 Batch 622/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.933, Loss: 1.476 Epoch 5 Batch 623/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.935, Loss: 1.532 Epoch 5 Batch 624/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.941, Loss: 1.510 Epoch 5 Batch 625/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.939, Loss: 1.384 Epoch 5 Batch 626/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.941, Loss: 1.412 Epoch 5 Batch 627/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.941, Loss: 1.454 Epoch 5 Batch 628/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.946, Loss: 1.553 Epoch 5 Batch 629/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.941, Loss: 1.388 Epoch 5 Batch 630/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.951, Loss: 1.428 Epoch 5 Batch 631/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.941, Loss: 1.476 Epoch 5 Batch 632/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.941, Loss: 1.398 Epoch 5 Batch 633/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.931, Loss: 1.377 Epoch 5 Batch 634/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.927, Loss: 1.506 Epoch 5 Batch 635/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.931, Loss: 1.389 Epoch 5 Batch 636/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.928, Loss: 1.398 Epoch 5 Batch 637/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.928, Loss: 1.410 Epoch 5 Batch 638/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.919, Loss: 1.457 Epoch 5 Batch 639/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.922, Loss: 1.404 Epoch 5 Batch 640/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.920, Loss: 1.453 Epoch 5 Batch 641/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.920, Loss: 1.407 Epoch 5 Batch 642/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.926, Loss: 1.452 Epoch 5 Batch 643/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.926, Loss: 1.432 Epoch 5 Batch 644/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.928, Loss: 1.502 Epoch 5 Batch 645/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.922, Loss: 1.489 Epoch 5 Batch 646/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.915, Loss: 1.468 Epoch 5 Batch 647/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.919, Loss: 1.438 Epoch 5 Batch 648/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.915, Loss: 1.317 Epoch 5 Batch 649/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.919, Loss: 1.462 Epoch 5 Batch 650/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.915, Loss: 1.398 Epoch 5 Batch 651/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.925, Loss: 1.400 Epoch 5 Batch 652/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.921, Loss: 1.531 Epoch 5 Batch 653/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.920, Loss: 1.448 Epoch 5 Batch 654/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.911, Loss: 1.397 Epoch 5 Batch 655/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.926, Loss: 1.513 Epoch 5 Batch 656/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.933, Loss: 1.432 Epoch 5 Batch 657/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.931, Loss: 1.439 Epoch 5 Batch 658/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.935, Loss: 1.474 Epoch 5 Batch 659/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.931, Loss: 1.469 Epoch 5 Batch 660/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.931, Loss: 1.462 Epoch 5 Batch 661/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.931, Loss: 1.508 Epoch 5 Batch 662/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.931, Loss: 1.446 Epoch 5 Batch 663/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.934, Loss: 1.474 Epoch 5 Batch 664/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.938, Loss: 1.410 Epoch 5 Batch 665/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.947, Loss: 1.483 Epoch 5 Batch 666/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.459 Epoch 5 Batch 667/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.954, Loss: 1.454 Epoch 5 Batch 668/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.945, Loss: 1.525 Epoch 5 Batch 669/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.945, Loss: 1.404 Epoch 5 Batch 670/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.941, Loss: 1.468 Epoch 5 Batch 671/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.935, Loss: 1.464 Epoch 5 Batch 672/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.937, Loss: 1.420 Epoch 5 Batch 673/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.942, Loss: 1.431 Epoch 5 Batch 674/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.936, Loss: 1.374 Epoch 5 Batch 675/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.936, Loss: 1.411 Epoch 5 Batch 676/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.936, Loss: 1.525 Epoch 5 Batch 677/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.936, Loss: 1.430 Epoch 5 Batch 678/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.945, Loss: 1.452 Epoch 5 Batch 679/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.948, Loss: 1.423 Epoch 5 Batch 680/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.938, Loss: 1.413 Epoch 5 Batch 681/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.934, Loss: 1.444 Epoch 5 Batch 682/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.929, Loss: 1.398 Epoch 5 Batch 683/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.938, Loss: 1.458 Epoch 5 Batch 684/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.935, Loss: 1.426 Epoch 5 Batch 685/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.933, Loss: 1.442 Epoch 5 Batch 686/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.935, Loss: 1.449 Epoch 5 Batch 687/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.933, Loss: 1.404 Epoch 5 Batch 688/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.929, Loss: 1.417 Epoch 5 Batch 689/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.920, Loss: 1.515 Epoch 5 Batch 690/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.929, Loss: 1.373 Epoch 5 Batch 691/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.929, Loss: 1.426 Epoch 5 Batch 692/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.939, Loss: 1.424 Epoch 5 Batch 693/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.939, Loss: 1.475 Epoch 5 Batch 694/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.936, Loss: 1.444 Epoch 5 Batch 695/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.930, Loss: 1.447 Epoch 5 Batch 696/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.927, Loss: 1.479 Epoch 5 Batch 697/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.927, Loss: 1.430 Epoch 5 Batch 698/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.919, Loss: 1.475 Epoch 5 Batch 699/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.919, Loss: 1.420 Epoch 5 Batch 700/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.924, Loss: 1.405 Epoch 5 Batch 701/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.924, Loss: 1.400 Epoch 5 Batch 702/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.921, Loss: 1.476 Epoch 5 Batch 703/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.921, Loss: 1.463 Epoch 5 Batch 704/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.926, Loss: 1.403 Epoch 5 Batch 705/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.933, Loss: 1.439 Epoch 5 Batch 706/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.936, Loss: 1.498 Epoch 5 Batch 707/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.932, Loss: 1.461 Epoch 5 Batch 708/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.931, Loss: 1.399 Epoch 5 Batch 709/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.929, Loss: 1.542 Epoch 5 Batch 710/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.919, Loss: 1.418 Epoch 5 Batch 711/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.929, Loss: 1.503 Epoch 5 Batch 712/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.929, Loss: 1.500 Epoch 5 Batch 713/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.925, Loss: 1.502 Epoch 5 Batch 714/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.933, Loss: 1.469 Epoch 5 Batch 715/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.931, Loss: 1.427 Epoch 5 Batch 716/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.943, Loss: 1.394 Epoch 5 Batch 717/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.947, Loss: 1.403 Epoch 5 Batch 718/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.950, Loss: 1.412 Epoch 5 Batch 719/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.952, Loss: 1.399 Epoch 5 Batch 720/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.952, Loss: 1.358 Epoch 5 Batch 721/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.947, Loss: 1.505 Epoch 5 Batch 722/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.950, Loss: 1.487 Epoch 5 Batch 723/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.395 Epoch 5 Batch 724/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.937, Loss: 1.482 Epoch 5 Batch 725/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.930, Loss: 1.446 Epoch 5 Batch 726/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.930, Loss: 1.440 Epoch 5 Batch 727/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.930, Loss: 1.469 Epoch 5 Batch 728/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.941, Loss: 1.406 Epoch 5 Batch 729/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.941, Loss: 1.512 Epoch 5 Batch 730/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.941, Loss: 1.522 Epoch 5 Batch 731/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.941, Loss: 1.452 Epoch 5 Batch 732/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.944, Loss: 1.416 Epoch 5 Batch 733/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.935, Loss: 1.316 Epoch 5 Batch 734/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.940, Loss: 1.472 Epoch 5 Batch 735/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.935, Loss: 1.445 Epoch 5 Batch 736/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.935, Loss: 1.411 Epoch 5 Batch 737/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.940, Loss: 1.372 Epoch 5 Batch 738/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.936, Loss: 1.386 Epoch 5 Batch 739/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.936, Loss: 1.523 Epoch 5 Batch 740/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.940, Loss: 1.496 Epoch 5 Batch 741/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.939, Loss: 1.439 Epoch 5 Batch 742/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.944, Loss: 1.384 Epoch 5 Batch 743/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.937, Loss: 1.483 Epoch 5 Batch 744/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.942, Loss: 1.406 Epoch 5 Batch 745/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.942, Loss: 1.443 Epoch 5 Batch 746/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.941, Loss: 1.450 Epoch 5 Batch 747/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.946, Loss: 1.407 Epoch 5 Batch 748/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.945, Loss: 1.528 Epoch 5 Batch 749/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.942, Loss: 1.438 Epoch 5 Batch 750/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.938, Loss: 1.475 Epoch 5 Batch 751/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.940, Loss: 1.393 Epoch 5 Batch 752/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.945, Loss: 1.479 Epoch 5 Batch 753/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.460 Epoch 5 Batch 754/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.946, Loss: 1.416 Epoch 5 Batch 755/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.946, Loss: 1.460 Epoch 5 Batch 756/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.946, Loss: 1.486 Epoch 5 Batch 757/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.946, Loss: 1.473 Epoch 5 Batch 758/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.940, Loss: 1.480 Epoch 5 Batch 759/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.942, Loss: 1.407 Epoch 5 Batch 760/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.947, Loss: 1.423 Epoch 5 Batch 761/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.948, Loss: 1.472 Epoch 5 Batch 762/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.947, Loss: 1.429 Epoch 5 Batch 763/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.946, Loss: 1.383 Epoch 5 Batch 764/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.952, Loss: 1.439 Epoch 5 Batch 765/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.947, Loss: 1.501 Epoch 5 Batch 766/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.947, Loss: 1.539 Epoch 5 Batch 767/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.943, Loss: 1.429 Epoch 5 Batch 768/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.395 Epoch 5 Batch 769/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.952, Loss: 1.407 Epoch 5 Batch 770/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.958, Loss: 1.399 Epoch 5 Batch 771/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.943, Loss: 1.480 Epoch 5 Batch 772/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.948, Loss: 1.397 Epoch 5 Batch 773/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.943, Loss: 1.501 Epoch 5 Batch 774/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.948, Loss: 1.460 Epoch 5 Batch 775/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.948, Loss: 1.345 Epoch 5 Batch 776/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.948, Loss: 1.452 Epoch 5 Batch 777/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.948, Loss: 1.467 Epoch 5 Batch 778/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.957, Loss: 1.431 Epoch 5 Batch 779/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.957, Loss: 1.431 Epoch 5 Batch 780/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.954, Loss: 1.411 Epoch 5 Batch 781/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.950, Loss: 1.403 Epoch 5 Batch 782/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.943, Loss: 1.432 Epoch 5 Batch 783/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.396 Epoch 5 Batch 784/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.939, Loss: 1.460 Epoch 5 Batch 785/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.934, Loss: 1.414 Epoch 5 Batch 786/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.934, Loss: 1.345 Epoch 5 Batch 787/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.940, Loss: 1.444 Epoch 5 Batch 788/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.943, Loss: 1.468 Epoch 5 Batch 789/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.939, Loss: 1.404 Epoch 5 Batch 790/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.938, Loss: 1.490 Epoch 5 Batch 791/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.931, Loss: 1.334 Epoch 5 Batch 792/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.938, Loss: 1.480 Epoch 5 Batch 793/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.946, Loss: 1.491 Epoch 5 Batch 794/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.935, Loss: 1.501 Epoch 5 Batch 795/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.936, Loss: 1.501 Epoch 5 Batch 796/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.937, Loss: 1.517 Epoch 5 Batch 797/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.937, Loss: 1.476 Epoch 5 Batch 798/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.937, Loss: 1.469 Epoch 5 Batch 799/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.938, Loss: 1.498 Epoch 5 Batch 800/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.938, Loss: 1.411 Epoch 5 Batch 801/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.938, Loss: 1.426 Epoch 5 Batch 802/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.938, Loss: 1.484 Epoch 5 Batch 803/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.935, Loss: 1.475 Epoch 5 Batch 804/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.940, Loss: 1.449 Epoch 5 Batch 805/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.942, Loss: 1.434 Epoch 5 Batch 806/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.936, Loss: 1.400 Epoch 5 Batch 807/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.936, Loss: 1.425 Epoch 5 Batch 808/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.936, Loss: 1.405 Epoch 5 Batch 809/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.936, Loss: 1.473 Epoch 5 Batch 810/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.937, Loss: 1.421 Epoch 5 Batch 811/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.937, Loss: 1.441 Epoch 5 Batch 812/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.934, Loss: 1.427 Epoch 5 Batch 813/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.938, Loss: 1.431 Epoch 5 Batch 814/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.935, Loss: 1.522 Epoch 5 Batch 815/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.933, Loss: 1.453 Epoch 5 Batch 816/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.938, Loss: 1.428 Epoch 5 Batch 817/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.938, Loss: 1.487 Epoch 5 Batch 818/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.949, Loss: 1.478 Epoch 5 Batch 819/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.954, Loss: 1.409 Epoch 5 Batch 820/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.433 Epoch 5 Batch 821/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.947, Loss: 1.471 Epoch 5 Batch 822/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.947, Loss: 1.474 Epoch 5 Batch 823/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.947, Loss: 1.562 Epoch 5 Batch 824/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.944, Loss: 1.464 Epoch 5 Batch 825/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.948, Loss: 1.439 Epoch 5 Batch 826/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.948, Loss: 1.504 Epoch 5 Batch 827/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.948, Loss: 1.405 Epoch 5 Batch 828/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.948, Loss: 1.402 Epoch 5 Batch 829/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.948, Loss: 1.493 Epoch 5 Batch 830/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.945, Loss: 1.425 Epoch 5 Batch 831/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.955, Loss: 1.441 Epoch 5 Batch 832/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.399 Epoch 5 Batch 833/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.950, Loss: 1.415 Epoch 5 Batch 834/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.960, Loss: 1.433 Epoch 5 Batch 835/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.958, Loss: 1.491 Epoch 5 Batch 836/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.958, Loss: 1.430 Epoch 5 Batch 837/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.956, Loss: 1.423 Epoch 5 Batch 838/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.951, Loss: 1.471 Epoch 5 Batch 839/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.955, Loss: 1.471 Epoch 5 Batch 840/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.448 Epoch 5 Batch 841/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.960, Loss: 1.425 Epoch 5 Batch 842/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.950, Loss: 1.443 Epoch 5 Batch 843/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.933, Loss: 1.437 Epoch 5 Batch 844/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.943, Loss: 1.511 Epoch 5 Batch 845/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.944, Loss: 1.471 Epoch 5 Batch 846/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.956, Loss: 1.465 Epoch 5 Batch 847/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.951, Loss: 1.369 Epoch 5 Batch 848/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.951, Loss: 1.448 Epoch 5 Batch 849/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.952, Loss: 1.407 Epoch 5 Batch 850/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.953, Loss: 1.462 Epoch 5 Batch 851/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.960, Loss: 1.458 Epoch 5 Batch 852/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.960, Loss: 1.406 Epoch 5 Batch 853/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.949, Loss: 1.495 Epoch 5 Batch 854/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.949, Loss: 1.451 Epoch 5 Batch 855/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.951, Loss: 1.475 Epoch 5 Batch 856/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.946, Loss: 1.371 Epoch 5 Batch 857/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.956, Loss: 1.386 Epoch 5 Batch 858/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.400 Epoch 5 Batch 859/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.953, Loss: 1.421 Epoch 5 Batch 860/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.949, Loss: 1.491 Epoch 5 Batch 861/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.949, Loss: 1.481 Epoch 5 Batch 862/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.955, Loss: 1.481 Epoch 5 Batch 863/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.943, Loss: 1.414 Epoch 5 Batch 864/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.943, Loss: 1.519 Epoch 5 Batch 865/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.943, Loss: 1.446 Epoch 5 Batch 866/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.944, Loss: 1.364 Epoch 5 Batch 867/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.944, Loss: 1.468 Epoch 5 Batch 868/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.944, Loss: 1.535 Epoch 5 Batch 869/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.944, Loss: 1.415 Epoch 5 Batch 870/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.946, Loss: 1.406 Epoch 5 Batch 871/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.950, Loss: 1.452 Epoch 5 Batch 872/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.946, Loss: 1.493 Epoch 5 Batch 873/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.945, Loss: 1.444 Epoch 5 Batch 874/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.933, Loss: 1.391 Epoch 5 Batch 875/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.935, Loss: 1.426 Epoch 5 Batch 876/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.943, Loss: 1.492 Epoch 5 Batch 877/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.946, Loss: 1.425 Epoch 5 Batch 878/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.946, Loss: 1.464 Epoch 5 Batch 879/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.950, Loss: 1.439 Epoch 5 Batch 880/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.946, Loss: 1.506 Epoch 5 Batch 881/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.952, Loss: 1.417 Epoch 5 Batch 882/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.960, Loss: 1.380 Epoch 5 Batch 883/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.956, Loss: 1.377 Epoch 5 Batch 884/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.956, Loss: 1.356 Epoch 5 Batch 885/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.956, Loss: 1.438 Epoch 5 Batch 886/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.956, Loss: 1.361 Epoch 5 Batch 887/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.957, Loss: 1.424 Epoch 5 Batch 888/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.951, Loss: 1.410 Epoch 5 Batch 889/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.953, Loss: 1.471 Epoch 5 Batch 890/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.947, Loss: 1.489 Epoch 5 Batch 891/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.942, Loss: 1.491 Epoch 5 Batch 892/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.938, Loss: 1.460 Epoch 5 Batch 893/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.936, Loss: 1.438 Epoch 5 Batch 894/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.936, Loss: 1.437 Epoch 5 Batch 895/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.941, Loss: 1.416 Epoch 5 Batch 896/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.941, Loss: 1.479 Epoch 5 Batch 897/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.946, Loss: 1.392 Epoch 5 Batch 898/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.951, Loss: 1.452 Epoch 5 Batch 899/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.956, Loss: 1.451 Epoch 5 Batch 900/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.960, Loss: 1.432 Epoch 5 Batch 901/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.960, Loss: 1.526 Epoch 5 Batch 902/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.960, Loss: 1.415 Epoch 5 Batch 903/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.955, Loss: 1.467 Epoch 5 Batch 904/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.954, Loss: 1.438 Epoch 5 Batch 905/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.954, Loss: 1.482 Epoch 5 Batch 906/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.954, Loss: 1.414 Epoch 5 Batch 907/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.954, Loss: 1.433 Epoch 5 Batch 908/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.956, Loss: 1.459 Epoch 5 Batch 909/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.959, Loss: 1.450 Epoch 5 Batch 910/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.958, Loss: 1.440 Epoch 5 Batch 911/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.961, Loss: 1.446 Epoch 5 Batch 912/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.968, Loss: 1.384 Epoch 5 Batch 913/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.967, Loss: 1.470 Epoch 5 Batch 914/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.961, Loss: 1.473 Epoch 5 Batch 915/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.956, Loss: 1.476 Epoch 5 Batch 916/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.947, Loss: 1.439 Epoch 5 Batch 917/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.948, Loss: 1.517 Epoch 5 Batch 918/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.953, Loss: 1.419 Epoch 5 Batch 919/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.504 Epoch 5 Batch 920/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.962, Loss: 1.438 Epoch 5 Batch 921/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.961, Loss: 1.436 Epoch 5 Batch 922/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.963, Loss: 1.496 Epoch 5 Batch 923/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.967, Loss: 1.441 Epoch 5 Batch 924/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.967, Loss: 1.508 Epoch 5 Batch 925/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.967, Loss: 1.529 Epoch 5 Batch 926/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.960, Loss: 1.394 Epoch 5 Batch 927/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.963, Loss: 1.406 Epoch 5 Batch 928/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.961, Loss: 1.390 Epoch 5 Batch 929/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.962, Loss: 1.509 Epoch 5 Batch 930/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.962, Loss: 1.468 Epoch 5 Batch 931/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.965, Loss: 1.467 Epoch 5 Batch 932/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.963, Loss: 1.481 Epoch 5 Batch 933/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.963, Loss: 1.377 Epoch 5 Batch 934/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.439 Epoch 5 Batch 935/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.968, Loss: 1.423 Epoch 5 Batch 936/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.977, Loss: 1.478 Epoch 5 Batch 937/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.970, Loss: 1.427 Epoch 5 Batch 938/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.970, Loss: 1.434 Epoch 5 Batch 939/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.963, Loss: 1.495 Epoch 5 Batch 940/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.426 Epoch 5 Batch 941/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.966, Loss: 1.479 Epoch 5 Batch 942/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.962, Loss: 1.450 Epoch 5 Batch 943/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.953, Loss: 1.443 Epoch 5 Batch 944/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.946, Loss: 1.459 Epoch 5 Batch 945/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.945, Loss: 1.376 Epoch 5 Batch 946/1077 - Train Accuracy: 0.994, Validation Accuracy: 0.945, Loss: 1.431 Epoch 5 Batch 947/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.941, Loss: 1.416 Epoch 5 Batch 948/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.946, Loss: 1.486 Epoch 5 Batch 949/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.945, Loss: 1.371 Epoch 5 Batch 950/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.951, Loss: 1.440 Epoch 5 Batch 951/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.950, Loss: 1.439 Epoch 5 Batch 952/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.947, Loss: 1.488 Epoch 5 Batch 953/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.948, Loss: 1.504 Epoch 5 Batch 954/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.952, Loss: 1.386 Epoch 5 Batch 955/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.947, Loss: 1.453 Epoch 5 Batch 956/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.947, Loss: 1.417 Epoch 5 Batch 957/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.947, Loss: 1.409 Epoch 5 Batch 958/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.950, Loss: 1.454 Epoch 5 Batch 959/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.945, Loss: 1.465 Epoch 5 Batch 960/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.945, Loss: 1.408 Epoch 5 Batch 961/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.950, Loss: 1.512 Epoch 5 Batch 962/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.950, Loss: 1.423 Epoch 5 Batch 963/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.945, Loss: 1.551 Epoch 5 Batch 964/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.945, Loss: 1.453 Epoch 5 Batch 965/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.951, Loss: 1.507 Epoch 5 Batch 966/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.475 Epoch 5 Batch 967/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.955, Loss: 1.487 Epoch 5 Batch 968/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.959, Loss: 1.380 Epoch 5 Batch 969/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.953, Loss: 1.412 Epoch 5 Batch 970/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.953, Loss: 1.429 Epoch 5 Batch 971/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.953, Loss: 1.524 Epoch 5 Batch 972/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.405 Epoch 5 Batch 973/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.951, Loss: 1.356 Epoch 5 Batch 974/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.951, Loss: 1.485 Epoch 5 Batch 975/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.955, Loss: 1.395 Epoch 5 Batch 976/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.436 Epoch 5 Batch 977/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.950, Loss: 1.366 Epoch 5 Batch 978/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.959, Loss: 1.463 Epoch 5 Batch 979/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.959, Loss: 1.381 Epoch 5 Batch 980/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.959, Loss: 1.447 Epoch 5 Batch 981/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.959, Loss: 1.426 Epoch 5 Batch 982/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.953, Loss: 1.426 Epoch 5 Batch 983/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.953, Loss: 1.359 Epoch 5 Batch 984/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.953, Loss: 1.485 Epoch 5 Batch 985/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.953, Loss: 1.395 Epoch 5 Batch 986/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.952, Loss: 1.412 Epoch 5 Batch 987/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.950, Loss: 1.412 Epoch 5 Batch 988/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.940, Loss: 1.464 Epoch 5 Batch 989/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.940, Loss: 1.409 Epoch 5 Batch 990/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.938, Loss: 1.517 Epoch 5 Batch 991/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.938, Loss: 1.421 Epoch 5 Batch 992/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.938, Loss: 1.416 Epoch 5 Batch 993/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.939, Loss: 1.384 Epoch 5 Batch 994/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.942, Loss: 1.467 Epoch 5 Batch 995/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.442 Epoch 5 Batch 996/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.445 Epoch 5 Batch 997/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.955, Loss: 1.474 Epoch 5 Batch 998/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.955, Loss: 1.389 Epoch 5 Batch 999/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.955, Loss: 1.524 Epoch 5 Batch 1000/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.955, Loss: 1.384 Epoch 5 Batch 1001/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.960, Loss: 1.390 Epoch 5 Batch 1002/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.365 Epoch 5 Batch 1003/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.433 Epoch 5 Batch 1004/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.457 Epoch 5 Batch 1005/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.951, Loss: 1.522 Epoch 5 Batch 1006/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.951, Loss: 1.410 Epoch 5 Batch 1007/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.951, Loss: 1.411 Epoch 5 Batch 1008/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.951, Loss: 1.402 Epoch 5 Batch 1009/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.951, Loss: 1.432 Epoch 5 Batch 1010/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.951, Loss: 1.376 Epoch 5 Batch 1011/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.956, Loss: 1.511 Epoch 5 Batch 1012/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.956, Loss: 1.379 Epoch 5 Batch 1013/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.946, Loss: 1.410 Epoch 5 Batch 1014/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.947, Loss: 1.413 Epoch 5 Batch 1015/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.946, Loss: 1.446 Epoch 5 Batch 1016/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.948, Loss: 1.490 Epoch 5 Batch 1017/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.946, Loss: 1.448 Epoch 5 Batch 1018/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.946, Loss: 1.503 Epoch 5 Batch 1019/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.944, Loss: 1.400 Epoch 5 Batch 1020/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.938, Loss: 1.410 Epoch 5 Batch 1021/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.935, Loss: 1.407 Epoch 5 Batch 1022/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.945, Loss: 1.419 Epoch 5 Batch 1023/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.946, Loss: 1.375 Epoch 5 Batch 1024/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.944, Loss: 1.461 Epoch 5 Batch 1025/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.949, Loss: 1.477 Epoch 5 Batch 1026/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.953, Loss: 1.476 Epoch 5 Batch 1027/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.948, Loss: 1.477 Epoch 5 Batch 1028/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.948, Loss: 1.480 Epoch 5 Batch 1029/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.942, Loss: 1.440 Epoch 5 Batch 1030/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.940, Loss: 1.455 Epoch 5 Batch 1031/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.940, Loss: 1.438 Epoch 5 Batch 1032/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.931, Loss: 1.423 Epoch 5 Batch 1033/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.924, Loss: 1.514 Epoch 5 Batch 1034/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.924, Loss: 1.521 Epoch 5 Batch 1035/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.924, Loss: 1.493 Epoch 5 Batch 1036/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.934, Loss: 1.421 Epoch 5 Batch 1037/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.934, Loss: 1.452 Epoch 5 Batch 1038/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.934, Loss: 1.449 Epoch 5 Batch 1039/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.939, Loss: 1.444 Epoch 5 Batch 1040/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.939, Loss: 1.450 Epoch 5 Batch 1041/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.946, Loss: 1.427 Epoch 5 Batch 1042/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.951, Loss: 1.423 Epoch 5 Batch 1043/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.948, Loss: 1.421 Epoch 5 Batch 1044/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.943, Loss: 1.467 Epoch 5 Batch 1045/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.944, Loss: 1.475 Epoch 5 Batch 1046/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.946, Loss: 1.385 Epoch 5 Batch 1047/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.951, Loss: 1.494 Epoch 5 Batch 1048/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.950, Loss: 1.411 Epoch 5 Batch 1049/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.950, Loss: 1.465 Epoch 5 Batch 1050/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.947, Loss: 1.317 Epoch 5 Batch 1051/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.947, Loss: 1.425 Epoch 5 Batch 1052/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.946, Loss: 1.424 Epoch 5 Batch 1053/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.946, Loss: 1.410 Epoch 5 Batch 1054/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.940, Loss: 1.585 Epoch 5 Batch 1055/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.939, Loss: 1.423 Epoch 5 Batch 1056/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.939, Loss: 1.444 Epoch 5 Batch 1057/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.935, Loss: 1.457 Epoch 5 Batch 1058/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.936, Loss: 1.505 Epoch 5 Batch 1059/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.936, Loss: 1.407 Epoch 5 Batch 1060/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.936, Loss: 1.447 Epoch 5 Batch 1061/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.936, Loss: 1.539 Epoch 5 Batch 1062/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.931, Loss: 1.382 Epoch 5 Batch 1063/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.939, Loss: 1.367 Epoch 5 Batch 1064/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.939, Loss: 1.435 Epoch 5 Batch 1065/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.934, Loss: 1.379 Epoch 5 Batch 1066/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.931, Loss: 1.381 Epoch 5 Batch 1067/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.931, Loss: 1.416 Epoch 5 Batch 1068/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.931, Loss: 1.497 Epoch 5 Batch 1069/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.926, Loss: 1.453 Epoch 5 Batch 1070/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.931, Loss: 1.461 Epoch 5 Batch 1071/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.927, Loss: 1.476 Epoch 5 Batch 1072/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.932, Loss: 1.488 Epoch 5 Batch 1073/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.932, Loss: 1.430 Epoch 5 Batch 1074/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.937, Loss: 1.422 Epoch 5 Batch 1075/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.936, Loss: 1.514 Epoch 6 Batch 0/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.936, Loss: 1.442 Epoch 6 Batch 1/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.941, Loss: 1.417 Epoch 6 Batch 2/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.941, Loss: 1.471 Epoch 6 Batch 3/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.944, Loss: 1.484 Epoch 6 Batch 4/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.944, Loss: 1.522 Epoch 6 Batch 5/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.947, Loss: 1.470 Epoch 6 Batch 6/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.946, Loss: 1.428 Epoch 6 Batch 7/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.951, Loss: 1.420 Epoch 6 Batch 8/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.949, Loss: 1.386 Epoch 6 Batch 9/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.946, Loss: 1.474 Epoch 6 Batch 10/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.950, Loss: 1.460 Epoch 6 Batch 11/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.950, Loss: 1.514 Epoch 6 Batch 12/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.945, Loss: 1.548 Epoch 6 Batch 13/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.945, Loss: 1.427 Epoch 6 Batch 14/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.940, Loss: 1.377 Epoch 6 Batch 15/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.935, Loss: 1.454 Epoch 6 Batch 16/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.935, Loss: 1.416 Epoch 6 Batch 17/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.939, Loss: 1.541 Epoch 6 Batch 18/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.939, Loss: 1.433 Epoch 6 Batch 19/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.932, Loss: 1.425 Epoch 6 Batch 20/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.935, Loss: 1.481 Epoch 6 Batch 21/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.940, Loss: 1.349 Epoch 6 Batch 22/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.946, Loss: 1.393 Epoch 6 Batch 23/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.946, Loss: 1.479 Epoch 6 Batch 24/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.950, Loss: 1.483 Epoch 6 Batch 25/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.950, Loss: 1.361 Epoch 6 Batch 26/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.946, Loss: 1.464 Epoch 6 Batch 27/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.944, Loss: 1.334 Epoch 6 Batch 28/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.955, Loss: 1.428 Epoch 6 Batch 29/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.414 Epoch 6 Batch 30/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.953, Loss: 1.436 Epoch 6 Batch 31/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.953, Loss: 1.474 Epoch 6 Batch 32/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.949, Loss: 1.518 Epoch 6 Batch 33/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.949, Loss: 1.454 Epoch 6 Batch 34/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.949, Loss: 1.477 Epoch 6 Batch 35/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.953, Loss: 1.485 Epoch 6 Batch 36/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.953, Loss: 1.479 Epoch 6 Batch 37/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.953, Loss: 1.466 Epoch 6 Batch 38/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.953, Loss: 1.431 Epoch 6 Batch 39/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.953, Loss: 1.469 Epoch 6 Batch 40/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.950, Loss: 1.395 Epoch 6 Batch 41/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.950, Loss: 1.407 Epoch 6 Batch 42/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.944, Loss: 1.387 Epoch 6 Batch 43/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.944, Loss: 1.484 Epoch 6 Batch 44/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.944, Loss: 1.366 Epoch 6 Batch 45/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.942, Loss: 1.461 Epoch 6 Batch 46/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.951, Loss: 1.384 Epoch 6 Batch 47/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.951, Loss: 1.459 Epoch 6 Batch 48/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.958, Loss: 1.516 Epoch 6 Batch 49/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.958, Loss: 1.424 Epoch 6 Batch 50/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.957, Loss: 1.380 Epoch 6 Batch 51/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.957, Loss: 1.416 Epoch 6 Batch 52/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.952, Loss: 1.477 Epoch 6 Batch 53/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.952, Loss: 1.382 Epoch 6 Batch 54/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.952, Loss: 1.462 Epoch 6 Batch 55/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.952, Loss: 1.452 Epoch 6 Batch 56/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.946, Loss: 1.461 Epoch 6 Batch 57/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.945, Loss: 1.504 Epoch 6 Batch 58/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.943, Loss: 1.374 Epoch 6 Batch 59/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.942, Loss: 1.418 Epoch 6 Batch 60/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.942, Loss: 1.455 Epoch 6 Batch 61/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.933, Loss: 1.485 Epoch 6 Batch 62/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.933, Loss: 1.403 Epoch 6 Batch 63/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.933, Loss: 1.510 Epoch 6 Batch 64/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.934, Loss: 1.448 Epoch 6 Batch 65/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.932, Loss: 1.459 Epoch 6 Batch 66/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.937, Loss: 1.424 Epoch 6 Batch 67/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.937, Loss: 1.477 Epoch 6 Batch 68/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.942, Loss: 1.451 Epoch 6 Batch 69/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.942, Loss: 1.449 Epoch 6 Batch 70/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.947, Loss: 1.460 Epoch 6 Batch 71/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.487 Epoch 6 Batch 72/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.947, Loss: 1.431 Epoch 6 Batch 73/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.949, Loss: 1.446 Epoch 6 Batch 74/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.949, Loss: 1.398 Epoch 6 Batch 75/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.944, Loss: 1.384 Epoch 6 Batch 76/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.949, Loss: 1.470 Epoch 6 Batch 77/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.942, Loss: 1.408 Epoch 6 Batch 78/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.938, Loss: 1.486 Epoch 6 Batch 79/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.938, Loss: 1.426 Epoch 6 Batch 80/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.942, Loss: 1.476 Epoch 6 Batch 81/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.945, Loss: 1.428 Epoch 6 Batch 82/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.940, Loss: 1.437 Epoch 6 Batch 83/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.933, Loss: 1.489 Epoch 6 Batch 84/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.933, Loss: 1.534 Epoch 6 Batch 85/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.942, Loss: 1.391 Epoch 6 Batch 86/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.942, Loss: 1.423 Epoch 6 Batch 87/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.943, Loss: 1.447 Epoch 6 Batch 88/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.946, Loss: 1.486 Epoch 6 Batch 89/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.951, Loss: 1.431 Epoch 6 Batch 90/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.953, Loss: 1.463 Epoch 6 Batch 91/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.952, Loss: 1.389 Epoch 6 Batch 92/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.426 Epoch 6 Batch 93/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.955, Loss: 1.434 Epoch 6 Batch 94/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.954, Loss: 1.411 Epoch 6 Batch 95/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.954, Loss: 1.456 Epoch 6 Batch 96/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.948, Loss: 1.444 Epoch 6 Batch 97/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.942, Loss: 1.439 Epoch 6 Batch 98/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.942, Loss: 1.486 Epoch 6 Batch 99/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.937, Loss: 1.351 Epoch 6 Batch 100/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.932, Loss: 1.372 Epoch 6 Batch 101/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.934, Loss: 1.430 Epoch 6 Batch 102/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.935, Loss: 1.376 Epoch 6 Batch 103/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.930, Loss: 1.443 Epoch 6 Batch 104/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.942, Loss: 1.505 Epoch 6 Batch 105/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.951, Loss: 1.508 Epoch 6 Batch 106/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.946, Loss: 1.393 Epoch 6 Batch 107/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.946, Loss: 1.417 Epoch 6 Batch 108/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.956, Loss: 1.445 Epoch 6 Batch 109/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.939, Loss: 1.414 Epoch 6 Batch 110/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.939, Loss: 1.507 Epoch 6 Batch 111/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.948, Loss: 1.394 Epoch 6 Batch 112/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.948, Loss: 1.470 Epoch 6 Batch 113/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.947, Loss: 1.493 Epoch 6 Batch 114/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.943, Loss: 1.372 Epoch 6 Batch 115/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.947, Loss: 1.469 Epoch 6 Batch 116/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.945, Loss: 1.452 Epoch 6 Batch 117/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.942, Loss: 1.455 Epoch 6 Batch 118/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.933, Loss: 1.364 Epoch 6 Batch 119/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.939, Loss: 1.402 Epoch 6 Batch 120/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.948, Loss: 1.390 Epoch 6 Batch 121/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.949, Loss: 1.390 Epoch 6 Batch 122/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.949, Loss: 1.461 Epoch 6 Batch 123/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.520 Epoch 6 Batch 124/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.951, Loss: 1.473 Epoch 6 Batch 125/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.958, Loss: 1.436 Epoch 6 Batch 126/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.963, Loss: 1.426 Epoch 6 Batch 127/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.958, Loss: 1.471 Epoch 6 Batch 128/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.967, Loss: 1.432 Epoch 6 Batch 129/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.964, Loss: 1.414 Epoch 6 Batch 130/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.959, Loss: 1.424 Epoch 6 Batch 131/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.955, Loss: 1.442 Epoch 6 Batch 132/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.960, Loss: 1.364 Epoch 6 Batch 133/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.960, Loss: 1.437 Epoch 6 Batch 134/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.960, Loss: 1.477 Epoch 6 Batch 135/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.950, Loss: 1.387 Epoch 6 Batch 136/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.955, Loss: 1.415 Epoch 6 Batch 137/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.361 Epoch 6 Batch 138/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.947, Loss: 1.380 Epoch 6 Batch 139/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.482 Epoch 6 Batch 140/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.456 Epoch 6 Batch 141/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.944, Loss: 1.401 Epoch 6 Batch 142/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.946, Loss: 1.441 Epoch 6 Batch 143/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.946, Loss: 1.454 Epoch 6 Batch 144/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.946, Loss: 1.462 Epoch 6 Batch 145/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.941, Loss: 1.411 Epoch 6 Batch 146/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.945, Loss: 1.410 Epoch 6 Batch 147/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.455 Epoch 6 Batch 148/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.944, Loss: 1.420 Epoch 6 Batch 149/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.939, Loss: 1.456 Epoch 6 Batch 150/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.939, Loss: 1.464 Epoch 6 Batch 151/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.939, Loss: 1.464 Epoch 6 Batch 152/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.939, Loss: 1.446 Epoch 6 Batch 153/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.939, Loss: 1.412 Epoch 6 Batch 154/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.944, Loss: 1.452 Epoch 6 Batch 155/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.945, Loss: 1.486 Epoch 6 Batch 156/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.950, Loss: 1.478 Epoch 6 Batch 157/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.950, Loss: 1.469 Epoch 6 Batch 158/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.949, Loss: 1.401 Epoch 6 Batch 159/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.949, Loss: 1.486 Epoch 6 Batch 160/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.950, Loss: 1.312 Epoch 6 Batch 161/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.942, Loss: 1.443 Epoch 6 Batch 162/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.943, Loss: 1.462 Epoch 6 Batch 163/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.948, Loss: 1.413 Epoch 6 Batch 164/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.435 Epoch 6 Batch 165/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.946, Loss: 1.432 Epoch 6 Batch 166/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.949, Loss: 1.405 Epoch 6 Batch 167/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.949, Loss: 1.504 Epoch 6 Batch 168/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.942, Loss: 1.496 Epoch 6 Batch 169/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.944, Loss: 1.431 Epoch 6 Batch 170/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.946, Loss: 1.443 Epoch 6 Batch 171/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.947, Loss: 1.452 Epoch 6 Batch 172/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.942, Loss: 1.414 Epoch 6 Batch 173/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.933, Loss: 1.440 Epoch 6 Batch 174/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.937, Loss: 1.406 Epoch 6 Batch 175/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.937, Loss: 1.511 Epoch 6 Batch 176/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.937, Loss: 1.411 Epoch 6 Batch 177/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.937, Loss: 1.426 Epoch 6 Batch 178/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.931, Loss: 1.354 Epoch 6 Batch 179/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.936, Loss: 1.484 Epoch 6 Batch 180/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.945, Loss: 1.434 Epoch 6 Batch 181/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.945, Loss: 1.478 Epoch 6 Batch 182/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.945, Loss: 1.430 Epoch 6 Batch 183/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.945, Loss: 1.411 Epoch 6 Batch 184/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.945, Loss: 1.397 Epoch 6 Batch 185/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.951, Loss: 1.479 Epoch 6 Batch 186/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.951, Loss: 1.385 Epoch 6 Batch 187/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.951, Loss: 1.437 Epoch 6 Batch 188/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.951, Loss: 1.421 Epoch 6 Batch 189/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.951, Loss: 1.416 Epoch 6 Batch 190/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.957, Loss: 1.428 Epoch 6 Batch 191/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.952, Loss: 1.429 Epoch 6 Batch 192/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.954, Loss: 1.453 Epoch 6 Batch 193/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.955, Loss: 1.462 Epoch 6 Batch 194/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.955, Loss: 1.420 Epoch 6 Batch 195/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.950, Loss: 1.382 Epoch 6 Batch 196/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.950, Loss: 1.420 Epoch 6 Batch 197/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.955, Loss: 1.424 Epoch 6 Batch 198/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.955, Loss: 1.399 Epoch 6 Batch 199/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.950, Loss: 1.518 Epoch 6 Batch 200/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.942, Loss: 1.457 Epoch 6 Batch 201/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.944, Loss: 1.334 Epoch 6 Batch 202/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.945, Loss: 1.397 Epoch 6 Batch 203/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.945, Loss: 1.418 Epoch 6 Batch 204/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.945, Loss: 1.362 Epoch 6 Batch 205/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.952, Loss: 1.481 Epoch 6 Batch 206/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.947, Loss: 1.405 Epoch 6 Batch 207/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.946, Loss: 1.384 Epoch 6 Batch 208/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.940, Loss: 1.444 Epoch 6 Batch 209/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.935, Loss: 1.469 Epoch 6 Batch 210/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.930, Loss: 1.438 Epoch 6 Batch 211/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.925, Loss: 1.481 Epoch 6 Batch 212/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.920, Loss: 1.396 Epoch 6 Batch 213/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.926, Loss: 1.490 Epoch 6 Batch 214/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.936, Loss: 1.520 Epoch 6 Batch 215/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.942, Loss: 1.400 Epoch 6 Batch 216/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.942, Loss: 1.418 Epoch 6 Batch 217/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.941, Loss: 1.423 Epoch 6 Batch 218/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.941, Loss: 1.424 Epoch 6 Batch 219/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.942, Loss: 1.363 Epoch 6 Batch 220/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.944, Loss: 1.452 Epoch 6 Batch 221/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.947, Loss: 1.375 Epoch 6 Batch 222/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.941, Loss: 1.409 Epoch 6 Batch 223/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.951, Loss: 1.476 Epoch 6 Batch 224/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.950, Loss: 1.485 Epoch 6 Batch 225/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.951, Loss: 1.431 Epoch 6 Batch 226/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.464 Epoch 6 Batch 227/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.956, Loss: 1.406 Epoch 6 Batch 228/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.954, Loss: 1.542 Epoch 6 Batch 229/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.954, Loss: 1.352 Epoch 6 Batch 230/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.959, Loss: 1.415 Epoch 6 Batch 231/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.957, Loss: 1.548 Epoch 6 Batch 232/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.438 Epoch 6 Batch 233/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.956, Loss: 1.511 Epoch 6 Batch 234/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.956, Loss: 1.474 Epoch 6 Batch 235/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.956, Loss: 1.458 Epoch 6 Batch 236/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.957, Loss: 1.452 Epoch 6 Batch 237/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.947, Loss: 1.378 Epoch 6 Batch 238/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.947, Loss: 1.515 Epoch 6 Batch 239/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.951, Loss: 1.438 Epoch 6 Batch 240/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.947, Loss: 1.480 Epoch 6 Batch 241/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.947, Loss: 1.449 Epoch 6 Batch 242/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.942, Loss: 1.421 Epoch 6 Batch 243/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.942, Loss: 1.442 Epoch 6 Batch 244/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.942, Loss: 1.454 Epoch 6 Batch 245/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.943, Loss: 1.410 Epoch 6 Batch 246/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.943, Loss: 1.405 Epoch 6 Batch 247/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.942, Loss: 1.456 Epoch 6 Batch 248/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.952, Loss: 1.489 Epoch 6 Batch 249/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.950, Loss: 1.474 Epoch 6 Batch 250/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.947, Loss: 1.425 Epoch 6 Batch 251/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.425 Epoch 6 Batch 252/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.942, Loss: 1.366 Epoch 6 Batch 253/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.942, Loss: 1.397 Epoch 6 Batch 254/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.942, Loss: 1.458 Epoch 6 Batch 255/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.948, Loss: 1.527 Epoch 6 Batch 256/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.946, Loss: 1.439 Epoch 6 Batch 257/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.951, Loss: 1.422 Epoch 6 Batch 258/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.946, Loss: 1.505 Epoch 6 Batch 259/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.951, Loss: 1.399 Epoch 6 Batch 260/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.951, Loss: 1.420 Epoch 6 Batch 261/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.955, Loss: 1.453 Epoch 6 Batch 262/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.961, Loss: 1.399 Epoch 6 Batch 263/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.963, Loss: 1.422 Epoch 6 Batch 264/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.958, Loss: 1.356 Epoch 6 Batch 265/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.952, Loss: 1.429 Epoch 6 Batch 266/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.946, Loss: 1.412 Epoch 6 Batch 267/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.946, Loss: 1.401 Epoch 6 Batch 268/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.941, Loss: 1.487 Epoch 6 Batch 269/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.444 Epoch 6 Batch 270/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.941, Loss: 1.506 Epoch 6 Batch 271/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.950, Loss: 1.387 Epoch 6 Batch 272/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.940, Loss: 1.512 Epoch 6 Batch 273/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.938, Loss: 1.438 Epoch 6 Batch 274/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.938, Loss: 1.486 Epoch 6 Batch 275/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.935, Loss: 1.482 Epoch 6 Batch 276/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.931, Loss: 1.447 Epoch 6 Batch 277/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.935, Loss: 1.401 Epoch 6 Batch 278/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.935, Loss: 1.415 Epoch 6 Batch 279/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.942, Loss: 1.410 Epoch 6 Batch 280/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.420 Epoch 6 Batch 281/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.460 Epoch 6 Batch 282/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.947, Loss: 1.402 Epoch 6 Batch 283/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.947, Loss: 1.432 Epoch 6 Batch 284/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.942, Loss: 1.381 Epoch 6 Batch 285/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.948, Loss: 1.392 Epoch 6 Batch 286/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.944, Loss: 1.509 Epoch 6 Batch 287/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.944, Loss: 1.452 Epoch 6 Batch 288/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.950, Loss: 1.384 Epoch 6 Batch 289/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.438 Epoch 6 Batch 290/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.952, Loss: 1.426 Epoch 6 Batch 291/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.943, Loss: 1.490 Epoch 6 Batch 292/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.943, Loss: 1.448 Epoch 6 Batch 293/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.953, Loss: 1.402 Epoch 6 Batch 294/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.455 Epoch 6 Batch 295/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.947, Loss: 1.460 Epoch 6 Batch 296/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.951, Loss: 1.346 Epoch 6 Batch 297/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.952, Loss: 1.415 Epoch 6 Batch 298/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.950, Loss: 1.500 Epoch 6 Batch 299/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.953, Loss: 1.388 Epoch 6 Batch 300/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.390 Epoch 6 Batch 301/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.957, Loss: 1.394 Epoch 6 Batch 302/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.957, Loss: 1.398 Epoch 6 Batch 303/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.382 Epoch 6 Batch 304/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.950, Loss: 1.388 Epoch 6 Batch 305/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.950, Loss: 1.421 Epoch 6 Batch 306/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.502 Epoch 6 Batch 307/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.392 Epoch 6 Batch 308/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.953, Loss: 1.459 Epoch 6 Batch 309/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.954, Loss: 1.338 Epoch 6 Batch 310/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.942, Loss: 1.426 Epoch 6 Batch 311/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.942, Loss: 1.449 Epoch 6 Batch 312/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.932, Loss: 1.517 Epoch 6 Batch 313/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.932, Loss: 1.412 Epoch 6 Batch 314/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.932, Loss: 1.432 Epoch 6 Batch 315/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.934, Loss: 1.503 Epoch 6 Batch 316/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.940, Loss: 1.370 Epoch 6 Batch 317/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.940, Loss: 1.446 Epoch 6 Batch 318/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.938, Loss: 1.401 Epoch 6 Batch 319/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.935, Loss: 1.432 Epoch 6 Batch 320/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.933, Loss: 1.449 Epoch 6 Batch 321/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.933, Loss: 1.393 Epoch 6 Batch 322/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.945, Loss: 1.423 Epoch 6 Batch 323/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.941, Loss: 1.444 Epoch 6 Batch 324/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.946, Loss: 1.435 Epoch 6 Batch 325/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.487 Epoch 6 Batch 326/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.390 Epoch 6 Batch 327/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.956, Loss: 1.370 Epoch 6 Batch 328/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.960, Loss: 1.411 Epoch 6 Batch 329/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.967, Loss: 1.503 Epoch 6 Batch 330/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.956, Loss: 1.445 Epoch 6 Batch 331/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.953, Loss: 1.472 Epoch 6 Batch 332/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.953, Loss: 1.425 Epoch 6 Batch 333/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.953, Loss: 1.457 Epoch 6 Batch 334/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.481 Epoch 6 Batch 335/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.374 Epoch 6 Batch 336/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.954, Loss: 1.473 Epoch 6 Batch 337/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.394 Epoch 6 Batch 338/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.950, Loss: 1.454 Epoch 6 Batch 339/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.945, Loss: 1.411 Epoch 6 Batch 340/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.944, Loss: 1.417 Epoch 6 Batch 341/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.943, Loss: 1.403 Epoch 6 Batch 342/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.948, Loss: 1.425 Epoch 6 Batch 343/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.948, Loss: 1.441 Epoch 6 Batch 344/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.954, Loss: 1.457 Epoch 6 Batch 345/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.958, Loss: 1.472 Epoch 6 Batch 346/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.957, Loss: 1.471 Epoch 6 Batch 347/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.949, Loss: 1.399 Epoch 6 Batch 348/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.958, Loss: 1.359 Epoch 6 Batch 349/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.958, Loss: 1.414 Epoch 6 Batch 350/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.943, Loss: 1.478 Epoch 6 Batch 351/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.941, Loss: 1.430 Epoch 6 Batch 352/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.941, Loss: 1.485 Epoch 6 Batch 353/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.941, Loss: 1.413 Epoch 6 Batch 354/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.935, Loss: 1.456 Epoch 6 Batch 355/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.935, Loss: 1.401 Epoch 6 Batch 356/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.940, Loss: 1.497 Epoch 6 Batch 357/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.936, Loss: 1.463 Epoch 6 Batch 358/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.952, Loss: 1.479 Epoch 6 Batch 359/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.952, Loss: 1.442 Epoch 6 Batch 360/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.952, Loss: 1.427 Epoch 6 Batch 361/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.950, Loss: 1.533 Epoch 6 Batch 362/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.944, Loss: 1.445 Epoch 6 Batch 363/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.939, Loss: 1.413 Epoch 6 Batch 364/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.942, Loss: 1.462 Epoch 6 Batch 365/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.942, Loss: 1.501 Epoch 6 Batch 366/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.942, Loss: 1.471 Epoch 6 Batch 367/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.937, Loss: 1.434 Epoch 6 Batch 368/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.932, Loss: 1.407 Epoch 6 Batch 369/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.928, Loss: 1.483 Epoch 6 Batch 370/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.932, Loss: 1.360 Epoch 6 Batch 371/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.927, Loss: 1.488 Epoch 6 Batch 372/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.932, Loss: 1.382 Epoch 6 Batch 373/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.932, Loss: 1.356 Epoch 6 Batch 374/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.935, Loss: 1.393 Epoch 6 Batch 375/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.941, Loss: 1.427 Epoch 6 Batch 376/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.941, Loss: 1.437 Epoch 6 Batch 377/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.947, Loss: 1.492 Epoch 6 Batch 378/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.947, Loss: 1.432 Epoch 6 Batch 379/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.952, Loss: 1.479 Epoch 6 Batch 380/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.951, Loss: 1.451 Epoch 6 Batch 381/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.946, Loss: 1.460 Epoch 6 Batch 382/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.951, Loss: 1.401 Epoch 6 Batch 383/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.952, Loss: 1.386 Epoch 6 Batch 384/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.417 Epoch 6 Batch 385/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.949, Loss: 1.467 Epoch 6 Batch 386/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.949, Loss: 1.432 Epoch 6 Batch 387/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.940, Loss: 1.404 Epoch 6 Batch 388/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.940, Loss: 1.403 Epoch 6 Batch 389/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.945, Loss: 1.496 Epoch 6 Batch 390/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.950, Loss: 1.524 Epoch 6 Batch 391/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.959, Loss: 1.417 Epoch 6 Batch 392/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.476 Epoch 6 Batch 393/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.958, Loss: 1.426 Epoch 6 Batch 394/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.960, Loss: 1.463 Epoch 6 Batch 395/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.965, Loss: 1.440 Epoch 6 Batch 396/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.960, Loss: 1.387 Epoch 6 Batch 397/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.952, Loss: 1.447 Epoch 6 Batch 398/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.946, Loss: 1.514 Epoch 6 Batch 399/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.937, Loss: 1.363 Epoch 6 Batch 400/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.946, Loss: 1.493 Epoch 6 Batch 401/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.941, Loss: 1.498 Epoch 6 Batch 402/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.940, Loss: 1.488 Epoch 6 Batch 403/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.936, Loss: 1.548 Epoch 6 Batch 404/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.943, Loss: 1.422 Epoch 6 Batch 405/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.936, Loss: 1.466 Epoch 6 Batch 406/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.934, Loss: 1.485 Epoch 6 Batch 407/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.934, Loss: 1.430 Epoch 6 Batch 408/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.944, Loss: 1.402 Epoch 6 Batch 409/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.947, Loss: 1.441 Epoch 6 Batch 410/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.947, Loss: 1.508 Epoch 6 Batch 411/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.944, Loss: 1.418 Epoch 6 Batch 412/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.949, Loss: 1.500 Epoch 6 Batch 413/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.949, Loss: 1.529 Epoch 6 Batch 414/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.949, Loss: 1.437 Epoch 6 Batch 415/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.949, Loss: 1.395 Epoch 6 Batch 416/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.949, Loss: 1.426 Epoch 6 Batch 417/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.475 Epoch 6 Batch 418/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.949, Loss: 1.432 Epoch 6 Batch 419/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.949, Loss: 1.452 Epoch 6 Batch 420/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.950, Loss: 1.329 Epoch 6 Batch 421/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.944, Loss: 1.425 Epoch 6 Batch 422/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.949, Loss: 1.485 Epoch 6 Batch 423/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.944, Loss: 1.442 Epoch 6 Batch 424/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.950, Loss: 1.384 Epoch 6 Batch 425/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.950, Loss: 1.430 Epoch 6 Batch 426/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.450 Epoch 6 Batch 427/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.949, Loss: 1.447 Epoch 6 Batch 428/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.949, Loss: 1.410 Epoch 6 Batch 429/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.949, Loss: 1.426 Epoch 6 Batch 430/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.949, Loss: 1.434 Epoch 6 Batch 431/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.949, Loss: 1.432 Epoch 6 Batch 432/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.951, Loss: 1.400 Epoch 6 Batch 433/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.943, Loss: 1.395 Epoch 6 Batch 434/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.939, Loss: 1.384 Epoch 6 Batch 435/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.941, Loss: 1.458 Epoch 6 Batch 436/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.941, Loss: 1.445 Epoch 6 Batch 437/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.941, Loss: 1.452 Epoch 6 Batch 438/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.942, Loss: 1.452 Epoch 6 Batch 439/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.943, Loss: 1.397 Epoch 6 Batch 440/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.942, Loss: 1.453 Epoch 6 Batch 441/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.942, Loss: 1.502 Epoch 6 Batch 442/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.946, Loss: 1.488 Epoch 6 Batch 443/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.442 Epoch 6 Batch 444/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.947, Loss: 1.419 Epoch 6 Batch 445/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.948, Loss: 1.460 Epoch 6 Batch 446/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.942, Loss: 1.429 Epoch 6 Batch 447/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.942, Loss: 1.527 Epoch 6 Batch 448/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.941, Loss: 1.437 Epoch 6 Batch 449/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.931, Loss: 1.452 Epoch 6 Batch 450/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.931, Loss: 1.427 Epoch 6 Batch 451/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.933, Loss: 1.319 Epoch 6 Batch 452/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.933, Loss: 1.472 Epoch 6 Batch 453/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.938, Loss: 1.519 Epoch 6 Batch 454/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.940, Loss: 1.453 Epoch 6 Batch 455/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.940, Loss: 1.412 Epoch 6 Batch 456/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.934, Loss: 1.485 Epoch 6 Batch 457/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.934, Loss: 1.412 Epoch 6 Batch 458/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.939, Loss: 1.365 Epoch 6 Batch 459/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.945, Loss: 1.434 Epoch 6 Batch 460/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.425 Epoch 6 Batch 461/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.949, Loss: 1.434 Epoch 6 Batch 462/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.944, Loss: 1.445 Epoch 6 Batch 463/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.943, Loss: 1.416 Epoch 6 Batch 464/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.944, Loss: 1.433 Epoch 6 Batch 465/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.939, Loss: 1.461 Epoch 6 Batch 466/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.939, Loss: 1.466 Epoch 6 Batch 467/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.939, Loss: 1.445 Epoch 6 Batch 468/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.944, Loss: 1.425 Epoch 6 Batch 469/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.946, Loss: 1.530 Epoch 6 Batch 470/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.945, Loss: 1.462 Epoch 6 Batch 471/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.945, Loss: 1.395 Epoch 6 Batch 472/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.945, Loss: 1.377 Epoch 6 Batch 473/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.940, Loss: 1.473 Epoch 6 Batch 474/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.945, Loss: 1.449 Epoch 6 Batch 475/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.936, Loss: 1.437 Epoch 6 Batch 476/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.945, Loss: 1.297 Epoch 6 Batch 477/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.944, Loss: 1.444 Epoch 6 Batch 478/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.415 Epoch 6 Batch 479/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.435 Epoch 6 Batch 480/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.958, Loss: 1.430 Epoch 6 Batch 481/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.958, Loss: 1.446 Epoch 6 Batch 482/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.962, Loss: 1.357 Epoch 6 Batch 483/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.958, Loss: 1.444 Epoch 6 Batch 484/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.962, Loss: 1.453 Epoch 6 Batch 485/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.962, Loss: 1.381 Epoch 6 Batch 486/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.446 Epoch 6 Batch 487/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.963, Loss: 1.311 Epoch 6 Batch 488/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.953, Loss: 1.449 Epoch 6 Batch 489/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.958, Loss: 1.394 Epoch 6 Batch 490/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.958, Loss: 1.516 Epoch 6 Batch 491/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.951, Loss: 1.454 Epoch 6 Batch 492/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.945, Loss: 1.413 Epoch 6 Batch 493/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.433 Epoch 6 Batch 494/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.945, Loss: 1.391 Epoch 6 Batch 495/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.945, Loss: 1.394 Epoch 6 Batch 496/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.945, Loss: 1.458 Epoch 6 Batch 497/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.945, Loss: 1.521 Epoch 6 Batch 498/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.448 Epoch 6 Batch 499/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.952, Loss: 1.411 Epoch 6 Batch 500/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.952, Loss: 1.454 Epoch 6 Batch 501/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.952, Loss: 1.489 Epoch 6 Batch 502/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.962, Loss: 1.443 Epoch 6 Batch 503/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.962, Loss: 1.416 Epoch 6 Batch 504/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.962, Loss: 1.512 Epoch 6 Batch 505/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.424 Epoch 6 Batch 506/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.958, Loss: 1.432 Epoch 6 Batch 507/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.953, Loss: 1.404 Epoch 6 Batch 508/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.953, Loss: 1.416 Epoch 6 Batch 509/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.953, Loss: 1.422 Epoch 6 Batch 510/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.953, Loss: 1.434 Epoch 6 Batch 511/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.949, Loss: 1.455 Epoch 6 Batch 512/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.949, Loss: 1.357 Epoch 6 Batch 513/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.949, Loss: 1.436 Epoch 6 Batch 514/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.942, Loss: 1.409 Epoch 6 Batch 515/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.942, Loss: 1.470 Epoch 6 Batch 516/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.944, Loss: 1.466 Epoch 6 Batch 517/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.937, Loss: 1.393 Epoch 6 Batch 518/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.937, Loss: 1.452 Epoch 6 Batch 519/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.937, Loss: 1.495 Epoch 6 Batch 520/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.950, Loss: 1.409 Epoch 6 Batch 521/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.955, Loss: 1.450 Epoch 6 Batch 522/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.955, Loss: 1.393 Epoch 6 Batch 523/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.454 Epoch 6 Batch 524/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.949, Loss: 1.368 Epoch 6 Batch 525/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.953, Loss: 1.531 Epoch 6 Batch 526/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.423 Epoch 6 Batch 527/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.951, Loss: 1.457 Epoch 6 Batch 528/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.951, Loss: 1.389 Epoch 6 Batch 529/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.954, Loss: 1.443 Epoch 6 Batch 530/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.954, Loss: 1.385 Epoch 6 Batch 531/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.959, Loss: 1.449 Epoch 6 Batch 532/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.950, Loss: 1.441 Epoch 6 Batch 533/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.946, Loss: 1.521 Epoch 6 Batch 534/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.937, Loss: 1.381 Epoch 6 Batch 535/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.946, Loss: 1.363 Epoch 6 Batch 536/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.941, Loss: 1.464 Epoch 6 Batch 537/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.950, Loss: 1.490 Epoch 6 Batch 538/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.950, Loss: 1.404 Epoch 6 Batch 539/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.949, Loss: 1.507 Epoch 6 Batch 540/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.954, Loss: 1.403 Epoch 6 Batch 541/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.476 Epoch 6 Batch 542/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.958, Loss: 1.395 Epoch 6 Batch 543/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.963, Loss: 1.407 Epoch 6 Batch 544/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.958, Loss: 1.461 Epoch 6 Batch 545/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.960, Loss: 1.503 Epoch 6 Batch 546/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.957, Loss: 1.463 Epoch 6 Batch 547/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.957, Loss: 1.395 Epoch 6 Batch 548/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.962, Loss: 1.502 Epoch 6 Batch 549/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.962, Loss: 1.434 Epoch 6 Batch 550/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.956, Loss: 1.441 Epoch 6 Batch 551/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.958, Loss: 1.433 Epoch 6 Batch 552/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.949, Loss: 1.465 Epoch 6 Batch 553/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.949, Loss: 1.449 Epoch 6 Batch 554/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.947, Loss: 1.420 Epoch 6 Batch 555/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.952, Loss: 1.393 Epoch 6 Batch 556/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.952, Loss: 1.439 Epoch 6 Batch 557/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.947, Loss: 1.401 Epoch 6 Batch 558/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.439 Epoch 6 Batch 559/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.959, Loss: 1.366 Epoch 6 Batch 560/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.960, Loss: 1.393 Epoch 6 Batch 561/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.402 Epoch 6 Batch 562/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.448 Epoch 6 Batch 563/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.949, Loss: 1.486 Epoch 6 Batch 564/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.949, Loss: 1.403 Epoch 6 Batch 565/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.945, Loss: 1.453 Epoch 6 Batch 566/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.945, Loss: 1.447 Epoch 6 Batch 567/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.452 Epoch 6 Batch 568/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.953, Loss: 1.434 Epoch 6 Batch 569/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.953, Loss: 1.402 Epoch 6 Batch 570/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.953, Loss: 1.392 Epoch 6 Batch 571/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.949, Loss: 1.423 Epoch 6 Batch 572/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.953, Loss: 1.375 Epoch 6 Batch 573/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.958, Loss: 1.384 Epoch 6 Batch 574/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.958, Loss: 1.494 Epoch 6 Batch 575/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.953, Loss: 1.427 Epoch 6 Batch 576/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.953, Loss: 1.454 Epoch 6 Batch 577/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.951, Loss: 1.414 Epoch 6 Batch 578/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.951, Loss: 1.419 Epoch 6 Batch 579/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.413 Epoch 6 Batch 580/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.950, Loss: 1.434 Epoch 6 Batch 581/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.950, Loss: 1.445 Epoch 6 Batch 582/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.950, Loss: 1.400 Epoch 6 Batch 583/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.945, Loss: 1.366 Epoch 6 Batch 584/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.945, Loss: 1.415 Epoch 6 Batch 585/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.943, Loss: 1.429 Epoch 6 Batch 586/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.417 Epoch 6 Batch 587/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.952, Loss: 1.460 Epoch 6 Batch 588/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.954, Loss: 1.493 Epoch 6 Batch 589/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.950, Loss: 1.370 Epoch 6 Batch 590/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.947, Loss: 1.474 Epoch 6 Batch 591/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.953, Loss: 1.503 Epoch 6 Batch 592/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.953, Loss: 1.461 Epoch 6 Batch 593/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.953, Loss: 1.516 Epoch 6 Batch 594/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.944, Loss: 1.403 Epoch 6 Batch 595/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.947, Loss: 1.410 Epoch 6 Batch 596/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.949, Loss: 1.420 Epoch 6 Batch 597/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.954, Loss: 1.498 Epoch 6 Batch 598/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.955, Loss: 1.457 Epoch 6 Batch 599/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.955, Loss: 1.403 Epoch 6 Batch 600/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.949, Loss: 1.420 Epoch 6 Batch 601/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.949, Loss: 1.381 Epoch 6 Batch 602/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.956, Loss: 1.397 Epoch 6 Batch 603/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.428 Epoch 6 Batch 604/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.955, Loss: 1.449 Epoch 6 Batch 605/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.469 Epoch 6 Batch 606/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.425 Epoch 6 Batch 607/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.960, Loss: 1.418 Epoch 6 Batch 608/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.411 Epoch 6 Batch 609/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.957, Loss: 1.544 Epoch 6 Batch 610/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.957, Loss: 1.456 Epoch 6 Batch 611/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.960, Loss: 1.496 Epoch 6 Batch 612/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.476 Epoch 6 Batch 613/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.955, Loss: 1.494 Epoch 6 Batch 614/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.955, Loss: 1.426 Epoch 6 Batch 615/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.949, Loss: 1.483 Epoch 6 Batch 616/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.949, Loss: 1.536 Epoch 6 Batch 617/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.945, Loss: 1.414 Epoch 6 Batch 618/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.945, Loss: 1.444 Epoch 6 Batch 619/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.947, Loss: 1.420 Epoch 6 Batch 620/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.945, Loss: 1.325 Epoch 6 Batch 621/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.945, Loss: 1.443 Epoch 6 Batch 622/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.945, Loss: 1.424 Epoch 6 Batch 623/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.947, Loss: 1.439 Epoch 6 Batch 624/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.952, Loss: 1.481 Epoch 6 Batch 625/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.957, Loss: 1.412 Epoch 6 Batch 626/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.948, Loss: 1.452 Epoch 6 Batch 627/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.948, Loss: 1.355 Epoch 6 Batch 628/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.957, Loss: 1.408 Epoch 6 Batch 629/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.957, Loss: 1.355 Epoch 6 Batch 630/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.957, Loss: 1.397 Epoch 6 Batch 631/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.957, Loss: 1.430 Epoch 6 Batch 632/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.951, Loss: 1.454 Epoch 6 Batch 633/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.951, Loss: 1.397 Epoch 6 Batch 634/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.432 Epoch 6 Batch 635/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.949, Loss: 1.469 Epoch 6 Batch 636/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.949, Loss: 1.424 Epoch 6 Batch 637/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.951, Loss: 1.503 Epoch 6 Batch 638/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.951, Loss: 1.438 Epoch 6 Batch 639/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.951, Loss: 1.463 Epoch 6 Batch 640/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.356 Epoch 6 Batch 641/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.956, Loss: 1.525 Epoch 6 Batch 642/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.956, Loss: 1.479 Epoch 6 Batch 643/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.385 Epoch 6 Batch 644/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.962, Loss: 1.354 Epoch 6 Batch 645/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.951, Loss: 1.465 Epoch 6 Batch 646/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.945, Loss: 1.442 Epoch 6 Batch 647/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.945, Loss: 1.419 Epoch 6 Batch 648/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.460 Epoch 6 Batch 649/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.940, Loss: 1.377 Epoch 6 Batch 650/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.477 Epoch 6 Batch 651/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.947, Loss: 1.425 Epoch 6 Batch 652/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.942, Loss: 1.406 Epoch 6 Batch 653/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.949, Loss: 1.482 Epoch 6 Batch 654/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.940, Loss: 1.422 Epoch 6 Batch 655/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.940, Loss: 1.376 Epoch 6 Batch 656/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.940, Loss: 1.417 Epoch 6 Batch 657/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.944, Loss: 1.390 Epoch 6 Batch 658/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.407 Epoch 6 Batch 659/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.951, Loss: 1.458 Epoch 6 Batch 660/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.950, Loss: 1.485 Epoch 6 Batch 661/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.952, Loss: 1.484 Epoch 6 Batch 662/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.947, Loss: 1.399 Epoch 6 Batch 663/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.954, Loss: 1.392 Epoch 6 Batch 664/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.949, Loss: 1.456 Epoch 6 Batch 665/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.951, Loss: 1.350 Epoch 6 Batch 666/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.941, Loss: 1.328 Epoch 6 Batch 667/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.932, Loss: 1.392 Epoch 6 Batch 668/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.934, Loss: 1.407 Epoch 6 Batch 669/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.939, Loss: 1.435 Epoch 6 Batch 670/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.938, Loss: 1.386 Epoch 6 Batch 671/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.947, Loss: 1.424 Epoch 6 Batch 672/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.953, Loss: 1.450 Epoch 6 Batch 673/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.953, Loss: 1.478 Epoch 6 Batch 674/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.952, Loss: 1.453 Epoch 6 Batch 675/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.952, Loss: 1.490 Epoch 6 Batch 676/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.953, Loss: 1.382 Epoch 6 Batch 677/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.953, Loss: 1.429 Epoch 6 Batch 678/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.396 Epoch 6 Batch 679/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.963, Loss: 1.415 Epoch 6 Batch 680/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.955, Loss: 1.467 Epoch 6 Batch 681/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.952, Loss: 1.436 Epoch 6 Batch 682/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.952, Loss: 1.370 Epoch 6 Batch 683/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.957, Loss: 1.368 Epoch 6 Batch 684/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.957, Loss: 1.374 Epoch 6 Batch 685/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.957, Loss: 1.453 Epoch 6 Batch 686/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.472 Epoch 6 Batch 687/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.451 Epoch 6 Batch 688/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.951, Loss: 1.420 Epoch 6 Batch 689/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.457 Epoch 6 Batch 690/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.952, Loss: 1.407 Epoch 6 Batch 691/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.961, Loss: 1.321 Epoch 6 Batch 692/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.463 Epoch 6 Batch 693/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.963, Loss: 1.464 Epoch 6 Batch 694/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.963, Loss: 1.449 Epoch 6 Batch 695/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.963, Loss: 1.415 Epoch 6 Batch 696/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.959, Loss: 1.383 Epoch 6 Batch 697/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.964, Loss: 1.420 Epoch 6 Batch 698/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.959, Loss: 1.469 Epoch 6 Batch 699/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.428 Epoch 6 Batch 700/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.959, Loss: 1.443 Epoch 6 Batch 701/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.954, Loss: 1.458 Epoch 6 Batch 702/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.954, Loss: 1.458 Epoch 6 Batch 703/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.407 Epoch 6 Batch 704/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.951, Loss: 1.440 Epoch 6 Batch 705/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.961, Loss: 1.499 Epoch 6 Batch 706/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.961, Loss: 1.483 Epoch 6 Batch 707/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.443 Epoch 6 Batch 708/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.954, Loss: 1.482 Epoch 6 Batch 709/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.954, Loss: 1.454 Epoch 6 Batch 710/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.949, Loss: 1.365 Epoch 6 Batch 711/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.950, Loss: 1.439 Epoch 6 Batch 712/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.953, Loss: 1.447 Epoch 6 Batch 713/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.956, Loss: 1.386 Epoch 6 Batch 714/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.479 Epoch 6 Batch 715/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.957, Loss: 1.433 Epoch 6 Batch 716/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.957, Loss: 1.422 Epoch 6 Batch 717/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.952, Loss: 1.416 Epoch 6 Batch 718/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.953, Loss: 1.393 Epoch 6 Batch 719/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.950, Loss: 1.526 Epoch 6 Batch 720/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.953, Loss: 1.484 Epoch 6 Batch 721/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.953, Loss: 1.340 Epoch 6 Batch 722/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.957, Loss: 1.469 Epoch 6 Batch 723/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.957, Loss: 1.401 Epoch 6 Batch 724/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.952, Loss: 1.426 Epoch 6 Batch 725/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.947, Loss: 1.467 Epoch 6 Batch 726/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.954, Loss: 1.486 Epoch 6 Batch 727/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.954, Loss: 1.482 Epoch 6 Batch 728/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.954, Loss: 1.435 Epoch 6 Batch 729/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.954, Loss: 1.343 Epoch 6 Batch 730/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.953, Loss: 1.382 Epoch 6 Batch 731/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.949, Loss: 1.382 Epoch 6 Batch 732/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.949, Loss: 1.437 Epoch 6 Batch 733/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.945, Loss: 1.438 Epoch 6 Batch 734/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.945, Loss: 1.429 Epoch 6 Batch 735/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.945, Loss: 1.465 Epoch 6 Batch 736/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.945, Loss: 1.462 Epoch 6 Batch 737/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.940, Loss: 1.461 Epoch 6 Batch 738/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.942, Loss: 1.370 Epoch 6 Batch 739/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.942, Loss: 1.382 Epoch 6 Batch 740/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.946, Loss: 1.392 Epoch 6 Batch 741/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.960, Loss: 1.438 Epoch 6 Batch 742/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.958, Loss: 1.436 Epoch 6 Batch 743/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.955, Loss: 1.427 Epoch 6 Batch 744/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.426 Epoch 6 Batch 745/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.954, Loss: 1.431 Epoch 6 Batch 746/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.954, Loss: 1.392 Epoch 6 Batch 747/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.953, Loss: 1.398 Epoch 6 Batch 748/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.953, Loss: 1.427 Epoch 6 Batch 749/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.438 Epoch 6 Batch 750/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.955, Loss: 1.422 Epoch 6 Batch 751/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.946, Loss: 1.395 Epoch 6 Batch 752/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.952, Loss: 1.398 Epoch 6 Batch 753/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.444 Epoch 6 Batch 754/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.947, Loss: 1.467 Epoch 6 Batch 755/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.947, Loss: 1.505 Epoch 6 Batch 756/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.950, Loss: 1.439 Epoch 6 Batch 757/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.950, Loss: 1.431 Epoch 6 Batch 758/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.950, Loss: 1.380 Epoch 6 Batch 759/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.950, Loss: 1.379 Epoch 6 Batch 760/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.952, Loss: 1.385 Epoch 6 Batch 761/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.950, Loss: 1.438 Epoch 6 Batch 762/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.954, Loss: 1.403 Epoch 6 Batch 763/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.949, Loss: 1.409 Epoch 6 Batch 764/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.954, Loss: 1.443 Epoch 6 Batch 765/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.958, Loss: 1.431 Epoch 6 Batch 766/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.953, Loss: 1.500 Epoch 6 Batch 767/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.958, Loss: 1.415 Epoch 6 Batch 768/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.448 Epoch 6 Batch 769/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.958, Loss: 1.403 Epoch 6 Batch 770/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.954, Loss: 1.465 Epoch 6 Batch 771/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.394 Epoch 6 Batch 772/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.426 Epoch 6 Batch 773/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.953, Loss: 1.483 Epoch 6 Batch 774/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.953, Loss: 1.454 Epoch 6 Batch 775/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.954, Loss: 1.509 Epoch 6 Batch 776/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.957, Loss: 1.382 Epoch 6 Batch 777/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.358 Epoch 6 Batch 778/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.961, Loss: 1.408 Epoch 6 Batch 779/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.964, Loss: 1.404 Epoch 6 Batch 780/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.964, Loss: 1.492 Epoch 6 Batch 781/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.955, Loss: 1.391 Epoch 6 Batch 782/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.959, Loss: 1.485 Epoch 6 Batch 783/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.955, Loss: 1.404 Epoch 6 Batch 784/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.950, Loss: 1.392 Epoch 6 Batch 785/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.950, Loss: 1.385 Epoch 6 Batch 786/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.382 Epoch 6 Batch 787/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.946, Loss: 1.403 Epoch 6 Batch 788/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.951, Loss: 1.386 Epoch 6 Batch 789/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.950, Loss: 1.441 Epoch 6 Batch 790/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.948, Loss: 1.397 Epoch 6 Batch 791/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.948, Loss: 1.512 Epoch 6 Batch 792/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.943, Loss: 1.476 Epoch 6 Batch 793/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.938, Loss: 1.410 Epoch 6 Batch 794/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.932, Loss: 1.405 Epoch 6 Batch 795/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.934, Loss: 1.431 Epoch 6 Batch 796/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.934, Loss: 1.476 Epoch 6 Batch 797/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.940, Loss: 1.418 Epoch 6 Batch 798/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.943, Loss: 1.394 Epoch 6 Batch 799/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.943, Loss: 1.530 Epoch 6 Batch 800/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.950, Loss: 1.469 Epoch 6 Batch 801/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.950, Loss: 1.467 Epoch 6 Batch 802/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.410 Epoch 6 Batch 803/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.965, Loss: 1.362 Epoch 6 Batch 804/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.965, Loss: 1.368 Epoch 6 Batch 805/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.960, Loss: 1.399 Epoch 6 Batch 806/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.956, Loss: 1.391 Epoch 6 Batch 807/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.956, Loss: 1.415 Epoch 6 Batch 808/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.954, Loss: 1.475 Epoch 6 Batch 809/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.952, Loss: 1.404 Epoch 6 Batch 810/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.952, Loss: 1.424 Epoch 6 Batch 811/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.947, Loss: 1.454 Epoch 6 Batch 812/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.436 Epoch 6 Batch 813/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.942, Loss: 1.418 Epoch 6 Batch 814/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.942, Loss: 1.433 Epoch 6 Batch 815/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.941, Loss: 1.514 Epoch 6 Batch 816/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.934, Loss: 1.475 Epoch 6 Batch 817/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.934, Loss: 1.430 Epoch 6 Batch 818/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.934, Loss: 1.462 Epoch 6 Batch 819/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.943, Loss: 1.497 Epoch 6 Batch 820/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.945, Loss: 1.453 Epoch 6 Batch 821/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.407 Epoch 6 Batch 822/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.954, Loss: 1.387 Epoch 6 Batch 823/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.384 Epoch 6 Batch 824/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.387 Epoch 6 Batch 825/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.956, Loss: 1.505 Epoch 6 Batch 826/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.958, Loss: 1.495 Epoch 6 Batch 827/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.949, Loss: 1.463 Epoch 6 Batch 828/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.948, Loss: 1.463 Epoch 6 Batch 829/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.949, Loss: 1.427 Epoch 6 Batch 830/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.942, Loss: 1.459 Epoch 6 Batch 831/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.950, Loss: 1.500 Epoch 6 Batch 832/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.357 Epoch 6 Batch 833/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.949, Loss: 1.447 Epoch 6 Batch 834/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.948, Loss: 1.378 Epoch 6 Batch 835/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.949, Loss: 1.423 Epoch 6 Batch 836/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.949, Loss: 1.497 Epoch 6 Batch 837/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.945, Loss: 1.434 Epoch 6 Batch 838/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.945, Loss: 1.379 Epoch 6 Batch 839/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.944, Loss: 1.426 Epoch 6 Batch 840/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.951, Loss: 1.416 Epoch 6 Batch 841/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.951, Loss: 1.367 Epoch 6 Batch 842/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.951, Loss: 1.462 Epoch 6 Batch 843/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.951, Loss: 1.479 Epoch 6 Batch 844/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.946, Loss: 1.489 Epoch 6 Batch 845/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.943, Loss: 1.452 Epoch 6 Batch 846/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.943, Loss: 1.442 Epoch 6 Batch 847/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.947, Loss: 1.489 Epoch 6 Batch 848/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.954, Loss: 1.538 Epoch 6 Batch 849/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.952, Loss: 1.536 Epoch 6 Batch 850/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.954, Loss: 1.451 Epoch 6 Batch 851/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.951, Loss: 1.393 Epoch 6 Batch 852/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.951, Loss: 1.412 Epoch 6 Batch 853/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.949, Loss: 1.474 Epoch 6 Batch 854/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.949, Loss: 1.539 Epoch 6 Batch 855/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.944, Loss: 1.516 Epoch 6 Batch 856/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.946, Loss: 1.432 Epoch 6 Batch 857/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.946, Loss: 1.393 Epoch 6 Batch 858/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.946, Loss: 1.463 Epoch 6 Batch 859/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.951, Loss: 1.422 Epoch 6 Batch 860/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.955, Loss: 1.372 Epoch 6 Batch 861/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.951, Loss: 1.444 Epoch 6 Batch 862/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.951, Loss: 1.525 Epoch 6 Batch 863/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.955, Loss: 1.419 Epoch 6 Batch 864/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.952, Loss: 1.434 Epoch 6 Batch 865/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.957, Loss: 1.401 Epoch 6 Batch 866/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.957, Loss: 1.435 Epoch 6 Batch 867/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.945, Loss: 1.465 Epoch 6 Batch 868/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.949, Loss: 1.459 Epoch 6 Batch 869/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.949, Loss: 1.404 Epoch 6 Batch 870/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.958, Loss: 1.438 Epoch 6 Batch 871/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.958, Loss: 1.492 Epoch 6 Batch 872/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.958, Loss: 1.466 Epoch 6 Batch 873/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.958, Loss: 1.436 Epoch 6 Batch 874/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.956, Loss: 1.377 Epoch 6 Batch 875/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.960, Loss: 1.407 Epoch 6 Batch 876/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.956, Loss: 1.458 Epoch 6 Batch 877/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.956, Loss: 1.375 Epoch 6 Batch 878/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.947, Loss: 1.392 Epoch 6 Batch 879/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.943, Loss: 1.425 Epoch 6 Batch 880/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.446 Epoch 6 Batch 881/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.447 Epoch 6 Batch 882/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.959, Loss: 1.381 Epoch 6 Batch 883/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.954, Loss: 1.556 Epoch 6 Batch 884/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.953, Loss: 1.446 Epoch 6 Batch 885/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.948, Loss: 1.383 Epoch 6 Batch 886/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.952, Loss: 1.458 Epoch 6 Batch 887/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.953, Loss: 1.385 Epoch 6 Batch 888/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.958, Loss: 1.456 Epoch 6 Batch 889/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.956, Loss: 1.450 Epoch 6 Batch 890/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.955, Loss: 1.439 Epoch 6 Batch 891/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.419 Epoch 6 Batch 892/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.952, Loss: 1.461 Epoch 6 Batch 893/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.952, Loss: 1.399 Epoch 6 Batch 894/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.947, Loss: 1.394 Epoch 6 Batch 895/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.947, Loss: 1.428 Epoch 6 Batch 896/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.947, Loss: 1.399 Epoch 6 Batch 897/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.947, Loss: 1.467 Epoch 6 Batch 898/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.952, Loss: 1.448 Epoch 6 Batch 899/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.952, Loss: 1.394 Epoch 6 Batch 900/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.952, Loss: 1.511 Epoch 6 Batch 901/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.952, Loss: 1.459 Epoch 6 Batch 902/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.953, Loss: 1.461 Epoch 6 Batch 903/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.953, Loss: 1.492 Epoch 6 Batch 904/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.959, Loss: 1.476 Epoch 6 Batch 905/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.960, Loss: 1.450 Epoch 6 Batch 906/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.960, Loss: 1.429 Epoch 6 Batch 907/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.425 Epoch 6 Batch 908/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.960, Loss: 1.509 Epoch 6 Batch 909/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.960, Loss: 1.454 Epoch 6 Batch 910/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.960, Loss: 1.417 Epoch 6 Batch 911/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.957, Loss: 1.438 Epoch 6 Batch 912/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.956, Loss: 1.437 Epoch 6 Batch 913/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.956, Loss: 1.489 Epoch 6 Batch 914/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.469 Epoch 6 Batch 915/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.954, Loss: 1.453 Epoch 6 Batch 916/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.965, Loss: 1.445 Epoch 6 Batch 917/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.965, Loss: 1.328 Epoch 6 Batch 918/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.965, Loss: 1.478 Epoch 6 Batch 919/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.961, Loss: 1.489 Epoch 6 Batch 920/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.956, Loss: 1.406 Epoch 6 Batch 921/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.956, Loss: 1.472 Epoch 6 Batch 922/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.961, Loss: 1.393 Epoch 6 Batch 923/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.456 Epoch 6 Batch 924/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.972, Loss: 1.368 Epoch 6 Batch 925/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.963, Loss: 1.450 Epoch 6 Batch 926/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.963, Loss: 1.490 Epoch 6 Batch 927/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.963, Loss: 1.427 Epoch 6 Batch 928/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.956, Loss: 1.511 Epoch 6 Batch 929/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.963, Loss: 1.467 Epoch 6 Batch 930/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.960, Loss: 1.495 Epoch 6 Batch 931/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.960, Loss: 1.407 Epoch 6 Batch 932/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.960, Loss: 1.407 Epoch 6 Batch 933/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.953, Loss: 1.439 Epoch 6 Batch 934/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.434 Epoch 6 Batch 935/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.960, Loss: 1.458 Epoch 6 Batch 936/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.960, Loss: 1.507 Epoch 6 Batch 937/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.962, Loss: 1.449 Epoch 6 Batch 938/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.419 Epoch 6 Batch 939/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.951, Loss: 1.429 Epoch 6 Batch 940/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.365 Epoch 6 Batch 941/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.447 Epoch 6 Batch 942/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.955, Loss: 1.440 Epoch 6 Batch 943/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.960, Loss: 1.469 Epoch 6 Batch 944/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.960, Loss: 1.432 Epoch 6 Batch 945/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.960, Loss: 1.413 Epoch 6 Batch 946/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.957, Loss: 1.484 Epoch 6 Batch 947/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.957, Loss: 1.487 Epoch 6 Batch 948/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.954, Loss: 1.519 Epoch 6 Batch 949/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.954, Loss: 1.459 Epoch 6 Batch 950/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.465 Epoch 6 Batch 951/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.961, Loss: 1.419 Epoch 6 Batch 952/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.960, Loss: 1.431 Epoch 6 Batch 953/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.960, Loss: 1.488 Epoch 6 Batch 954/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.960, Loss: 1.397 Epoch 6 Batch 955/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.391 Epoch 6 Batch 956/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.947, Loss: 1.474 Epoch 6 Batch 957/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.948, Loss: 1.359 Epoch 6 Batch 958/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.948, Loss: 1.413 Epoch 6 Batch 959/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.943, Loss: 1.335 Epoch 6 Batch 960/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.943, Loss: 1.416 Epoch 6 Batch 961/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.943, Loss: 1.410 Epoch 6 Batch 962/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.938, Loss: 1.360 Epoch 6 Batch 963/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.944, Loss: 1.370 Epoch 6 Batch 964/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.945, Loss: 1.454 Epoch 6 Batch 965/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.955, Loss: 1.465 Epoch 6 Batch 966/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.957, Loss: 1.403 Epoch 6 Batch 967/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.955, Loss: 1.457 Epoch 6 Batch 968/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.955, Loss: 1.421 Epoch 6 Batch 969/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.950, Loss: 1.500 Epoch 6 Batch 970/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.946, Loss: 1.468 Epoch 6 Batch 971/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.946, Loss: 1.475 Epoch 6 Batch 972/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.946, Loss: 1.379 Epoch 6 Batch 973/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.951, Loss: 1.432 Epoch 6 Batch 974/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.957, Loss: 1.427 Epoch 6 Batch 975/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.963, Loss: 1.419 Epoch 6 Batch 976/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.953, Loss: 1.504 Epoch 6 Batch 977/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.953, Loss: 1.468 Epoch 6 Batch 978/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.949, Loss: 1.549 Epoch 6 Batch 979/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.955, Loss: 1.402 Epoch 6 Batch 980/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.953, Loss: 1.506 Epoch 6 Batch 981/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.958, Loss: 1.453 Epoch 6 Batch 982/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.958, Loss: 1.401 Epoch 6 Batch 983/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.958, Loss: 1.411 Epoch 6 Batch 984/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.958, Loss: 1.360 Epoch 6 Batch 985/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.957, Loss: 1.399 Epoch 6 Batch 986/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.957, Loss: 1.411 Epoch 6 Batch 987/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.947, Loss: 1.517 Epoch 6 Batch 988/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.954, Loss: 1.351 Epoch 6 Batch 989/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.383 Epoch 6 Batch 990/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.948, Loss: 1.485 Epoch 6 Batch 991/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.943, Loss: 1.432 Epoch 6 Batch 992/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.943, Loss: 1.438 Epoch 6 Batch 993/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.943, Loss: 1.493 Epoch 6 Batch 994/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.948, Loss: 1.471 Epoch 6 Batch 995/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.949, Loss: 1.481 Epoch 6 Batch 996/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.949, Loss: 1.373 Epoch 6 Batch 997/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.949, Loss: 1.388 Epoch 6 Batch 998/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.939, Loss: 1.393 Epoch 6 Batch 999/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.939, Loss: 1.376 Epoch 6 Batch 1000/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.939, Loss: 1.507 Epoch 6 Batch 1001/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.934, Loss: 1.407 Epoch 6 Batch 1002/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.939, Loss: 1.489 Epoch 6 Batch 1003/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.936, Loss: 1.430 Epoch 6 Batch 1004/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.942, Loss: 1.438 Epoch 6 Batch 1005/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.943, Loss: 1.462 Epoch 6 Batch 1006/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.943, Loss: 1.414 Epoch 6 Batch 1007/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.943, Loss: 1.485 Epoch 6 Batch 1008/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.943, Loss: 1.371 Epoch 6 Batch 1009/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.943, Loss: 1.441 Epoch 6 Batch 1010/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.943, Loss: 1.398 Epoch 6 Batch 1011/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.947, Loss: 1.467 Epoch 6 Batch 1012/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.943, Loss: 1.448 Epoch 6 Batch 1013/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.938, Loss: 1.403 Epoch 6 Batch 1014/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.937, Loss: 1.475 Epoch 6 Batch 1015/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.935, Loss: 1.448 Epoch 6 Batch 1016/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.942, Loss: 1.406 Epoch 6 Batch 1017/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.942, Loss: 1.419 Epoch 6 Batch 1018/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.941, Loss: 1.466 Epoch 6 Batch 1019/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.940, Loss: 1.403 Epoch 6 Batch 1020/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.940, Loss: 1.353 Epoch 6 Batch 1021/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.942, Loss: 1.445 Epoch 6 Batch 1022/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.947, Loss: 1.401 Epoch 6 Batch 1023/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.951, Loss: 1.400 Epoch 6 Batch 1024/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.949, Loss: 1.429 Epoch 6 Batch 1025/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.944, Loss: 1.417 Epoch 6 Batch 1026/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.944, Loss: 1.402 Epoch 6 Batch 1027/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.954, Loss: 1.442 Epoch 6 Batch 1028/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.965, Loss: 1.460 Epoch 6 Batch 1029/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.969, Loss: 1.398 Epoch 6 Batch 1030/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.964, Loss: 1.488 Epoch 6 Batch 1031/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.964, Loss: 1.420 Epoch 6 Batch 1032/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.964, Loss: 1.449 Epoch 6 Batch 1033/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.964, Loss: 1.417 Epoch 6 Batch 1034/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.964, Loss: 1.428 Epoch 6 Batch 1035/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.383 Epoch 6 Batch 1036/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.395 Epoch 6 Batch 1037/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.946, Loss: 1.469 Epoch 6 Batch 1038/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.947, Loss: 1.419 Epoch 6 Batch 1039/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.947, Loss: 1.394 Epoch 6 Batch 1040/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.947, Loss: 1.471 Epoch 6 Batch 1041/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.947, Loss: 1.517 Epoch 6 Batch 1042/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.483 Epoch 6 Batch 1043/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.952, Loss: 1.491 Epoch 6 Batch 1044/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.444 Epoch 6 Batch 1045/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.347 Epoch 6 Batch 1046/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.961, Loss: 1.462 Epoch 6 Batch 1047/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.959, Loss: 1.413 Epoch 6 Batch 1048/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.963, Loss: 1.458 Epoch 6 Batch 1049/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.961, Loss: 1.428 Epoch 6 Batch 1050/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.964, Loss: 1.445 Epoch 6 Batch 1051/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.964, Loss: 1.509 Epoch 6 Batch 1052/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.966, Loss: 1.402 Epoch 6 Batch 1053/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.966, Loss: 1.361 Epoch 6 Batch 1054/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.963, Loss: 1.344 Epoch 6 Batch 1055/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.352 Epoch 6 Batch 1056/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.958, Loss: 1.367 Epoch 6 Batch 1057/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.952, Loss: 1.399 Epoch 6 Batch 1058/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.475 Epoch 6 Batch 1059/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.961, Loss: 1.455 Epoch 6 Batch 1060/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.958, Loss: 1.381 Epoch 6 Batch 1061/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.953, Loss: 1.413 Epoch 6 Batch 1062/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.949, Loss: 1.415 Epoch 6 Batch 1063/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.949, Loss: 1.417 Epoch 6 Batch 1064/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.953, Loss: 1.491 Epoch 6 Batch 1065/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.949, Loss: 1.399 Epoch 6 Batch 1066/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.948, Loss: 1.413 Epoch 6 Batch 1067/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.938, Loss: 1.500 Epoch 6 Batch 1068/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.938, Loss: 1.433 Epoch 6 Batch 1069/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.928, Loss: 1.422 Epoch 6 Batch 1070/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.928, Loss: 1.535 Epoch 6 Batch 1071/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.932, Loss: 1.509 Epoch 6 Batch 1072/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.932, Loss: 1.399 Epoch 6 Batch 1073/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.932, Loss: 1.411 Epoch 6 Batch 1074/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.934, Loss: 1.413 Epoch 6 Batch 1075/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.931, Loss: 1.457 Epoch 7 Batch 0/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.932, Loss: 1.532 Epoch 7 Batch 1/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.936, Loss: 1.480 Epoch 7 Batch 2/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.936, Loss: 1.396 Epoch 7 Batch 3/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.936, Loss: 1.452 Epoch 7 Batch 4/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.936, Loss: 1.371 Epoch 7 Batch 5/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.937, Loss: 1.467 Epoch 7 Batch 6/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.937, Loss: 1.523 Epoch 7 Batch 7/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.953, Loss: 1.448 Epoch 7 Batch 8/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.958, Loss: 1.391 Epoch 7 Batch 9/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.956, Loss: 1.322 Epoch 7 Batch 10/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.956, Loss: 1.409 Epoch 7 Batch 11/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.958, Loss: 1.435 Epoch 7 Batch 12/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.956, Loss: 1.402 Epoch 7 Batch 13/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.958, Loss: 1.518 Epoch 7 Batch 14/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.960, Loss: 1.370 Epoch 7 Batch 15/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.960, Loss: 1.472 Epoch 7 Batch 16/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.955, Loss: 1.378 Epoch 7 Batch 17/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.393 Epoch 7 Batch 18/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.437 Epoch 7 Batch 19/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.430 Epoch 7 Batch 20/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.960, Loss: 1.371 Epoch 7 Batch 21/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.962, Loss: 1.437 Epoch 7 Batch 22/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.958, Loss: 1.454 Epoch 7 Batch 23/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.370 Epoch 7 Batch 24/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.396 Epoch 7 Batch 25/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.950, Loss: 1.415 Epoch 7 Batch 26/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.509 Epoch 7 Batch 27/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.369 Epoch 7 Batch 28/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.949, Loss: 1.387 Epoch 7 Batch 29/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.947, Loss: 1.434 Epoch 7 Batch 30/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.947, Loss: 1.417 Epoch 7 Batch 31/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.953, Loss: 1.468 Epoch 7 Batch 32/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.949, Loss: 1.460 Epoch 7 Batch 33/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.951, Loss: 1.483 Epoch 7 Batch 34/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.388 Epoch 7 Batch 35/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.420 Epoch 7 Batch 36/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.950, Loss: 1.408 Epoch 7 Batch 37/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.939, Loss: 1.465 Epoch 7 Batch 38/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.937, Loss: 1.401 Epoch 7 Batch 39/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.937, Loss: 1.448 Epoch 7 Batch 40/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.928, Loss: 1.394 Epoch 7 Batch 41/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.932, Loss: 1.417 Epoch 7 Batch 42/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.927, Loss: 1.476 Epoch 7 Batch 43/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.927, Loss: 1.450 Epoch 7 Batch 44/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.929, Loss: 1.393 Epoch 7 Batch 45/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.929, Loss: 1.415 Epoch 7 Batch 46/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.938, Loss: 1.485 Epoch 7 Batch 47/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.947, Loss: 1.366 Epoch 7 Batch 48/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.951, Loss: 1.450 Epoch 7 Batch 49/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.953, Loss: 1.417 Epoch 7 Batch 50/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.949, Loss: 1.470 Epoch 7 Batch 51/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.950, Loss: 1.364 Epoch 7 Batch 52/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.950, Loss: 1.484 Epoch 7 Batch 53/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.950, Loss: 1.451 Epoch 7 Batch 54/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.955, Loss: 1.445 Epoch 7 Batch 55/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.420 Epoch 7 Batch 56/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.954, Loss: 1.417 Epoch 7 Batch 57/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.954, Loss: 1.471 Epoch 7 Batch 58/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.949, Loss: 1.407 Epoch 7 Batch 59/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.941, Loss: 1.466 Epoch 7 Batch 60/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.941, Loss: 1.494 Epoch 7 Batch 61/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.941, Loss: 1.395 Epoch 7 Batch 62/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.937, Loss: 1.451 Epoch 7 Batch 63/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.940, Loss: 1.476 Epoch 7 Batch 64/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.940, Loss: 1.445 Epoch 7 Batch 65/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.940, Loss: 1.448 Epoch 7 Batch 66/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.940, Loss: 1.498 Epoch 7 Batch 67/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.940, Loss: 1.403 Epoch 7 Batch 68/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.935, Loss: 1.510 Epoch 7 Batch 69/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.947, Loss: 1.481 Epoch 7 Batch 70/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.942, Loss: 1.437 Epoch 7 Batch 71/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.946, Loss: 1.486 Epoch 7 Batch 72/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.951, Loss: 1.492 Epoch 7 Batch 73/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.946, Loss: 1.468 Epoch 7 Batch 74/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.946, Loss: 1.507 Epoch 7 Batch 75/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.951, Loss: 1.436 Epoch 7 Batch 76/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.951, Loss: 1.405 Epoch 7 Batch 77/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.951, Loss: 1.394 Epoch 7 Batch 78/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.460 Epoch 7 Batch 79/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.943, Loss: 1.506 Epoch 7 Batch 80/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.939, Loss: 1.404 Epoch 7 Batch 81/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.946, Loss: 1.378 Epoch 7 Batch 82/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.941, Loss: 1.434 Epoch 7 Batch 83/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.946, Loss: 1.494 Epoch 7 Batch 84/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.946, Loss: 1.374 Epoch 7 Batch 85/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.941, Loss: 1.397 Epoch 7 Batch 86/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.940, Loss: 1.503 Epoch 7 Batch 87/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.940, Loss: 1.462 Epoch 7 Batch 88/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.950, Loss: 1.463 Epoch 7 Batch 89/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.451 Epoch 7 Batch 90/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.952, Loss: 1.393 Epoch 7 Batch 91/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.952, Loss: 1.438 Epoch 7 Batch 92/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.952, Loss: 1.414 Epoch 7 Batch 93/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.341 Epoch 7 Batch 94/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.952, Loss: 1.378 Epoch 7 Batch 95/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.964, Loss: 1.441 Epoch 7 Batch 96/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.959, Loss: 1.408 Epoch 7 Batch 97/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.954, Loss: 1.390 Epoch 7 Batch 98/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.959, Loss: 1.432 Epoch 7 Batch 99/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.959, Loss: 1.430 Epoch 7 Batch 100/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.954, Loss: 1.435 Epoch 7 Batch 101/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.948, Loss: 1.457 Epoch 7 Batch 102/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.944, Loss: 1.394 Epoch 7 Batch 103/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.941, Loss: 1.432 Epoch 7 Batch 104/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.941, Loss: 1.385 Epoch 7 Batch 105/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.941, Loss: 1.441 Epoch 7 Batch 106/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.941, Loss: 1.477 Epoch 7 Batch 107/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.934, Loss: 1.471 Epoch 7 Batch 108/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.935, Loss: 1.466 Epoch 7 Batch 109/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.940, Loss: 1.436 Epoch 7 Batch 110/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.944, Loss: 1.433 Epoch 7 Batch 111/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.945, Loss: 1.513 Epoch 7 Batch 112/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.953, Loss: 1.455 Epoch 7 Batch 113/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.949, Loss: 1.458 Epoch 7 Batch 114/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.452 Epoch 7 Batch 115/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.950, Loss: 1.424 Epoch 7 Batch 116/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.954, Loss: 1.537 Epoch 7 Batch 117/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.954, Loss: 1.424 Epoch 7 Batch 118/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.954, Loss: 1.423 Epoch 7 Batch 119/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.954, Loss: 1.436 Epoch 7 Batch 120/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.954, Loss: 1.351 Epoch 7 Batch 121/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.954, Loss: 1.473 Epoch 7 Batch 122/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.954, Loss: 1.455 Epoch 7 Batch 123/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.954, Loss: 1.408 Epoch 7 Batch 124/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.956, Loss: 1.441 Epoch 7 Batch 125/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.951, Loss: 1.435 Epoch 7 Batch 126/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.949, Loss: 1.363 Epoch 7 Batch 127/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.949, Loss: 1.436 Epoch 7 Batch 128/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.949, Loss: 1.327 Epoch 7 Batch 129/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.944, Loss: 1.432 Epoch 7 Batch 130/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.944, Loss: 1.431 Epoch 7 Batch 131/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.942, Loss: 1.356 Epoch 7 Batch 132/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.944, Loss: 1.405 Epoch 7 Batch 133/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.939, Loss: 1.373 Epoch 7 Batch 134/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.935, Loss: 1.443 Epoch 7 Batch 135/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.934, Loss: 1.456 Epoch 7 Batch 136/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.930, Loss: 1.424 Epoch 7 Batch 137/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.930, Loss: 1.385 Epoch 7 Batch 138/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.930, Loss: 1.462 Epoch 7 Batch 139/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.930, Loss: 1.419 Epoch 7 Batch 140/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.934, Loss: 1.418 Epoch 7 Batch 141/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.929, Loss: 1.384 Epoch 7 Batch 142/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.937, Loss: 1.436 Epoch 7 Batch 143/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.932, Loss: 1.476 Epoch 7 Batch 144/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.936, Loss: 1.493 Epoch 7 Batch 145/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.936, Loss: 1.378 Epoch 7 Batch 146/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.936, Loss: 1.426 Epoch 7 Batch 147/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.936, Loss: 1.437 Epoch 7 Batch 148/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.938, Loss: 1.463 Epoch 7 Batch 149/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.939, Loss: 1.380 Epoch 7 Batch 150/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.938, Loss: 1.376 Epoch 7 Batch 151/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.934, Loss: 1.442 Epoch 7 Batch 152/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.934, Loss: 1.435 Epoch 7 Batch 153/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.929, Loss: 1.457 Epoch 7 Batch 154/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.933, Loss: 1.443 Epoch 7 Batch 155/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.938, Loss: 1.411 Epoch 7 Batch 156/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.938, Loss: 1.430 Epoch 7 Batch 157/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.938, Loss: 1.437 Epoch 7 Batch 158/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.942, Loss: 1.418 Epoch 7 Batch 159/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.945, Loss: 1.516 Epoch 7 Batch 160/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.939, Loss: 1.418 Epoch 7 Batch 161/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.937, Loss: 1.449 Epoch 7 Batch 162/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.948, Loss: 1.542 Epoch 7 Batch 163/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.948, Loss: 1.501 Epoch 7 Batch 164/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.948, Loss: 1.438 Epoch 7 Batch 165/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.948, Loss: 1.439 Epoch 7 Batch 166/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.950, Loss: 1.428 Epoch 7 Batch 167/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.402 Epoch 7 Batch 168/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.950, Loss: 1.398 Epoch 7 Batch 169/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.949, Loss: 1.440 Epoch 7 Batch 170/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.953, Loss: 1.443 Epoch 7 Batch 171/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.953, Loss: 1.430 Epoch 7 Batch 172/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.942, Loss: 1.467 Epoch 7 Batch 173/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.937, Loss: 1.444 Epoch 7 Batch 174/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.939, Loss: 1.408 Epoch 7 Batch 175/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.941, Loss: 1.402 Epoch 7 Batch 176/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.941, Loss: 1.438 Epoch 7 Batch 177/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.941, Loss: 1.472 Epoch 7 Batch 178/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.941, Loss: 1.462 Epoch 7 Batch 179/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.941, Loss: 1.394 Epoch 7 Batch 180/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.941, Loss: 1.436 Epoch 7 Batch 181/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.941, Loss: 1.391 Epoch 7 Batch 182/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.951, Loss: 1.445 Epoch 7 Batch 183/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.424 Epoch 7 Batch 184/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.485 Epoch 7 Batch 185/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.945, Loss: 1.467 Epoch 7 Batch 186/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.945, Loss: 1.396 Epoch 7 Batch 187/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.944, Loss: 1.444 Epoch 7 Batch 188/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.944, Loss: 1.380 Epoch 7 Batch 189/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.944, Loss: 1.495 Epoch 7 Batch 190/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.944, Loss: 1.426 Epoch 7 Batch 191/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.950, Loss: 1.380 Epoch 7 Batch 192/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.950, Loss: 1.520 Epoch 7 Batch 193/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.945, Loss: 1.420 Epoch 7 Batch 194/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.945, Loss: 1.437 Epoch 7 Batch 195/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.948, Loss: 1.407 Epoch 7 Batch 196/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.948, Loss: 1.391 Epoch 7 Batch 197/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.950, Loss: 1.407 Epoch 7 Batch 198/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.949, Loss: 1.484 Epoch 7 Batch 199/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.383 Epoch 7 Batch 200/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.947, Loss: 1.470 Epoch 7 Batch 201/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.942, Loss: 1.467 Epoch 7 Batch 202/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.942, Loss: 1.384 Epoch 7 Batch 203/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.938, Loss: 1.420 Epoch 7 Batch 204/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.938, Loss: 1.362 Epoch 7 Batch 205/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.938, Loss: 1.407 Epoch 7 Batch 206/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.934, Loss: 1.379 Epoch 7 Batch 207/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.934, Loss: 1.403 Epoch 7 Batch 208/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.942, Loss: 1.485 Epoch 7 Batch 209/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.947, Loss: 1.457 Epoch 7 Batch 210/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.949, Loss: 1.466 Epoch 7 Batch 211/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.950, Loss: 1.379 Epoch 7 Batch 212/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.400 Epoch 7 Batch 213/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.955, Loss: 1.413 Epoch 7 Batch 214/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.941, Loss: 1.417 Epoch 7 Batch 215/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.938, Loss: 1.502 Epoch 7 Batch 216/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.942, Loss: 1.382 Epoch 7 Batch 217/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.940, Loss: 1.351 Epoch 7 Batch 218/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.940, Loss: 1.417 Epoch 7 Batch 219/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.945, Loss: 1.497 Epoch 7 Batch 220/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.945, Loss: 1.484 Epoch 7 Batch 221/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.945, Loss: 1.503 Epoch 7 Batch 222/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.950, Loss: 1.308 Epoch 7 Batch 223/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.950, Loss: 1.475 Epoch 7 Batch 224/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.952, Loss: 1.425 Epoch 7 Batch 225/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.949, Loss: 1.544 Epoch 7 Batch 226/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.953, Loss: 1.397 Epoch 7 Batch 227/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.953, Loss: 1.494 Epoch 7 Batch 228/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.459 Epoch 7 Batch 229/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.465 Epoch 7 Batch 230/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.957, Loss: 1.345 Epoch 7 Batch 231/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.957, Loss: 1.406 Epoch 7 Batch 232/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.957, Loss: 1.408 Epoch 7 Batch 233/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.952, Loss: 1.528 Epoch 7 Batch 234/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.951, Loss: 1.503 Epoch 7 Batch 235/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.952, Loss: 1.472 Epoch 7 Batch 236/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.951, Loss: 1.393 Epoch 7 Batch 237/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.951, Loss: 1.462 Epoch 7 Batch 238/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.951, Loss: 1.440 Epoch 7 Batch 239/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.947, Loss: 1.403 Epoch 7 Batch 240/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.943, Loss: 1.398 Epoch 7 Batch 241/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.948, Loss: 1.427 Epoch 7 Batch 242/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.958, Loss: 1.466 Epoch 7 Batch 243/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.958, Loss: 1.400 Epoch 7 Batch 244/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.956, Loss: 1.463 Epoch 7 Batch 245/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.963, Loss: 1.518 Epoch 7 Batch 246/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.960, Loss: 1.489 Epoch 7 Batch 247/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.955, Loss: 1.399 Epoch 7 Batch 248/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.950, Loss: 1.362 Epoch 7 Batch 249/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.945, Loss: 1.437 Epoch 7 Batch 250/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.371 Epoch 7 Batch 251/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.945, Loss: 1.426 Epoch 7 Batch 252/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.945, Loss: 1.413 Epoch 7 Batch 253/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.946, Loss: 1.416 Epoch 7 Batch 254/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.439 Epoch 7 Batch 255/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.952, Loss: 1.452 Epoch 7 Batch 256/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.533 Epoch 7 Batch 257/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.950, Loss: 1.418 Epoch 7 Batch 258/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.369 Epoch 7 Batch 259/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.422 Epoch 7 Batch 260/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.960, Loss: 1.348 Epoch 7 Batch 261/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.961, Loss: 1.396 Epoch 7 Batch 262/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.391 Epoch 7 Batch 263/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.956, Loss: 1.398 Epoch 7 Batch 264/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.956, Loss: 1.450 Epoch 7 Batch 265/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.516 Epoch 7 Batch 266/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.402 Epoch 7 Batch 267/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.955, Loss: 1.425 Epoch 7 Batch 268/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.955, Loss: 1.415 Epoch 7 Batch 269/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.529 Epoch 7 Batch 270/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.963, Loss: 1.425 Epoch 7 Batch 271/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.957, Loss: 1.386 Epoch 7 Batch 272/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.578 Epoch 7 Batch 273/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.957, Loss: 1.494 Epoch 7 Batch 274/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.955, Loss: 1.353 Epoch 7 Batch 275/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.413 Epoch 7 Batch 276/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.944, Loss: 1.496 Epoch 7 Batch 277/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.940, Loss: 1.428 Epoch 7 Batch 278/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.940, Loss: 1.447 Epoch 7 Batch 279/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.943, Loss: 1.390 Epoch 7 Batch 280/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.943, Loss: 1.407 Epoch 7 Batch 281/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.943, Loss: 1.519 Epoch 7 Batch 282/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.948, Loss: 1.450 Epoch 7 Batch 283/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.948, Loss: 1.540 Epoch 7 Batch 284/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.948, Loss: 1.452 Epoch 7 Batch 285/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.948, Loss: 1.432 Epoch 7 Batch 286/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.948, Loss: 1.399 Epoch 7 Batch 287/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.943, Loss: 1.378 Epoch 7 Batch 288/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.944, Loss: 1.480 Epoch 7 Batch 289/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.950, Loss: 1.425 Epoch 7 Batch 290/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.952, Loss: 1.457 Epoch 7 Batch 291/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.947, Loss: 1.376 Epoch 7 Batch 292/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.944, Loss: 1.395 Epoch 7 Batch 293/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.950, Loss: 1.407 Epoch 7 Batch 294/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.955, Loss: 1.467 Epoch 7 Batch 295/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.350 Epoch 7 Batch 296/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.955, Loss: 1.457 Epoch 7 Batch 297/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.950, Loss: 1.379 Epoch 7 Batch 298/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.949, Loss: 1.459 Epoch 7 Batch 299/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.949, Loss: 1.407 Epoch 7 Batch 300/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.949, Loss: 1.410 Epoch 7 Batch 301/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.949, Loss: 1.396 Epoch 7 Batch 302/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.949, Loss: 1.472 Epoch 7 Batch 303/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.944, Loss: 1.383 Epoch 7 Batch 304/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.939, Loss: 1.407 Epoch 7 Batch 305/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.950, Loss: 1.428 Epoch 7 Batch 306/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.946, Loss: 1.450 Epoch 7 Batch 307/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.945, Loss: 1.348 Epoch 7 Batch 308/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.940, Loss: 1.490 Epoch 7 Batch 309/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.940, Loss: 1.400 Epoch 7 Batch 310/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.949, Loss: 1.476 Epoch 7 Batch 311/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.409 Epoch 7 Batch 312/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.950, Loss: 1.513 Epoch 7 Batch 313/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.955, Loss: 1.451 Epoch 7 Batch 314/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.955, Loss: 1.411 Epoch 7 Batch 315/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.404 Epoch 7 Batch 316/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.955, Loss: 1.433 Epoch 7 Batch 317/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.482 Epoch 7 Batch 318/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.958, Loss: 1.496 Epoch 7 Batch 319/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.958, Loss: 1.441 Epoch 7 Batch 320/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.953, Loss: 1.392 Epoch 7 Batch 321/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.953, Loss: 1.399 Epoch 7 Batch 322/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.958, Loss: 1.483 Epoch 7 Batch 323/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.388 Epoch 7 Batch 324/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.961, Loss: 1.402 Epoch 7 Batch 325/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.956, Loss: 1.447 Epoch 7 Batch 326/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.957, Loss: 1.431 Epoch 7 Batch 327/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.966, Loss: 1.467 Epoch 7 Batch 328/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.437 Epoch 7 Batch 329/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.960, Loss: 1.418 Epoch 7 Batch 330/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.956, Loss: 1.461 Epoch 7 Batch 331/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.946, Loss: 1.394 Epoch 7 Batch 332/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.946, Loss: 1.399 Epoch 7 Batch 333/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.946, Loss: 1.419 Epoch 7 Batch 334/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.951, Loss: 1.429 Epoch 7 Batch 335/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.951, Loss: 1.434 Epoch 7 Batch 336/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.951, Loss: 1.460 Epoch 7 Batch 337/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.952, Loss: 1.387 Epoch 7 Batch 338/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.957, Loss: 1.449 Epoch 7 Batch 339/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.950, Loss: 1.391 Epoch 7 Batch 340/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.481 Epoch 7 Batch 341/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.955, Loss: 1.445 Epoch 7 Batch 342/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.955, Loss: 1.408 Epoch 7 Batch 343/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.955, Loss: 1.399 Epoch 7 Batch 344/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.950, Loss: 1.371 Epoch 7 Batch 345/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.952, Loss: 1.501 Epoch 7 Batch 346/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.952, Loss: 1.365 Epoch 7 Batch 347/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.952, Loss: 1.388 Epoch 7 Batch 348/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.946, Loss: 1.437 Epoch 7 Batch 349/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.946, Loss: 1.398 Epoch 7 Batch 350/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.946, Loss: 1.425 Epoch 7 Batch 351/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.951, Loss: 1.460 Epoch 7 Batch 352/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.940, Loss: 1.360 Epoch 7 Batch 353/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.935, Loss: 1.414 Epoch 7 Batch 354/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.935, Loss: 1.494 Epoch 7 Batch 355/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.941, Loss: 1.444 Epoch 7 Batch 356/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.943, Loss: 1.423 Epoch 7 Batch 357/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.943, Loss: 1.436 Epoch 7 Batch 358/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.948, Loss: 1.490 Epoch 7 Batch 359/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.948, Loss: 1.315 Epoch 7 Batch 360/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.947, Loss: 1.409 Epoch 7 Batch 361/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.442 Epoch 7 Batch 362/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.952, Loss: 1.480 Epoch 7 Batch 363/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.955, Loss: 1.482 Epoch 7 Batch 364/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.954, Loss: 1.444 Epoch 7 Batch 365/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.958, Loss: 1.476 Epoch 7 Batch 366/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.301 Epoch 7 Batch 367/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.410 Epoch 7 Batch 368/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.959, Loss: 1.435 Epoch 7 Batch 369/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.543 Epoch 7 Batch 370/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.435 Epoch 7 Batch 371/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.953, Loss: 1.359 Epoch 7 Batch 372/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.953, Loss: 1.404 Epoch 7 Batch 373/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.953, Loss: 1.395 Epoch 7 Batch 374/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.953, Loss: 1.482 Epoch 7 Batch 375/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.464 Epoch 7 Batch 376/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.952, Loss: 1.440 Epoch 7 Batch 377/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.947, Loss: 1.325 Epoch 7 Batch 378/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.947, Loss: 1.386 Epoch 7 Batch 379/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.952, Loss: 1.452 Epoch 7 Batch 380/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.425 Epoch 7 Batch 381/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.495 Epoch 7 Batch 382/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.952, Loss: 1.381 Epoch 7 Batch 383/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.954, Loss: 1.424 Epoch 7 Batch 384/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.945, Loss: 1.443 Epoch 7 Batch 385/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.939, Loss: 1.399 Epoch 7 Batch 386/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.940, Loss: 1.399 Epoch 7 Batch 387/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.939, Loss: 1.384 Epoch 7 Batch 388/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.945, Loss: 1.414 Epoch 7 Batch 389/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.946, Loss: 1.416 Epoch 7 Batch 390/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.951, Loss: 1.463 Epoch 7 Batch 391/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.944, Loss: 1.484 Epoch 7 Batch 392/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.944, Loss: 1.433 Epoch 7 Batch 393/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.944, Loss: 1.459 Epoch 7 Batch 394/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.941, Loss: 1.430 Epoch 7 Batch 395/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.941, Loss: 1.390 Epoch 7 Batch 396/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.937, Loss: 1.438 Epoch 7 Batch 397/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.936, Loss: 1.455 Epoch 7 Batch 398/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.946, Loss: 1.468 Epoch 7 Batch 399/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.951, Loss: 1.443 Epoch 7 Batch 400/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.951, Loss: 1.417 Epoch 7 Batch 401/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.960, Loss: 1.383 Epoch 7 Batch 402/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.960, Loss: 1.391 Epoch 7 Batch 403/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.960, Loss: 1.489 Epoch 7 Batch 404/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.951, Loss: 1.387 Epoch 7 Batch 405/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.492 Epoch 7 Batch 406/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.416 Epoch 7 Batch 407/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.955, Loss: 1.478 Epoch 7 Batch 408/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.949, Loss: 1.487 Epoch 7 Batch 409/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.949, Loss: 1.452 Epoch 7 Batch 410/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.956, Loss: 1.420 Epoch 7 Batch 411/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.958, Loss: 1.524 Epoch 7 Batch 412/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.958, Loss: 1.381 Epoch 7 Batch 413/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.958, Loss: 1.387 Epoch 7 Batch 414/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.958, Loss: 1.491 Epoch 7 Batch 415/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.518 Epoch 7 Batch 416/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.950, Loss: 1.438 Epoch 7 Batch 417/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.460 Epoch 7 Batch 418/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.950, Loss: 1.418 Epoch 7 Batch 419/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.950, Loss: 1.457 Epoch 7 Batch 420/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.944, Loss: 1.487 Epoch 7 Batch 421/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.944, Loss: 1.496 Epoch 7 Batch 422/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.945, Loss: 1.413 Epoch 7 Batch 423/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.935, Loss: 1.460 Epoch 7 Batch 424/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.935, Loss: 1.425 Epoch 7 Batch 425/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.940, Loss: 1.416 Epoch 7 Batch 426/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.939, Loss: 1.462 Epoch 7 Batch 427/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.941, Loss: 1.421 Epoch 7 Batch 428/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.951, Loss: 1.467 Epoch 7 Batch 429/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.951, Loss: 1.452 Epoch 7 Batch 430/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.946, Loss: 1.411 Epoch 7 Batch 431/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.950, Loss: 1.440 Epoch 7 Batch 432/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.950, Loss: 1.435 Epoch 7 Batch 433/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.423 Epoch 7 Batch 434/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.956, Loss: 1.392 Epoch 7 Batch 435/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.405 Epoch 7 Batch 436/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.955, Loss: 1.392 Epoch 7 Batch 437/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.954, Loss: 1.414 Epoch 7 Batch 438/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.955, Loss: 1.492 Epoch 7 Batch 439/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.490 Epoch 7 Batch 440/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.950, Loss: 1.396 Epoch 7 Batch 441/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.950, Loss: 1.465 Epoch 7 Batch 442/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.950, Loss: 1.463 Epoch 7 Batch 443/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.945, Loss: 1.465 Epoch 7 Batch 444/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.950, Loss: 1.400 Epoch 7 Batch 445/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.950, Loss: 1.440 Epoch 7 Batch 446/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.950, Loss: 1.405 Epoch 7 Batch 447/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.946, Loss: 1.423 Epoch 7 Batch 448/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.946, Loss: 1.545 Epoch 7 Batch 449/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.946, Loss: 1.433 Epoch 7 Batch 450/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.947, Loss: 1.395 Epoch 7 Batch 451/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.951, Loss: 1.382 Epoch 7 Batch 452/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.487 Epoch 7 Batch 453/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.962, Loss: 1.502 Epoch 7 Batch 454/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.962, Loss: 1.356 Epoch 7 Batch 455/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.966, Loss: 1.514 Epoch 7 Batch 456/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.966, Loss: 1.353 Epoch 7 Batch 457/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.966, Loss: 1.475 Epoch 7 Batch 458/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.966, Loss: 1.413 Epoch 7 Batch 459/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.971, Loss: 1.421 Epoch 7 Batch 460/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.967, Loss: 1.392 Epoch 7 Batch 461/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.965, Loss: 1.501 Epoch 7 Batch 462/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.965, Loss: 1.440 Epoch 7 Batch 463/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.965, Loss: 1.382 Epoch 7 Batch 464/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.965, Loss: 1.512 Epoch 7 Batch 465/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.960, Loss: 1.441 Epoch 7 Batch 466/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.960, Loss: 1.391 Epoch 7 Batch 467/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.955, Loss: 1.418 Epoch 7 Batch 468/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.457 Epoch 7 Batch 469/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.396 Epoch 7 Batch 470/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.956, Loss: 1.449 Epoch 7 Batch 471/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.956, Loss: 1.427 Epoch 7 Batch 472/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.956, Loss: 1.510 Epoch 7 Batch 473/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.950, Loss: 1.406 Epoch 7 Batch 474/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.955, Loss: 1.409 Epoch 7 Batch 475/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.955, Loss: 1.507 Epoch 7 Batch 476/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.956, Loss: 1.446 Epoch 7 Batch 477/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.956, Loss: 1.433 Epoch 7 Batch 478/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.449 Epoch 7 Batch 479/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.413 Epoch 7 Batch 480/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.952, Loss: 1.405 Epoch 7 Batch 481/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.428 Epoch 7 Batch 482/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.951, Loss: 1.444 Epoch 7 Batch 483/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.956, Loss: 1.421 Epoch 7 Batch 484/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.425 Epoch 7 Batch 485/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.392 Epoch 7 Batch 486/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.390 Epoch 7 Batch 487/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.951, Loss: 1.446 Epoch 7 Batch 488/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.951, Loss: 1.438 Epoch 7 Batch 489/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.951, Loss: 1.521 Epoch 7 Batch 490/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.951, Loss: 1.466 Epoch 7 Batch 491/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.947, Loss: 1.459 Epoch 7 Batch 492/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.945, Loss: 1.426 Epoch 7 Batch 493/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.945, Loss: 1.447 Epoch 7 Batch 494/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.945, Loss: 1.388 Epoch 7 Batch 495/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.944, Loss: 1.491 Epoch 7 Batch 496/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.950, Loss: 1.417 Epoch 7 Batch 497/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.950, Loss: 1.531 Epoch 7 Batch 498/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.945, Loss: 1.472 Epoch 7 Batch 499/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.945, Loss: 1.429 Epoch 7 Batch 500/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.945, Loss: 1.402 Epoch 7 Batch 501/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.952, Loss: 1.440 Epoch 7 Batch 502/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.952, Loss: 1.485 Epoch 7 Batch 503/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.946, Loss: 1.418 Epoch 7 Batch 504/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.947, Loss: 1.437 Epoch 7 Batch 505/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.952, Loss: 1.446 Epoch 7 Batch 506/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.961, Loss: 1.402 Epoch 7 Batch 507/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.955, Loss: 1.471 Epoch 7 Batch 508/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.449 Epoch 7 Batch 509/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.955, Loss: 1.428 Epoch 7 Batch 510/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.431 Epoch 7 Batch 511/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.405 Epoch 7 Batch 512/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.955, Loss: 1.344 Epoch 7 Batch 513/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.438 Epoch 7 Batch 514/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.953, Loss: 1.462 Epoch 7 Batch 515/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.953, Loss: 1.407 Epoch 7 Batch 516/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.426 Epoch 7 Batch 517/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.953, Loss: 1.458 Epoch 7 Batch 518/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.952, Loss: 1.447 Epoch 7 Batch 519/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.958, Loss: 1.434 Epoch 7 Batch 520/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.958, Loss: 1.447 Epoch 7 Batch 521/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.958, Loss: 1.429 Epoch 7 Batch 522/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.958, Loss: 1.437 Epoch 7 Batch 523/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.960, Loss: 1.461 Epoch 7 Batch 524/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.960, Loss: 1.463 Epoch 7 Batch 525/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.961, Loss: 1.441 Epoch 7 Batch 526/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.461 Epoch 7 Batch 527/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.965, Loss: 1.475 Epoch 7 Batch 528/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.966, Loss: 1.443 Epoch 7 Batch 529/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.971, Loss: 1.433 Epoch 7 Batch 530/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.971, Loss: 1.423 Epoch 7 Batch 531/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.964, Loss: 1.449 Epoch 7 Batch 532/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.968, Loss: 1.360 Epoch 7 Batch 533/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.967, Loss: 1.352 Epoch 7 Batch 534/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.967, Loss: 1.399 Epoch 7 Batch 535/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.967, Loss: 1.373 Epoch 7 Batch 536/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.967, Loss: 1.380 Epoch 7 Batch 537/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.971, Loss: 1.369 Epoch 7 Batch 538/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.972, Loss: 1.405 Epoch 7 Batch 539/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.972, Loss: 1.466 Epoch 7 Batch 540/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.977, Loss: 1.428 Epoch 7 Batch 541/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.977, Loss: 1.464 Epoch 7 Batch 542/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.966, Loss: 1.412 Epoch 7 Batch 543/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.971, Loss: 1.403 Epoch 7 Batch 544/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.965, Loss: 1.382 Epoch 7 Batch 545/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.965, Loss: 1.419 Epoch 7 Batch 546/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.965, Loss: 1.399 Epoch 7 Batch 547/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.960, Loss: 1.442 Epoch 7 Batch 548/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.966, Loss: 1.544 Epoch 7 Batch 549/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.966, Loss: 1.501 Epoch 7 Batch 550/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.966, Loss: 1.381 Epoch 7 Batch 551/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.967, Loss: 1.457 Epoch 7 Batch 552/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.965, Loss: 1.413 Epoch 7 Batch 553/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.966, Loss: 1.364 Epoch 7 Batch 554/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.966, Loss: 1.430 Epoch 7 Batch 555/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.966, Loss: 1.430 Epoch 7 Batch 556/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.971, Loss: 1.399 Epoch 7 Batch 557/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.966, Loss: 1.435 Epoch 7 Batch 558/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.966, Loss: 1.459 Epoch 7 Batch 559/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.966, Loss: 1.517 Epoch 7 Batch 560/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.966, Loss: 1.460 Epoch 7 Batch 561/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.966, Loss: 1.418 Epoch 7 Batch 562/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.966, Loss: 1.494 Epoch 7 Batch 563/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.961, Loss: 1.487 Epoch 7 Batch 564/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.961, Loss: 1.380 Epoch 7 Batch 565/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.971, Loss: 1.498 Epoch 7 Batch 566/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.971, Loss: 1.443 Epoch 7 Batch 567/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.967, Loss: 1.425 Epoch 7 Batch 568/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.967, Loss: 1.421 Epoch 7 Batch 569/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.960, Loss: 1.435 Epoch 7 Batch 570/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.960, Loss: 1.490 Epoch 7 Batch 571/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.440 Epoch 7 Batch 572/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.960, Loss: 1.419 Epoch 7 Batch 573/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.961, Loss: 1.448 Epoch 7 Batch 574/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.432 Epoch 7 Batch 575/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.951, Loss: 1.429 Epoch 7 Batch 576/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.415 Epoch 7 Batch 577/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.458 Epoch 7 Batch 578/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.472 Epoch 7 Batch 579/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.954, Loss: 1.398 Epoch 7 Batch 580/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.954, Loss: 1.376 Epoch 7 Batch 581/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.371 Epoch 7 Batch 582/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.448 Epoch 7 Batch 583/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.957, Loss: 1.421 Epoch 7 Batch 584/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.957, Loss: 1.431 Epoch 7 Batch 585/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.955, Loss: 1.372 Epoch 7 Batch 586/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.960, Loss: 1.449 Epoch 7 Batch 587/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.960, Loss: 1.401 Epoch 7 Batch 588/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.960, Loss: 1.443 Epoch 7 Batch 589/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.966, Loss: 1.514 Epoch 7 Batch 590/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.966, Loss: 1.388 Epoch 7 Batch 591/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.966, Loss: 1.451 Epoch 7 Batch 592/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.960, Loss: 1.454 Epoch 7 Batch 593/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.967, Loss: 1.444 Epoch 7 Batch 594/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.962, Loss: 1.449 Epoch 7 Batch 595/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.960, Loss: 1.372 Epoch 7 Batch 596/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.964, Loss: 1.403 Epoch 7 Batch 597/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.962, Loss: 1.357 Epoch 7 Batch 598/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.964, Loss: 1.427 Epoch 7 Batch 599/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.964, Loss: 1.408 Epoch 7 Batch 600/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.969, Loss: 1.403 Epoch 7 Batch 601/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.969, Loss: 1.425 Epoch 7 Batch 602/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.968, Loss: 1.464 Epoch 7 Batch 603/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.973, Loss: 1.357 Epoch 7 Batch 604/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.968, Loss: 1.499 Epoch 7 Batch 605/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.423 Epoch 7 Batch 606/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.963, Loss: 1.464 Epoch 7 Batch 607/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.965, Loss: 1.449 Epoch 7 Batch 608/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.963, Loss: 1.366 Epoch 7 Batch 609/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.963, Loss: 1.447 Epoch 7 Batch 610/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.962, Loss: 1.432 Epoch 7 Batch 611/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.966, Loss: 1.398 Epoch 7 Batch 612/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.957, Loss: 1.456 Epoch 7 Batch 613/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.952, Loss: 1.444 Epoch 7 Batch 614/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.947, Loss: 1.385 Epoch 7 Batch 615/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.952, Loss: 1.455 Epoch 7 Batch 616/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.957, Loss: 1.444 Epoch 7 Batch 617/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.957, Loss: 1.371 Epoch 7 Batch 618/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.959, Loss: 1.472 Epoch 7 Batch 619/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.959, Loss: 1.481 Epoch 7 Batch 620/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.452 Epoch 7 Batch 621/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.960, Loss: 1.436 Epoch 7 Batch 622/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.964, Loss: 1.450 Epoch 7 Batch 623/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.964, Loss: 1.433 Epoch 7 Batch 624/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.964, Loss: 1.494 Epoch 7 Batch 625/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.963, Loss: 1.389 Epoch 7 Batch 626/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.962, Loss: 1.481 Epoch 7 Batch 627/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.958, Loss: 1.439 Epoch 7 Batch 628/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.958, Loss: 1.400 Epoch 7 Batch 629/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.958, Loss: 1.399 Epoch 7 Batch 630/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.370 Epoch 7 Batch 631/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.958, Loss: 1.403 Epoch 7 Batch 632/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.962, Loss: 1.403 Epoch 7 Batch 633/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.962, Loss: 1.486 Epoch 7 Batch 634/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.962, Loss: 1.360 Epoch 7 Batch 635/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.962, Loss: 1.423 Epoch 7 Batch 636/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.962, Loss: 1.303 Epoch 7 Batch 637/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.459 Epoch 7 Batch 638/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.957, Loss: 1.429 Epoch 7 Batch 639/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.477 Epoch 7 Batch 640/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.962, Loss: 1.416 Epoch 7 Batch 641/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.962, Loss: 1.471 Epoch 7 Batch 642/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.962, Loss: 1.451 Epoch 7 Batch 643/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.963, Loss: 1.409 Epoch 7 Batch 644/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.963, Loss: 1.507 Epoch 7 Batch 645/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.969, Loss: 1.404 Epoch 7 Batch 646/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.969, Loss: 1.367 Epoch 7 Batch 647/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.963, Loss: 1.397 Epoch 7 Batch 648/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.963, Loss: 1.492 Epoch 7 Batch 649/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.961, Loss: 1.459 Epoch 7 Batch 650/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.955, Loss: 1.459 Epoch 7 Batch 651/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.359 Epoch 7 Batch 652/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.953, Loss: 1.372 Epoch 7 Batch 653/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.953, Loss: 1.401 Epoch 7 Batch 654/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.949, Loss: 1.431 Epoch 7 Batch 655/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.953, Loss: 1.385 Epoch 7 Batch 656/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.944, Loss: 1.497 Epoch 7 Batch 657/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.944, Loss: 1.400 Epoch 7 Batch 658/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.463 Epoch 7 Batch 659/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.957, Loss: 1.391 Epoch 7 Batch 660/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.947, Loss: 1.460 Epoch 7 Batch 661/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.942, Loss: 1.397 Epoch 7 Batch 662/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.944, Loss: 1.439 Epoch 7 Batch 663/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.949, Loss: 1.393 Epoch 7 Batch 664/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.944, Loss: 1.440 Epoch 7 Batch 665/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.944, Loss: 1.465 Epoch 7 Batch 666/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.948, Loss: 1.468 Epoch 7 Batch 667/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.949, Loss: 1.453 Epoch 7 Batch 668/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.492 Epoch 7 Batch 669/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.966, Loss: 1.374 Epoch 7 Batch 670/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.961, Loss: 1.426 Epoch 7 Batch 671/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.966, Loss: 1.509 Epoch 7 Batch 672/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.407 Epoch 7 Batch 673/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.964, Loss: 1.484 Epoch 7 Batch 674/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.964, Loss: 1.470 Epoch 7 Batch 675/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.969, Loss: 1.429 Epoch 7 Batch 676/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.973, Loss: 1.442 Epoch 7 Batch 677/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.973, Loss: 1.465 Epoch 7 Batch 678/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.973, Loss: 1.493 Epoch 7 Batch 679/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.974, Loss: 1.391 Epoch 7 Batch 680/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.979, Loss: 1.456 Epoch 7 Batch 681/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.974, Loss: 1.403 Epoch 7 Batch 682/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.969, Loss: 1.376 Epoch 7 Batch 683/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.974, Loss: 1.393 Epoch 7 Batch 684/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.972, Loss: 1.428 Epoch 7 Batch 685/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.492 Epoch 7 Batch 686/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.947, Loss: 1.462 Epoch 7 Batch 687/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.396 Epoch 7 Batch 688/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.420 Epoch 7 Batch 689/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.423 Epoch 7 Batch 690/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.952, Loss: 1.525 Epoch 7 Batch 691/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.420 Epoch 7 Batch 692/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.952, Loss: 1.411 Epoch 7 Batch 693/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.469 Epoch 7 Batch 694/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.466 Epoch 7 Batch 695/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.963, Loss: 1.422 Epoch 7 Batch 696/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.968, Loss: 1.448 Epoch 7 Batch 697/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.968, Loss: 1.468 Epoch 7 Batch 698/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.417 Epoch 7 Batch 699/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.958, Loss: 1.378 Epoch 7 Batch 700/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.957, Loss: 1.439 Epoch 7 Batch 701/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.417 Epoch 7 Batch 702/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.952, Loss: 1.437 Epoch 7 Batch 703/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.954, Loss: 1.446 Epoch 7 Batch 704/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.949, Loss: 1.415 Epoch 7 Batch 705/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.957, Loss: 1.405 Epoch 7 Batch 706/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.955, Loss: 1.421 Epoch 7 Batch 707/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.961, Loss: 1.381 Epoch 7 Batch 708/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.961, Loss: 1.542 Epoch 7 Batch 709/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.961, Loss: 1.357 Epoch 7 Batch 710/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.961, Loss: 1.481 Epoch 7 Batch 711/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.961, Loss: 1.456 Epoch 7 Batch 712/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.961, Loss: 1.395 Epoch 7 Batch 713/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.971, Loss: 1.421 Epoch 7 Batch 714/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.966, Loss: 1.408 Epoch 7 Batch 715/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.966, Loss: 1.410 Epoch 7 Batch 716/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.468 Epoch 7 Batch 717/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.961, Loss: 1.437 Epoch 7 Batch 718/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.967, Loss: 1.363 Epoch 7 Batch 719/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.972, Loss: 1.394 Epoch 7 Batch 720/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.972, Loss: 1.397 Epoch 7 Batch 721/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.972, Loss: 1.430 Epoch 7 Batch 722/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.971, Loss: 1.410 Epoch 7 Batch 723/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.970, Loss: 1.455 Epoch 7 Batch 724/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.972, Loss: 1.412 Epoch 7 Batch 725/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.972, Loss: 1.499 Epoch 7 Batch 726/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.977, Loss: 1.382 Epoch 7 Batch 727/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.977, Loss: 1.391 Epoch 7 Batch 728/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.972, Loss: 1.385 Epoch 7 Batch 729/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.965, Loss: 1.378 Epoch 7 Batch 730/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.965, Loss: 1.436 Epoch 7 Batch 731/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.961, Loss: 1.474 Epoch 7 Batch 732/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.961, Loss: 1.502 Epoch 7 Batch 733/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.961, Loss: 1.442 Epoch 7 Batch 734/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.955, Loss: 1.426 Epoch 7 Batch 735/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.955, Loss: 1.410 Epoch 7 Batch 736/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.953, Loss: 1.397 Epoch 7 Batch 737/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.953, Loss: 1.399 Epoch 7 Batch 738/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.953, Loss: 1.403 Epoch 7 Batch 739/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.953, Loss: 1.441 Epoch 7 Batch 740/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.953, Loss: 1.413 Epoch 7 Batch 741/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.949, Loss: 1.397 Epoch 7 Batch 742/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.462 Epoch 7 Batch 743/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.949, Loss: 1.344 Epoch 7 Batch 744/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.949, Loss: 1.487 Epoch 7 Batch 745/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.947, Loss: 1.453 Epoch 7 Batch 746/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.947, Loss: 1.380 Epoch 7 Batch 747/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.951, Loss: 1.460 Epoch 7 Batch 748/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.951, Loss: 1.432 Epoch 7 Batch 749/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.956, Loss: 1.451 Epoch 7 Batch 750/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.956, Loss: 1.420 Epoch 7 Batch 751/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.523 Epoch 7 Batch 752/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.958, Loss: 1.402 Epoch 7 Batch 753/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.969, Loss: 1.374 Epoch 7 Batch 754/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.970, Loss: 1.443 Epoch 7 Batch 755/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.970, Loss: 1.396 Epoch 7 Batch 756/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.970, Loss: 1.485 Epoch 7 Batch 757/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.974, Loss: 1.465 Epoch 7 Batch 758/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.974, Loss: 1.440 Epoch 7 Batch 759/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.975, Loss: 1.499 Epoch 7 Batch 760/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.971, Loss: 1.444 Epoch 7 Batch 761/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.971, Loss: 1.456 Epoch 7 Batch 762/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.975, Loss: 1.432 Epoch 7 Batch 763/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.975, Loss: 1.374 Epoch 7 Batch 764/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.975, Loss: 1.436 Epoch 7 Batch 765/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.975, Loss: 1.444 Epoch 7 Batch 766/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.973, Loss: 1.433 Epoch 7 Batch 767/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.975, Loss: 1.429 Epoch 7 Batch 768/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.971, Loss: 1.375 Epoch 7 Batch 769/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.966, Loss: 1.438 Epoch 7 Batch 770/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.968, Loss: 1.378 Epoch 7 Batch 771/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.968, Loss: 1.446 Epoch 7 Batch 772/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.452 Epoch 7 Batch 773/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.960, Loss: 1.506 Epoch 7 Batch 774/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.360 Epoch 7 Batch 775/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.451 Epoch 7 Batch 776/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.955, Loss: 1.356 Epoch 7 Batch 777/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.456 Epoch 7 Batch 778/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.956, Loss: 1.396 Epoch 7 Batch 779/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.953, Loss: 1.448 Epoch 7 Batch 780/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.948, Loss: 1.429 Epoch 7 Batch 781/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.948, Loss: 1.424 Epoch 7 Batch 782/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.956, Loss: 1.461 Epoch 7 Batch 783/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.948, Loss: 1.469 Epoch 7 Batch 784/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.948, Loss: 1.385 Epoch 7 Batch 785/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.948, Loss: 1.407 Epoch 7 Batch 786/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.948, Loss: 1.379 Epoch 7 Batch 787/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.395 Epoch 7 Batch 788/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.952, Loss: 1.402 Epoch 7 Batch 789/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.952, Loss: 1.371 Epoch 7 Batch 790/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.958, Loss: 1.495 Epoch 7 Batch 791/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.959, Loss: 1.492 Epoch 7 Batch 792/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.955, Loss: 1.476 Epoch 7 Batch 793/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.962, Loss: 1.484 Epoch 7 Batch 794/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.956, Loss: 1.489 Epoch 7 Batch 795/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.372 Epoch 7 Batch 796/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.955, Loss: 1.446 Epoch 7 Batch 797/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.485 Epoch 7 Batch 798/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.960, Loss: 1.462 Epoch 7 Batch 799/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.950, Loss: 1.467 Epoch 7 Batch 800/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.404 Epoch 7 Batch 801/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.458 Epoch 7 Batch 802/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.955, Loss: 1.374 Epoch 7 Batch 803/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.951, Loss: 1.437 Epoch 7 Batch 804/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.397 Epoch 7 Batch 805/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.952, Loss: 1.422 Epoch 7 Batch 806/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.456 Epoch 7 Batch 807/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.962, Loss: 1.422 Epoch 7 Batch 808/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.962, Loss: 1.405 Epoch 7 Batch 809/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.436 Epoch 7 Batch 810/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.959, Loss: 1.387 Epoch 7 Batch 811/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.959, Loss: 1.519 Epoch 7 Batch 812/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.959, Loss: 1.352 Epoch 7 Batch 813/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.959, Loss: 1.456 Epoch 7 Batch 814/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.434 Epoch 7 Batch 815/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.355 Epoch 7 Batch 816/1077 - Train Accuracy: 0.992, Validation Accuracy: 0.967, Loss: 1.483 Epoch 7 Batch 817/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.967, Loss: 1.409 Epoch 7 Batch 818/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.967, Loss: 1.466 Epoch 7 Batch 819/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.962, Loss: 1.360 Epoch 7 Batch 820/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.962, Loss: 1.409 Epoch 7 Batch 821/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.963, Loss: 1.461 Epoch 7 Batch 822/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.962, Loss: 1.469 Epoch 7 Batch 823/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.962, Loss: 1.415 Epoch 7 Batch 824/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.967, Loss: 1.422 Epoch 7 Batch 825/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.955, Loss: 1.371 Epoch 7 Batch 826/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.957, Loss: 1.454 Epoch 7 Batch 827/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.957, Loss: 1.435 Epoch 7 Batch 828/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.957, Loss: 1.445 Epoch 7 Batch 829/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.957, Loss: 1.438 Epoch 7 Batch 830/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.961, Loss: 1.367 Epoch 7 Batch 831/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.957, Loss: 1.438 Epoch 7 Batch 832/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.954, Loss: 1.432 Epoch 7 Batch 833/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.959, Loss: 1.440 Epoch 7 Batch 834/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.959, Loss: 1.550 Epoch 7 Batch 835/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.959, Loss: 1.386 Epoch 7 Batch 836/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.386 Epoch 7 Batch 837/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.968, Loss: 1.402 Epoch 7 Batch 838/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.968, Loss: 1.441 Epoch 7 Batch 839/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.973, Loss: 1.392 Epoch 7 Batch 840/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.972, Loss: 1.479 Epoch 7 Batch 841/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.960, Loss: 1.402 Epoch 7 Batch 842/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.958, Loss: 1.400 Epoch 7 Batch 843/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.478 Epoch 7 Batch 844/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.965, Loss: 1.439 Epoch 7 Batch 845/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.967, Loss: 1.417 Epoch 7 Batch 846/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.967, Loss: 1.441 Epoch 7 Batch 847/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.963, Loss: 1.434 Epoch 7 Batch 848/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.967, Loss: 1.294 Epoch 7 Batch 849/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.968, Loss: 1.425 Epoch 7 Batch 850/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.972, Loss: 1.463 Epoch 7 Batch 851/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.972, Loss: 1.433 Epoch 7 Batch 852/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.972, Loss: 1.452 Epoch 7 Batch 853/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.964, Loss: 1.460 Epoch 7 Batch 854/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.965, Loss: 1.450 Epoch 7 Batch 855/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.965, Loss: 1.397 Epoch 7 Batch 856/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.958, Loss: 1.513 Epoch 7 Batch 857/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.968, Loss: 1.382 Epoch 7 Batch 858/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.968, Loss: 1.450 Epoch 7 Batch 859/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.499 Epoch 7 Batch 860/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.970, Loss: 1.485 Epoch 7 Batch 861/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.967, Loss: 1.511 Epoch 7 Batch 862/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.967, Loss: 1.385 Epoch 7 Batch 863/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.972, Loss: 1.467 Epoch 7 Batch 864/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.972, Loss: 1.510 Epoch 7 Batch 865/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.975, Loss: 1.437 Epoch 7 Batch 866/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.975, Loss: 1.423 Epoch 7 Batch 867/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.970, Loss: 1.460 Epoch 7 Batch 868/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.974, Loss: 1.417 Epoch 7 Batch 869/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.975, Loss: 1.341 Epoch 7 Batch 870/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.980, Loss: 1.389 Epoch 7 Batch 871/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.980, Loss: 1.531 Epoch 7 Batch 872/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.980, Loss: 1.472 Epoch 7 Batch 873/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.971, Loss: 1.347 Epoch 7 Batch 874/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.971, Loss: 1.377 Epoch 7 Batch 875/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.964, Loss: 1.397 Epoch 7 Batch 876/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.965, Loss: 1.450 Epoch 7 Batch 877/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.963, Loss: 1.389 Epoch 7 Batch 878/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.962, Loss: 1.359 Epoch 7 Batch 879/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.962, Loss: 1.396 Epoch 7 Batch 880/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.964, Loss: 1.402 Epoch 7 Batch 881/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.962, Loss: 1.466 Epoch 7 Batch 882/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.960, Loss: 1.450 Epoch 7 Batch 883/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.422 Epoch 7 Batch 884/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.960, Loss: 1.458 Epoch 7 Batch 885/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.966, Loss: 1.485 Epoch 7 Batch 886/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.966, Loss: 1.408 Epoch 7 Batch 887/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.400 Epoch 7 Batch 888/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.403 Epoch 7 Batch 889/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.968, Loss: 1.376 Epoch 7 Batch 890/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.964, Loss: 1.382 Epoch 7 Batch 891/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.962, Loss: 1.420 Epoch 7 Batch 892/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.951, Loss: 1.411 Epoch 7 Batch 893/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.956, Loss: 1.493 Epoch 7 Batch 894/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.956, Loss: 1.455 Epoch 7 Batch 895/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.955, Loss: 1.449 Epoch 7 Batch 896/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.950, Loss: 1.409 Epoch 7 Batch 897/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.950, Loss: 1.471 Epoch 7 Batch 898/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.404 Epoch 7 Batch 899/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.477 Epoch 7 Batch 900/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.960, Loss: 1.488 Epoch 7 Batch 901/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.957, Loss: 1.445 Epoch 7 Batch 902/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.957, Loss: 1.462 Epoch 7 Batch 903/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.953, Loss: 1.424 Epoch 7 Batch 904/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.422 Epoch 7 Batch 905/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.433 Epoch 7 Batch 906/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.950, Loss: 1.447 Epoch 7 Batch 907/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.463 Epoch 7 Batch 908/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.950, Loss: 1.436 Epoch 7 Batch 909/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.506 Epoch 7 Batch 910/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.955, Loss: 1.482 Epoch 7 Batch 911/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.955, Loss: 1.421 Epoch 7 Batch 912/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.955, Loss: 1.415 Epoch 7 Batch 913/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.967, Loss: 1.435 Epoch 7 Batch 914/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.972, Loss: 1.407 Epoch 7 Batch 915/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.972, Loss: 1.410 Epoch 7 Batch 916/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.972, Loss: 1.412 Epoch 7 Batch 917/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.972, Loss: 1.488 Epoch 7 Batch 918/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.972, Loss: 1.416 Epoch 7 Batch 919/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.972, Loss: 1.417 Epoch 7 Batch 920/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.972, Loss: 1.423 Epoch 7 Batch 921/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.972, Loss: 1.436 Epoch 7 Batch 922/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.972, Loss: 1.376 Epoch 7 Batch 923/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.972, Loss: 1.501 Epoch 7 Batch 924/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.973, Loss: 1.468 Epoch 7 Batch 925/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.972, Loss: 1.410 Epoch 7 Batch 926/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.972, Loss: 1.397 Epoch 7 Batch 927/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.977, Loss: 1.474 Epoch 7 Batch 928/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.977, Loss: 1.521 Epoch 7 Batch 929/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.977, Loss: 1.450 Epoch 7 Batch 930/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.977, Loss: 1.376 Epoch 7 Batch 931/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.977, Loss: 1.391 Epoch 7 Batch 932/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.977, Loss: 1.507 Epoch 7 Batch 933/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.979, Loss: 1.497 Epoch 7 Batch 934/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.979, Loss: 1.454 Epoch 7 Batch 935/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.984, Loss: 1.404 Epoch 7 Batch 936/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.984, Loss: 1.415 Epoch 7 Batch 937/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.979, Loss: 1.421 Epoch 7 Batch 938/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.979, Loss: 1.394 Epoch 7 Batch 939/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.974, Loss: 1.451 Epoch 7 Batch 940/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.965, Loss: 1.469 Epoch 7 Batch 941/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.965, Loss: 1.490 Epoch 7 Batch 942/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.965, Loss: 1.493 Epoch 7 Batch 943/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.970, Loss: 1.362 Epoch 7 Batch 944/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.970, Loss: 1.450 Epoch 7 Batch 945/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.974, Loss: 1.421 Epoch 7 Batch 946/1077 - Train Accuracy: 0.994, Validation Accuracy: 0.974, Loss: 1.418 Epoch 7 Batch 947/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.974, Loss: 1.370 Epoch 7 Batch 948/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.969, Loss: 1.441 Epoch 7 Batch 949/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.970, Loss: 1.405 Epoch 7 Batch 950/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.980, Loss: 1.423 Epoch 7 Batch 951/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.980, Loss: 1.337 Epoch 7 Batch 952/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.980, Loss: 1.422 Epoch 7 Batch 953/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.977, Loss: 1.487 Epoch 7 Batch 954/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.977, Loss: 1.483 Epoch 7 Batch 955/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.977, Loss: 1.485 Epoch 7 Batch 956/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.977, Loss: 1.437 Epoch 7 Batch 957/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.977, Loss: 1.394 Epoch 7 Batch 958/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.972, Loss: 1.455 Epoch 7 Batch 959/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.410 Epoch 7 Batch 960/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.969, Loss: 1.375 Epoch 7 Batch 961/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.969, Loss: 1.428 Epoch 7 Batch 962/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.429 Epoch 7 Batch 963/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.430 Epoch 7 Batch 964/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.955, Loss: 1.395 Epoch 7 Batch 965/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.955, Loss: 1.445 Epoch 7 Batch 966/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.365 Epoch 7 Batch 967/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.961, Loss: 1.435 Epoch 7 Batch 968/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.961, Loss: 1.415 Epoch 7 Batch 969/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.961, Loss: 1.338 Epoch 7 Batch 970/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.434 Epoch 7 Batch 971/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.966, Loss: 1.367 Epoch 7 Batch 972/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.965, Loss: 1.406 Epoch 7 Batch 973/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.965, Loss: 1.426 Epoch 7 Batch 974/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.970, Loss: 1.413 Epoch 7 Batch 975/1077 - Train Accuracy: 0.992, Validation Accuracy: 0.965, Loss: 1.439 Epoch 7 Batch 976/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.966, Loss: 1.420 Epoch 7 Batch 977/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.966, Loss: 1.428 Epoch 7 Batch 978/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.966, Loss: 1.422 Epoch 7 Batch 979/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.966, Loss: 1.433 Epoch 7 Batch 980/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.968, Loss: 1.457 Epoch 7 Batch 981/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.968, Loss: 1.417 Epoch 7 Batch 982/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.968, Loss: 1.483 Epoch 7 Batch 983/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.968, Loss: 1.455 Epoch 7 Batch 984/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.971, Loss: 1.377 Epoch 7 Batch 985/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.967, Loss: 1.441 Epoch 7 Batch 986/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.967, Loss: 1.452 Epoch 7 Batch 987/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.967, Loss: 1.505 Epoch 7 Batch 988/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.962, Loss: 1.443 Epoch 7 Batch 989/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.962, Loss: 1.430 Epoch 7 Batch 990/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.962, Loss: 1.377 Epoch 7 Batch 991/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.962, Loss: 1.380 Epoch 7 Batch 992/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.957, Loss: 1.425 Epoch 7 Batch 993/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.455 Epoch 7 Batch 994/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.966, Loss: 1.445 Epoch 7 Batch 995/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.969, Loss: 1.425 Epoch 7 Batch 996/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.969, Loss: 1.402 Epoch 7 Batch 997/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.969, Loss: 1.459 Epoch 7 Batch 998/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.969, Loss: 1.406 Epoch 7 Batch 999/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.969, Loss: 1.403 Epoch 7 Batch 1000/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.973, Loss: 1.418 Epoch 7 Batch 1001/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.973, Loss: 1.419 Epoch 7 Batch 1002/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.973, Loss: 1.371 Epoch 7 Batch 1003/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.973, Loss: 1.490 Epoch 7 Batch 1004/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.973, Loss: 1.409 Epoch 7 Batch 1005/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.973, Loss: 1.413 Epoch 7 Batch 1006/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.978, Loss: 1.356 Epoch 7 Batch 1007/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.974, Loss: 1.444 Epoch 7 Batch 1008/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.974, Loss: 1.488 Epoch 7 Batch 1009/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.974, Loss: 1.408 Epoch 7 Batch 1010/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.374 Epoch 7 Batch 1011/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.976, Loss: 1.398 Epoch 7 Batch 1012/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.976, Loss: 1.440 Epoch 7 Batch 1013/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.978, Loss: 1.441 Epoch 7 Batch 1014/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.973, Loss: 1.369 Epoch 7 Batch 1015/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.973, Loss: 1.435 Epoch 7 Batch 1016/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.973, Loss: 1.483 Epoch 7 Batch 1017/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.973, Loss: 1.444 Epoch 7 Batch 1018/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.441 Epoch 7 Batch 1019/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.484 Epoch 7 Batch 1020/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.968, Loss: 1.425 Epoch 7 Batch 1021/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.443 Epoch 7 Batch 1022/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.964, Loss: 1.432 Epoch 7 Batch 1023/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.964, Loss: 1.485 Epoch 7 Batch 1024/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.968, Loss: 1.465 Epoch 7 Batch 1025/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.968, Loss: 1.344 Epoch 7 Batch 1026/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.969, Loss: 1.554 Epoch 7 Batch 1027/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.969, Loss: 1.418 Epoch 7 Batch 1028/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.971, Loss: 1.411 Epoch 7 Batch 1029/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.971, Loss: 1.474 Epoch 7 Batch 1030/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.971, Loss: 1.357 Epoch 7 Batch 1031/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.971, Loss: 1.369 Epoch 7 Batch 1032/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.971, Loss: 1.454 Epoch 7 Batch 1033/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.971, Loss: 1.441 Epoch 7 Batch 1034/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.966, Loss: 1.435 Epoch 7 Batch 1035/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.967, Loss: 1.413 Epoch 7 Batch 1036/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.965, Loss: 1.445 Epoch 7 Batch 1037/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.961, Loss: 1.413 Epoch 7 Batch 1038/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.961, Loss: 1.461 Epoch 7 Batch 1039/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.956, Loss: 1.454 Epoch 7 Batch 1040/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.433 Epoch 7 Batch 1041/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.963, Loss: 1.478 Epoch 7 Batch 1042/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.426 Epoch 7 Batch 1043/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.972, Loss: 1.414 Epoch 7 Batch 1044/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.972, Loss: 1.388 Epoch 7 Batch 1045/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.972, Loss: 1.462 Epoch 7 Batch 1046/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.967, Loss: 1.411 Epoch 7 Batch 1047/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.967, Loss: 1.394 Epoch 7 Batch 1048/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.967, Loss: 1.381 Epoch 7 Batch 1049/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.967, Loss: 1.388 Epoch 7 Batch 1050/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.962, Loss: 1.397 Epoch 7 Batch 1051/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.962, Loss: 1.432 Epoch 7 Batch 1052/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.965, Loss: 1.335 Epoch 7 Batch 1053/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.965, Loss: 1.530 Epoch 7 Batch 1054/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.965, Loss: 1.464 Epoch 7 Batch 1055/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.964, Loss: 1.340 Epoch 7 Batch 1056/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.964, Loss: 1.507 Epoch 7 Batch 1057/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.964, Loss: 1.447 Epoch 7 Batch 1058/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.964, Loss: 1.421 Epoch 7 Batch 1059/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.969, Loss: 1.463 Epoch 7 Batch 1060/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.969, Loss: 1.417 Epoch 7 Batch 1061/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.965, Loss: 1.471 Epoch 7 Batch 1062/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.964, Loss: 1.437 Epoch 7 Batch 1063/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.962, Loss: 1.401 Epoch 7 Batch 1064/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.962, Loss: 1.372 Epoch 7 Batch 1065/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.957, Loss: 1.294 Epoch 7 Batch 1066/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.957, Loss: 1.341 Epoch 7 Batch 1067/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.957, Loss: 1.432 Epoch 7 Batch 1068/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.399 Epoch 7 Batch 1069/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.950, Loss: 1.393 Epoch 7 Batch 1070/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.460 Epoch 7 Batch 1071/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.960, Loss: 1.411 Epoch 7 Batch 1072/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.960, Loss: 1.388 Epoch 7 Batch 1073/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.960, Loss: 1.428 Epoch 7 Batch 1074/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.960, Loss: 1.428 Epoch 7 Batch 1075/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.965, Loss: 1.361 Epoch 8 Batch 0/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.970, Loss: 1.486 Epoch 8 Batch 1/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.968, Loss: 1.411 Epoch 8 Batch 2/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.977, Loss: 1.392 Epoch 8 Batch 3/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.972, Loss: 1.435 Epoch 8 Batch 4/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.409 Epoch 8 Batch 5/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.967, Loss: 1.413 Epoch 8 Batch 6/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.967, Loss: 1.363 Epoch 8 Batch 7/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.967, Loss: 1.424 Epoch 8 Batch 8/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.967, Loss: 1.435 Epoch 8 Batch 9/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.972, Loss: 1.421 Epoch 8 Batch 10/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.969, Loss: 1.392 Epoch 8 Batch 11/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.964, Loss: 1.450 Epoch 8 Batch 12/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.443 Epoch 8 Batch 13/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.967, Loss: 1.458 Epoch 8 Batch 14/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.967, Loss: 1.478 Epoch 8 Batch 15/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.967, Loss: 1.363 Epoch 8 Batch 16/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.967, Loss: 1.412 Epoch 8 Batch 17/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.967, Loss: 1.454 Epoch 8 Batch 18/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.972, Loss: 1.427 Epoch 8 Batch 19/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.977, Loss: 1.443 Epoch 8 Batch 20/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.972, Loss: 1.433 Epoch 8 Batch 21/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.972, Loss: 1.432 Epoch 8 Batch 22/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.972, Loss: 1.397 Epoch 8 Batch 23/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.461 Epoch 8 Batch 24/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.374 Epoch 8 Batch 25/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.963, Loss: 1.348 Epoch 8 Batch 26/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.963, Loss: 1.385 Epoch 8 Batch 27/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.419 Epoch 8 Batch 28/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.968, Loss: 1.474 Epoch 8 Batch 29/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.966, Loss: 1.444 Epoch 8 Batch 30/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.966, Loss: 1.403 Epoch 8 Batch 31/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.966, Loss: 1.432 Epoch 8 Batch 32/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.959, Loss: 1.438 Epoch 8 Batch 33/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.367 Epoch 8 Batch 34/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.954, Loss: 1.416 Epoch 8 Batch 35/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.949, Loss: 1.447 Epoch 8 Batch 36/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.949, Loss: 1.403 Epoch 8 Batch 37/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.954, Loss: 1.349 Epoch 8 Batch 38/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.954, Loss: 1.472 Epoch 8 Batch 39/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.954, Loss: 1.406 Epoch 8 Batch 40/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.960, Loss: 1.450 Epoch 8 Batch 41/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.373 Epoch 8 Batch 42/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.461 Epoch 8 Batch 43/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.959, Loss: 1.411 Epoch 8 Batch 44/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.959, Loss: 1.416 Epoch 8 Batch 45/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.446 Epoch 8 Batch 46/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.963, Loss: 1.440 Epoch 8 Batch 47/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.958, Loss: 1.408 Epoch 8 Batch 48/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.954, Loss: 1.499 Epoch 8 Batch 49/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.952, Loss: 1.466 Epoch 8 Batch 50/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.956, Loss: 1.396 Epoch 8 Batch 51/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.412 Epoch 8 Batch 52/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.954, Loss: 1.433 Epoch 8 Batch 53/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.954, Loss: 1.431 Epoch 8 Batch 54/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.954, Loss: 1.530 Epoch 8 Batch 55/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.383 Epoch 8 Batch 56/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.947, Loss: 1.449 Epoch 8 Batch 57/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.947, Loss: 1.459 Epoch 8 Batch 58/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.952, Loss: 1.461 Epoch 8 Batch 59/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.952, Loss: 1.407 Epoch 8 Batch 60/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.952, Loss: 1.395 Epoch 8 Batch 61/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.952, Loss: 1.422 Epoch 8 Batch 62/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.954, Loss: 1.390 Epoch 8 Batch 63/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.966, Loss: 1.411 Epoch 8 Batch 64/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.960, Loss: 1.428 Epoch 8 Batch 65/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.960, Loss: 1.432 Epoch 8 Batch 66/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.965, Loss: 1.433 Epoch 8 Batch 67/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.964, Loss: 1.469 Epoch 8 Batch 68/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.964, Loss: 1.417 Epoch 8 Batch 69/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.964, Loss: 1.445 Epoch 8 Batch 70/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.968, Loss: 1.397 Epoch 8 Batch 71/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.968, Loss: 1.406 Epoch 8 Batch 72/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.968, Loss: 1.451 Epoch 8 Batch 73/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.968, Loss: 1.377 Epoch 8 Batch 74/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.962, Loss: 1.419 Epoch 8 Batch 75/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.962, Loss: 1.445 Epoch 8 Batch 76/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.963, Loss: 1.415 Epoch 8 Batch 77/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.959, Loss: 1.445 Epoch 8 Batch 78/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.959, Loss: 1.365 Epoch 8 Batch 79/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.959, Loss: 1.407 Epoch 8 Batch 80/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.959, Loss: 1.437 Epoch 8 Batch 81/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.959, Loss: 1.296 Epoch 8 Batch 82/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.957, Loss: 1.481 Epoch 8 Batch 83/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.957, Loss: 1.446 Epoch 8 Batch 84/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.372 Epoch 8 Batch 85/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.393 Epoch 8 Batch 86/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.955, Loss: 1.490 Epoch 8 Batch 87/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.950, Loss: 1.390 Epoch 8 Batch 88/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.956, Loss: 1.490 Epoch 8 Batch 89/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.440 Epoch 8 Batch 90/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.394 Epoch 8 Batch 91/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.956, Loss: 1.492 Epoch 8 Batch 92/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.489 Epoch 8 Batch 93/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.956, Loss: 1.493 Epoch 8 Batch 94/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.961, Loss: 1.395 Epoch 8 Batch 95/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.963, Loss: 1.401 Epoch 8 Batch 96/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.958, Loss: 1.354 Epoch 8 Batch 97/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.962, Loss: 1.423 Epoch 8 Batch 98/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.963, Loss: 1.493 Epoch 8 Batch 99/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.963, Loss: 1.410 Epoch 8 Batch 100/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.968, Loss: 1.367 Epoch 8 Batch 101/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.964, Loss: 1.406 Epoch 8 Batch 102/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.964, Loss: 1.343 Epoch 8 Batch 103/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.419 Epoch 8 Batch 104/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.958, Loss: 1.501 Epoch 8 Batch 105/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.961, Loss: 1.430 Epoch 8 Batch 106/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.478 Epoch 8 Batch 107/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.956, Loss: 1.420 Epoch 8 Batch 108/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.950, Loss: 1.436 Epoch 8 Batch 109/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.440 Epoch 8 Batch 110/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.954, Loss: 1.454 Epoch 8 Batch 111/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.473 Epoch 8 Batch 112/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.954, Loss: 1.422 Epoch 8 Batch 113/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.391 Epoch 8 Batch 114/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.947, Loss: 1.380 Epoch 8 Batch 115/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.953, Loss: 1.406 Epoch 8 Batch 116/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.958, Loss: 1.401 Epoch 8 Batch 117/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.427 Epoch 8 Batch 118/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.965, Loss: 1.408 Epoch 8 Batch 119/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.964, Loss: 1.473 Epoch 8 Batch 120/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.960, Loss: 1.546 Epoch 8 Batch 121/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.447 Epoch 8 Batch 122/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.950, Loss: 1.450 Epoch 8 Batch 123/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.953, Loss: 1.418 Epoch 8 Batch 124/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.953, Loss: 1.432 Epoch 8 Batch 125/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.952, Loss: 1.435 Epoch 8 Batch 126/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.954, Loss: 1.491 Epoch 8 Batch 127/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.959, Loss: 1.434 Epoch 8 Batch 128/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.959, Loss: 1.419 Epoch 8 Batch 129/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.959, Loss: 1.467 Epoch 8 Batch 130/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.958, Loss: 1.392 Epoch 8 Batch 131/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.958, Loss: 1.367 Epoch 8 Batch 132/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.961, Loss: 1.407 Epoch 8 Batch 133/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.961, Loss: 1.441 Epoch 8 Batch 134/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.961, Loss: 1.493 Epoch 8 Batch 135/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.959, Loss: 1.477 Epoch 8 Batch 136/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.959, Loss: 1.379 Epoch 8 Batch 137/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.961, Loss: 1.466 Epoch 8 Batch 138/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.505 Epoch 8 Batch 139/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.966, Loss: 1.421 Epoch 8 Batch 140/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.966, Loss: 1.358 Epoch 8 Batch 141/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.966, Loss: 1.473 Epoch 8 Batch 142/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.959, Loss: 1.399 Epoch 8 Batch 143/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.953, Loss: 1.476 Epoch 8 Batch 144/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.952, Loss: 1.452 Epoch 8 Batch 145/1077 - Train Accuracy: 0.992, Validation Accuracy: 0.951, Loss: 1.401 Epoch 8 Batch 146/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.946, Loss: 1.466 Epoch 8 Batch 147/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.946, Loss: 1.372 Epoch 8 Batch 148/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.956, Loss: 1.400 Epoch 8 Batch 149/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.468 Epoch 8 Batch 150/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.492 Epoch 8 Batch 151/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.957, Loss: 1.455 Epoch 8 Batch 152/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.455 Epoch 8 Batch 153/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.955, Loss: 1.473 Epoch 8 Batch 154/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.955, Loss: 1.518 Epoch 8 Batch 155/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.445 Epoch 8 Batch 156/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.376 Epoch 8 Batch 157/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.955, Loss: 1.416 Epoch 8 Batch 158/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.954, Loss: 1.415 Epoch 8 Batch 159/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.497 Epoch 8 Batch 160/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.952, Loss: 1.438 Epoch 8 Batch 161/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.953, Loss: 1.373 Epoch 8 Batch 162/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.953, Loss: 1.455 Epoch 8 Batch 163/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.954, Loss: 1.437 Epoch 8 Batch 164/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.954, Loss: 1.476 Epoch 8 Batch 165/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.405 Epoch 8 Batch 166/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.949, Loss: 1.323 Epoch 8 Batch 167/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.956, Loss: 1.419 Epoch 8 Batch 168/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.951, Loss: 1.426 Epoch 8 Batch 169/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.951, Loss: 1.434 Epoch 8 Batch 170/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.951, Loss: 1.390 Epoch 8 Batch 171/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.959, Loss: 1.496 Epoch 8 Batch 172/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.959, Loss: 1.420 Epoch 8 Batch 173/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.467 Epoch 8 Batch 174/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.418 Epoch 8 Batch 175/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.960, Loss: 1.466 Epoch 8 Batch 176/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.960, Loss: 1.329 Epoch 8 Batch 177/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.961, Loss: 1.503 Epoch 8 Batch 178/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.961, Loss: 1.450 Epoch 8 Batch 179/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.962, Loss: 1.343 Epoch 8 Batch 180/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.959, Loss: 1.377 Epoch 8 Batch 181/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.959, Loss: 1.395 Epoch 8 Batch 182/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.959, Loss: 1.403 Epoch 8 Batch 183/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.428 Epoch 8 Batch 184/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.954, Loss: 1.422 Epoch 8 Batch 185/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.462 Epoch 8 Batch 186/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.954, Loss: 1.449 Epoch 8 Batch 187/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.951, Loss: 1.362 Epoch 8 Batch 188/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.951, Loss: 1.497 Epoch 8 Batch 189/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.951, Loss: 1.392 Epoch 8 Batch 190/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.956, Loss: 1.471 Epoch 8 Batch 191/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.422 Epoch 8 Batch 192/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.957, Loss: 1.450 Epoch 8 Batch 193/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.483 Epoch 8 Batch 194/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.961, Loss: 1.421 Epoch 8 Batch 195/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.962, Loss: 1.429 Epoch 8 Batch 196/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.963, Loss: 1.439 Epoch 8 Batch 197/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.449 Epoch 8 Batch 198/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.401 Epoch 8 Batch 199/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.956, Loss: 1.470 Epoch 8 Batch 200/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.959, Loss: 1.434 Epoch 8 Batch 201/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.960, Loss: 1.411 Epoch 8 Batch 202/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.962, Loss: 1.401 Epoch 8 Batch 203/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.963, Loss: 1.387 Epoch 8 Batch 204/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.961, Loss: 1.444 Epoch 8 Batch 205/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.961, Loss: 1.382 Epoch 8 Batch 206/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.957, Loss: 1.488 Epoch 8 Batch 207/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.398 Epoch 8 Batch 208/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.957, Loss: 1.378 Epoch 8 Batch 209/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.960, Loss: 1.388 Epoch 8 Batch 210/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.442 Epoch 8 Batch 211/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.965, Loss: 1.445 Epoch 8 Batch 212/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.970, Loss: 1.421 Epoch 8 Batch 213/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.963, Loss: 1.552 Epoch 8 Batch 214/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.374 Epoch 8 Batch 215/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.963, Loss: 1.422 Epoch 8 Batch 216/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.960, Loss: 1.462 Epoch 8 Batch 217/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.958, Loss: 1.411 Epoch 8 Batch 218/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.958, Loss: 1.441 Epoch 8 Batch 219/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.955, Loss: 1.412 Epoch 8 Batch 220/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.959, Loss: 1.412 Epoch 8 Batch 221/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.492 Epoch 8 Batch 222/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.950, Loss: 1.487 Epoch 8 Batch 223/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.949, Loss: 1.494 Epoch 8 Batch 224/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.950, Loss: 1.428 Epoch 8 Batch 225/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.950, Loss: 1.411 Epoch 8 Batch 226/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.955, Loss: 1.417 Epoch 8 Batch 227/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.959, Loss: 1.427 Epoch 8 Batch 228/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.959, Loss: 1.498 Epoch 8 Batch 229/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.959, Loss: 1.486 Epoch 8 Batch 230/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.412 Epoch 8 Batch 231/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.955, Loss: 1.379 Epoch 8 Batch 232/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.955, Loss: 1.446 Epoch 8 Batch 233/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.955, Loss: 1.430 Epoch 8 Batch 234/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.955, Loss: 1.417 Epoch 8 Batch 235/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.953, Loss: 1.405 Epoch 8 Batch 236/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.953, Loss: 1.382 Epoch 8 Batch 237/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.435 Epoch 8 Batch 238/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.956, Loss: 1.311 Epoch 8 Batch 239/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.952, Loss: 1.381 Epoch 8 Batch 240/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.947, Loss: 1.461 Epoch 8 Batch 241/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.947, Loss: 1.475 Epoch 8 Batch 242/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.954, Loss: 1.393 Epoch 8 Batch 243/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.958, Loss: 1.387 Epoch 8 Batch 244/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.958, Loss: 1.451 Epoch 8 Batch 245/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.960, Loss: 1.336 Epoch 8 Batch 246/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.965, Loss: 1.390 Epoch 8 Batch 247/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.965, Loss: 1.426 Epoch 8 Batch 248/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.966, Loss: 1.516 Epoch 8 Batch 249/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.966, Loss: 1.421 Epoch 8 Batch 250/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.966, Loss: 1.383 Epoch 8 Batch 251/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.966, Loss: 1.446 Epoch 8 Batch 252/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.966, Loss: 1.422 Epoch 8 Batch 253/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.967, Loss: 1.486 Epoch 8 Batch 254/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.967, Loss: 1.479 Epoch 8 Batch 255/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.967, Loss: 1.405 Epoch 8 Batch 256/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.968, Loss: 1.411 Epoch 8 Batch 257/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.968, Loss: 1.493 Epoch 8 Batch 258/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.968, Loss: 1.473 Epoch 8 Batch 259/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.966, Loss: 1.445 Epoch 8 Batch 260/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.966, Loss: 1.406 Epoch 8 Batch 261/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.966, Loss: 1.474 Epoch 8 Batch 262/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.966, Loss: 1.383 Epoch 8 Batch 263/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.966, Loss: 1.425 Epoch 8 Batch 264/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.961, Loss: 1.423 Epoch 8 Batch 265/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.961, Loss: 1.415 Epoch 8 Batch 266/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.954, Loss: 1.411 Epoch 8 Batch 267/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.954, Loss: 1.439 Epoch 8 Batch 268/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.948, Loss: 1.472 Epoch 8 Batch 269/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.948, Loss: 1.442 Epoch 8 Batch 270/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.948, Loss: 1.414 Epoch 8 Batch 271/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.948, Loss: 1.353 Epoch 8 Batch 272/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.948, Loss: 1.519 Epoch 8 Batch 273/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.948, Loss: 1.430 Epoch 8 Batch 274/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.948, Loss: 1.439 Epoch 8 Batch 275/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.948, Loss: 1.488 Epoch 8 Batch 276/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.949, Loss: 1.463 Epoch 8 Batch 277/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.949, Loss: 1.460 Epoch 8 Batch 278/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.949, Loss: 1.420 Epoch 8 Batch 279/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.949, Loss: 1.481 Epoch 8 Batch 280/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.949, Loss: 1.420 Epoch 8 Batch 281/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.949, Loss: 1.446 Epoch 8 Batch 282/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.949, Loss: 1.504 Epoch 8 Batch 283/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.949, Loss: 1.399 Epoch 8 Batch 284/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.953, Loss: 1.402 Epoch 8 Batch 285/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.946, Loss: 1.385 Epoch 8 Batch 286/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.951, Loss: 1.397 Epoch 8 Batch 287/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.951, Loss: 1.382 Epoch 8 Batch 288/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.952, Loss: 1.427 Epoch 8 Batch 289/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.952, Loss: 1.492 Epoch 8 Batch 290/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.952, Loss: 1.401 Epoch 8 Batch 291/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.952, Loss: 1.461 Epoch 8 Batch 292/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.435 Epoch 8 Batch 293/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.384 Epoch 8 Batch 294/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.960, Loss: 1.395 Epoch 8 Batch 295/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.960, Loss: 1.550 Epoch 8 Batch 296/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.960, Loss: 1.364 Epoch 8 Batch 297/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.960, Loss: 1.432 Epoch 8 Batch 298/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.530 Epoch 8 Batch 299/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.401 Epoch 8 Batch 300/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.967, Loss: 1.511 Epoch 8 Batch 301/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.967, Loss: 1.487 Epoch 8 Batch 302/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.962, Loss: 1.368 Epoch 8 Batch 303/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.962, Loss: 1.491 Epoch 8 Batch 304/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.957, Loss: 1.452 Epoch 8 Batch 305/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.364 Epoch 8 Batch 306/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.961, Loss: 1.351 Epoch 8 Batch 307/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.957, Loss: 1.445 Epoch 8 Batch 308/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.957, Loss: 1.538 Epoch 8 Batch 309/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.952, Loss: 1.408 Epoch 8 Batch 310/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.957, Loss: 1.420 Epoch 8 Batch 311/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.957, Loss: 1.401 Epoch 8 Batch 312/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.952, Loss: 1.490 Epoch 8 Batch 313/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.952, Loss: 1.504 Epoch 8 Batch 314/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.348 Epoch 8 Batch 315/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.952, Loss: 1.374 Epoch 8 Batch 316/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.951, Loss: 1.378 Epoch 8 Batch 317/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.951, Loss: 1.492 Epoch 8 Batch 318/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.956, Loss: 1.406 Epoch 8 Batch 319/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.961, Loss: 1.439 Epoch 8 Batch 320/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.415 Epoch 8 Batch 321/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.452 Epoch 8 Batch 322/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.941, Loss: 1.405 Epoch 8 Batch 323/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.946, Loss: 1.424 Epoch 8 Batch 324/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.948, Loss: 1.422 Epoch 8 Batch 325/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.944, Loss: 1.381 Epoch 8 Batch 326/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.944, Loss: 1.432 Epoch 8 Batch 327/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.944, Loss: 1.486 Epoch 8 Batch 328/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.944, Loss: 1.434 Epoch 8 Batch 329/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.944, Loss: 1.495 Epoch 8 Batch 330/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.944, Loss: 1.464 Epoch 8 Batch 331/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.944, Loss: 1.494 Epoch 8 Batch 332/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.944, Loss: 1.345 Epoch 8 Batch 333/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.949, Loss: 1.393 Epoch 8 Batch 334/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.953, Loss: 1.373 Epoch 8 Batch 335/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.446 Epoch 8 Batch 336/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.958, Loss: 1.468 Epoch 8 Batch 337/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.457 Epoch 8 Batch 338/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.968, Loss: 1.459 Epoch 8 Batch 339/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.968, Loss: 1.460 Epoch 8 Batch 340/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.966, Loss: 1.402 Epoch 8 Batch 341/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.963, Loss: 1.422 Epoch 8 Batch 342/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.451 Epoch 8 Batch 343/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.953, Loss: 1.404 Epoch 8 Batch 344/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.953, Loss: 1.516 Epoch 8 Batch 345/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.951, Loss: 1.422 Epoch 8 Batch 346/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.947, Loss: 1.446 Epoch 8 Batch 347/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.947, Loss: 1.417 Epoch 8 Batch 348/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.946, Loss: 1.394 Epoch 8 Batch 349/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.946, Loss: 1.402 Epoch 8 Batch 350/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.949, Loss: 1.485 Epoch 8 Batch 351/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.949, Loss: 1.420 Epoch 8 Batch 352/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.944, Loss: 1.432 Epoch 8 Batch 353/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.942, Loss: 1.458 Epoch 8 Batch 354/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.938, Loss: 1.473 Epoch 8 Batch 355/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.933, Loss: 1.432 Epoch 8 Batch 356/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.939, Loss: 1.392 Epoch 8 Batch 357/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.935, Loss: 1.436 Epoch 8 Batch 358/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.940, Loss: 1.504 Epoch 8 Batch 359/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.940, Loss: 1.413 Epoch 8 Batch 360/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.940, Loss: 1.430 Epoch 8 Batch 361/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.950, Loss: 1.376 Epoch 8 Batch 362/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.416 Epoch 8 Batch 363/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.954, Loss: 1.489 Epoch 8 Batch 364/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.954, Loss: 1.435 Epoch 8 Batch 365/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.958, Loss: 1.431 Epoch 8 Batch 366/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.456 Epoch 8 Batch 367/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.958, Loss: 1.422 Epoch 8 Batch 368/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.956, Loss: 1.382 Epoch 8 Batch 369/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.956, Loss: 1.406 Epoch 8 Batch 370/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.961, Loss: 1.460 Epoch 8 Batch 371/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.956, Loss: 1.429 Epoch 8 Batch 372/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.956, Loss: 1.471 Epoch 8 Batch 373/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.951, Loss: 1.556 Epoch 8 Batch 374/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.956, Loss: 1.431 Epoch 8 Batch 375/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.956, Loss: 1.404 Epoch 8 Batch 376/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.465 Epoch 8 Batch 377/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.440 Epoch 8 Batch 378/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.390 Epoch 8 Batch 379/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.427 Epoch 8 Batch 380/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.956, Loss: 1.369 Epoch 8 Batch 381/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.956, Loss: 1.423 Epoch 8 Batch 382/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.956, Loss: 1.444 Epoch 8 Batch 383/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.435 Epoch 8 Batch 384/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.956, Loss: 1.405 Epoch 8 Batch 385/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.956, Loss: 1.415 Epoch 8 Batch 386/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.954, Loss: 1.420 Epoch 8 Batch 387/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.954, Loss: 1.485 Epoch 8 Batch 388/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.955, Loss: 1.474 Epoch 8 Batch 389/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.518 Epoch 8 Batch 390/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.958, Loss: 1.439 Epoch 8 Batch 391/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.487 Epoch 8 Batch 392/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.436 Epoch 8 Batch 393/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.958, Loss: 1.456 Epoch 8 Batch 394/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.958, Loss: 1.475 Epoch 8 Batch 395/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.959, Loss: 1.506 Epoch 8 Batch 396/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.418 Epoch 8 Batch 397/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.953, Loss: 1.442 Epoch 8 Batch 398/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.948, Loss: 1.417 Epoch 8 Batch 399/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.948, Loss: 1.441 Epoch 8 Batch 400/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.431 Epoch 8 Batch 401/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.952, Loss: 1.475 Epoch 8 Batch 402/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.398 Epoch 8 Batch 403/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.955, Loss: 1.420 Epoch 8 Batch 404/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.950, Loss: 1.431 Epoch 8 Batch 405/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.950, Loss: 1.400 Epoch 8 Batch 406/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.432 Epoch 8 Batch 407/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.949, Loss: 1.337 Epoch 8 Batch 408/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.410 Epoch 8 Batch 409/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.949, Loss: 1.474 Epoch 8 Batch 410/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.953, Loss: 1.508 Epoch 8 Batch 411/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.953, Loss: 1.479 Epoch 8 Batch 412/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.954, Loss: 1.414 Epoch 8 Batch 413/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.954, Loss: 1.417 Epoch 8 Batch 414/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.954, Loss: 1.374 Epoch 8 Batch 415/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.959, Loss: 1.441 Epoch 8 Batch 416/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.958, Loss: 1.316 Epoch 8 Batch 417/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.435 Epoch 8 Batch 418/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.383 Epoch 8 Batch 419/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.956, Loss: 1.434 Epoch 8 Batch 420/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.956, Loss: 1.349 Epoch 8 Batch 421/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.395 Epoch 8 Batch 422/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.369 Epoch 8 Batch 423/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.956, Loss: 1.419 Epoch 8 Batch 424/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.956, Loss: 1.328 Epoch 8 Batch 425/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.958, Loss: 1.413 Epoch 8 Batch 426/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.505 Epoch 8 Batch 427/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.965, Loss: 1.368 Epoch 8 Batch 428/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.966, Loss: 1.452 Epoch 8 Batch 429/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.966, Loss: 1.365 Epoch 8 Batch 430/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.966, Loss: 1.439 Epoch 8 Batch 431/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.966, Loss: 1.470 Epoch 8 Batch 432/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.966, Loss: 1.428 Epoch 8 Batch 433/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.966, Loss: 1.471 Epoch 8 Batch 434/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.960, Loss: 1.438 Epoch 8 Batch 435/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.955, Loss: 1.442 Epoch 8 Batch 436/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.514 Epoch 8 Batch 437/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.955, Loss: 1.372 Epoch 8 Batch 438/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.955, Loss: 1.407 Epoch 8 Batch 439/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.405 Epoch 8 Batch 440/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.958, Loss: 1.432 Epoch 8 Batch 441/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.953, Loss: 1.476 Epoch 8 Batch 442/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.955, Loss: 1.417 Epoch 8 Batch 443/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.954, Loss: 1.456 Epoch 8 Batch 444/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.954, Loss: 1.385 Epoch 8 Batch 445/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.953, Loss: 1.412 Epoch 8 Batch 446/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.953, Loss: 1.409 Epoch 8 Batch 447/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.956, Loss: 1.409 Epoch 8 Batch 448/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.429 Epoch 8 Batch 449/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.431 Epoch 8 Batch 450/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.441 Epoch 8 Batch 451/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.945, Loss: 1.446 Epoch 8 Batch 452/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.945, Loss: 1.372 Epoch 8 Batch 453/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.945, Loss: 1.408 Epoch 8 Batch 454/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.945, Loss: 1.364 Epoch 8 Batch 455/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.355 Epoch 8 Batch 456/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.945, Loss: 1.453 Epoch 8 Batch 457/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.945, Loss: 1.385 Epoch 8 Batch 458/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.945, Loss: 1.403 Epoch 8 Batch 459/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.430 Epoch 8 Batch 460/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.362 Epoch 8 Batch 461/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.435 Epoch 8 Batch 462/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.954, Loss: 1.366 Epoch 8 Batch 463/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.447 Epoch 8 Batch 464/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.405 Epoch 8 Batch 465/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.954, Loss: 1.505 Epoch 8 Batch 466/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.954, Loss: 1.428 Epoch 8 Batch 467/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.954, Loss: 1.455 Epoch 8 Batch 468/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.392 Epoch 8 Batch 469/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.950, Loss: 1.437 Epoch 8 Batch 470/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.945, Loss: 1.455 Epoch 8 Batch 471/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.946, Loss: 1.419 Epoch 8 Batch 472/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.946, Loss: 1.413 Epoch 8 Batch 473/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.950, Loss: 1.415 Epoch 8 Batch 474/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.389 Epoch 8 Batch 475/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.953, Loss: 1.468 Epoch 8 Batch 476/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.957, Loss: 1.369 Epoch 8 Batch 477/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.953, Loss: 1.423 Epoch 8 Batch 478/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.953, Loss: 1.416 Epoch 8 Batch 479/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.455 Epoch 8 Batch 480/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.957, Loss: 1.408 Epoch 8 Batch 481/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.952, Loss: 1.440 Epoch 8 Batch 482/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.435 Epoch 8 Batch 483/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.954, Loss: 1.508 Epoch 8 Batch 484/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.959, Loss: 1.425 Epoch 8 Batch 485/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.412 Epoch 8 Batch 486/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.961, Loss: 1.433 Epoch 8 Batch 487/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.961, Loss: 1.429 Epoch 8 Batch 488/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.400 Epoch 8 Batch 489/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.952, Loss: 1.423 Epoch 8 Batch 490/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.941, Loss: 1.376 Epoch 8 Batch 491/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.945, Loss: 1.435 Epoch 8 Batch 492/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.945, Loss: 1.478 Epoch 8 Batch 493/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.945, Loss: 1.427 Epoch 8 Batch 494/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.945, Loss: 1.424 Epoch 8 Batch 495/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.945, Loss: 1.390 Epoch 8 Batch 496/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.945, Loss: 1.412 Epoch 8 Batch 497/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.945, Loss: 1.407 Epoch 8 Batch 498/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.959, Loss: 1.446 Epoch 8 Batch 499/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.964, Loss: 1.411 Epoch 8 Batch 500/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.964, Loss: 1.425 Epoch 8 Batch 501/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.968, Loss: 1.383 Epoch 8 Batch 502/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.968, Loss: 1.499 Epoch 8 Batch 503/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.975, Loss: 1.481 Epoch 8 Batch 504/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.970, Loss: 1.465 Epoch 8 Batch 505/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.399 Epoch 8 Batch 506/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.960, Loss: 1.464 Epoch 8 Batch 507/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.960, Loss: 1.483 Epoch 8 Batch 508/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.959, Loss: 1.388 Epoch 8 Batch 509/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.959, Loss: 1.411 Epoch 8 Batch 510/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.959, Loss: 1.394 Epoch 8 Batch 511/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.959, Loss: 1.382 Epoch 8 Batch 512/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.965, Loss: 1.411 Epoch 8 Batch 513/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.960, Loss: 1.487 Epoch 8 Batch 514/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.967, Loss: 1.453 Epoch 8 Batch 515/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.967, Loss: 1.442 Epoch 8 Batch 516/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.967, Loss: 1.426 Epoch 8 Batch 517/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.967, Loss: 1.416 Epoch 8 Batch 518/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.967, Loss: 1.418 Epoch 8 Batch 519/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.959, Loss: 1.372 Epoch 8 Batch 520/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.959, Loss: 1.414 Epoch 8 Batch 521/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.959, Loss: 1.427 Epoch 8 Batch 522/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.954, Loss: 1.416 Epoch 8 Batch 523/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.367 Epoch 8 Batch 524/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.424 Epoch 8 Batch 525/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.962, Loss: 1.452 Epoch 8 Batch 526/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.962, Loss: 1.423 Epoch 8 Batch 527/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.963, Loss: 1.467 Epoch 8 Batch 528/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.963, Loss: 1.460 Epoch 8 Batch 529/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.967, Loss: 1.464 Epoch 8 Batch 530/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.960, Loss: 1.455 Epoch 8 Batch 531/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.960, Loss: 1.458 Epoch 8 Batch 532/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.955, Loss: 1.431 Epoch 8 Batch 533/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.955, Loss: 1.436 Epoch 8 Batch 534/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.959, Loss: 1.439 Epoch 8 Batch 535/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.428 Epoch 8 Batch 536/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.963, Loss: 1.384 Epoch 8 Batch 537/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.963, Loss: 1.425 Epoch 8 Batch 538/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.434 Epoch 8 Batch 539/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.963, Loss: 1.463 Epoch 8 Batch 540/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.958, Loss: 1.384 Epoch 8 Batch 541/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.963, Loss: 1.369 Epoch 8 Batch 542/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.384 Epoch 8 Batch 543/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.964, Loss: 1.407 Epoch 8 Batch 544/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.963, Loss: 1.482 Epoch 8 Batch 545/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.968, Loss: 1.340 Epoch 8 Batch 546/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.968, Loss: 1.422 Epoch 8 Batch 547/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.968, Loss: 1.438 Epoch 8 Batch 548/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.968, Loss: 1.482 Epoch 8 Batch 549/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.973, Loss: 1.467 Epoch 8 Batch 550/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.968, Loss: 1.434 Epoch 8 Batch 551/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.963, Loss: 1.437 Epoch 8 Batch 552/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.509 Epoch 8 Batch 553/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.963, Loss: 1.433 Epoch 8 Batch 554/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.963, Loss: 1.408 Epoch 8 Batch 555/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.959, Loss: 1.384 Epoch 8 Batch 556/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.499 Epoch 8 Batch 557/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.964, Loss: 1.413 Epoch 8 Batch 558/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.331 Epoch 8 Batch 559/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.964, Loss: 1.430 Epoch 8 Batch 560/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.964, Loss: 1.417 Epoch 8 Batch 561/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.964, Loss: 1.371 Epoch 8 Batch 562/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.962, Loss: 1.380 Epoch 8 Batch 563/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.962, Loss: 1.441 Epoch 8 Batch 564/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.962, Loss: 1.460 Epoch 8 Batch 565/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.962, Loss: 1.408 Epoch 8 Batch 566/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.425 Epoch 8 Batch 567/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.963, Loss: 1.433 Epoch 8 Batch 568/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.963, Loss: 1.374 Epoch 8 Batch 569/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.968, Loss: 1.404 Epoch 8 Batch 570/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.968, Loss: 1.466 Epoch 8 Batch 571/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.973, Loss: 1.441 Epoch 8 Batch 572/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.970, Loss: 1.489 Epoch 8 Batch 573/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.966, Loss: 1.375 Epoch 8 Batch 574/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.447 Epoch 8 Batch 575/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.484 Epoch 8 Batch 576/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.959, Loss: 1.460 Epoch 8 Batch 577/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.396 Epoch 8 Batch 578/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.399 Epoch 8 Batch 579/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.967, Loss: 1.534 Epoch 8 Batch 580/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.968, Loss: 1.435 Epoch 8 Batch 581/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.967, Loss: 1.425 Epoch 8 Batch 582/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.968, Loss: 1.475 Epoch 8 Batch 583/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.968, Loss: 1.386 Epoch 8 Batch 584/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.474 Epoch 8 Batch 585/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.973, Loss: 1.386 Epoch 8 Batch 586/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.973, Loss: 1.469 Epoch 8 Batch 587/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.973, Loss: 1.459 Epoch 8 Batch 588/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.972, Loss: 1.398 Epoch 8 Batch 589/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.966, Loss: 1.458 Epoch 8 Batch 590/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.966, Loss: 1.450 Epoch 8 Batch 591/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.966, Loss: 1.411 Epoch 8 Batch 592/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.971, Loss: 1.356 Epoch 8 Batch 593/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.971, Loss: 1.379 Epoch 8 Batch 594/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.971, Loss: 1.406 Epoch 8 Batch 595/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.971, Loss: 1.356 Epoch 8 Batch 596/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.970, Loss: 1.463 Epoch 8 Batch 597/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.970, Loss: 1.422 Epoch 8 Batch 598/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.970, Loss: 1.368 Epoch 8 Batch 599/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.972, Loss: 1.463 Epoch 8 Batch 600/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.972, Loss: 1.391 Epoch 8 Batch 601/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.972, Loss: 1.451 Epoch 8 Batch 602/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.974, Loss: 1.408 Epoch 8 Batch 603/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.972, Loss: 1.440 Epoch 8 Batch 604/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.965, Loss: 1.482 Epoch 8 Batch 605/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.965, Loss: 1.467 Epoch 8 Batch 606/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.965, Loss: 1.316 Epoch 8 Batch 607/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.965, Loss: 1.423 Epoch 8 Batch 608/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.475 Epoch 8 Batch 609/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.957, Loss: 1.394 Epoch 8 Batch 610/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.433 Epoch 8 Batch 611/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.961, Loss: 1.387 Epoch 8 Batch 612/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.961, Loss: 1.386 Epoch 8 Batch 613/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.957, Loss: 1.419 Epoch 8 Batch 614/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.957, Loss: 1.427 Epoch 8 Batch 615/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.957, Loss: 1.462 Epoch 8 Batch 616/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.961, Loss: 1.434 Epoch 8 Batch 617/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.974, Loss: 1.427 Epoch 8 Batch 618/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.969, Loss: 1.414 Epoch 8 Batch 619/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.971, Loss: 1.479 Epoch 8 Batch 620/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.969, Loss: 1.507 Epoch 8 Batch 621/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.970, Loss: 1.434 Epoch 8 Batch 622/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.970, Loss: 1.368 Epoch 8 Batch 623/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.969, Loss: 1.496 Epoch 8 Batch 624/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.969, Loss: 1.365 Epoch 8 Batch 625/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.963, Loss: 1.480 Epoch 8 Batch 626/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.963, Loss: 1.399 Epoch 8 Batch 627/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.963, Loss: 1.381 Epoch 8 Batch 628/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.381 Epoch 8 Batch 629/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.958, Loss: 1.401 Epoch 8 Batch 630/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.958, Loss: 1.381 Epoch 8 Batch 631/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.352 Epoch 8 Batch 632/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.460 Epoch 8 Batch 633/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.958, Loss: 1.442 Epoch 8 Batch 634/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.958, Loss: 1.432 Epoch 8 Batch 635/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.958, Loss: 1.375 Epoch 8 Batch 636/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.958, Loss: 1.406 Epoch 8 Batch 637/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.958, Loss: 1.460 Epoch 8 Batch 638/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.456 Epoch 8 Batch 639/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.487 Epoch 8 Batch 640/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.382 Epoch 8 Batch 641/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.963, Loss: 1.447 Epoch 8 Batch 642/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.963, Loss: 1.444 Epoch 8 Batch 643/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.968, Loss: 1.459 Epoch 8 Batch 644/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.966, Loss: 1.423 Epoch 8 Batch 645/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.966, Loss: 1.460 Epoch 8 Batch 646/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.961, Loss: 1.498 Epoch 8 Batch 647/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.472 Epoch 8 Batch 648/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.955, Loss: 1.416 Epoch 8 Batch 649/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.351 Epoch 8 Batch 650/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.961, Loss: 1.443 Epoch 8 Batch 651/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.402 Epoch 8 Batch 652/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.966, Loss: 1.426 Epoch 8 Batch 653/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.966, Loss: 1.431 Epoch 8 Batch 654/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.963, Loss: 1.369 Epoch 8 Batch 655/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.961, Loss: 1.331 Epoch 8 Batch 656/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.435 Epoch 8 Batch 657/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.958, Loss: 1.464 Epoch 8 Batch 658/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.965, Loss: 1.427 Epoch 8 Batch 659/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.970, Loss: 1.445 Epoch 8 Batch 660/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.970, Loss: 1.480 Epoch 8 Batch 661/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.970, Loss: 1.450 Epoch 8 Batch 662/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.970, Loss: 1.437 Epoch 8 Batch 663/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.974, Loss: 1.443 Epoch 8 Batch 664/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.974, Loss: 1.404 Epoch 8 Batch 665/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.974, Loss: 1.496 Epoch 8 Batch 666/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.972, Loss: 1.493 Epoch 8 Batch 667/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.457 Epoch 8 Batch 668/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.972, Loss: 1.415 Epoch 8 Batch 669/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.972, Loss: 1.457 Epoch 8 Batch 670/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.962, Loss: 1.380 Epoch 8 Batch 671/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.963, Loss: 1.342 Epoch 8 Batch 672/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.425 Epoch 8 Batch 673/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.971, Loss: 1.395 Epoch 8 Batch 674/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.975, Loss: 1.422 Epoch 8 Batch 675/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.975, Loss: 1.454 Epoch 8 Batch 676/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.973, Loss: 1.390 Epoch 8 Batch 677/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.978, Loss: 1.482 Epoch 8 Batch 678/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.973, Loss: 1.363 Epoch 8 Batch 679/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.973, Loss: 1.456 Epoch 8 Batch 680/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.973, Loss: 1.474 Epoch 8 Batch 681/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.973, Loss: 1.392 Epoch 8 Batch 682/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.959, Loss: 1.425 Epoch 8 Batch 683/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.353 Epoch 8 Batch 684/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.959, Loss: 1.436 Epoch 8 Batch 685/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.959, Loss: 1.366 Epoch 8 Batch 686/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.949, Loss: 1.481 Epoch 8 Batch 687/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.949, Loss: 1.457 Epoch 8 Batch 688/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.941, Loss: 1.402 Epoch 8 Batch 689/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.941, Loss: 1.381 Epoch 8 Batch 690/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.941, Loss: 1.366 Epoch 8 Batch 691/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.941, Loss: 1.378 Epoch 8 Batch 692/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.952, Loss: 1.373 Epoch 8 Batch 693/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.424 Epoch 8 Batch 694/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.952, Loss: 1.456 Epoch 8 Batch 695/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.954, Loss: 1.365 Epoch 8 Batch 696/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.955, Loss: 1.448 Epoch 8 Batch 697/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.423 Epoch 8 Batch 698/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.950, Loss: 1.435 Epoch 8 Batch 699/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.950, Loss: 1.447 Epoch 8 Batch 700/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.954, Loss: 1.373 Epoch 8 Batch 701/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.959, Loss: 1.384 Epoch 8 Batch 702/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.959, Loss: 1.467 Epoch 8 Batch 703/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.959, Loss: 1.479 Epoch 8 Batch 704/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.959, Loss: 1.515 Epoch 8 Batch 705/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.964, Loss: 1.498 Epoch 8 Batch 706/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.964, Loss: 1.459 Epoch 8 Batch 707/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.968, Loss: 1.411 Epoch 8 Batch 708/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.404 Epoch 8 Batch 709/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.968, Loss: 1.544 Epoch 8 Batch 710/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.968, Loss: 1.412 Epoch 8 Batch 711/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.968, Loss: 1.465 Epoch 8 Batch 712/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.963, Loss: 1.414 Epoch 8 Batch 713/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.958, Loss: 1.395 Epoch 8 Batch 714/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.958, Loss: 1.474 Epoch 8 Batch 715/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.425 Epoch 8 Batch 716/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.463 Epoch 8 Batch 717/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.955, Loss: 1.424 Epoch 8 Batch 718/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.430 Epoch 8 Batch 719/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.957, Loss: 1.374 Epoch 8 Batch 720/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.959, Loss: 1.414 Epoch 8 Batch 721/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.959, Loss: 1.385 Epoch 8 Batch 722/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.964, Loss: 1.392 Epoch 8 Batch 723/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.964, Loss: 1.436 Epoch 8 Batch 724/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.511 Epoch 8 Batch 725/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.963, Loss: 1.475 Epoch 8 Batch 726/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.961, Loss: 1.358 Epoch 8 Batch 727/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.960, Loss: 1.442 Epoch 8 Batch 728/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.955, Loss: 1.524 Epoch 8 Batch 729/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.955, Loss: 1.408 Epoch 8 Batch 730/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.949, Loss: 1.448 Epoch 8 Batch 731/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.951, Loss: 1.546 Epoch 8 Batch 732/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.951, Loss: 1.369 Epoch 8 Batch 733/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.949, Loss: 1.503 Epoch 8 Batch 734/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.425 Epoch 8 Batch 735/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.942, Loss: 1.424 Epoch 8 Batch 736/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.939, Loss: 1.384 Epoch 8 Batch 737/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.940, Loss: 1.446 Epoch 8 Batch 738/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.941, Loss: 1.483 Epoch 8 Batch 739/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.941, Loss: 1.443 Epoch 8 Batch 740/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.941, Loss: 1.401 Epoch 8 Batch 741/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.453 Epoch 8 Batch 742/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.960, Loss: 1.516 Epoch 8 Batch 743/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.435 Epoch 8 Batch 744/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.477 Epoch 8 Batch 745/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.957, Loss: 1.456 Epoch 8 Batch 746/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.957, Loss: 1.401 Epoch 8 Batch 747/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.498 Epoch 8 Batch 748/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.964, Loss: 1.411 Epoch 8 Batch 749/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.969, Loss: 1.485 Epoch 8 Batch 750/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.969, Loss: 1.447 Epoch 8 Batch 751/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.969, Loss: 1.392 Epoch 8 Batch 752/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.968, Loss: 1.357 Epoch 8 Batch 753/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.966, Loss: 1.329 Epoch 8 Batch 754/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.962, Loss: 1.491 Epoch 8 Batch 755/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.962, Loss: 1.435 Epoch 8 Batch 756/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.967, Loss: 1.404 Epoch 8 Batch 757/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.967, Loss: 1.363 Epoch 8 Batch 758/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.965, Loss: 1.463 Epoch 8 Batch 759/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.965, Loss: 1.423 Epoch 8 Batch 760/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.965, Loss: 1.524 Epoch 8 Batch 761/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.971, Loss: 1.411 Epoch 8 Batch 762/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.971, Loss: 1.501 Epoch 8 Batch 763/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.971, Loss: 1.443 Epoch 8 Batch 764/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.971, Loss: 1.414 Epoch 8 Batch 765/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.982, Loss: 1.462 Epoch 8 Batch 766/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.982, Loss: 1.531 Epoch 8 Batch 767/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.968, Loss: 1.534 Epoch 8 Batch 768/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.968, Loss: 1.422 Epoch 8 Batch 769/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.416 Epoch 8 Batch 770/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.412 Epoch 8 Batch 771/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.958, Loss: 1.400 Epoch 8 Batch 772/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.958, Loss: 1.524 Epoch 8 Batch 773/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.489 Epoch 8 Batch 774/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.958, Loss: 1.441 Epoch 8 Batch 775/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.969, Loss: 1.383 Epoch 8 Batch 776/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.970, Loss: 1.396 Epoch 8 Batch 777/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.963, Loss: 1.442 Epoch 8 Batch 778/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.488 Epoch 8 Batch 779/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.958, Loss: 1.343 Epoch 8 Batch 780/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.953, Loss: 1.441 Epoch 8 Batch 781/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.963, Loss: 1.467 Epoch 8 Batch 782/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.958, Loss: 1.433 Epoch 8 Batch 783/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.361 Epoch 8 Batch 784/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.963, Loss: 1.534 Epoch 8 Batch 785/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.957, Loss: 1.426 Epoch 8 Batch 786/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.952, Loss: 1.416 Epoch 8 Batch 787/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.952, Loss: 1.420 Epoch 8 Batch 788/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.950, Loss: 1.424 Epoch 8 Batch 789/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.945, Loss: 1.501 Epoch 8 Batch 790/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.945, Loss: 1.493 Epoch 8 Batch 791/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.950, Loss: 1.393 Epoch 8 Batch 792/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.949, Loss: 1.558 Epoch 8 Batch 793/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.950, Loss: 1.392 Epoch 8 Batch 794/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.953, Loss: 1.450 Epoch 8 Batch 795/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.953, Loss: 1.433 Epoch 8 Batch 796/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.953, Loss: 1.411 Epoch 8 Batch 797/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.953, Loss: 1.410 Epoch 8 Batch 798/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.953, Loss: 1.436 Epoch 8 Batch 799/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.953, Loss: 1.433 Epoch 8 Batch 800/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.954, Loss: 1.445 Epoch 8 Batch 801/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.945, Loss: 1.370 Epoch 8 Batch 802/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.941, Loss: 1.473 Epoch 8 Batch 803/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.497 Epoch 8 Batch 804/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.952, Loss: 1.424 Epoch 8 Batch 805/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.407 Epoch 8 Batch 806/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.953, Loss: 1.439 Epoch 8 Batch 807/1077 - Train Accuracy: 0.994, Validation Accuracy: 0.955, Loss: 1.475 Epoch 8 Batch 808/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.955, Loss: 1.598 Epoch 8 Batch 809/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.955, Loss: 1.455 Epoch 8 Batch 810/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.955, Loss: 1.475 Epoch 8 Batch 811/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.955, Loss: 1.523 Epoch 8 Batch 812/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.426 Epoch 8 Batch 813/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.953, Loss: 1.493 Epoch 8 Batch 814/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.953, Loss: 1.478 Epoch 8 Batch 815/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.480 Epoch 8 Batch 816/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.956, Loss: 1.360 Epoch 8 Batch 817/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.953, Loss: 1.450 Epoch 8 Batch 818/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.953, Loss: 1.410 Epoch 8 Batch 819/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.958, Loss: 1.410 Epoch 8 Batch 820/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.430 Epoch 8 Batch 821/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.965, Loss: 1.455 Epoch 8 Batch 822/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.962, Loss: 1.419 Epoch 8 Batch 823/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.967, Loss: 1.456 Epoch 8 Batch 824/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.968, Loss: 1.417 Epoch 8 Batch 825/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.974, Loss: 1.404 Epoch 8 Batch 826/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.972, Loss: 1.401 Epoch 8 Batch 827/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.974, Loss: 1.420 Epoch 8 Batch 828/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.972, Loss: 1.467 Epoch 8 Batch 829/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.972, Loss: 1.443 Epoch 8 Batch 830/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.972, Loss: 1.405 Epoch 8 Batch 831/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.972, Loss: 1.490 Epoch 8 Batch 832/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.972, Loss: 1.342 Epoch 8 Batch 833/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.971, Loss: 1.425 Epoch 8 Batch 834/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.972, Loss: 1.395 Epoch 8 Batch 835/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.972, Loss: 1.465 Epoch 8 Batch 836/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.972, Loss: 1.449 Epoch 8 Batch 837/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.972, Loss: 1.495 Epoch 8 Batch 838/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.977, Loss: 1.354 Epoch 8 Batch 839/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.976, Loss: 1.403 Epoch 8 Batch 840/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.981, Loss: 1.433 Epoch 8 Batch 841/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.976, Loss: 1.444 Epoch 8 Batch 842/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.976, Loss: 1.397 Epoch 8 Batch 843/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.971, Loss: 1.386 Epoch 8 Batch 844/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.969, Loss: 1.453 Epoch 8 Batch 845/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.969, Loss: 1.412 Epoch 8 Batch 846/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.964, Loss: 1.431 Epoch 8 Batch 847/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.960, Loss: 1.466 Epoch 8 Batch 848/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.960, Loss: 1.484 Epoch 8 Batch 849/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.964, Loss: 1.370 Epoch 8 Batch 850/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.964, Loss: 1.405 Epoch 8 Batch 851/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.969, Loss: 1.386 Epoch 8 Batch 852/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.969, Loss: 1.437 Epoch 8 Batch 853/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.967, Loss: 1.434 Epoch 8 Batch 854/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.962, Loss: 1.449 Epoch 8 Batch 855/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.969, Loss: 1.454 Epoch 8 Batch 856/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.404 Epoch 8 Batch 857/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.402 Epoch 8 Batch 858/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.373 Epoch 8 Batch 859/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.963, Loss: 1.501 Epoch 8 Batch 860/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.958, Loss: 1.388 Epoch 8 Batch 861/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.963, Loss: 1.396 Epoch 8 Batch 862/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.963, Loss: 1.374 Epoch 8 Batch 863/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.968, Loss: 1.433 Epoch 8 Batch 864/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.962, Loss: 1.348 Epoch 8 Batch 865/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.961, Loss: 1.418 Epoch 8 Batch 866/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.437 Epoch 8 Batch 867/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.963, Loss: 1.489 Epoch 8 Batch 868/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.969, Loss: 1.435 Epoch 8 Batch 869/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.974, Loss: 1.448 Epoch 8 Batch 870/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.976, Loss: 1.496 Epoch 8 Batch 871/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.976, Loss: 1.369 Epoch 8 Batch 872/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.976, Loss: 1.416 Epoch 8 Batch 873/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.976, Loss: 1.463 Epoch 8 Batch 874/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.974, Loss: 1.428 Epoch 8 Batch 875/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.974, Loss: 1.451 Epoch 8 Batch 876/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.974, Loss: 1.457 Epoch 8 Batch 877/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.973, Loss: 1.342 Epoch 8 Batch 878/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.973, Loss: 1.452 Epoch 8 Batch 879/1077 - Train Accuracy: 0.996, Validation Accuracy: 0.973, Loss: 1.412 Epoch 8 Batch 880/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.967, Loss: 1.373 Epoch 8 Batch 881/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.967, Loss: 1.385 Epoch 8 Batch 882/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.968, Loss: 1.386 Epoch 8 Batch 883/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.968, Loss: 1.397 Epoch 8 Batch 884/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.967, Loss: 1.406 Epoch 8 Batch 885/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.967, Loss: 1.394 Epoch 8 Batch 886/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.383 Epoch 8 Batch 887/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.961, Loss: 1.398 Epoch 8 Batch 888/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.966, Loss: 1.400 Epoch 8 Batch 889/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.968, Loss: 1.413 Epoch 8 Batch 890/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.968, Loss: 1.433 Epoch 8 Batch 891/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.968, Loss: 1.463 Epoch 8 Batch 892/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.968, Loss: 1.449 Epoch 8 Batch 893/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.968, Loss: 1.451 Epoch 8 Batch 894/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.969, Loss: 1.362 Epoch 8 Batch 895/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.969, Loss: 1.464 Epoch 8 Batch 896/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.969, Loss: 1.477 Epoch 8 Batch 897/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.969, Loss: 1.461 Epoch 8 Batch 898/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.969, Loss: 1.437 Epoch 8 Batch 899/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.969, Loss: 1.436 Epoch 8 Batch 900/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.970, Loss: 1.455 Epoch 8 Batch 901/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.970, Loss: 1.447 Epoch 8 Batch 902/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.968, Loss: 1.393 Epoch 8 Batch 903/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.964, Loss: 1.437 Epoch 8 Batch 904/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.964, Loss: 1.416 Epoch 8 Batch 905/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.968, Loss: 1.482 Epoch 8 Batch 906/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.968, Loss: 1.407 Epoch 8 Batch 907/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.974, Loss: 1.395 Epoch 8 Batch 908/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.969, Loss: 1.449 Epoch 8 Batch 909/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.969, Loss: 1.458 Epoch 8 Batch 910/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.969, Loss: 1.419 Epoch 8 Batch 911/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.969, Loss: 1.410 Epoch 8 Batch 912/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.969, Loss: 1.363 Epoch 8 Batch 913/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.968, Loss: 1.492 Epoch 8 Batch 914/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.968, Loss: 1.431 Epoch 8 Batch 915/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.968, Loss: 1.395 Epoch 8 Batch 916/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.968, Loss: 1.448 Epoch 8 Batch 917/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.970, Loss: 1.354 Epoch 8 Batch 918/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.969, Loss: 1.424 Epoch 8 Batch 919/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.974, Loss: 1.392 Epoch 8 Batch 920/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.970, Loss: 1.474 Epoch 8 Batch 921/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.967, Loss: 1.449 Epoch 8 Batch 922/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.967, Loss: 1.337 Epoch 8 Batch 923/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.963, Loss: 1.425 Epoch 8 Batch 924/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.963, Loss: 1.391 Epoch 8 Batch 925/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.968, Loss: 1.420 Epoch 8 Batch 926/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.969, Loss: 1.404 Epoch 8 Batch 927/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.974, Loss: 1.432 Epoch 8 Batch 928/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.973, Loss: 1.371 Epoch 8 Batch 929/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.973, Loss: 1.439 Epoch 8 Batch 930/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.973, Loss: 1.452 Epoch 8 Batch 931/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.972, Loss: 1.410 Epoch 8 Batch 932/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.977, Loss: 1.348 Epoch 8 Batch 933/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.966, Loss: 1.386 Epoch 8 Batch 934/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.968, Loss: 1.433 Epoch 8 Batch 935/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.968, Loss: 1.452 Epoch 8 Batch 936/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.968, Loss: 1.433 Epoch 8 Batch 937/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.968, Loss: 1.482 Epoch 8 Batch 938/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.963, Loss: 1.415 Epoch 8 Batch 939/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.966, Loss: 1.481 Epoch 8 Batch 940/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.435 Epoch 8 Batch 941/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.968, Loss: 1.424 Epoch 8 Batch 942/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.965, Loss: 1.398 Epoch 8 Batch 943/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.970, Loss: 1.464 Epoch 8 Batch 944/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.970, Loss: 1.424 Epoch 8 Batch 945/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.975, Loss: 1.405 Epoch 8 Batch 946/1077 - Train Accuracy: 0.995, Validation Accuracy: 0.981, Loss: 1.420 Epoch 8 Batch 947/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.979, Loss: 1.442 Epoch 8 Batch 948/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.975, Loss: 1.507 Epoch 8 Batch 949/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.975, Loss: 1.450 Epoch 8 Batch 950/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.975, Loss: 1.391 Epoch 8 Batch 951/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.974, Loss: 1.461 Epoch 8 Batch 952/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.973, Loss: 1.490 Epoch 8 Batch 953/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.973, Loss: 1.421 Epoch 8 Batch 954/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.972, Loss: 1.397 Epoch 8 Batch 955/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.967, Loss: 1.422 Epoch 8 Batch 956/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.967, Loss: 1.350 Epoch 8 Batch 957/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.967, Loss: 1.421 Epoch 8 Batch 958/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.962, Loss: 1.449 Epoch 8 Batch 959/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.962, Loss: 1.453 Epoch 8 Batch 960/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.358 Epoch 8 Batch 961/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.436 Epoch 8 Batch 962/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.964, Loss: 1.423 Epoch 8 Batch 963/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.967, Loss: 1.409 Epoch 8 Batch 964/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.407 Epoch 8 Batch 965/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.958, Loss: 1.485 Epoch 8 Batch 966/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.958, Loss: 1.427 Epoch 8 Batch 967/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.394 Epoch 8 Batch 968/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.963, Loss: 1.390 Epoch 8 Batch 969/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.963, Loss: 1.375 Epoch 8 Batch 970/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.963, Loss: 1.315 Epoch 8 Batch 971/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.963, Loss: 1.394 Epoch 8 Batch 972/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.962, Loss: 1.436 Epoch 8 Batch 973/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.962, Loss: 1.480 Epoch 8 Batch 974/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.962, Loss: 1.424 Epoch 8 Batch 975/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.972, Loss: 1.457 Epoch 8 Batch 976/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.971, Loss: 1.437 Epoch 8 Batch 977/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.971, Loss: 1.423 Epoch 8 Batch 978/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.974, Loss: 1.364 Epoch 8 Batch 979/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.974, Loss: 1.426 Epoch 8 Batch 980/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.974, Loss: 1.369 Epoch 8 Batch 981/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.969, Loss: 1.387 Epoch 8 Batch 982/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.515 Epoch 8 Batch 983/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.964, Loss: 1.430 Epoch 8 Batch 984/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.964, Loss: 1.430 Epoch 8 Batch 985/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.964, Loss: 1.433 Epoch 8 Batch 986/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.964, Loss: 1.409 Epoch 8 Batch 987/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.965, Loss: 1.435 Epoch 8 Batch 988/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.965, Loss: 1.340 Epoch 8 Batch 989/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.974, Loss: 1.380 Epoch 8 Batch 990/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.974, Loss: 1.405 Epoch 8 Batch 991/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.969, Loss: 1.382 Epoch 8 Batch 992/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.543 Epoch 8 Batch 993/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.967, Loss: 1.482 Epoch 8 Batch 994/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.972, Loss: 1.453 Epoch 8 Batch 995/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.451 Epoch 8 Batch 996/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.969, Loss: 1.421 Epoch 8 Batch 997/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.970, Loss: 1.434 Epoch 8 Batch 998/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.970, Loss: 1.409 Epoch 8 Batch 999/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.970, Loss: 1.438 Epoch 8 Batch 1000/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.975, Loss: 1.363 Epoch 8 Batch 1001/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.975, Loss: 1.397 Epoch 8 Batch 1002/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.975, Loss: 1.374 Epoch 8 Batch 1003/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.975, Loss: 1.494 Epoch 8 Batch 1004/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.970, Loss: 1.431 Epoch 8 Batch 1005/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.965, Loss: 1.417 Epoch 8 Batch 1006/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.970, Loss: 1.514 Epoch 8 Batch 1007/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.970, Loss: 1.398 Epoch 8 Batch 1008/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.970, Loss: 1.390 Epoch 8 Batch 1009/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.968, Loss: 1.393 Epoch 8 Batch 1010/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.963, Loss: 1.450 Epoch 8 Batch 1011/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.963, Loss: 1.412 Epoch 8 Batch 1012/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.388 Epoch 8 Batch 1013/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.963, Loss: 1.361 Epoch 8 Batch 1014/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.963, Loss: 1.465 Epoch 8 Batch 1015/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.963, Loss: 1.428 Epoch 8 Batch 1016/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.963, Loss: 1.404 Epoch 8 Batch 1017/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.411 Epoch 8 Batch 1018/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.963, Loss: 1.391 Epoch 8 Batch 1019/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.963, Loss: 1.466 Epoch 8 Batch 1020/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.959, Loss: 1.366 Epoch 8 Batch 1021/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.959, Loss: 1.468 Epoch 8 Batch 1022/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.963, Loss: 1.379 Epoch 8 Batch 1023/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.968, Loss: 1.467 Epoch 8 Batch 1024/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.968, Loss: 1.377 Epoch 8 Batch 1025/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.974, Loss: 1.426 Epoch 8 Batch 1026/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.970, Loss: 1.486 Epoch 8 Batch 1027/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.970, Loss: 1.430 Epoch 8 Batch 1028/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.970, Loss: 1.428 Epoch 8 Batch 1029/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.970, Loss: 1.454 Epoch 8 Batch 1030/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.970, Loss: 1.403 Epoch 8 Batch 1031/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.969, Loss: 1.426 Epoch 8 Batch 1032/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.967, Loss: 1.412 Epoch 8 Batch 1033/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.962, Loss: 1.483 Epoch 8 Batch 1034/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.962, Loss: 1.490 Epoch 8 Batch 1035/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.962, Loss: 1.449 Epoch 8 Batch 1036/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.964, Loss: 1.480 Epoch 8 Batch 1037/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.964, Loss: 1.446 Epoch 8 Batch 1038/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.960, Loss: 1.433 Epoch 8 Batch 1039/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.395 Epoch 8 Batch 1040/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.954, Loss: 1.402 Epoch 8 Batch 1041/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.957, Loss: 1.466 Epoch 8 Batch 1042/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.953, Loss: 1.472 Epoch 8 Batch 1043/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.958, Loss: 1.426 Epoch 8 Batch 1044/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.958, Loss: 1.436 Epoch 8 Batch 1045/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.408 Epoch 8 Batch 1046/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.957, Loss: 1.407 Epoch 8 Batch 1047/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.957, Loss: 1.416 Epoch 8 Batch 1048/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.334 Epoch 8 Batch 1049/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.416 Epoch 8 Batch 1050/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.971, Loss: 1.491 Epoch 8 Batch 1051/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.434 Epoch 8 Batch 1052/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.966, Loss: 1.381 Epoch 8 Batch 1053/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.966, Loss: 1.497 Epoch 8 Batch 1054/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.963, Loss: 1.433 Epoch 8 Batch 1055/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.963, Loss: 1.440 Epoch 8 Batch 1056/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.958, Loss: 1.421 Epoch 8 Batch 1057/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.949, Loss: 1.433 Epoch 8 Batch 1058/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.953, Loss: 1.363 Epoch 8 Batch 1059/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.958, Loss: 1.399 Epoch 8 Batch 1060/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.953, Loss: 1.444 Epoch 8 Batch 1061/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.954, Loss: 1.406 Epoch 8 Batch 1062/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.388 Epoch 8 Batch 1063/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.955, Loss: 1.422 Epoch 8 Batch 1064/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.954, Loss: 1.411 Epoch 8 Batch 1065/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.413 Epoch 8 Batch 1066/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.956, Loss: 1.376 Epoch 8 Batch 1067/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.956, Loss: 1.453 Epoch 8 Batch 1068/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.394 Epoch 8 Batch 1069/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.961, Loss: 1.436 Epoch 8 Batch 1070/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.961, Loss: 1.452 Epoch 8 Batch 1071/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.417 Epoch 8 Batch 1072/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.956, Loss: 1.494 Epoch 8 Batch 1073/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.577 Epoch 8 Batch 1074/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.956, Loss: 1.440 Epoch 8 Batch 1075/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.483 Epoch 9 Batch 0/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.474 Epoch 9 Batch 1/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.956, Loss: 1.392 Epoch 9 Batch 2/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.950, Loss: 1.433 Epoch 9 Batch 3/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.950, Loss: 1.398 Epoch 9 Batch 4/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.956, Loss: 1.461 Epoch 9 Batch 5/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.950, Loss: 1.385 Epoch 9 Batch 6/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.950, Loss: 1.482 Epoch 9 Batch 7/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.475 Epoch 9 Batch 8/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.954, Loss: 1.498 Epoch 9 Batch 9/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.954, Loss: 1.377 Epoch 9 Batch 10/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.961, Loss: 1.448 Epoch 9 Batch 11/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.962, Loss: 1.413 Epoch 9 Batch 12/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.960, Loss: 1.390 Epoch 9 Batch 13/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.412 Epoch 9 Batch 14/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.407 Epoch 9 Batch 15/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.961, Loss: 1.419 Epoch 9 Batch 16/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.961, Loss: 1.494 Epoch 9 Batch 17/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.469 Epoch 9 Batch 18/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.962, Loss: 1.413 Epoch 9 Batch 19/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.962, Loss: 1.456 Epoch 9 Batch 20/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.385 Epoch 9 Batch 21/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.968, Loss: 1.431 Epoch 9 Batch 22/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.968, Loss: 1.487 Epoch 9 Batch 23/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.968, Loss: 1.451 Epoch 9 Batch 24/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.963, Loss: 1.417 Epoch 9 Batch 25/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.347 Epoch 9 Batch 26/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.968, Loss: 1.342 Epoch 9 Batch 27/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.973, Loss: 1.428 Epoch 9 Batch 28/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.967, Loss: 1.472 Epoch 9 Batch 29/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.967, Loss: 1.393 Epoch 9 Batch 30/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.967, Loss: 1.429 Epoch 9 Batch 31/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.429 Epoch 9 Batch 32/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.959, Loss: 1.490 Epoch 9 Batch 33/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.464 Epoch 9 Batch 34/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.955, Loss: 1.402 Epoch 9 Batch 35/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.956, Loss: 1.443 Epoch 9 Batch 36/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.954, Loss: 1.430 Epoch 9 Batch 37/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.954, Loss: 1.423 Epoch 9 Batch 38/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.954, Loss: 1.411 Epoch 9 Batch 39/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.953, Loss: 1.388 Epoch 9 Batch 40/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.960, Loss: 1.418 Epoch 9 Batch 41/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.959, Loss: 1.471 Epoch 9 Batch 42/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.954, Loss: 1.386 Epoch 9 Batch 43/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.953, Loss: 1.362 Epoch 9 Batch 44/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.958, Loss: 1.356 Epoch 9 Batch 45/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.486 Epoch 9 Batch 46/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.963, Loss: 1.388 Epoch 9 Batch 47/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.960, Loss: 1.340 Epoch 9 Batch 48/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.376 Epoch 9 Batch 49/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.952, Loss: 1.466 Epoch 9 Batch 50/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.419 Epoch 9 Batch 51/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.952, Loss: 1.452 Epoch 9 Batch 52/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.947, Loss: 1.458 Epoch 9 Batch 53/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.950, Loss: 1.428 Epoch 9 Batch 54/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.947, Loss: 1.440 Epoch 9 Batch 55/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.946, Loss: 1.436 Epoch 9 Batch 56/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.946, Loss: 1.430 Epoch 9 Batch 57/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.947, Loss: 1.441 Epoch 9 Batch 58/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.949, Loss: 1.469 Epoch 9 Batch 59/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.949, Loss: 1.468 Epoch 9 Batch 60/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.953, Loss: 1.407 Epoch 9 Batch 61/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.953, Loss: 1.410 Epoch 9 Batch 62/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.951, Loss: 1.412 Epoch 9 Batch 63/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.954, Loss: 1.485 Epoch 9 Batch 64/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.958, Loss: 1.402 Epoch 9 Batch 65/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.956, Loss: 1.506 Epoch 9 Batch 66/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.956, Loss: 1.404 Epoch 9 Batch 67/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.956, Loss: 1.406 Epoch 9 Batch 68/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.956, Loss: 1.418 Epoch 9 Batch 69/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.378 Epoch 9 Batch 70/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.428 Epoch 9 Batch 71/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.961, Loss: 1.463 Epoch 9 Batch 72/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.961, Loss: 1.409 Epoch 9 Batch 73/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.961, Loss: 1.384 Epoch 9 Batch 74/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.961, Loss: 1.454 Epoch 9 Batch 75/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.334 Epoch 9 Batch 76/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.961, Loss: 1.496 Epoch 9 Batch 77/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.411 Epoch 9 Batch 78/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.961, Loss: 1.444 Epoch 9 Batch 79/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.961, Loss: 1.447 Epoch 9 Batch 80/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.961, Loss: 1.358 Epoch 9 Batch 81/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.959, Loss: 1.382 Epoch 9 Batch 82/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.959, Loss: 1.419 Epoch 9 Batch 83/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.949, Loss: 1.441 Epoch 9 Batch 84/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.480 Epoch 9 Batch 85/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.949, Loss: 1.472 Epoch 9 Batch 86/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.954, Loss: 1.428 Epoch 9 Batch 87/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.959, Loss: 1.353 Epoch 9 Batch 88/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.959, Loss: 1.438 Epoch 9 Batch 89/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.959, Loss: 1.450 Epoch 9 Batch 90/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.958, Loss: 1.438 Epoch 9 Batch 91/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.427 Epoch 9 Batch 92/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.466 Epoch 9 Batch 93/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.955, Loss: 1.400 Epoch 9 Batch 94/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.433 Epoch 9 Batch 95/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.958, Loss: 1.474 Epoch 9 Batch 96/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.963, Loss: 1.349 Epoch 9 Batch 97/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.962, Loss: 1.437 Epoch 9 Batch 98/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.963, Loss: 1.414 Epoch 9 Batch 99/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.958, Loss: 1.367 Epoch 9 Batch 100/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.958, Loss: 1.379 Epoch 9 Batch 101/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.958, Loss: 1.438 Epoch 9 Batch 102/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.958, Loss: 1.509 Epoch 9 Batch 103/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.953, Loss: 1.419 Epoch 9 Batch 104/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.950, Loss: 1.409 Epoch 9 Batch 105/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.429 Epoch 9 Batch 106/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.950, Loss: 1.421 Epoch 9 Batch 107/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.950, Loss: 1.450 Epoch 9 Batch 108/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.950, Loss: 1.439 Epoch 9 Batch 109/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.950, Loss: 1.438 Epoch 9 Batch 110/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.954, Loss: 1.354 Epoch 9 Batch 111/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.952, Loss: 1.357 Epoch 9 Batch 112/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.947, Loss: 1.432 Epoch 9 Batch 113/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.935, Loss: 1.430 Epoch 9 Batch 114/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.935, Loss: 1.339 Epoch 9 Batch 115/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.936, Loss: 1.454 Epoch 9 Batch 116/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.938, Loss: 1.431 Epoch 9 Batch 117/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.933, Loss: 1.360 Epoch 9 Batch 118/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.943, Loss: 1.408 Epoch 9 Batch 119/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.952, Loss: 1.445 Epoch 9 Batch 120/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.446 Epoch 9 Batch 121/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.952, Loss: 1.430 Epoch 9 Batch 122/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.952, Loss: 1.381 Epoch 9 Batch 123/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.952, Loss: 1.406 Epoch 9 Batch 124/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.958, Loss: 1.461 Epoch 9 Batch 125/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.956, Loss: 1.404 Epoch 9 Batch 126/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.956, Loss: 1.403 Epoch 9 Batch 127/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.407 Epoch 9 Batch 128/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.956, Loss: 1.472 Epoch 9 Batch 129/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.442 Epoch 9 Batch 130/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.958, Loss: 1.450 Epoch 9 Batch 131/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.958, Loss: 1.445 Epoch 9 Batch 132/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.962, Loss: 1.460 Epoch 9 Batch 133/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.959, Loss: 1.392 Epoch 9 Batch 134/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.959, Loss: 1.461 Epoch 9 Batch 135/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.961, Loss: 1.420 Epoch 9 Batch 136/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.956, Loss: 1.403 Epoch 9 Batch 137/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.951, Loss: 1.463 Epoch 9 Batch 138/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.957, Loss: 1.501 Epoch 9 Batch 139/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.957, Loss: 1.398 Epoch 9 Batch 140/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.957, Loss: 1.478 Epoch 9 Batch 141/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.957, Loss: 1.480 Epoch 9 Batch 142/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.957, Loss: 1.403 Epoch 9 Batch 143/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.955, Loss: 1.424 Epoch 9 Batch 144/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.955, Loss: 1.488 Epoch 9 Batch 145/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.960, Loss: 1.410 Epoch 9 Batch 146/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.965, Loss: 1.464 Epoch 9 Batch 147/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.961, Loss: 1.469 Epoch 9 Batch 148/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.432 Epoch 9 Batch 149/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.956, Loss: 1.419 Epoch 9 Batch 150/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.412 Epoch 9 Batch 151/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.434 Epoch 9 Batch 152/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.964, Loss: 1.384 Epoch 9 Batch 153/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.964, Loss: 1.389 Epoch 9 Batch 154/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.964, Loss: 1.348 Epoch 9 Batch 155/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.959, Loss: 1.477 Epoch 9 Batch 156/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.417 Epoch 9 Batch 157/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.955, Loss: 1.361 Epoch 9 Batch 158/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.417 Epoch 9 Batch 159/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.950, Loss: 1.415 Epoch 9 Batch 160/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.954, Loss: 1.425 Epoch 9 Batch 161/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.954, Loss: 1.330 Epoch 9 Batch 162/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.959, Loss: 1.483 Epoch 9 Batch 163/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.965, Loss: 1.437 Epoch 9 Batch 164/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.965, Loss: 1.364 Epoch 9 Batch 165/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.465 Epoch 9 Batch 166/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.950, Loss: 1.461 Epoch 9 Batch 167/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.950, Loss: 1.492 Epoch 9 Batch 168/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.950, Loss: 1.435 Epoch 9 Batch 169/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.950, Loss: 1.433 Epoch 9 Batch 170/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.950, Loss: 1.465 Epoch 9 Batch 171/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.955, Loss: 1.381 Epoch 9 Batch 172/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.441 Epoch 9 Batch 173/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.950, Loss: 1.438 Epoch 9 Batch 174/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.950, Loss: 1.370 Epoch 9 Batch 175/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.947, Loss: 1.310 Epoch 9 Batch 176/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.947, Loss: 1.454 Epoch 9 Batch 177/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.952, Loss: 1.448 Epoch 9 Batch 178/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.952, Loss: 1.438 Epoch 9 Batch 179/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.950, Loss: 1.421 Epoch 9 Batch 180/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.953, Loss: 1.390 Epoch 9 Batch 181/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.953, Loss: 1.460 Epoch 9 Batch 182/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.953, Loss: 1.408 Epoch 9 Batch 183/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.392 Epoch 9 Batch 184/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.960, Loss: 1.453 Epoch 9 Batch 185/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.954, Loss: 1.451 Epoch 9 Batch 186/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.393 Epoch 9 Batch 187/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.964, Loss: 1.322 Epoch 9 Batch 188/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.964, Loss: 1.455 Epoch 9 Batch 189/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.955, Loss: 1.482 Epoch 9 Batch 190/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.960, Loss: 1.432 Epoch 9 Batch 191/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.435 Epoch 9 Batch 192/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.960, Loss: 1.397 Epoch 9 Batch 193/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.960, Loss: 1.425 Epoch 9 Batch 194/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.469 Epoch 9 Batch 195/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.380 Epoch 9 Batch 196/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.959, Loss: 1.439 Epoch 9 Batch 197/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.964, Loss: 1.433 Epoch 9 Batch 198/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.417 Epoch 9 Batch 199/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.963, Loss: 1.396 Epoch 9 Batch 200/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.365 Epoch 9 Batch 201/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.965, Loss: 1.367 Epoch 9 Batch 202/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.970, Loss: 1.436 Epoch 9 Batch 203/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.965, Loss: 1.434 Epoch 9 Batch 204/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.958, Loss: 1.461 Epoch 9 Batch 205/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.953, Loss: 1.446 Epoch 9 Batch 206/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.953, Loss: 1.466 Epoch 9 Batch 207/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.958, Loss: 1.402 Epoch 9 Batch 208/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.968, Loss: 1.397 Epoch 9 Batch 209/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.968, Loss: 1.459 Epoch 9 Batch 210/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.968, Loss: 1.317 Epoch 9 Batch 211/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.968, Loss: 1.444 Epoch 9 Batch 212/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.965, Loss: 1.425 Epoch 9 Batch 213/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.965, Loss: 1.381 Epoch 9 Batch 214/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.964, Loss: 1.454 Epoch 9 Batch 215/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.964, Loss: 1.429 Epoch 9 Batch 216/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.966, Loss: 1.348 Epoch 9 Batch 217/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.969, Loss: 1.432 Epoch 9 Batch 218/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.964, Loss: 1.569 Epoch 9 Batch 219/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.955, Loss: 1.492 Epoch 9 Batch 220/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.954, Loss: 1.502 Epoch 9 Batch 221/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.379 Epoch 9 Batch 222/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.952, Loss: 1.471 Epoch 9 Batch 223/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.947, Loss: 1.440 Epoch 9 Batch 224/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.379 Epoch 9 Batch 225/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.956, Loss: 1.427 Epoch 9 Batch 226/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.956, Loss: 1.387 Epoch 9 Batch 227/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.956, Loss: 1.416 Epoch 9 Batch 228/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.955, Loss: 1.386 Epoch 9 Batch 229/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.436 Epoch 9 Batch 230/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.955, Loss: 1.424 Epoch 9 Batch 231/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.431 Epoch 9 Batch 232/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.955, Loss: 1.380 Epoch 9 Batch 233/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.957, Loss: 1.415 Epoch 9 Batch 234/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.959, Loss: 1.412 Epoch 9 Batch 235/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.957, Loss: 1.391 Epoch 9 Batch 236/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.952, Loss: 1.427 Epoch 9 Batch 237/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.449 Epoch 9 Batch 238/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.954, Loss: 1.445 Epoch 9 Batch 239/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.954, Loss: 1.411 Epoch 9 Batch 240/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.959, Loss: 1.379 Epoch 9 Batch 241/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.959, Loss: 1.501 Epoch 9 Batch 242/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.959, Loss: 1.367 Epoch 9 Batch 243/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.964, Loss: 1.455 Epoch 9 Batch 244/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.965, Loss: 1.477 Epoch 9 Batch 245/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.971, Loss: 1.386 Epoch 9 Batch 246/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.967, Loss: 1.417 Epoch 9 Batch 247/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.967, Loss: 1.444 Epoch 9 Batch 248/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.967, Loss: 1.383 Epoch 9 Batch 249/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.965, Loss: 1.402 Epoch 9 Batch 250/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.963, Loss: 1.404 Epoch 9 Batch 251/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.967, Loss: 1.422 Epoch 9 Batch 252/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.967, Loss: 1.359 Epoch 9 Batch 253/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.967, Loss: 1.421 Epoch 9 Batch 254/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.967, Loss: 1.413 Epoch 9 Batch 255/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.962, Loss: 1.451 Epoch 9 Batch 256/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.962, Loss: 1.494 Epoch 9 Batch 257/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.962, Loss: 1.428 Epoch 9 Batch 258/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.956, Loss: 1.423 Epoch 9 Batch 259/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.958, Loss: 1.352 Epoch 9 Batch 260/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.444 Epoch 9 Batch 261/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.524 Epoch 9 Batch 262/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.969, Loss: 1.461 Epoch 9 Batch 263/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.969, Loss: 1.480 Epoch 9 Batch 264/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.969, Loss: 1.413 Epoch 9 Batch 265/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.974, Loss: 1.501 Epoch 9 Batch 266/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.972, Loss: 1.442 Epoch 9 Batch 267/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.961, Loss: 1.455 Epoch 9 Batch 268/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.447 Epoch 9 Batch 269/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.426 Epoch 9 Batch 270/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.961, Loss: 1.426 Epoch 9 Batch 271/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.963, Loss: 1.401 Epoch 9 Batch 272/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.962, Loss: 1.441 Epoch 9 Batch 273/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.962, Loss: 1.519 Epoch 9 Batch 274/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.962, Loss: 1.395 Epoch 9 Batch 275/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.962, Loss: 1.481 Epoch 9 Batch 276/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.962, Loss: 1.425 Epoch 9 Batch 277/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.962, Loss: 1.366 Epoch 9 Batch 278/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.962, Loss: 1.442 Epoch 9 Batch 279/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.962, Loss: 1.477 Epoch 9 Batch 280/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.961, Loss: 1.397 Epoch 9 Batch 281/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.961, Loss: 1.438 Epoch 9 Batch 282/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.423 Epoch 9 Batch 283/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.961, Loss: 1.445 Epoch 9 Batch 284/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.955, Loss: 1.494 Epoch 9 Batch 285/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.960, Loss: 1.415 Epoch 9 Batch 286/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.960, Loss: 1.464 Epoch 9 Batch 287/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.953, Loss: 1.442 Epoch 9 Batch 288/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.953, Loss: 1.443 Epoch 9 Batch 289/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.953, Loss: 1.447 Epoch 9 Batch 290/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.953, Loss: 1.411 Epoch 9 Batch 291/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.953, Loss: 1.450 Epoch 9 Batch 292/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.949, Loss: 1.438 Epoch 9 Batch 293/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.944, Loss: 1.414 Epoch 9 Batch 294/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.948, Loss: 1.412 Epoch 9 Batch 295/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.452 Epoch 9 Batch 296/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.951, Loss: 1.403 Epoch 9 Batch 297/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.951, Loss: 1.441 Epoch 9 Batch 298/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.951, Loss: 1.412 Epoch 9 Batch 299/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.951, Loss: 1.350 Epoch 9 Batch 300/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.952, Loss: 1.395 Epoch 9 Batch 301/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.960, Loss: 1.353 Epoch 9 Batch 302/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.960, Loss: 1.444 Epoch 9 Batch 303/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.960, Loss: 1.411 Epoch 9 Batch 304/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.965, Loss: 1.409 Epoch 9 Batch 305/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.960, Loss: 1.365 Epoch 9 Batch 306/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.961, Loss: 1.423 Epoch 9 Batch 307/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.959, Loss: 1.384 Epoch 9 Batch 308/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.959, Loss: 1.435 Epoch 9 Batch 309/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.961, Loss: 1.410 Epoch 9 Batch 310/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.391 Epoch 9 Batch 311/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.356 Epoch 9 Batch 312/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.960, Loss: 1.515 Epoch 9 Batch 313/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.957, Loss: 1.423 Epoch 9 Batch 314/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.951, Loss: 1.520 Epoch 9 Batch 315/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.351 Epoch 9 Batch 316/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.958, Loss: 1.379 Epoch 9 Batch 317/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.464 Epoch 9 Batch 318/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.960, Loss: 1.327 Epoch 9 Batch 319/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.398 Epoch 9 Batch 320/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.960, Loss: 1.434 Epoch 9 Batch 321/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.960, Loss: 1.407 Epoch 9 Batch 322/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.960, Loss: 1.417 Epoch 9 Batch 323/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.960, Loss: 1.451 Epoch 9 Batch 324/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.960, Loss: 1.407 Epoch 9 Batch 325/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.966, Loss: 1.419 Epoch 9 Batch 326/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.966, Loss: 1.339 Epoch 9 Batch 327/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.961, Loss: 1.456 Epoch 9 Batch 328/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.488 Epoch 9 Batch 329/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.961, Loss: 1.431 Epoch 9 Batch 330/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.430 Epoch 9 Batch 331/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.955, Loss: 1.463 Epoch 9 Batch 332/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.486 Epoch 9 Batch 333/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.955, Loss: 1.415 Epoch 9 Batch 334/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.959, Loss: 1.366 Epoch 9 Batch 335/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.957, Loss: 1.392 Epoch 9 Batch 336/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.956, Loss: 1.394 Epoch 9 Batch 337/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.956, Loss: 1.412 Epoch 9 Batch 338/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.400 Epoch 9 Batch 339/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.405 Epoch 9 Batch 340/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.490 Epoch 9 Batch 341/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.952, Loss: 1.454 Epoch 9 Batch 342/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.433 Epoch 9 Batch 343/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.952, Loss: 1.436 Epoch 9 Batch 344/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.952, Loss: 1.417 Epoch 9 Batch 345/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.951, Loss: 1.443 Epoch 9 Batch 346/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.957, Loss: 1.452 Epoch 9 Batch 347/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.957, Loss: 1.493 Epoch 9 Batch 348/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.366 Epoch 9 Batch 349/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.963, Loss: 1.361 Epoch 9 Batch 350/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.961, Loss: 1.468 Epoch 9 Batch 351/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.958, Loss: 1.434 Epoch 9 Batch 352/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.958, Loss: 1.420 Epoch 9 Batch 353/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.954, Loss: 1.482 Epoch 9 Batch 354/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.952, Loss: 1.452 Epoch 9 Batch 355/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.952, Loss: 1.404 Epoch 9 Batch 356/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.952, Loss: 1.455 Epoch 9 Batch 357/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.947, Loss: 1.462 Epoch 9 Batch 358/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.952, Loss: 1.475 Epoch 9 Batch 359/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.952, Loss: 1.352 Epoch 9 Batch 360/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.952, Loss: 1.454 Epoch 9 Batch 361/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.957, Loss: 1.401 Epoch 9 Batch 362/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.452 Epoch 9 Batch 363/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.388 Epoch 9 Batch 364/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.953, Loss: 1.398 Epoch 9 Batch 365/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.955, Loss: 1.351 Epoch 9 Batch 366/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.952, Loss: 1.385 Epoch 9 Batch 367/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.952, Loss: 1.453 Epoch 9 Batch 368/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.952, Loss: 1.445 Epoch 9 Batch 369/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.952, Loss: 1.405 Epoch 9 Batch 370/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.466 Epoch 9 Batch 371/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.505 Epoch 9 Batch 372/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.952, Loss: 1.389 Epoch 9 Batch 373/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.952, Loss: 1.461 Epoch 9 Batch 374/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.361 Epoch 9 Batch 375/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.955, Loss: 1.491 Epoch 9 Batch 376/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.955, Loss: 1.417 Epoch 9 Batch 377/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.418 Epoch 9 Batch 378/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.442 Epoch 9 Batch 379/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.955, Loss: 1.481 Epoch 9 Batch 380/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.955, Loss: 1.443 Epoch 9 Batch 381/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.955, Loss: 1.453 Epoch 9 Batch 382/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.949, Loss: 1.432 Epoch 9 Batch 383/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.949, Loss: 1.384 Epoch 9 Batch 384/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.947, Loss: 1.419 Epoch 9 Batch 385/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.947, Loss: 1.400 Epoch 9 Batch 386/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.947, Loss: 1.409 Epoch 9 Batch 387/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.947, Loss: 1.374 Epoch 9 Batch 388/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.948, Loss: 1.373 Epoch 9 Batch 389/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.952, Loss: 1.432 Epoch 9 Batch 390/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.948, Loss: 1.452 Epoch 9 Batch 391/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.948, Loss: 1.396 Epoch 9 Batch 392/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.948, Loss: 1.509 Epoch 9 Batch 393/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.948, Loss: 1.419 Epoch 9 Batch 394/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.953, Loss: 1.386 Epoch 9 Batch 395/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.950, Loss: 1.387 Epoch 9 Batch 396/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.941, Loss: 1.377 Epoch 9 Batch 397/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.940, Loss: 1.445 Epoch 9 Batch 398/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.940, Loss: 1.476 Epoch 9 Batch 399/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.940, Loss: 1.443 Epoch 9 Batch 400/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.945, Loss: 1.443 Epoch 9 Batch 401/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.950, Loss: 1.469 Epoch 9 Batch 402/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.355 Epoch 9 Batch 403/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.440 Epoch 9 Batch 404/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.950, Loss: 1.401 Epoch 9 Batch 405/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.946, Loss: 1.475 Epoch 9 Batch 406/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.945, Loss: 1.430 Epoch 9 Batch 407/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.945, Loss: 1.415 Epoch 9 Batch 408/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.944, Loss: 1.468 Epoch 9 Batch 409/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.949, Loss: 1.422 Epoch 9 Batch 410/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.949, Loss: 1.352 Epoch 9 Batch 411/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.949, Loss: 1.379 Epoch 9 Batch 412/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.953, Loss: 1.402 Epoch 9 Batch 413/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.953, Loss: 1.357 Epoch 9 Batch 414/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.953, Loss: 1.389 Epoch 9 Batch 415/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.465 Epoch 9 Batch 416/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.953, Loss: 1.348 Epoch 9 Batch 417/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.959, Loss: 1.428 Epoch 9 Batch 418/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.959, Loss: 1.410 Epoch 9 Batch 419/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.440 Epoch 9 Batch 420/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.952, Loss: 1.415 Epoch 9 Batch 421/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.954, Loss: 1.439 Epoch 9 Batch 422/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.954, Loss: 1.442 Epoch 9 Batch 423/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.408 Epoch 9 Batch 424/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.966, Loss: 1.394 Epoch 9 Batch 425/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.962, Loss: 1.500 Epoch 9 Batch 426/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.962, Loss: 1.438 Epoch 9 Batch 427/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.427 Epoch 9 Batch 428/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.955, Loss: 1.478 Epoch 9 Batch 429/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.957, Loss: 1.436 Epoch 9 Batch 430/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.957, Loss: 1.444 Epoch 9 Batch 431/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.957, Loss: 1.454 Epoch 9 Batch 432/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.447 Epoch 9 Batch 433/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.962, Loss: 1.508 Epoch 9 Batch 434/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.962, Loss: 1.447 Epoch 9 Batch 435/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.962, Loss: 1.371 Epoch 9 Batch 436/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.957, Loss: 1.433 Epoch 9 Batch 437/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.960, Loss: 1.469 Epoch 9 Batch 438/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.960, Loss: 1.423 Epoch 9 Batch 439/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.429 Epoch 9 Batch 440/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.407 Epoch 9 Batch 441/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.437 Epoch 9 Batch 442/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.955, Loss: 1.424 Epoch 9 Batch 443/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.955, Loss: 1.463 Epoch 9 Batch 444/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.962, Loss: 1.431 Epoch 9 Batch 445/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.962, Loss: 1.435 Epoch 9 Batch 446/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.962, Loss: 1.367 Epoch 9 Batch 447/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.961, Loss: 1.374 Epoch 9 Batch 448/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.422 Epoch 9 Batch 449/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.446 Epoch 9 Batch 450/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.961, Loss: 1.427 Epoch 9 Batch 451/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.966, Loss: 1.411 Epoch 9 Batch 452/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.361 Epoch 9 Batch 453/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.960, Loss: 1.468 Epoch 9 Batch 454/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.960, Loss: 1.418 Epoch 9 Batch 455/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.957, Loss: 1.508 Epoch 9 Batch 456/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.963, Loss: 1.370 Epoch 9 Batch 457/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.961, Loss: 1.441 Epoch 9 Batch 458/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.483 Epoch 9 Batch 459/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.956, Loss: 1.402 Epoch 9 Batch 460/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.956, Loss: 1.413 Epoch 9 Batch 461/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.445 Epoch 9 Batch 462/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.494 Epoch 9 Batch 463/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.956, Loss: 1.453 Epoch 9 Batch 464/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.382 Epoch 9 Batch 465/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.429 Epoch 9 Batch 466/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.956, Loss: 1.560 Epoch 9 Batch 467/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.342 Epoch 9 Batch 468/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.953, Loss: 1.456 Epoch 9 Batch 469/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.949, Loss: 1.408 Epoch 9 Batch 470/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.949, Loss: 1.337 Epoch 9 Batch 471/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.949, Loss: 1.459 Epoch 9 Batch 472/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.950, Loss: 1.457 Epoch 9 Batch 473/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.950, Loss: 1.458 Epoch 9 Batch 474/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.955, Loss: 1.462 Epoch 9 Batch 475/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.415 Epoch 9 Batch 476/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.955, Loss: 1.435 Epoch 9 Batch 477/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.389 Epoch 9 Batch 478/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.952, Loss: 1.370 Epoch 9 Batch 479/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.957, Loss: 1.406 Epoch 9 Batch 480/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.960, Loss: 1.462 Epoch 9 Batch 481/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.965, Loss: 1.417 Epoch 9 Batch 482/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.440 Epoch 9 Batch 483/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.960, Loss: 1.413 Epoch 9 Batch 484/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.381 Epoch 9 Batch 485/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.962, Loss: 1.427 Epoch 9 Batch 486/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.962, Loss: 1.369 Epoch 9 Batch 487/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.961, Loss: 1.502 Epoch 9 Batch 488/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.359 Epoch 9 Batch 489/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.357 Epoch 9 Batch 490/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.406 Epoch 9 Batch 491/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.960, Loss: 1.467 Epoch 9 Batch 492/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.960, Loss: 1.444 Epoch 9 Batch 493/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.960, Loss: 1.375 Epoch 9 Batch 494/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.965, Loss: 1.415 Epoch 9 Batch 495/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.965, Loss: 1.452 Epoch 9 Batch 496/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.965, Loss: 1.462 Epoch 9 Batch 497/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.962, Loss: 1.449 Epoch 9 Batch 498/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.962, Loss: 1.445 Epoch 9 Batch 499/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.962, Loss: 1.358 Epoch 9 Batch 500/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.962, Loss: 1.406 Epoch 9 Batch 501/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.962, Loss: 1.373 Epoch 9 Batch 502/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.477 Epoch 9 Batch 503/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.957, Loss: 1.426 Epoch 9 Batch 504/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.956, Loss: 1.522 Epoch 9 Batch 505/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.956, Loss: 1.401 Epoch 9 Batch 506/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.950, Loss: 1.331 Epoch 9 Batch 507/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.955, Loss: 1.324 Epoch 9 Batch 508/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.955, Loss: 1.426 Epoch 9 Batch 509/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.955, Loss: 1.422 Epoch 9 Batch 510/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.950, Loss: 1.422 Epoch 9 Batch 511/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.950, Loss: 1.412 Epoch 9 Batch 512/1077 - Train Accuracy: 0.996, Validation Accuracy: 0.955, Loss: 1.413 Epoch 9 Batch 513/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.951, Loss: 1.367 Epoch 9 Batch 514/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.951, Loss: 1.481 Epoch 9 Batch 515/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.951, Loss: 1.329 Epoch 9 Batch 516/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.955, Loss: 1.413 Epoch 9 Batch 517/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.956, Loss: 1.434 Epoch 9 Batch 518/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.960, Loss: 1.439 Epoch 9 Batch 519/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.965, Loss: 1.412 Epoch 9 Batch 520/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.971, Loss: 1.390 Epoch 9 Batch 521/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.971, Loss: 1.381 Epoch 9 Batch 522/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.966, Loss: 1.433 Epoch 9 Batch 523/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.966, Loss: 1.444 Epoch 9 Batch 524/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.966, Loss: 1.386 Epoch 9 Batch 525/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.961, Loss: 1.387 Epoch 9 Batch 526/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.967, Loss: 1.405 Epoch 9 Batch 527/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.966, Loss: 1.461 Epoch 9 Batch 528/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.970, Loss: 1.399 Epoch 9 Batch 529/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.971, Loss: 1.456 Epoch 9 Batch 530/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.966, Loss: 1.426 Epoch 9 Batch 531/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.966, Loss: 1.432 Epoch 9 Batch 532/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.966, Loss: 1.489 Epoch 9 Batch 533/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.968, Loss: 1.422 Epoch 9 Batch 534/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.966, Loss: 1.374 Epoch 9 Batch 535/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.966, Loss: 1.445 Epoch 9 Batch 536/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.971, Loss: 1.443 Epoch 9 Batch 537/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.966, Loss: 1.410 Epoch 9 Batch 538/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.961, Loss: 1.384 Epoch 9 Batch 539/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.960, Loss: 1.407 Epoch 9 Batch 540/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.958, Loss: 1.419 Epoch 9 Batch 541/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.958, Loss: 1.471 Epoch 9 Batch 542/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.354 Epoch 9 Batch 543/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.958, Loss: 1.395 Epoch 9 Batch 544/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.428 Epoch 9 Batch 545/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.964, Loss: 1.391 Epoch 9 Batch 546/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.964, Loss: 1.471 Epoch 9 Batch 547/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.974, Loss: 1.355 Epoch 9 Batch 548/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.974, Loss: 1.409 Epoch 9 Batch 549/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.969, Loss: 1.387 Epoch 9 Batch 550/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.964, Loss: 1.398 Epoch 9 Batch 551/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.959, Loss: 1.433 Epoch 9 Batch 552/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.964, Loss: 1.386 Epoch 9 Batch 553/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.961, Loss: 1.397 Epoch 9 Batch 554/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.962, Loss: 1.433 Epoch 9 Batch 555/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.971, Loss: 1.438 Epoch 9 Batch 556/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.972, Loss: 1.342 Epoch 9 Batch 557/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.971, Loss: 1.416 Epoch 9 Batch 558/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.971, Loss: 1.448 Epoch 9 Batch 559/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.972, Loss: 1.395 Epoch 9 Batch 560/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.967, Loss: 1.374 Epoch 9 Batch 561/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.971, Loss: 1.419 Epoch 9 Batch 562/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.971, Loss: 1.410 Epoch 9 Batch 563/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.971, Loss: 1.343 Epoch 9 Batch 564/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.971, Loss: 1.481 Epoch 9 Batch 565/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.969, Loss: 1.481 Epoch 9 Batch 566/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.973, Loss: 1.411 Epoch 9 Batch 567/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.973, Loss: 1.437 Epoch 9 Batch 568/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.968, Loss: 1.389 Epoch 9 Batch 569/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.968, Loss: 1.377 Epoch 9 Batch 570/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.966, Loss: 1.413 Epoch 9 Batch 571/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.966, Loss: 1.461 Epoch 9 Batch 572/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.961, Loss: 1.421 Epoch 9 Batch 573/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.966, Loss: 1.486 Epoch 9 Batch 574/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.966, Loss: 1.434 Epoch 9 Batch 575/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.966, Loss: 1.462 Epoch 9 Batch 576/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.964, Loss: 1.457 Epoch 9 Batch 577/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.969, Loss: 1.434 Epoch 9 Batch 578/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.968, Loss: 1.339 Epoch 9 Batch 579/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.968, Loss: 1.390 Epoch 9 Batch 580/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.351 Epoch 9 Batch 581/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.959, Loss: 1.464 Epoch 9 Batch 582/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.959, Loss: 1.454 Epoch 9 Batch 583/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.397 Epoch 9 Batch 584/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.961, Loss: 1.398 Epoch 9 Batch 585/1077 - Train Accuracy: 0.995, Validation Accuracy: 0.965, Loss: 1.354 Epoch 9 Batch 586/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.965, Loss: 1.462 Epoch 9 Batch 587/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.965, Loss: 1.454 Epoch 9 Batch 588/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.965, Loss: 1.406 Epoch 9 Batch 589/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.965, Loss: 1.474 Epoch 9 Batch 590/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.963, Loss: 1.422 Epoch 9 Batch 591/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.382 Epoch 9 Batch 592/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.965, Loss: 1.409 Epoch 9 Batch 593/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.960, Loss: 1.422 Epoch 9 Batch 594/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.452 Epoch 9 Batch 595/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.960, Loss: 1.404 Epoch 9 Batch 596/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.960, Loss: 1.468 Epoch 9 Batch 597/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.953, Loss: 1.369 Epoch 9 Batch 598/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.955, Loss: 1.377 Epoch 9 Batch 599/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.389 Epoch 9 Batch 600/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.472 Epoch 9 Batch 601/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.963, Loss: 1.379 Epoch 9 Batch 602/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.969, Loss: 1.386 Epoch 9 Batch 603/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.965, Loss: 1.423 Epoch 9 Batch 604/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.965, Loss: 1.499 Epoch 9 Batch 605/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.960, Loss: 1.472 Epoch 9 Batch 606/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.964, Loss: 1.368 Epoch 9 Batch 607/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.964, Loss: 1.394 Epoch 9 Batch 608/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.964, Loss: 1.374 Epoch 9 Batch 609/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.440 Epoch 9 Batch 610/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.432 Epoch 9 Batch 611/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.964, Loss: 1.432 Epoch 9 Batch 612/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.964, Loss: 1.421 Epoch 9 Batch 613/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.964, Loss: 1.449 Epoch 9 Batch 614/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.964, Loss: 1.404 Epoch 9 Batch 615/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.958, Loss: 1.421 Epoch 9 Batch 616/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.958, Loss: 1.423 Epoch 9 Batch 617/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.397 Epoch 9 Batch 618/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.963, Loss: 1.349 Epoch 9 Batch 619/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.968, Loss: 1.554 Epoch 9 Batch 620/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.968, Loss: 1.354 Epoch 9 Batch 621/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.459 Epoch 9 Batch 622/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.413 Epoch 9 Batch 623/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.330 Epoch 9 Batch 624/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.439 Epoch 9 Batch 625/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.964, Loss: 1.382 Epoch 9 Batch 626/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.402 Epoch 9 Batch 627/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.964, Loss: 1.472 Epoch 9 Batch 628/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.964, Loss: 1.438 Epoch 9 Batch 629/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.409 Epoch 9 Batch 630/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.959, Loss: 1.393 Epoch 9 Batch 631/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.959, Loss: 1.384 Epoch 9 Batch 632/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.412 Epoch 9 Batch 633/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.957, Loss: 1.393 Epoch 9 Batch 634/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.399 Epoch 9 Batch 635/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.957, Loss: 1.450 Epoch 9 Batch 636/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.962, Loss: 1.412 Epoch 9 Batch 637/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.959, Loss: 1.446 Epoch 9 Batch 638/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.959, Loss: 1.402 Epoch 9 Batch 639/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.959, Loss: 1.396 Epoch 9 Batch 640/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.959, Loss: 1.391 Epoch 9 Batch 641/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.405 Epoch 9 Batch 642/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.962, Loss: 1.431 Epoch 9 Batch 643/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.962, Loss: 1.513 Epoch 9 Batch 644/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.962, Loss: 1.384 Epoch 9 Batch 645/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.964, Loss: 1.356 Epoch 9 Batch 646/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.400 Epoch 9 Batch 647/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.953, Loss: 1.371 Epoch 9 Batch 648/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.953, Loss: 1.456 Epoch 9 Batch 649/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.953, Loss: 1.458 Epoch 9 Batch 650/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.960, Loss: 1.422 Epoch 9 Batch 651/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.955, Loss: 1.390 Epoch 9 Batch 652/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.959, Loss: 1.416 Epoch 9 Batch 653/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.441 Epoch 9 Batch 654/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.955, Loss: 1.387 Epoch 9 Batch 655/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.955, Loss: 1.481 Epoch 9 Batch 656/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.955, Loss: 1.441 Epoch 9 Batch 657/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.955, Loss: 1.507 Epoch 9 Batch 658/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.950, Loss: 1.381 Epoch 9 Batch 659/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.952, Loss: 1.415 Epoch 9 Batch 660/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.952, Loss: 1.426 Epoch 9 Batch 661/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.958, Loss: 1.464 Epoch 9 Batch 662/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.962, Loss: 1.400 Epoch 9 Batch 663/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.962, Loss: 1.397 Epoch 9 Batch 664/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.962, Loss: 1.409 Epoch 9 Batch 665/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.964, Loss: 1.334 Epoch 9 Batch 666/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.964, Loss: 1.431 Epoch 9 Batch 667/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.964, Loss: 1.449 Epoch 9 Batch 668/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.964, Loss: 1.492 Epoch 9 Batch 669/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.962, Loss: 1.402 Epoch 9 Batch 670/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.966, Loss: 1.537 Epoch 9 Batch 671/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.962, Loss: 1.464 Epoch 9 Batch 672/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.962, Loss: 1.480 Epoch 9 Batch 673/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.400 Epoch 9 Batch 674/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.958, Loss: 1.468 Epoch 9 Batch 675/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.956, Loss: 1.536 Epoch 9 Batch 676/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.956, Loss: 1.428 Epoch 9 Batch 677/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.417 Epoch 9 Batch 678/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.956, Loss: 1.341 Epoch 9 Batch 679/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.956, Loss: 1.428 Epoch 9 Batch 680/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.956, Loss: 1.503 Epoch 9 Batch 681/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.953, Loss: 1.404 Epoch 9 Batch 682/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.953, Loss: 1.464 Epoch 9 Batch 683/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.953, Loss: 1.410 Epoch 9 Batch 684/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.951, Loss: 1.484 Epoch 9 Batch 685/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.956, Loss: 1.404 Epoch 9 Batch 686/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.956, Loss: 1.501 Epoch 9 Batch 687/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.961, Loss: 1.462 Epoch 9 Batch 688/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.961, Loss: 1.432 Epoch 9 Batch 689/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.966, Loss: 1.428 Epoch 9 Batch 690/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.966, Loss: 1.371 Epoch 9 Batch 691/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.964, Loss: 1.392 Epoch 9 Batch 692/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.964, Loss: 1.455 Epoch 9 Batch 693/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.964, Loss: 1.424 Epoch 9 Batch 694/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.440 Epoch 9 Batch 695/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.452 Epoch 9 Batch 696/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.964, Loss: 1.411 Epoch 9 Batch 697/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.964, Loss: 1.492 Epoch 9 Batch 698/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.969, Loss: 1.406 Epoch 9 Batch 699/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.969, Loss: 1.463 Epoch 9 Batch 700/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.969, Loss: 1.462 Epoch 9 Batch 701/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.969, Loss: 1.375 Epoch 9 Batch 702/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.969, Loss: 1.442 Epoch 9 Batch 703/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.960, Loss: 1.462 Epoch 9 Batch 704/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.461 Epoch 9 Batch 705/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.384 Epoch 9 Batch 706/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.966, Loss: 1.493 Epoch 9 Batch 707/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.965, Loss: 1.456 Epoch 9 Batch 708/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.965, Loss: 1.397 Epoch 9 Batch 709/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.960, Loss: 1.417 Epoch 9 Batch 710/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.954, Loss: 1.423 Epoch 9 Batch 711/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.954, Loss: 1.468 Epoch 9 Batch 712/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.955, Loss: 1.417 Epoch 9 Batch 713/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.955, Loss: 1.460 Epoch 9 Batch 714/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.955, Loss: 1.462 Epoch 9 Batch 715/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.959, Loss: 1.426 Epoch 9 Batch 716/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.964, Loss: 1.371 Epoch 9 Batch 717/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.964, Loss: 1.428 Epoch 9 Batch 718/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.475 Epoch 9 Batch 719/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.960, Loss: 1.440 Epoch 9 Batch 720/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.403 Epoch 9 Batch 721/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.410 Epoch 9 Batch 722/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.955, Loss: 1.430 Epoch 9 Batch 723/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.955, Loss: 1.451 Epoch 9 Batch 724/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.956, Loss: 1.396 Epoch 9 Batch 725/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.951, Loss: 1.420 Epoch 9 Batch 726/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.958, Loss: 1.431 Epoch 9 Batch 727/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.958, Loss: 1.389 Epoch 9 Batch 728/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.428 Epoch 9 Batch 729/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.961, Loss: 1.445 Epoch 9 Batch 730/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.965, Loss: 1.476 Epoch 9 Batch 731/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.965, Loss: 1.553 Epoch 9 Batch 732/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.970, Loss: 1.373 Epoch 9 Batch 733/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.970, Loss: 1.542 Epoch 9 Batch 734/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.965, Loss: 1.458 Epoch 9 Batch 735/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.965, Loss: 1.412 Epoch 9 Batch 736/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.965, Loss: 1.420 Epoch 9 Batch 737/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.965, Loss: 1.425 Epoch 9 Batch 738/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.967, Loss: 1.363 Epoch 9 Batch 739/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.439 Epoch 9 Batch 740/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.963, Loss: 1.408 Epoch 9 Batch 741/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.343 Epoch 9 Batch 742/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.366 Epoch 9 Batch 743/1077 - Train Accuracy: 0.997, Validation Accuracy: 0.963, Loss: 1.342 Epoch 9 Batch 744/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.972, Loss: 1.399 Epoch 9 Batch 745/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.972, Loss: 1.435 Epoch 9 Batch 746/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.977, Loss: 1.454 Epoch 9 Batch 747/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.977, Loss: 1.441 Epoch 9 Batch 748/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.977, Loss: 1.412 Epoch 9 Batch 749/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.977, Loss: 1.431 Epoch 9 Batch 750/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.977, Loss: 1.348 Epoch 9 Batch 751/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.982, Loss: 1.439 Epoch 9 Batch 752/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.982, Loss: 1.443 Epoch 9 Batch 753/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.982, Loss: 1.391 Epoch 9 Batch 754/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.982, Loss: 1.364 Epoch 9 Batch 755/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.982, Loss: 1.472 Epoch 9 Batch 756/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.982, Loss: 1.442 Epoch 9 Batch 757/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.982, Loss: 1.412 Epoch 9 Batch 758/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.982, Loss: 1.364 Epoch 9 Batch 759/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.983, Loss: 1.448 Epoch 9 Batch 760/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.983, Loss: 1.423 Epoch 9 Batch 761/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.983, Loss: 1.375 Epoch 9 Batch 762/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.983, Loss: 1.486 Epoch 9 Batch 763/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.982, Loss: 1.432 Epoch 9 Batch 764/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.977, Loss: 1.491 Epoch 9 Batch 765/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.977, Loss: 1.437 Epoch 9 Batch 766/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.978, Loss: 1.420 Epoch 9 Batch 767/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.978, Loss: 1.440 Epoch 9 Batch 768/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.978, Loss: 1.420 Epoch 9 Batch 769/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.978, Loss: 1.293 Epoch 9 Batch 770/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.973, Loss: 1.430 Epoch 9 Batch 771/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.970, Loss: 1.418 Epoch 9 Batch 772/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.961, Loss: 1.387 Epoch 9 Batch 773/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.961, Loss: 1.428 Epoch 9 Batch 774/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.961, Loss: 1.434 Epoch 9 Batch 775/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.957, Loss: 1.461 Epoch 9 Batch 776/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.451 Epoch 9 Batch 777/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.957, Loss: 1.418 Epoch 9 Batch 778/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.956, Loss: 1.408 Epoch 9 Batch 779/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.959, Loss: 1.396 Epoch 9 Batch 780/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.959, Loss: 1.443 Epoch 9 Batch 781/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.956, Loss: 1.439 Epoch 9 Batch 782/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.385 Epoch 9 Batch 783/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.457 Epoch 9 Batch 784/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.965, Loss: 1.425 Epoch 9 Batch 785/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.965, Loss: 1.363 Epoch 9 Batch 786/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.963, Loss: 1.496 Epoch 9 Batch 787/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.960, Loss: 1.440 Epoch 9 Batch 788/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.429 Epoch 9 Batch 789/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.371 Epoch 9 Batch 790/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.964, Loss: 1.502 Epoch 9 Batch 791/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.388 Epoch 9 Batch 792/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.963, Loss: 1.403 Epoch 9 Batch 793/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.458 Epoch 9 Batch 794/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.960, Loss: 1.429 Epoch 9 Batch 795/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.417 Epoch 9 Batch 796/1077 - Train Accuracy: 0.994, Validation Accuracy: 0.956, Loss: 1.368 Epoch 9 Batch 797/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.411 Epoch 9 Batch 798/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.951, Loss: 1.497 Epoch 9 Batch 799/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.951, Loss: 1.443 Epoch 9 Batch 800/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.953, Loss: 1.438 Epoch 9 Batch 801/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.953, Loss: 1.391 Epoch 9 Batch 802/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.953, Loss: 1.443 Epoch 9 Batch 803/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.953, Loss: 1.396 Epoch 9 Batch 804/1077 - Train Accuracy: 0.992, Validation Accuracy: 0.962, Loss: 1.448 Epoch 9 Batch 805/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.967, Loss: 1.393 Epoch 9 Batch 806/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.967, Loss: 1.451 Epoch 9 Batch 807/1077 - Train Accuracy: 0.994, Validation Accuracy: 0.967, Loss: 1.408 Epoch 9 Batch 808/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.967, Loss: 1.469 Epoch 9 Batch 809/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.968, Loss: 1.466 Epoch 9 Batch 810/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.966, Loss: 1.410 Epoch 9 Batch 811/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.966, Loss: 1.416 Epoch 9 Batch 812/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.966, Loss: 1.411 Epoch 9 Batch 813/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.966, Loss: 1.450 Epoch 9 Batch 814/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.965, Loss: 1.423 Epoch 9 Batch 815/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.966, Loss: 1.416 Epoch 9 Batch 816/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.966, Loss: 1.473 Epoch 9 Batch 817/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.481 Epoch 9 Batch 818/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.447 Epoch 9 Batch 819/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.959, Loss: 1.444 Epoch 9 Batch 820/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.958, Loss: 1.363 Epoch 9 Batch 821/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.958, Loss: 1.419 Epoch 9 Batch 822/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.958, Loss: 1.419 Epoch 9 Batch 823/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.966, Loss: 1.419 Epoch 9 Batch 824/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.970, Loss: 1.445 Epoch 9 Batch 825/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.970, Loss: 1.442 Epoch 9 Batch 826/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.969, Loss: 1.440 Epoch 9 Batch 827/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.969, Loss: 1.340 Epoch 9 Batch 828/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.969, Loss: 1.369 Epoch 9 Batch 829/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.969, Loss: 1.439 Epoch 9 Batch 830/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.970, Loss: 1.432 Epoch 9 Batch 831/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.970, Loss: 1.441 Epoch 9 Batch 832/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.972, Loss: 1.469 Epoch 9 Batch 833/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.972, Loss: 1.447 Epoch 9 Batch 834/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.972, Loss: 1.478 Epoch 9 Batch 835/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.967, Loss: 1.413 Epoch 9 Batch 836/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.967, Loss: 1.491 Epoch 9 Batch 837/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.967, Loss: 1.487 Epoch 9 Batch 838/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.967, Loss: 1.416 Epoch 9 Batch 839/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.967, Loss: 1.454 Epoch 9 Batch 840/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.957, Loss: 1.419 Epoch 9 Batch 841/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.439 Epoch 9 Batch 842/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.476 Epoch 9 Batch 843/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.958, Loss: 1.351 Epoch 9 Batch 844/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.357 Epoch 9 Batch 845/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.958, Loss: 1.422 Epoch 9 Batch 846/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.378 Epoch 9 Batch 847/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.963, Loss: 1.541 Epoch 9 Batch 848/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.961, Loss: 1.487 Epoch 9 Batch 849/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.961, Loss: 1.444 Epoch 9 Batch 850/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.961, Loss: 1.484 Epoch 9 Batch 851/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.953, Loss: 1.430 Epoch 9 Batch 852/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.953, Loss: 1.498 Epoch 9 Batch 853/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.478 Epoch 9 Batch 854/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.953, Loss: 1.451 Epoch 9 Batch 855/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.958, Loss: 1.493 Epoch 9 Batch 856/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.958, Loss: 1.454 Epoch 9 Batch 857/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.428 Epoch 9 Batch 858/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.964, Loss: 1.438 Epoch 9 Batch 859/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.959, Loss: 1.403 Epoch 9 Batch 860/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.958, Loss: 1.407 Epoch 9 Batch 861/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.963, Loss: 1.486 Epoch 9 Batch 862/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.968, Loss: 1.454 Epoch 9 Batch 863/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.968, Loss: 1.405 Epoch 9 Batch 864/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.972, Loss: 1.343 Epoch 9 Batch 865/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.972, Loss: 1.522 Epoch 9 Batch 866/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.430 Epoch 9 Batch 867/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.972, Loss: 1.479 Epoch 9 Batch 868/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.972, Loss: 1.445 Epoch 9 Batch 869/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.973, Loss: 1.408 Epoch 9 Batch 870/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.974, Loss: 1.429 Epoch 9 Batch 871/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.974, Loss: 1.375 Epoch 9 Batch 872/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.974, Loss: 1.443 Epoch 9 Batch 873/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.969, Loss: 1.433 Epoch 9 Batch 874/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.969, Loss: 1.345 Epoch 9 Batch 875/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.974, Loss: 1.433 Epoch 9 Batch 876/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.974, Loss: 1.387 Epoch 9 Batch 877/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.974, Loss: 1.431 Epoch 9 Batch 878/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.974, Loss: 1.409 Epoch 9 Batch 879/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.974, Loss: 1.419 Epoch 9 Batch 880/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.974, Loss: 1.451 Epoch 9 Batch 881/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.974, Loss: 1.388 Epoch 9 Batch 882/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.974, Loss: 1.461 Epoch 9 Batch 883/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.974, Loss: 1.405 Epoch 9 Batch 884/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.974, Loss: 1.452 Epoch 9 Batch 885/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.974, Loss: 1.403 Epoch 9 Batch 886/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.976, Loss: 1.377 Epoch 9 Batch 887/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.976, Loss: 1.418 Epoch 9 Batch 888/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.976, Loss: 1.454 Epoch 9 Batch 889/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.974, Loss: 1.411 Epoch 9 Batch 890/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.970, Loss: 1.427 Epoch 9 Batch 891/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.975, Loss: 1.431 Epoch 9 Batch 892/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.975, Loss: 1.372 Epoch 9 Batch 893/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.975, Loss: 1.502 Epoch 9 Batch 894/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.975, Loss: 1.420 Epoch 9 Batch 895/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.978, Loss: 1.403 Epoch 9 Batch 896/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.973, Loss: 1.409 Epoch 9 Batch 897/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.978, Loss: 1.433 Epoch 9 Batch 898/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.982, Loss: 1.464 Epoch 9 Batch 899/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.982, Loss: 1.322 Epoch 9 Batch 900/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.978, Loss: 1.432 Epoch 9 Batch 901/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.978, Loss: 1.413 Epoch 9 Batch 902/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.978, Loss: 1.436 Epoch 9 Batch 903/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.978, Loss: 1.476 Epoch 9 Batch 904/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.977, Loss: 1.410 Epoch 9 Batch 905/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.977, Loss: 1.356 Epoch 9 Batch 906/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.973, Loss: 1.410 Epoch 9 Batch 907/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.973, Loss: 1.405 Epoch 9 Batch 908/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.973, Loss: 1.390 Epoch 9 Batch 909/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.973, Loss: 1.403 Epoch 9 Batch 910/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.977, Loss: 1.434 Epoch 9 Batch 911/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.977, Loss: 1.409 Epoch 9 Batch 912/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.977, Loss: 1.424 Epoch 9 Batch 913/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.977, Loss: 1.469 Epoch 9 Batch 914/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.977, Loss: 1.520 Epoch 9 Batch 915/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.977, Loss: 1.446 Epoch 9 Batch 916/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.977, Loss: 1.437 Epoch 9 Batch 917/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.977, Loss: 1.424 Epoch 9 Batch 918/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.972, Loss: 1.358 Epoch 9 Batch 919/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.972, Loss: 1.404 Epoch 9 Batch 920/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.972, Loss: 1.481 Epoch 9 Batch 921/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.972, Loss: 1.397 Epoch 9 Batch 922/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.972, Loss: 1.380 Epoch 9 Batch 923/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.972, Loss: 1.419 Epoch 9 Batch 924/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.973, Loss: 1.376 Epoch 9 Batch 925/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.973, Loss: 1.324 Epoch 9 Batch 926/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.973, Loss: 1.400 Epoch 9 Batch 927/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.973, Loss: 1.469 Epoch 9 Batch 928/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.978, Loss: 1.509 Epoch 9 Batch 929/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.975, Loss: 1.412 Epoch 9 Batch 930/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.975, Loss: 1.409 Epoch 9 Batch 931/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.975, Loss: 1.400 Epoch 9 Batch 932/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.975, Loss: 1.437 Epoch 9 Batch 933/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.975, Loss: 1.429 Epoch 9 Batch 934/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.975, Loss: 1.376 Epoch 9 Batch 935/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.974, Loss: 1.417 Epoch 9 Batch 936/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.974, Loss: 1.451 Epoch 9 Batch 937/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.974, Loss: 1.462 Epoch 9 Batch 938/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.974, Loss: 1.437 Epoch 9 Batch 939/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.974, Loss: 1.409 Epoch 9 Batch 940/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.974, Loss: 1.376 Epoch 9 Batch 941/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.974, Loss: 1.408 Epoch 9 Batch 942/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.972, Loss: 1.376 Epoch 9 Batch 943/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.972, Loss: 1.457 Epoch 9 Batch 944/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.972, Loss: 1.409 Epoch 9 Batch 945/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.972, Loss: 1.469 Epoch 9 Batch 946/1077 - Train Accuracy: 0.995, Validation Accuracy: 0.970, Loss: 1.391 Epoch 9 Batch 947/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.970, Loss: 1.503 Epoch 9 Batch 948/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.970, Loss: 1.380 Epoch 9 Batch 949/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.970, Loss: 1.429 Epoch 9 Batch 950/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.966, Loss: 1.368 Epoch 9 Batch 951/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.966, Loss: 1.445 Epoch 9 Batch 952/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.966, Loss: 1.401 Epoch 9 Batch 953/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.966, Loss: 1.472 Epoch 9 Batch 954/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.966, Loss: 1.374 Epoch 9 Batch 955/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.966, Loss: 1.437 Epoch 9 Batch 956/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.966, Loss: 1.500 Epoch 9 Batch 957/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.956, Loss: 1.415 Epoch 9 Batch 958/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.956, Loss: 1.396 Epoch 9 Batch 959/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.956, Loss: 1.389 Epoch 9 Batch 960/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.956, Loss: 1.382 Epoch 9 Batch 961/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.956, Loss: 1.434 Epoch 9 Batch 962/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.966, Loss: 1.366 Epoch 9 Batch 963/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.966, Loss: 1.369 Epoch 9 Batch 964/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.966, Loss: 1.409 Epoch 9 Batch 965/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.968, Loss: 1.367 Epoch 9 Batch 966/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.968, Loss: 1.426 Epoch 9 Batch 967/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.969, Loss: 1.401 Epoch 9 Batch 968/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.969, Loss: 1.434 Epoch 9 Batch 969/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.968, Loss: 1.390 Epoch 9 Batch 970/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.968, Loss: 1.493 Epoch 9 Batch 971/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.405 Epoch 9 Batch 972/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.437 Epoch 9 Batch 973/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.955, Loss: 1.497 Epoch 9 Batch 974/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.960, Loss: 1.435 Epoch 9 Batch 975/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.960, Loss: 1.418 Epoch 9 Batch 976/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.960, Loss: 1.445 Epoch 9 Batch 977/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.960, Loss: 1.427 Epoch 9 Batch 978/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.958, Loss: 1.381 Epoch 9 Batch 979/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.960, Loss: 1.478 Epoch 9 Batch 980/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.960, Loss: 1.413 Epoch 9 Batch 981/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.961, Loss: 1.408 Epoch 9 Batch 982/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.958, Loss: 1.387 Epoch 9 Batch 983/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.431 Epoch 9 Batch 984/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.972, Loss: 1.392 Epoch 9 Batch 985/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.967, Loss: 1.427 Epoch 9 Batch 986/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.966, Loss: 1.391 Epoch 9 Batch 987/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.965, Loss: 1.476 Epoch 9 Batch 988/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.965, Loss: 1.416 Epoch 9 Batch 989/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.965, Loss: 1.386 Epoch 9 Batch 990/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.960, Loss: 1.435 Epoch 9 Batch 991/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.964, Loss: 1.430 Epoch 9 Batch 992/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.967, Loss: 1.464 Epoch 9 Batch 993/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.962, Loss: 1.380 Epoch 9 Batch 994/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.967, Loss: 1.409 Epoch 9 Batch 995/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.967, Loss: 1.446 Epoch 9 Batch 996/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.967, Loss: 1.534 Epoch 9 Batch 997/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.966, Loss: 1.383 Epoch 9 Batch 998/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.961, Loss: 1.402 Epoch 9 Batch 999/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.966, Loss: 1.460 Epoch 9 Batch 1000/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.962, Loss: 1.445 Epoch 9 Batch 1001/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.966, Loss: 1.547 Epoch 9 Batch 1002/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.967, Loss: 1.361 Epoch 9 Batch 1003/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.957, Loss: 1.377 Epoch 9 Batch 1004/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.957, Loss: 1.467 Epoch 9 Batch 1005/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.961, Loss: 1.399 Epoch 9 Batch 1006/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.432 Epoch 9 Batch 1007/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.961, Loss: 1.399 Epoch 9 Batch 1008/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.962, Loss: 1.486 Epoch 9 Batch 1009/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.959, Loss: 1.353 Epoch 9 Batch 1010/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.959, Loss: 1.355 Epoch 9 Batch 1011/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.959, Loss: 1.443 Epoch 9 Batch 1012/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.966, Loss: 1.355 Epoch 9 Batch 1013/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.971, Loss: 1.453 Epoch 9 Batch 1014/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.969, Loss: 1.391 Epoch 9 Batch 1015/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.973, Loss: 1.452 Epoch 9 Batch 1016/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.968, Loss: 1.469 Epoch 9 Batch 1017/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.973, Loss: 1.473 Epoch 9 Batch 1018/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.973, Loss: 1.457 Epoch 9 Batch 1019/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.973, Loss: 1.521 Epoch 9 Batch 1020/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.969, Loss: 1.422 Epoch 9 Batch 1021/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.969, Loss: 1.478 Epoch 9 Batch 1022/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.967, Loss: 1.302 Epoch 9 Batch 1023/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.967, Loss: 1.448 Epoch 9 Batch 1024/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.967, Loss: 1.426 Epoch 9 Batch 1025/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.971, Loss: 1.374 Epoch 9 Batch 1026/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.976, Loss: 1.400 Epoch 9 Batch 1027/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.976, Loss: 1.456 Epoch 9 Batch 1028/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.980, Loss: 1.410 Epoch 9 Batch 1029/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.985, Loss: 1.458 Epoch 9 Batch 1030/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.983, Loss: 1.451 Epoch 9 Batch 1031/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.974, Loss: 1.480 Epoch 9 Batch 1032/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.974, Loss: 1.436 Epoch 9 Batch 1033/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.971, Loss: 1.512 Epoch 9 Batch 1034/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.972, Loss: 1.462 Epoch 9 Batch 1035/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.972, Loss: 1.445 Epoch 9 Batch 1036/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.972, Loss: 1.450 Epoch 9 Batch 1037/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.972, Loss: 1.380 Epoch 9 Batch 1038/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.972, Loss: 1.477 Epoch 9 Batch 1039/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.965, Loss: 1.499 Epoch 9 Batch 1040/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.479 Epoch 9 Batch 1041/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.432 Epoch 9 Batch 1042/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.968, Loss: 1.462 Epoch 9 Batch 1043/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.968, Loss: 1.433 Epoch 9 Batch 1044/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.967, Loss: 1.462 Epoch 9 Batch 1045/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.967, Loss: 1.423 Epoch 9 Batch 1046/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.967, Loss: 1.428 Epoch 9 Batch 1047/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.967, Loss: 1.420 Epoch 9 Batch 1048/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.967, Loss: 1.450 Epoch 9 Batch 1049/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.967, Loss: 1.429 Epoch 9 Batch 1050/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.967, Loss: 1.419 Epoch 9 Batch 1051/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.968, Loss: 1.445 Epoch 9 Batch 1052/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.447 Epoch 9 Batch 1053/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.968, Loss: 1.422 Epoch 9 Batch 1054/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.966, Loss: 1.388 Epoch 9 Batch 1055/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.969, Loss: 1.463 Epoch 9 Batch 1056/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.969, Loss: 1.433 Epoch 9 Batch 1057/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.969, Loss: 1.331 Epoch 9 Batch 1058/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.969, Loss: 1.450 Epoch 9 Batch 1059/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.969, Loss: 1.443 Epoch 9 Batch 1060/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.964, Loss: 1.413 Epoch 9 Batch 1061/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.969, Loss: 1.438 Epoch 9 Batch 1062/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.969, Loss: 1.377 Epoch 9 Batch 1063/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.969, Loss: 1.360 Epoch 9 Batch 1064/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.969, Loss: 1.385 Epoch 9 Batch 1065/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.969, Loss: 1.464 Epoch 9 Batch 1066/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.964, Loss: 1.321 Epoch 9 Batch 1067/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.964, Loss: 1.411 Epoch 9 Batch 1068/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.452 Epoch 9 Batch 1069/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.964, Loss: 1.494 Epoch 9 Batch 1070/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.964, Loss: 1.486 Epoch 9 Batch 1071/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.440 Epoch 9 Batch 1072/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.966, Loss: 1.427 Epoch 9 Batch 1073/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.966, Loss: 1.565 Epoch 9 Batch 1074/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.443 Epoch 9 Batch 1075/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.968, Loss: 1.380 Model Trained and Saved Save Parameters Save the batch_size and save_path parameters for inference. \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" # Save parameters for checkpoint helper . save_params(save_path) Checkpoint \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" import tensorflow as tf import numpy as np import helper import problem_unittests as tests _, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = helper . load_preprocess() load_path = helper . load_params() Sentence to Sequence To feed a sentence into the model for translation, you first need to preprocess it. Implement the function sentence_to_seq() to preprocess new sentences. Convert the sentence to lowercase Convert words into ids using vocab_to_int Convert words not in the vocabulary, to the <UNK> word id. def sentence_to_seq (sentence, vocab_to_int): \"\"\" Convert a sentence to a sequence of ids :param sentence: String :param vocab_to_int: Dictionary to go from the words to an id :return: List of word ids \"\"\" # TODO: Implement Function sequence = [vocab_to_int . get(word, vocab_to_int[ '<UNK>' ]) for word in sentence . lower() . split()] return sequence \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_sentence_to_seq(sentence_to_seq) Tests Passed Translate This will translate translate_sentence from English to French. translate_sentence = 'he saw a old yellow truck .' \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int) loaded_graph = tf . Graph() with tf . Session(graph = loaded_graph) as sess: # Load saved model loader = tf . train . import_meta_graph(load_path + '.meta' ) loader . restore(sess, load_path) input_data = loaded_graph . get_tensor_by_name( 'input:0' ) logits = loaded_graph . get_tensor_by_name( 'logits:0' ) keep_prob = loaded_graph . get_tensor_by_name( 'keep_prob:0' ) translate_logits = sess . run(logits, {input_data: [translate_sentence], keep_prob: 1.0 })[ 0 ] print ( 'Input' ) print ( ' Word Ids: {}' . format([i for i in translate_sentence])) print ( ' English Words: {}' . format([source_int_to_vocab[i] for i in translate_sentence])) print ( ' \\n Prediction' ) print ( ' Word Ids: {}' . format([i for i in np . argmax(translate_logits, 1 )])) print ( ' French Words: {}' . format([target_int_to_vocab[i] for i in np . argmax(translate_logits, 1 )])) Input Word Ids: [86, 172, 194, 216, 38, 155, 7] English Words: ['he', 'saw', 'a', 'old', 'yellow', 'truck', '.'] Prediction Word Ids: [202, 157, 46, 271, 49, 141, 352, 4, 1] French Words: ['il', 'a', 'vu', 'un', 'vieux', 'camion', 'jaune', '.', '&lt;EOS&gt;'] Imperfect Translation You might notice that some sentences translate better than others. Since the dataset you're using only has a vocabulary of 227 English words of the thousands that you use, you're only going to see good results using these words. For this project, you don't need a perfect translation. However, if you want to create a better translation model, you'll need better data. You can train on the WMT10 French-English corpus . This dataset has more vocabulary and richer in topics discussed. However, this will take you days to train, so make sure you've a GPU and the neural network is performing well on dataset we provided. Just make sure you play with the WMT10 corpus after you've submitted this project. Submitting This Project When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_language_translation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission.","title":"Language Translator"},{"location":"dl/translator/dlnd_language_translation/#language-translation","text":"In this project, you\u2019re going to take a peek into the realm of neural network machine translation. You\u2019ll be training a sequence to sequence model on a dataset of English and French sentences that can translate new sentences from English to French.","title":"Language Translation"},{"location":"dl/translator/dlnd_language_translation/#get-the-data","text":"Since translating the whole language of English to French will take lots of time to train, we have provided you with a small portion of the English corpus. \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" import helper import problem_unittests as tests source_path = 'data/small_vocab_en' target_path = 'data/small_vocab_fr' source_text = helper . load_data(source_path) target_text = helper . load_data(target_path)","title":"Get the Data"},{"location":"dl/translator/dlnd_language_translation/#explore-the-data","text":"Play around with view_sentence_range to view different parts of the data. view_sentence_range = ( 0 , 10 ) \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" import numpy as np print ( 'Dataset Stats' ) print ( 'Roughly the number of unique words: {}' . format( len ({word: None for word in source_text . split()}))) sentences = source_text . split( ' \\n ' ) word_counts = [ len (sentence . split()) for sentence in sentences] print ( 'Number of sentences: {}' . format( len (sentences))) print ( 'Average number of words in a sentence: {}' . format(np . average(word_counts))) print () print ( 'English sentences {} to {}:' . format( * view_sentence_range)) print ( ' \\n ' . join(source_text . split( ' \\n ' )[view_sentence_range[ 0 ]:view_sentence_range[ 1 ]])) print () print ( 'French sentences {} to {}:' . format( * view_sentence_range)) print ( ' \\n ' . join(target_text . split( ' \\n ' )[view_sentence_range[ 0 ]:view_sentence_range[ 1 ]])) Dataset Stats Roughly the number of unique words: 227 Number of sentences: 137861 Average number of words in a sentence: 13.225277634719028 English sentences 0 to 10: new jersey is sometimes quiet during autumn , and it is snowy in april . the united states is usually chilly during july , and it is usually freezing in november . california is usually quiet during march , and it is usually hot in june . the united states is sometimes mild during june , and it is cold in september . your least liked fruit is the grape , but my least liked is the apple . his favorite fruit is the orange , but my favorite is the grape . paris is relaxing during december , but it is usually chilly in july . new jersey is busy during spring , and it is never hot in march . our least liked fruit is the lemon , but my least liked is the grape . the united states is sometimes busy during january , and it is sometimes warm in november . French sentences 0 to 10: new jersey est parfois calme pendant l' automne , et il est neigeux en avril . les \u00e9tats-unis est g\u00e9n\u00e9ralement froid en juillet , et il g\u00e8le habituellement en novembre . california est g\u00e9n\u00e9ralement calme en mars , et il est g\u00e9n\u00e9ralement chaud en juin . les \u00e9tats-unis est parfois l\u00e9g\u00e8re en juin , et il fait froid en septembre . votre moins aim\u00e9 fruit est le raisin , mais mon moins aim\u00e9 est la pomme . son fruit pr\u00e9f\u00e9r\u00e9 est l'orange , mais mon pr\u00e9f\u00e9r\u00e9 est le raisin . paris est relaxant en d\u00e9cembre , mais il est g\u00e9n\u00e9ralement froid en juillet . new jersey est occup\u00e9 au printemps , et il est jamais chaude en mars . notre fruit est moins aim\u00e9 le citron , mais mon moins aim\u00e9 est le raisin . les \u00e9tats-unis est parfois occup\u00e9 en janvier , et il est parfois chaud en novembre .","title":"Explore the Data"},{"location":"dl/translator/dlnd_language_translation/#implement-preprocessing-function","text":"","title":"Implement Preprocessing Function"},{"location":"dl/translator/dlnd_language_translation/#text-to-word-ids","text":"As you did with other RNNs, you must turn the text into a number so the computer can understand it. In the function text_to_ids() , you'll turn source_text and target_text from words to ids. However, you need to add the <EOS> word id at the end of each sentence from target_text . This will help the neural network predict when the sentence should end. You can get the <EOS> word id by doing: target_vocab_to_int[ '<EOS>' ] You can get other word ids using source_vocab_to_int and target_vocab_to_int . def text_to_ids (source_text, target_text, source_vocab_to_int, target_vocab_to_int): \"\"\" Convert source and target text to proper word ids :param source_text: String that contains all the source text. :param target_text: String that contains all the target text. :param source_vocab_to_int: Dictionary to go from the source words to an id :param target_vocab_to_int: Dictionary to go from the target words to an id :return: A tuple of lists (source_id_text, target_id_text) \"\"\" # TODO: Implement Function '''st = sentence; wd = word''' st_source = [st for st in source_text . split( ' \\n ' )] st_target = [st + ' <EOS>' for st in target_text . split( ' \\n ' )] source_id_text = [[source_vocab_to_int[wd] for wd in st . split()] for st in st_source] target_id_text = [[target_vocab_to_int[wd] for wd in st . split()] for st in st_target] return source_id_text, target_id_text \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_text_to_ids(text_to_ids) Tests Passed","title":"Text to Word Ids"},{"location":"dl/translator/dlnd_language_translation/#preprocess-all-the-data-and-save-it","text":"Running the code cell below will preprocess all the data and save it to file. \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" helper . preprocess_and_save_data(source_path, target_path, text_to_ids)","title":"Preprocess all the data and save it"},{"location":"dl/translator/dlnd_language_translation/#check-point","text":"This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk. \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" import numpy as np import helper (source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper . load_preprocess()","title":"Check Point"},{"location":"dl/translator/dlnd_language_translation/#check-the-version-of-tensorflow-and-access-to-gpu","text":"This will check to make sure you have the correct version of TensorFlow and access to a GPU \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" from distutils.version import LooseVersion import warnings import tensorflow as tf # Check TensorFlow Version assert LooseVersion(tf . __version__) in [LooseVersion( '1.0.0' ), LooseVersion( '1.0.1' )], 'This project requires TensorFlow version 1.0 You are using {}' . format(tf . __version__) print ( 'TensorFlow Version: {}' . format(tf . __version__)) # Check for a GPU if not tf . test . gpu_device_name(): warnings . warn( 'No GPU found. Please use a GPU to train your neural network.' ) else : print ( 'Default GPU Device: {}' . format(tf . test . gpu_device_name())) TensorFlow Version: 1.0.0 Default GPU Device: /gpu:0","title":"Check the Version of TensorFlow and Access to GPU"},{"location":"dl/translator/dlnd_language_translation/#build-the-neural-network","text":"You'll build the components necessary to build a Sequence-to-Sequence model by implementing the following functions below: - model_inputs - process_decoding_input - encoding_layer - decoding_layer_train - decoding_layer_infer - decoding_layer - seq2seq_model","title":"Build the Neural Network"},{"location":"dl/translator/dlnd_language_translation/#input","text":"Implement the model_inputs() function to create TF Placeholders for the Neural Network. It should create the following placeholders: Input text placeholder named \"input\" using the TF Placeholder name parameter with rank 2. Targets placeholder with rank 2. Learning rate placeholder with rank 0. Keep probability placeholder named \"keep_prob\" using the TF Placeholder name parameter with rank 0. Return the placeholders in the following the tuple (Input, Targets, Learing Rate, Keep Probability) def model_inputs (): \"\"\" Create TF Placeholders for input, targets, and learning rate. :return: Tuple (input, targets, learning rate, keep probability) \"\"\" # TODO: Implement Function input_ = tf . placeholder(tf . int32, [ None , None ], 'input' ) target_ = tf . placeholder(tf . int32, [ None , None ], 'target' ) learning_rate_ = tf . placeholder(tf . float32, None , 'lr' ) keep_prob_ = tf . placeholder(tf . float32, None , 'keep_prob' ) return input_, target_, learning_rate_, keep_prob_ \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_model_inputs(model_inputs) Tests Passed","title":"Input"},{"location":"dl/translator/dlnd_language_translation/#process-decoding-input","text":"Implement process_decoding_input using TensorFlow to remove the last word id from each batch in target_data and concat the GO ID to the begining of each batch. def process_decoding_input (target_data, target_vocab_to_int, batch_size): \"\"\" Preprocess target data for dencoding :param target_data: Target Placehoder :param target_vocab_to_int: Dictionary to go from the target words to an id :param batch_size: Batch Size :return: Preprocessed target data \"\"\" # TODO: Implement Function A = tf . strided_slice(target_data, [ 0 , 0 ], [batch_size, - 1 ], [ 1 , 1 ]) B = tf . fill([batch_size, 1 ], target_vocab_to_int[ '<GO>' ]) C = tf . concat([B, A], 1 ) return C \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_process_decoding_input(process_decoding_input) Tests Passed","title":"Process Decoding Input"},{"location":"dl/translator/dlnd_language_translation/#encoding","text":"Implement encoding_layer() to create a Encoder RNN layer using tf.nn.dynamic_rnn() . def encoding_layer (rnn_inputs, rnn_size, num_layers, keep_prob): \"\"\" Create encoding layer :param rnn_inputs: Inputs for the RNN :param rnn_size: RNN Size :param num_layers: Number of layers :param keep_prob: Dropout keep probability :return: RNN state \"\"\" # TODO: Implement Function lstm = tf . contrib . rnn . BasicLSTMCell(rnn_size) dropout = tf . contrib . rnn . DropoutWrapper(lstm, keep_prob) cell = tf . contrib . rnn . MultiRNNCell([dropout] * num_layers) outputs, final_state = tf . nn . dynamic_rnn(cell, rnn_inputs, dtype = tf . float32) return final_state \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_encoding_layer(encoding_layer) Tests Passed","title":"Encoding"},{"location":"dl/translator/dlnd_language_translation/#decoding-training","text":"Create training logits using tf.contrib.seq2seq.simple_decoder_fn_train() and tf.contrib.seq2seq.dynamic_rnn_decoder() . Apply the output_fn to the tf.contrib.seq2seq.dynamic_rnn_decoder() outputs. def decoding_layer_train (encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob): \"\"\" Create a decoding layer for training :param encoder_state: Encoder State :param dec_cell: Decoder RNN Cell :param dec_embed_input: Decoder embedded input :param sequence_length: Sequence Length :param decoding_scope: TenorFlow Variable Scope for decoding :param output_fn: Function to apply the output layer :param keep_prob: Dropout keep probability :return: Train Logits \"\"\" # TODO: Implement Function # 1. Create training logits using tf.contrib.seq2seq.simple_decoder_fn_train() simple_dec_fn_train = tf . contrib . seq2seq . simple_decoder_fn_train(encoder_state) # 2. and tf.contrib.seq2seq.dynamic_rnn_decoder() and # Apply the output_fn to the tf.contrib.seq2seq.dynamic_rnn_decoder() outputs. outputs_train, final_state, final_context_state = \\ tf . contrib . seq2seq . dynamic_rnn_decoder(cell = dec_cell, decoder_fn = simple_dec_fn_train, inputs = dec_embed_input, sequence_length = sequence_length, scope = decoding_scope) logits_train = output_fn(outputs_train) logits = tf . nn . dropout(logits_train, keep_prob) return logits \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_decoding_layer_train(decoding_layer_train) Tests Passed","title":"Decoding - Training"},{"location":"dl/translator/dlnd_language_translation/#decoding-inference","text":"Create inference logits using tf.contrib.seq2seq.simple_decoder_fn_inference() and tf.contrib.seq2seq.dynamic_rnn_decoder() . def decoding_layer_infer (encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size, decoding_scope, output_fn, keep_prob): \"\"\" Create a decoding layer for inference :param encoder_state: Encoder state :param dec_cell: Decoder RNN Cell :param dec_embeddings: Decoder embeddings :param start_of_sequence_id: GO ID :param end_of_sequence_id: EOS Id :param maximum_length: The maximum allowed time steps to decode :param vocab_size: Size of vocabulary :param decoding_scope: TensorFlow Variable Scope for decoding :param output_fn: Function to apply the output layer :param keep_prob: Dropout keep probability :return: Inference Logits \"\"\" # TODO: Implement Function # Create inference logits using tf.contrib.seq2seq.simple_decoder_fn_inference() simple_dec_fn_infer = tf . contrib . seq2seq . simple_decoder_fn_inference(output_fn, encoder_state, dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size) # add dropout layer dropout = tf . contrib . rnn . DropoutWrapper(dec_cell, keep_prob) # and use tf.contrib.seq2seq.dynamic_rnn_decoder(). logits_infer,final_state, final_context_state = \\ tf . contrib . seq2seq . dynamic_rnn_decoder(dropout, simple_dec_fn_infer, sequence_length = maximum_length, scope = decoding_scope) return logits_infer \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_decoding_layer_infer(decoding_layer_infer) Tests Passed","title":"Decoding - Inference"},{"location":"dl/translator/dlnd_language_translation/#build-the-decoding-layer","text":"Implement decoding_layer() to create a Decoder RNN layer. Create RNN cell for decoding using rnn_size and num_layers . Create the output fuction using lambda to transform it's input, logits, to class logits. Use the your decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob) function to get the training logits. Use your decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size, decoding_scope, output_fn, keep_prob) function to get the inference logits. Note: You'll need to use tf.variable_scope to share variables between training and inference. def decoding_layer (dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size, num_layers, target_vocab_to_int, keep_prob): \"\"\" Create decoding layer :param dec_embed_input: Decoder embedded input :param dec_embeddings: Decoder embeddings :param encoder_state: The encoded state :param vocab_size: Size of vocabulary :param sequence_length: Sequence Length :param rnn_size: RNN Size :param num_layers: Number of layers :param target_vocab_to_int: Dictionary to go from the target words to an id :param keep_prob: Dropout keep probability :return: Tuple of (Training Logits, Inference Logits) \"\"\" # TODO: Implement Function # 1. Create RNN cell for decoding using rnn_size and num_layers. lstm = tf . contrib . rnn . BasicLSTMCell(rnn_size) dropout = tf . contrib . rnn . DropoutWrapper(lstm, keep_prob) dec_cell = tf . contrib . rnn . MultiRNNCell([dropout] * num_layers) max_target_sentence_length = max ([ len (sentence) for sentence in source_int_text]) # 2. Create the output fuction using lambda to transform it's input, logits, to class logits. with tf . variable_scope( 'decoding_layer' ) as decoding_scope: output_fn = lambda x: tf . contrib . layers . fully_connected(x,\\ num_outputs = vocab_size,\\ activation_fn = None ,\\ scope = decoding_scope) # 3. Use the your decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, # decoding_scope, output_fn, keep_prob) function to get the training logits. with tf . variable_scope( 'decoding_layer' ) as decoding_scope: logits_train = decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob) # 4. Use your decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, # end_of_sequence_id, maximum_length, vocab_size, decoding_scope, output_fn, keep_prob) # function to get the inference logits. with tf . variable_scope( 'decoding_layer' , reuse = True ) as decoding_scope: logits_infer = decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, target_vocab_to_int[ '<GO>' ], target_vocab_to_int[ '<EOS>' ], max_target_sentence_length, vocab_size, decoding_scope, output_fn, keep_prob) return logits_train, logits_infer \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_decoding_layer(decoding_layer) Tests Passed","title":"Build the Decoding Layer"},{"location":"dl/translator/dlnd_language_translation/#build-the-neural-network_1","text":"Apply the functions you implemented above to: Apply embedding to the input data for the encoder. Encode the input using your encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob) . Process target data using your process_decoding_input(target_data, target_vocab_to_int, batch_size) function. Apply embedding to the target data for the decoder. Decode the encoded input using your decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size, num_layers, target_vocab_to_int, keep_prob) . def seq2seq_model (input_data, target_data, keep_prob, batch_size, sequence_length, source_vocab_size, target_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers, target_vocab_to_int): \"\"\" Build the Sequence-to-Sequence part of the neural network :param input_data: Input placeholder :param target_data: Target placeholder :param keep_prob: Dropout keep probability placeholder :param batch_size: Batch Size :param sequence_length: Sequence Length :param source_vocab_size: Source vocabulary size :param target_vocab_size: Target vocabulary size :param enc_embedding_size: Decoder embedding size :param dec_embedding_size: Encoder embedding size :param rnn_size: RNN Size :param num_layers: Number of layers :param target_vocab_to_int: Dictionary to go from the target words to an id :return: Tuple of (Training Logits, Inference Logits) \"\"\" # TODO: Implement Function # 1. Apply embedding to the input data for the encoder. enc_embed_input = tf . contrib . layers . embed_sequence(input_data,\\ source_vocab_size,\\ enc_embedding_size) # 2. Encode the input using your encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob). enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob) # 3. Process target data using your process_decoding_input(target_data, target_vocab_to_int, batch_size) function. dec_input = process_decoding_input(target_data, target_vocab_to_int, batch_size) # 4. Apply embedding to the target data for the decoder. dec_embeddings = tf . Variable(tf . truncated_normal([target_vocab_size, dec_embedding_size], stddev = 0.01 )) # 5. Decode the encoded input using your decoding_layer(dec_embed_input, dec_embeddings, encoder_state, # vocab_size, sequence_length, rnn_size, num_layers, target_vocab_to_int, keep_prob). dec_embed_input = tf . nn . embedding_lookup(dec_embeddings, dec_input) logits_train, logits_infer = decoding_layer(dec_embed_input, dec_embeddings, enc_state, target_vocab_size, sequence_length, rnn_size, num_layers, target_vocab_to_int, keep_prob) return logits_train, logits_infer \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_seq2seq_model(seq2seq_model) Tests Passed","title":"Build the Neural Network"},{"location":"dl/translator/dlnd_language_translation/#neural-network-training","text":"","title":"Neural Network Training"},{"location":"dl/translator/dlnd_language_translation/#hyperparameters","text":"Tune the following parameters: Set epochs to the number of epochs. Set batch_size to the batch size. Set rnn_size to the size of the RNNs. Set num_layers to the number of layers. Set encoding_embedding_size to the size of the embedding for the encoder. Set decoding_embedding_size to the size of the embedding for the decoder. Set learning_rate to the learning rate. Set keep_probability to the Dropout keep probability # Number of Epochs epochs = 10 # Batch Size batch_size = 128 # RNN Size rnn_size = 300 # Number of Layers num_layers = 3 # Embedding Size encoding_embedding_size = 160 decoding_embedding_size = 160 # Learning Rate learning_rate = 0.001 # Dropout Keep Probability keep_probability = 0.70","title":"Hyperparameters"},{"location":"dl/translator/dlnd_language_translation/#build-the-graph","text":"Build the graph using the neural network you implemented. \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" save_path = 'checkpoints/dev' (source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper . load_preprocess() max_source_sentence_length = max ([ len (sentence) for sentence in source_int_text]) train_graph = tf . Graph() with train_graph . as_default(): input_data, targets, lr, keep_prob = model_inputs() sequence_length = tf . placeholder_with_default(max_source_sentence_length, None , name = 'sequence_length' ) input_shape = tf . shape(input_data) train_logits, inference_logits = seq2seq_model( tf . reverse(input_data, [ - 1 ]), targets, keep_prob, batch_size, sequence_length, len (source_vocab_to_int), len (target_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, target_vocab_to_int) tf . identity(inference_logits, 'logits' ) with tf . name_scope( \"optimization\" ): # Loss function cost = tf . contrib . seq2seq . sequence_loss( train_logits, targets, tf . ones([input_shape[ 0 ], sequence_length])) # Optimizer optimizer = tf . train . AdamOptimizer(lr) # Gradient Clipping gradients = optimizer . compute_gradients(cost) capped_gradients = [(tf . clip_by_value(grad, - 1. , 1. ), var) for grad, var in gradients if grad is not None ] train_op = optimizer . apply_gradients(capped_gradients)","title":"Build the Graph"},{"location":"dl/translator/dlnd_language_translation/#train","text":"Train the neural network on the preprocessed data. If you have a hard time getting a good loss, check the forms to see if anyone is having the same problem. \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" import time def get_accuracy (target, logits): \"\"\" Calculate accuracy \"\"\" max_seq = max (target . shape[ 1 ], logits . shape[ 1 ]) if max_seq - target . shape[ 1 ]: target = np . pad( target, [( 0 , 0 ),( 0 ,max_seq - target . shape[ 1 ])], 'constant' ) if max_seq - logits . shape[ 1 ]: logits = np . pad( logits, [( 0 , 0 ),( 0 ,max_seq - logits . shape[ 1 ]), ( 0 , 0 )], 'constant' ) return np . mean(np . equal(target, np . argmax(logits, 2 ))) train_source = source_int_text[batch_size:] train_target = target_int_text[batch_size:] valid_source = helper . pad_sentence_batch(source_int_text[:batch_size]) valid_target = helper . pad_sentence_batch(target_int_text[:batch_size]) with tf . Session(graph = train_graph) as sess: sess . run(tf . global_variables_initializer()) for epoch_i in range (epochs): for batch_i, (source_batch, target_batch) in enumerate ( helper . batch_data(train_source, train_target, batch_size)): start_time = time . time() _, loss = sess . run( [train_op, cost], {input_data: source_batch, targets: target_batch, lr: learning_rate, sequence_length: target_batch . shape[ 1 ], keep_prob: keep_probability}) batch_train_logits = sess . run( inference_logits, {input_data: source_batch, keep_prob: 1.0 }) batch_valid_logits = sess . run( inference_logits, {input_data: valid_source, keep_prob: 1.0 }) train_acc = get_accuracy(target_batch, batch_train_logits) valid_acc = get_accuracy(np . array(valid_target), batch_valid_logits) end_time = time . time() print ( 'Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.3f}, Validation Accuracy: {:>6.3f}, Loss: {:>6.3f}' . format(epoch_i, batch_i, len (source_int_text) // batch_size, train_acc, valid_acc, loss)) # Save Model saver = tf . train . Saver() saver . save(sess, save_path) print ( 'Model Trained and Saved' ) Epoch 0 Batch 0/1077 - Train Accuracy: 0.324, Validation Accuracy: 0.335, Loss: 5.881 Epoch 0 Batch 1/1077 - Train Accuracy: 0.255, Validation Accuracy: 0.335, Loss: 5.863 Epoch 0 Batch 2/1077 - Train Accuracy: 0.244, Validation Accuracy: 0.335, Loss: 5.794 Epoch 0 Batch 3/1077 - Train Accuracy: 0.250, Validation Accuracy: 0.319, Loss: 5.441 Epoch 0 Batch 4/1077 - Train Accuracy: 0.262, Validation Accuracy: 0.336, Loss: 4.921 Epoch 0 Batch 5/1077 - Train Accuracy: 0.296, Validation Accuracy: 0.339, Loss: 4.560 Epoch 0 Batch 6/1077 - Train Accuracy: 0.283, Validation Accuracy: 0.343, Loss: 4.586 Epoch 0 Batch 7/1077 - Train Accuracy: 0.262, Validation Accuracy: 0.337, Loss: 4.475 Epoch 0 Batch 8/1077 - Train Accuracy: 0.279, Validation Accuracy: 0.347, Loss: 4.324 Epoch 0 Batch 9/1077 - Train Accuracy: 0.315, Validation Accuracy: 0.375, Loss: 4.254 Epoch 0 Batch 10/1077 - Train Accuracy: 0.270, Validation Accuracy: 0.368, Loss: 4.311 Epoch 0 Batch 11/1077 - Train Accuracy: 0.329, Validation Accuracy: 0.368, Loss: 4.150 Epoch 0 Batch 12/1077 - Train Accuracy: 0.311, Validation Accuracy: 0.376, Loss: 4.131 Epoch 0 Batch 13/1077 - Train Accuracy: 0.358, Validation Accuracy: 0.377, Loss: 3.920 Epoch 0 Batch 14/1077 - Train Accuracy: 0.331, Validation Accuracy: 0.374, Loss: 3.901 Epoch 0 Batch 15/1077 - Train Accuracy: 0.337, Validation Accuracy: 0.401, Loss: 4.001 Epoch 0 Batch 16/1077 - Train Accuracy: 0.352, Validation Accuracy: 0.397, Loss: 3.933 Epoch 0 Batch 17/1077 - Train Accuracy: 0.368, Validation Accuracy: 0.412, Loss: 3.860 Epoch 0 Batch 18/1077 - Train Accuracy: 0.354, Validation Accuracy: 0.419, Loss: 3.938 Epoch 0 Batch 19/1077 - Train Accuracy: 0.378, Validation Accuracy: 0.422, Loss: 3.882 Epoch 0 Batch 20/1077 - Train Accuracy: 0.355, Validation Accuracy: 0.411, Loss: 3.861 Epoch 0 Batch 21/1077 - Train Accuracy: 0.333, Validation Accuracy: 0.413, Loss: 3.866 Epoch 0 Batch 22/1077 - Train Accuracy: 0.355, Validation Accuracy: 0.421, Loss: 3.920 Epoch 0 Batch 23/1077 - Train Accuracy: 0.365, Validation Accuracy: 0.425, Loss: 3.797 Epoch 0 Batch 24/1077 - Train Accuracy: 0.371, Validation Accuracy: 0.424, Loss: 3.814 Epoch 0 Batch 25/1077 - Train Accuracy: 0.364, Validation Accuracy: 0.422, Loss: 3.780 Epoch 0 Batch 26/1077 - Train Accuracy: 0.361, Validation Accuracy: 0.429, Loss: 3.723 Epoch 0 Batch 27/1077 - Train Accuracy: 0.420, Validation Accuracy: 0.438, Loss: 3.564 Epoch 0 Batch 28/1077 - Train Accuracy: 0.398, Validation Accuracy: 0.436, Loss: 3.627 Epoch 0 Batch 29/1077 - Train Accuracy: 0.384, Validation Accuracy: 0.434, Loss: 3.669 Epoch 0 Batch 30/1077 - Train Accuracy: 0.391, Validation Accuracy: 0.453, Loss: 3.635 Epoch 0 Batch 31/1077 - Train Accuracy: 0.388, Validation Accuracy: 0.457, Loss: 3.665 Epoch 0 Batch 32/1077 - Train Accuracy: 0.435, Validation Accuracy: 0.454, Loss: 3.496 Epoch 0 Batch 33/1077 - Train Accuracy: 0.422, Validation Accuracy: 0.454, Loss: 3.503 Epoch 0 Batch 34/1077 - Train Accuracy: 0.398, Validation Accuracy: 0.458, Loss: 3.511 Epoch 0 Batch 35/1077 - Train Accuracy: 0.406, Validation Accuracy: 0.456, Loss: 3.537 Epoch 0 Batch 36/1077 - Train Accuracy: 0.407, Validation Accuracy: 0.459, Loss: 3.471 Epoch 0 Batch 37/1077 - Train Accuracy: 0.387, Validation Accuracy: 0.445, Loss: 3.545 Epoch 0 Batch 38/1077 - Train Accuracy: 0.378, Validation Accuracy: 0.469, Loss: 3.708 Epoch 0 Batch 39/1077 - Train Accuracy: 0.395, Validation Accuracy: 0.453, Loss: 3.466 Epoch 0 Batch 40/1077 - Train Accuracy: 0.411, Validation Accuracy: 0.467, Loss: 3.501 Epoch 0 Batch 41/1077 - Train Accuracy: 0.420, Validation Accuracy: 0.454, Loss: 3.426 Epoch 0 Batch 42/1077 - Train Accuracy: 0.416, Validation Accuracy: 0.473, Loss: 3.434 Epoch 0 Batch 43/1077 - Train Accuracy: 0.396, Validation Accuracy: 0.462, Loss: 3.523 Epoch 0 Batch 44/1077 - Train Accuracy: 0.372, Validation Accuracy: 0.474, Loss: 3.563 Epoch 0 Batch 45/1077 - Train Accuracy: 0.403, Validation Accuracy: 0.471, Loss: 3.408 Epoch 0 Batch 46/1077 - Train Accuracy: 0.396, Validation Accuracy: 0.465, Loss: 3.455 Epoch 0 Batch 47/1077 - Train Accuracy: 0.430, Validation Accuracy: 0.475, Loss: 3.303 Epoch 0 Batch 48/1077 - Train Accuracy: 0.410, Validation Accuracy: 0.464, Loss: 3.370 Epoch 0 Batch 49/1077 - Train Accuracy: 0.411, Validation Accuracy: 0.477, Loss: 3.365 Epoch 0 Batch 50/1077 - Train Accuracy: 0.363, Validation Accuracy: 0.445, Loss: 3.501 Epoch 0 Batch 51/1077 - Train Accuracy: 0.435, Validation Accuracy: 0.479, Loss: 3.434 Epoch 0 Batch 52/1077 - Train Accuracy: 0.407, Validation Accuracy: 0.471, Loss: 3.370 Epoch 0 Batch 53/1077 - Train Accuracy: 0.393, Validation Accuracy: 0.440, Loss: 3.417 Epoch 0 Batch 54/1077 - Train Accuracy: 0.375, Validation Accuracy: 0.467, Loss: 3.545 Epoch 0 Batch 55/1077 - Train Accuracy: 0.452, Validation Accuracy: 0.478, Loss: 3.253 Epoch 0 Batch 56/1077 - Train Accuracy: 0.405, Validation Accuracy: 0.466, Loss: 3.331 Epoch 0 Batch 57/1077 - Train Accuracy: 0.454, Validation Accuracy: 0.459, Loss: 3.199 Epoch 0 Batch 58/1077 - Train Accuracy: 0.424, Validation Accuracy: 0.474, Loss: 3.389 Epoch 0 Batch 59/1077 - Train Accuracy: 0.395, Validation Accuracy: 0.485, Loss: 3.356 Epoch 0 Batch 60/1077 - Train Accuracy: 0.406, Validation Accuracy: 0.461, Loss: 3.284 Epoch 0 Batch 61/1077 - Train Accuracy: 0.406, Validation Accuracy: 0.477, Loss: 3.289 Epoch 0 Batch 62/1077 - Train Accuracy: 0.410, Validation Accuracy: 0.486, Loss: 3.302 Epoch 0 Batch 63/1077 - Train Accuracy: 0.446, Validation Accuracy: 0.473, Loss: 3.155 Epoch 0 Batch 64/1077 - Train Accuracy: 0.389, Validation Accuracy: 0.466, Loss: 3.280 Epoch 0 Batch 65/1077 - Train Accuracy: 0.400, Validation Accuracy: 0.480, Loss: 3.324 Epoch 0 Batch 66/1077 - Train Accuracy: 0.415, Validation Accuracy: 0.485, Loss: 3.242 Epoch 0 Batch 67/1077 - Train Accuracy: 0.429, Validation Accuracy: 0.462, Loss: 3.176 Epoch 0 Batch 68/1077 - Train Accuracy: 0.396, Validation Accuracy: 0.484, Loss: 3.244 Epoch 0 Batch 69/1077 - Train Accuracy: 0.459, Validation Accuracy: 0.488, Loss: 3.250 Epoch 0 Batch 70/1077 - Train Accuracy: 0.377, Validation Accuracy: 0.480, Loss: 3.334 Epoch 0 Batch 71/1077 - Train Accuracy: 0.388, Validation Accuracy: 0.459, Loss: 3.225 Epoch 0 Batch 72/1077 - Train Accuracy: 0.424, Validation Accuracy: 0.483, Loss: 3.248 Epoch 0 Batch 73/1077 - Train Accuracy: 0.427, Validation Accuracy: 0.479, Loss: 3.168 Epoch 0 Batch 74/1077 - Train Accuracy: 0.420, Validation Accuracy: 0.466, Loss: 3.127 Epoch 0 Batch 75/1077 - Train Accuracy: 0.424, Validation Accuracy: 0.471, Loss: 3.063 Epoch 0 Batch 76/1077 - Train Accuracy: 0.435, Validation Accuracy: 0.494, Loss: 3.139 Epoch 0 Batch 77/1077 - Train Accuracy: 0.416, Validation Accuracy: 0.492, Loss: 3.183 Epoch 0 Batch 78/1077 - Train Accuracy: 0.386, Validation Accuracy: 0.482, Loss: 3.269 Epoch 0 Batch 79/1077 - Train Accuracy: 0.417, Validation Accuracy: 0.482, Loss: 3.213 Epoch 0 Batch 80/1077 - Train Accuracy: 0.440, Validation Accuracy: 0.484, Loss: 3.123 Epoch 0 Batch 81/1077 - Train Accuracy: 0.452, Validation Accuracy: 0.488, Loss: 3.096 Epoch 0 Batch 82/1077 - Train Accuracy: 0.473, Validation Accuracy: 0.484, Loss: 3.003 Epoch 0 Batch 83/1077 - Train Accuracy: 0.428, Validation Accuracy: 0.505, Loss: 3.298 Epoch 0 Batch 84/1077 - Train Accuracy: 0.454, Validation Accuracy: 0.504, Loss: 3.154 Epoch 0 Batch 85/1077 - Train Accuracy: 0.443, Validation Accuracy: 0.487, Loss: 3.131 Epoch 0 Batch 86/1077 - Train Accuracy: 0.425, Validation Accuracy: 0.484, Loss: 3.144 Epoch 0 Batch 87/1077 - Train Accuracy: 0.430, Validation Accuracy: 0.496, Loss: 3.128 Epoch 0 Batch 88/1077 - Train Accuracy: 0.425, Validation Accuracy: 0.491, Loss: 3.140 Epoch 0 Batch 89/1077 - Train Accuracy: 0.417, Validation Accuracy: 0.488, Loss: 3.143 Epoch 0 Batch 90/1077 - Train Accuracy: 0.424, Validation Accuracy: 0.493, Loss: 3.137 Epoch 0 Batch 91/1077 - Train Accuracy: 0.483, Validation Accuracy: 0.493, Loss: 2.936 Epoch 0 Batch 92/1077 - Train Accuracy: 0.439, Validation Accuracy: 0.485, Loss: 3.099 Epoch 0 Batch 93/1077 - Train Accuracy: 0.439, Validation Accuracy: 0.500, Loss: 3.179 Epoch 0 Batch 94/1077 - Train Accuracy: 0.467, Validation Accuracy: 0.517, Loss: 3.060 Epoch 0 Batch 95/1077 - Train Accuracy: 0.480, Validation Accuracy: 0.504, Loss: 3.046 Epoch 0 Batch 96/1077 - Train Accuracy: 0.445, Validation Accuracy: 0.487, Loss: 3.108 Epoch 0 Batch 97/1077 - Train Accuracy: 0.441, Validation Accuracy: 0.484, Loss: 3.093 Epoch 0 Batch 98/1077 - Train Accuracy: 0.469, Validation Accuracy: 0.480, Loss: 2.999 Epoch 0 Batch 99/1077 - Train Accuracy: 0.414, Validation Accuracy: 0.489, Loss: 3.196 Epoch 0 Batch 100/1077 - Train Accuracy: 0.444, Validation Accuracy: 0.500, Loss: 3.061 Epoch 0 Batch 101/1077 - Train Accuracy: 0.444, Validation Accuracy: 0.505, Loss: 3.070 Epoch 0 Batch 102/1077 - Train Accuracy: 0.478, Validation Accuracy: 0.513, Loss: 3.038 Epoch 0 Batch 103/1077 - Train Accuracy: 0.391, Validation Accuracy: 0.493, Loss: 3.172 Epoch 0 Batch 104/1077 - Train Accuracy: 0.373, Validation Accuracy: 0.495, Loss: 3.147 Epoch 0 Batch 105/1077 - Train Accuracy: 0.450, Validation Accuracy: 0.496, Loss: 3.061 Epoch 0 Batch 106/1077 - Train Accuracy: 0.440, Validation Accuracy: 0.494, Loss: 3.163 Epoch 0 Batch 107/1077 - Train Accuracy: 0.467, Validation Accuracy: 0.494, Loss: 3.001 Epoch 0 Batch 108/1077 - Train Accuracy: 0.483, Validation Accuracy: 0.505, Loss: 2.941 Epoch 0 Batch 109/1077 - Train Accuracy: 0.459, Validation Accuracy: 0.512, Loss: 3.009 Epoch 0 Batch 110/1077 - Train Accuracy: 0.471, Validation Accuracy: 0.503, Loss: 3.016 Epoch 0 Batch 111/1077 - Train Accuracy: 0.430, Validation Accuracy: 0.494, Loss: 2.948 Epoch 0 Batch 112/1077 - Train Accuracy: 0.453, Validation Accuracy: 0.507, Loss: 3.041 Epoch 0 Batch 113/1077 - Train Accuracy: 0.444, Validation Accuracy: 0.517, Loss: 3.017 Epoch 0 Batch 114/1077 - Train Accuracy: 0.431, Validation Accuracy: 0.475, Loss: 2.991 Epoch 0 Batch 115/1077 - Train Accuracy: 0.449, Validation Accuracy: 0.504, Loss: 3.087 Epoch 0 Batch 116/1077 - Train Accuracy: 0.423, Validation Accuracy: 0.512, Loss: 3.088 Epoch 0 Batch 117/1077 - Train Accuracy: 0.400, Validation Accuracy: 0.502, Loss: 3.048 Epoch 0 Batch 118/1077 - Train Accuracy: 0.410, Validation Accuracy: 0.507, Loss: 3.071 Epoch 0 Batch 119/1077 - Train Accuracy: 0.425, Validation Accuracy: 0.494, Loss: 2.953 Epoch 0 Batch 120/1077 - Train Accuracy: 0.450, Validation Accuracy: 0.506, Loss: 2.916 Epoch 0 Batch 121/1077 - Train Accuracy: 0.450, Validation Accuracy: 0.501, Loss: 2.997 Epoch 0 Batch 122/1077 - Train Accuracy: 0.455, Validation Accuracy: 0.513, Loss: 2.959 Epoch 0 Batch 123/1077 - Train Accuracy: 0.471, Validation Accuracy: 0.503, Loss: 2.831 Epoch 0 Batch 124/1077 - Train Accuracy: 0.461, Validation Accuracy: 0.515, Loss: 2.974 Epoch 0 Batch 125/1077 - Train Accuracy: 0.480, Validation Accuracy: 0.516, Loss: 2.979 Epoch 0 Batch 126/1077 - Train Accuracy: 0.460, Validation Accuracy: 0.512, Loss: 2.883 Epoch 0 Batch 127/1077 - Train Accuracy: 0.473, Validation Accuracy: 0.508, Loss: 2.933 Epoch 0 Batch 128/1077 - Train Accuracy: 0.510, Validation Accuracy: 0.516, Loss: 2.865 Epoch 0 Batch 129/1077 - Train Accuracy: 0.476, Validation Accuracy: 0.520, Loss: 2.884 Epoch 0 Batch 130/1077 - Train Accuracy: 0.464, Validation Accuracy: 0.495, Loss: 2.766 Epoch 0 Batch 131/1077 - Train Accuracy: 0.412, Validation Accuracy: 0.485, Loss: 3.086 Epoch 0 Batch 132/1077 - Train Accuracy: 0.438, Validation Accuracy: 0.496, Loss: 3.078 Epoch 0 Batch 133/1077 - Train Accuracy: 0.402, Validation Accuracy: 0.478, Loss: 2.878 Epoch 0 Batch 134/1077 - Train Accuracy: 0.442, Validation Accuracy: 0.505, Loss: 2.896 Epoch 0 Batch 135/1077 - Train Accuracy: 0.422, Validation Accuracy: 0.496, Loss: 2.939 Epoch 0 Batch 136/1077 - Train Accuracy: 0.441, Validation Accuracy: 0.500, Loss: 2.915 Epoch 0 Batch 137/1077 - Train Accuracy: 0.482, Validation Accuracy: 0.512, Loss: 2.830 Epoch 0 Batch 138/1077 - Train Accuracy: 0.400, Validation Accuracy: 0.458, Loss: 2.795 Epoch 0 Batch 139/1077 - Train Accuracy: 0.457, Validation Accuracy: 0.523, Loss: 2.912 Epoch 0 Batch 140/1077 - Train Accuracy: 0.435, Validation Accuracy: 0.517, Loss: 2.929 Epoch 0 Batch 141/1077 - Train Accuracy: 0.428, Validation Accuracy: 0.495, Loss: 2.901 Epoch 0 Batch 142/1077 - Train Accuracy: 0.451, Validation Accuracy: 0.491, Loss: 2.780 Epoch 0 Batch 143/1077 - Train Accuracy: 0.459, Validation Accuracy: 0.503, Loss: 2.844 Epoch 0 Batch 144/1077 - Train Accuracy: 0.431, Validation Accuracy: 0.517, Loss: 2.852 Epoch 0 Batch 145/1077 - Train Accuracy: 0.458, Validation Accuracy: 0.494, Loss: 2.820 Epoch 0 Batch 146/1077 - Train Accuracy: 0.475, Validation Accuracy: 0.511, Loss: 2.766 Epoch 0 Batch 147/1077 - Train Accuracy: 0.454, Validation Accuracy: 0.520, Loss: 2.858 Epoch 0 Batch 148/1077 - Train Accuracy: 0.473, Validation Accuracy: 0.526, Loss: 2.847 Epoch 0 Batch 149/1077 - Train Accuracy: 0.447, Validation Accuracy: 0.520, Loss: 2.797 Epoch 0 Batch 150/1077 - Train Accuracy: 0.474, Validation Accuracy: 0.504, Loss: 2.802 Epoch 0 Batch 151/1077 - Train Accuracy: 0.465, Validation Accuracy: 0.493, Loss: 2.763 Epoch 0 Batch 152/1077 - Train Accuracy: 0.457, Validation Accuracy: 0.515, Loss: 2.793 Epoch 0 Batch 153/1077 - Train Accuracy: 0.464, Validation Accuracy: 0.527, Loss: 2.790 Epoch 0 Batch 154/1077 - Train Accuracy: 0.429, Validation Accuracy: 0.517, Loss: 2.857 Epoch 0 Batch 155/1077 - Train Accuracy: 0.461, Validation Accuracy: 0.519, Loss: 2.711 Epoch 0 Batch 156/1077 - Train Accuracy: 0.440, Validation Accuracy: 0.487, Loss: 2.786 Epoch 0 Batch 157/1077 - Train Accuracy: 0.481, Validation Accuracy: 0.522, Loss: 2.722 Epoch 0 Batch 158/1077 - Train Accuracy: 0.465, Validation Accuracy: 0.513, Loss: 2.696 Epoch 0 Batch 159/1077 - Train Accuracy: 0.475, Validation Accuracy: 0.514, Loss: 2.706 Epoch 0 Batch 160/1077 - Train Accuracy: 0.461, Validation Accuracy: 0.516, Loss: 2.727 Epoch 0 Batch 161/1077 - Train Accuracy: 0.421, Validation Accuracy: 0.500, Loss: 2.726 Epoch 0 Batch 162/1077 - Train Accuracy: 0.410, Validation Accuracy: 0.458, Loss: 2.779 Epoch 0 Batch 163/1077 - Train Accuracy: 0.404, Validation Accuracy: 0.494, Loss: 2.744 Epoch 0 Batch 164/1077 - Train Accuracy: 0.452, Validation Accuracy: 0.501, Loss: 2.767 Epoch 0 Batch 165/1077 - Train Accuracy: 0.413, Validation Accuracy: 0.482, Loss: 2.738 Epoch 0 Batch 166/1077 - Train Accuracy: 0.440, Validation Accuracy: 0.460, Loss: 2.720 Epoch 0 Batch 167/1077 - Train Accuracy: 0.421, Validation Accuracy: 0.503, Loss: 2.758 Epoch 0 Batch 168/1077 - Train Accuracy: 0.417, Validation Accuracy: 0.486, Loss: 2.875 Epoch 0 Batch 169/1077 - Train Accuracy: 0.462, Validation Accuracy: 0.467, Loss: 2.651 Epoch 0 Batch 170/1077 - Train Accuracy: 0.417, Validation Accuracy: 0.501, Loss: 2.728 Epoch 0 Batch 171/1077 - Train Accuracy: 0.492, Validation Accuracy: 0.490, Loss: 2.636 Epoch 0 Batch 172/1077 - Train Accuracy: 0.447, Validation Accuracy: 0.485, Loss: 2.644 Epoch 0 Batch 173/1077 - Train Accuracy: 0.397, Validation Accuracy: 0.507, Loss: 2.783 Epoch 0 Batch 174/1077 - Train Accuracy: 0.464, Validation Accuracy: 0.500, Loss: 2.660 Epoch 0 Batch 175/1077 - Train Accuracy: 0.451, Validation Accuracy: 0.500, Loss: 2.614 Epoch 0 Batch 176/1077 - Train Accuracy: 0.420, Validation Accuracy: 0.504, Loss: 2.628 Epoch 0 Batch 177/1077 - Train Accuracy: 0.409, Validation Accuracy: 0.479, Loss: 2.617 Epoch 0 Batch 178/1077 - Train Accuracy: 0.461, Validation Accuracy: 0.495, Loss: 2.629 Epoch 0 Batch 179/1077 - Train Accuracy: 0.457, Validation Accuracy: 0.503, Loss: 2.729 Epoch 0 Batch 180/1077 - Train Accuracy: 0.427, Validation Accuracy: 0.499, Loss: 2.572 Epoch 0 Batch 181/1077 - Train Accuracy: 0.448, Validation Accuracy: 0.490, Loss: 2.650 Epoch 0 Batch 182/1077 - Train Accuracy: 0.453, Validation Accuracy: 0.494, Loss: 2.675 Epoch 0 Batch 183/1077 - Train Accuracy: 0.405, Validation Accuracy: 0.473, Loss: 2.686 Epoch 0 Batch 184/1077 - Train Accuracy: 0.418, Validation Accuracy: 0.471, Loss: 2.578 Epoch 0 Batch 185/1077 - Train Accuracy: 0.429, Validation Accuracy: 0.488, Loss: 2.613 Epoch 0 Batch 186/1077 - Train Accuracy: 0.467, Validation Accuracy: 0.491, Loss: 2.695 Epoch 0 Batch 187/1077 - Train Accuracy: 0.441, Validation Accuracy: 0.511, Loss: 2.600 Epoch 0 Batch 188/1077 - Train Accuracy: 0.488, Validation Accuracy: 0.522, Loss: 2.568 Epoch 0 Batch 189/1077 - Train Accuracy: 0.470, Validation Accuracy: 0.513, Loss: 2.628 Epoch 0 Batch 190/1077 - Train Accuracy: 0.447, Validation Accuracy: 0.495, Loss: 2.651 Epoch 0 Batch 191/1077 - Train Accuracy: 0.483, Validation Accuracy: 0.499, Loss: 2.526 Epoch 0 Batch 192/1077 - Train Accuracy: 0.487, Validation Accuracy: 0.508, Loss: 2.561 Epoch 0 Batch 193/1077 - Train Accuracy: 0.483, Validation Accuracy: 0.500, Loss: 2.508 Epoch 0 Batch 194/1077 - Train Accuracy: 0.474, Validation Accuracy: 0.498, Loss: 2.527 Epoch 0 Batch 195/1077 - Train Accuracy: 0.432, Validation Accuracy: 0.494, Loss: 2.622 Epoch 0 Batch 196/1077 - Train Accuracy: 0.463, Validation Accuracy: 0.493, Loss: 2.579 Epoch 0 Batch 197/1077 - Train Accuracy: 0.470, Validation Accuracy: 0.512, Loss: 2.551 Epoch 0 Batch 198/1077 - Train Accuracy: 0.465, Validation Accuracy: 0.516, Loss: 2.452 Epoch 0 Batch 199/1077 - Train Accuracy: 0.446, Validation Accuracy: 0.502, Loss: 2.616 Epoch 0 Batch 200/1077 - Train Accuracy: 0.427, Validation Accuracy: 0.502, Loss: 2.638 Epoch 0 Batch 201/1077 - Train Accuracy: 0.465, Validation Accuracy: 0.506, Loss: 2.516 Epoch 0 Batch 202/1077 - Train Accuracy: 0.466, Validation Accuracy: 0.512, Loss: 2.592 Epoch 0 Batch 203/1077 - Train Accuracy: 0.444, Validation Accuracy: 0.510, Loss: 2.571 Epoch 0 Batch 204/1077 - Train Accuracy: 0.453, Validation Accuracy: 0.503, Loss: 2.534 Epoch 0 Batch 205/1077 - Train Accuracy: 0.501, Validation Accuracy: 0.520, Loss: 2.555 Epoch 0 Batch 206/1077 - Train Accuracy: 0.440, Validation Accuracy: 0.498, Loss: 2.576 Epoch 0 Batch 207/1077 - Train Accuracy: 0.416, Validation Accuracy: 0.498, Loss: 2.555 Epoch 0 Batch 208/1077 - Train Accuracy: 0.456, Validation Accuracy: 0.498, Loss: 2.488 Epoch 0 Batch 209/1077 - Train Accuracy: 0.487, Validation Accuracy: 0.517, Loss: 2.456 Epoch 0 Batch 210/1077 - Train Accuracy: 0.489, Validation Accuracy: 0.507, Loss: 2.489 Epoch 0 Batch 211/1077 - Train Accuracy: 0.452, Validation Accuracy: 0.506, Loss: 2.500 Epoch 0 Batch 212/1077 - Train Accuracy: 0.492, Validation Accuracy: 0.509, Loss: 2.491 Epoch 0 Batch 213/1077 - Train Accuracy: 0.466, Validation Accuracy: 0.495, Loss: 2.541 Epoch 0 Batch 214/1077 - Train Accuracy: 0.458, Validation Accuracy: 0.522, Loss: 2.521 Epoch 0 Batch 215/1077 - Train Accuracy: 0.441, Validation Accuracy: 0.511, Loss: 2.559 Epoch 0 Batch 216/1077 - Train Accuracy: 0.447, Validation Accuracy: 0.514, Loss: 2.551 Epoch 0 Batch 217/1077 - Train Accuracy: 0.507, Validation Accuracy: 0.526, Loss: 2.466 Epoch 0 Batch 218/1077 - Train Accuracy: 0.436, Validation Accuracy: 0.511, Loss: 2.608 Epoch 0 Batch 219/1077 - Train Accuracy: 0.478, Validation Accuracy: 0.501, Loss: 2.484 Epoch 0 Batch 220/1077 - Train Accuracy: 0.387, Validation Accuracy: 0.466, Loss: 2.624 Epoch 0 Batch 221/1077 - Train Accuracy: 0.428, Validation Accuracy: 0.475, Loss: 2.521 Epoch 0 Batch 222/1077 - Train Accuracy: 0.400, Validation Accuracy: 0.472, Loss: 2.802 Epoch 0 Batch 223/1077 - Train Accuracy: 0.456, Validation Accuracy: 0.489, Loss: 2.458 Epoch 0 Batch 224/1077 - Train Accuracy: 0.460, Validation Accuracy: 0.489, Loss: 2.531 Epoch 0 Batch 225/1077 - Train Accuracy: 0.421, Validation Accuracy: 0.474, Loss: 2.490 Epoch 0 Batch 226/1077 - Train Accuracy: 0.454, Validation Accuracy: 0.479, Loss: 2.438 Epoch 0 Batch 227/1077 - Train Accuracy: 0.430, Validation Accuracy: 0.481, Loss: 2.647 Epoch 0 Batch 228/1077 - Train Accuracy: 0.474, Validation Accuracy: 0.491, Loss: 2.517 Epoch 0 Batch 229/1077 - Train Accuracy: 0.488, Validation Accuracy: 0.495, Loss: 2.410 Epoch 0 Batch 230/1077 - Train Accuracy: 0.469, Validation Accuracy: 0.500, Loss: 2.465 Epoch 0 Batch 231/1077 - Train Accuracy: 0.479, Validation Accuracy: 0.509, Loss: 2.504 Epoch 0 Batch 232/1077 - Train Accuracy: 0.441, Validation Accuracy: 0.521, Loss: 2.536 Epoch 0 Batch 233/1077 - Train Accuracy: 0.448, Validation Accuracy: 0.520, Loss: 2.596 Epoch 0 Batch 234/1077 - Train Accuracy: 0.504, Validation Accuracy: 0.517, Loss: 2.499 Epoch 0 Batch 235/1077 - Train Accuracy: 0.487, Validation Accuracy: 0.489, Loss: 2.387 Epoch 0 Batch 236/1077 - Train Accuracy: 0.439, Validation Accuracy: 0.487, Loss: 2.506 Epoch 0 Batch 237/1077 - Train Accuracy: 0.474, Validation Accuracy: 0.485, Loss: 2.439 Epoch 0 Batch 238/1077 - Train Accuracy: 0.495, Validation Accuracy: 0.511, Loss: 2.472 Epoch 0 Batch 239/1077 - Train Accuracy: 0.475, Validation Accuracy: 0.500, Loss: 2.405 Epoch 0 Batch 240/1077 - Train Accuracy: 0.464, Validation Accuracy: 0.518, Loss: 2.482 Epoch 0 Batch 241/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.523, Loss: 2.443 Epoch 0 Batch 242/1077 - Train Accuracy: 0.439, Validation Accuracy: 0.515, Loss: 2.458 Epoch 0 Batch 243/1077 - Train Accuracy: 0.425, Validation Accuracy: 0.510, Loss: 2.519 Epoch 0 Batch 244/1077 - Train Accuracy: 0.505, Validation Accuracy: 0.489, Loss: 2.400 Epoch 0 Batch 245/1077 - Train Accuracy: 0.462, Validation Accuracy: 0.491, Loss: 2.388 Epoch 0 Batch 246/1077 - Train Accuracy: 0.416, Validation Accuracy: 0.485, Loss: 2.460 Epoch 0 Batch 247/1077 - Train Accuracy: 0.448, Validation Accuracy: 0.469, Loss: 2.405 Epoch 0 Batch 248/1077 - Train Accuracy: 0.433, Validation Accuracy: 0.477, Loss: 2.421 Epoch 0 Batch 249/1077 - Train Accuracy: 0.478, Validation Accuracy: 0.498, Loss: 2.431 Epoch 0 Batch 250/1077 - Train Accuracy: 0.487, Validation Accuracy: 0.509, Loss: 2.365 Epoch 0 Batch 251/1077 - Train Accuracy: 0.485, Validation Accuracy: 0.493, Loss: 2.412 Epoch 0 Batch 252/1077 - Train Accuracy: 0.460, Validation Accuracy: 0.480, Loss: 2.385 Epoch 0 Batch 253/1077 - Train Accuracy: 0.498, Validation Accuracy: 0.508, Loss: 2.384 Epoch 0 Batch 254/1077 - Train Accuracy: 0.448, Validation Accuracy: 0.495, Loss: 2.477 Epoch 0 Batch 255/1077 - Train Accuracy: 0.437, Validation Accuracy: 0.461, Loss: 2.371 Epoch 0 Batch 256/1077 - Train Accuracy: 0.415, Validation Accuracy: 0.483, Loss: 2.489 Epoch 0 Batch 257/1077 - Train Accuracy: 0.465, Validation Accuracy: 0.498, Loss: 2.416 Epoch 0 Batch 258/1077 - Train Accuracy: 0.471, Validation Accuracy: 0.495, Loss: 2.289 Epoch 0 Batch 259/1077 - Train Accuracy: 0.425, Validation Accuracy: 0.482, Loss: 2.361 Epoch 0 Batch 260/1077 - Train Accuracy: 0.443, Validation Accuracy: 0.450, Loss: 2.374 Epoch 0 Batch 261/1077 - Train Accuracy: 0.448, Validation Accuracy: 0.451, Loss: 2.415 Epoch 0 Batch 262/1077 - Train Accuracy: 0.438, Validation Accuracy: 0.473, Loss: 2.495 Epoch 0 Batch 263/1077 - Train Accuracy: 0.496, Validation Accuracy: 0.478, Loss: 2.344 Epoch 0 Batch 264/1077 - Train Accuracy: 0.420, Validation Accuracy: 0.467, Loss: 2.482 Epoch 0 Batch 265/1077 - Train Accuracy: 0.441, Validation Accuracy: 0.464, Loss: 2.425 Epoch 0 Batch 266/1077 - Train Accuracy: 0.453, Validation Accuracy: 0.472, Loss: 2.342 Epoch 0 Batch 267/1077 - Train Accuracy: 0.464, Validation Accuracy: 0.483, Loss: 2.280 Epoch 0 Batch 268/1077 - Train Accuracy: 0.470, Validation Accuracy: 0.497, Loss: 2.363 Epoch 0 Batch 269/1077 - Train Accuracy: 0.423, Validation Accuracy: 0.506, Loss: 2.423 Epoch 0 Batch 270/1077 - Train Accuracy: 0.441, Validation Accuracy: 0.499, Loss: 2.378 Epoch 0 Batch 271/1077 - Train Accuracy: 0.432, Validation Accuracy: 0.489, Loss: 2.392 Epoch 0 Batch 272/1077 - Train Accuracy: 0.457, Validation Accuracy: 0.490, Loss: 2.350 Epoch 0 Batch 273/1077 - Train Accuracy: 0.458, Validation Accuracy: 0.494, Loss: 2.377 Epoch 0 Batch 274/1077 - Train Accuracy: 0.464, Validation Accuracy: 0.497, Loss: 2.381 Epoch 0 Batch 275/1077 - Train Accuracy: 0.472, Validation Accuracy: 0.495, Loss: 2.283 Epoch 0 Batch 276/1077 - Train Accuracy: 0.459, Validation Accuracy: 0.518, Loss: 2.413 Epoch 0 Batch 277/1077 - Train Accuracy: 0.482, Validation Accuracy: 0.476, Loss: 2.297 Epoch 0 Batch 278/1077 - Train Accuracy: 0.420, Validation Accuracy: 0.465, Loss: 2.365 Epoch 0 Batch 279/1077 - Train Accuracy: 0.433, Validation Accuracy: 0.482, Loss: 2.397 Epoch 0 Batch 280/1077 - Train Accuracy: 0.452, Validation Accuracy: 0.494, Loss: 2.420 Epoch 0 Batch 281/1077 - Train Accuracy: 0.447, Validation Accuracy: 0.496, Loss: 2.414 Epoch 0 Batch 282/1077 - Train Accuracy: 0.436, Validation Accuracy: 0.487, Loss: 2.399 Epoch 0 Batch 283/1077 - Train Accuracy: 0.459, Validation Accuracy: 0.489, Loss: 2.398 Epoch 0 Batch 284/1077 - Train Accuracy: 0.457, Validation Accuracy: 0.483, Loss: 2.361 Epoch 0 Batch 285/1077 - Train Accuracy: 0.466, Validation Accuracy: 0.471, Loss: 2.320 Epoch 0 Batch 286/1077 - Train Accuracy: 0.468, Validation Accuracy: 0.479, Loss: 2.293 Epoch 0 Batch 287/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.538, Loss: 2.251 Epoch 0 Batch 288/1077 - Train Accuracy: 0.470, Validation Accuracy: 0.534, Loss: 2.360 Epoch 0 Batch 289/1077 - Train Accuracy: 0.491, Validation Accuracy: 0.528, Loss: 2.336 Epoch 0 Batch 290/1077 - Train Accuracy: 0.446, Validation Accuracy: 0.513, Loss: 2.392 Epoch 0 Batch 291/1077 - Train Accuracy: 0.477, Validation Accuracy: 0.510, Loss: 2.332 Epoch 0 Batch 292/1077 - Train Accuracy: 0.512, Validation Accuracy: 0.501, Loss: 2.338 Epoch 0 Batch 293/1077 - Train Accuracy: 0.407, Validation Accuracy: 0.480, Loss: 2.418 Epoch 0 Batch 294/1077 - Train Accuracy: 0.498, Validation Accuracy: 0.492, Loss: 2.256 Epoch 0 Batch 295/1077 - Train Accuracy: 0.455, Validation Accuracy: 0.487, Loss: 2.356 Epoch 0 Batch 296/1077 - Train Accuracy: 0.481, Validation Accuracy: 0.492, Loss: 2.292 Epoch 0 Batch 297/1077 - Train Accuracy: 0.431, Validation Accuracy: 0.489, Loss: 2.382 Epoch 0 Batch 298/1077 - Train Accuracy: 0.470, Validation Accuracy: 0.484, Loss: 2.368 Epoch 0 Batch 299/1077 - Train Accuracy: 0.447, Validation Accuracy: 0.476, Loss: 2.335 Epoch 0 Batch 300/1077 - Train Accuracy: 0.450, Validation Accuracy: 0.491, Loss: 2.421 Epoch 0 Batch 301/1077 - Train Accuracy: 0.448, Validation Accuracy: 0.491, Loss: 2.342 Epoch 0 Batch 302/1077 - Train Accuracy: 0.483, Validation Accuracy: 0.482, Loss: 2.246 Epoch 0 Batch 303/1077 - Train Accuracy: 0.436, Validation Accuracy: 0.496, Loss: 2.305 Epoch 0 Batch 304/1077 - Train Accuracy: 0.478, Validation Accuracy: 0.539, Loss: 2.279 Epoch 0 Batch 305/1077 - Train Accuracy: 0.463, Validation Accuracy: 0.519, Loss: 2.369 Epoch 0 Batch 306/1077 - Train Accuracy: 0.453, Validation Accuracy: 0.501, Loss: 2.239 Epoch 0 Batch 307/1077 - Train Accuracy: 0.444, Validation Accuracy: 0.517, Loss: 2.376 Epoch 0 Batch 308/1077 - Train Accuracy: 0.430, Validation Accuracy: 0.509, Loss: 2.343 Epoch 0 Batch 309/1077 - Train Accuracy: 0.470, Validation Accuracy: 0.514, Loss: 2.239 Epoch 0 Batch 310/1077 - Train Accuracy: 0.446, Validation Accuracy: 0.525, Loss: 2.325 Epoch 0 Batch 311/1077 - Train Accuracy: 0.553, Validation Accuracy: 0.540, Loss: 2.274 Epoch 0 Batch 312/1077 - Train Accuracy: 0.488, Validation Accuracy: 0.508, Loss: 2.362 Epoch 0 Batch 313/1077 - Train Accuracy: 0.446, Validation Accuracy: 0.498, Loss: 2.364 Epoch 0 Batch 314/1077 - Train Accuracy: 0.509, Validation Accuracy: 0.495, Loss: 2.359 Epoch 0 Batch 315/1077 - Train Accuracy: 0.487, Validation Accuracy: 0.499, Loss: 2.248 Epoch 0 Batch 316/1077 - Train Accuracy: 0.478, Validation Accuracy: 0.505, Loss: 2.327 Epoch 0 Batch 317/1077 - Train Accuracy: 0.452, Validation Accuracy: 0.502, Loss: 2.404 Epoch 0 Batch 318/1077 - Train Accuracy: 0.448, Validation Accuracy: 0.489, Loss: 2.357 Epoch 0 Batch 319/1077 - Train Accuracy: 0.439, Validation Accuracy: 0.477, Loss: 2.330 Epoch 0 Batch 320/1077 - Train Accuracy: 0.445, Validation Accuracy: 0.489, Loss: 2.394 Epoch 0 Batch 321/1077 - Train Accuracy: 0.450, Validation Accuracy: 0.496, Loss: 2.359 Epoch 0 Batch 322/1077 - Train Accuracy: 0.469, Validation Accuracy: 0.500, Loss: 2.224 Epoch 0 Batch 323/1077 - Train Accuracy: 0.518, Validation Accuracy: 0.512, Loss: 2.424 Epoch 0 Batch 324/1077 - Train Accuracy: 0.468, Validation Accuracy: 0.532, Loss: 2.284 Epoch 0 Batch 325/1077 - Train Accuracy: 0.528, Validation Accuracy: 0.544, Loss: 2.197 Epoch 0 Batch 326/1077 - Train Accuracy: 0.510, Validation Accuracy: 0.526, Loss: 2.215 Epoch 0 Batch 327/1077 - Train Accuracy: 0.444, Validation Accuracy: 0.496, Loss: 2.323 Epoch 0 Batch 328/1077 - Train Accuracy: 0.523, Validation Accuracy: 0.520, Loss: 2.237 Epoch 0 Batch 329/1077 - Train Accuracy: 0.491, Validation Accuracy: 0.556, Loss: 2.298 Epoch 0 Batch 330/1077 - Train Accuracy: 0.525, Validation Accuracy: 0.549, Loss: 2.376 Epoch 0 Batch 331/1077 - Train Accuracy: 0.482, Validation Accuracy: 0.536, Loss: 2.231 Epoch 0 Batch 332/1077 - Train Accuracy: 0.486, Validation Accuracy: 0.530, Loss: 2.238 Epoch 0 Batch 333/1077 - Train Accuracy: 0.512, Validation Accuracy: 0.536, Loss: 2.331 Epoch 0 Batch 334/1077 - Train Accuracy: 0.498, Validation Accuracy: 0.544, Loss: 2.271 Epoch 0 Batch 335/1077 - Train Accuracy: 0.544, Validation Accuracy: 0.538, Loss: 2.192 Epoch 0 Batch 336/1077 - Train Accuracy: 0.494, Validation Accuracy: 0.511, Loss: 2.264 Epoch 0 Batch 337/1077 - Train Accuracy: 0.463, Validation Accuracy: 0.500, Loss: 2.247 Epoch 0 Batch 338/1077 - Train Accuracy: 0.475, Validation Accuracy: 0.506, Loss: 2.264 Epoch 0 Batch 339/1077 - Train Accuracy: 0.489, Validation Accuracy: 0.509, Loss: 2.251 Epoch 0 Batch 340/1077 - Train Accuracy: 0.470, Validation Accuracy: 0.534, Loss: 2.299 Epoch 0 Batch 341/1077 - Train Accuracy: 0.509, Validation Accuracy: 0.542, Loss: 2.303 Epoch 0 Batch 342/1077 - Train Accuracy: 0.473, Validation Accuracy: 0.506, Loss: 2.243 Epoch 0 Batch 343/1077 - Train Accuracy: 0.438, Validation Accuracy: 0.494, Loss: 2.252 Epoch 0 Batch 344/1077 - Train Accuracy: 0.458, Validation Accuracy: 0.500, Loss: 2.273 Epoch 0 Batch 345/1077 - Train Accuracy: 0.485, Validation Accuracy: 0.499, Loss: 2.182 Epoch 0 Batch 346/1077 - Train Accuracy: 0.493, Validation Accuracy: 0.552, Loss: 2.240 Epoch 0 Batch 347/1077 - Train Accuracy: 0.508, Validation Accuracy: 0.536, Loss: 2.215 Epoch 0 Batch 348/1077 - Train Accuracy: 0.515, Validation Accuracy: 0.542, Loss: 2.172 Epoch 0 Batch 349/1077 - Train Accuracy: 0.477, Validation Accuracy: 0.523, Loss: 2.300 Epoch 0 Batch 350/1077 - Train Accuracy: 0.475, Validation Accuracy: 0.554, Loss: 2.260 Epoch 0 Batch 351/1077 - Train Accuracy: 0.468, Validation Accuracy: 0.542, Loss: 2.328 Epoch 0 Batch 352/1077 - Train Accuracy: 0.431, Validation Accuracy: 0.539, Loss: 2.226 Epoch 0 Batch 353/1077 - Train Accuracy: 0.456, Validation Accuracy: 0.534, Loss: 2.236 Epoch 0 Batch 354/1077 - Train Accuracy: 0.490, Validation Accuracy: 0.546, Loss: 2.213 Epoch 0 Batch 355/1077 - Train Accuracy: 0.501, Validation Accuracy: 0.549, Loss: 2.221 Epoch 0 Batch 356/1077 - Train Accuracy: 0.490, Validation Accuracy: 0.539, Loss: 2.203 Epoch 0 Batch 357/1077 - Train Accuracy: 0.530, Validation Accuracy: 0.534, Loss: 2.147 Epoch 0 Batch 358/1077 - Train Accuracy: 0.463, Validation Accuracy: 0.543, Loss: 2.345 Epoch 0 Batch 359/1077 - Train Accuracy: 0.507, Validation Accuracy: 0.553, Loss: 2.317 Epoch 0 Batch 360/1077 - Train Accuracy: 0.514, Validation Accuracy: 0.550, Loss: 2.274 Epoch 0 Batch 361/1077 - Train Accuracy: 0.499, Validation Accuracy: 0.555, Loss: 2.307 Epoch 0 Batch 362/1077 - Train Accuracy: 0.475, Validation Accuracy: 0.535, Loss: 2.233 Epoch 0 Batch 363/1077 - Train Accuracy: 0.471, Validation Accuracy: 0.525, Loss: 2.198 Epoch 0 Batch 364/1077 - Train Accuracy: 0.486, Validation Accuracy: 0.527, Loss: 2.281 Epoch 0 Batch 365/1077 - Train Accuracy: 0.491, Validation Accuracy: 0.544, Loss: 2.219 Epoch 0 Batch 366/1077 - Train Accuracy: 0.491, Validation Accuracy: 0.555, Loss: 2.373 Epoch 0 Batch 367/1077 - Train Accuracy: 0.519, Validation Accuracy: 0.564, Loss: 2.201 Epoch 0 Batch 368/1077 - Train Accuracy: 0.511, Validation Accuracy: 0.554, Loss: 2.171 Epoch 0 Batch 369/1077 - Train Accuracy: 0.509, Validation Accuracy: 0.547, Loss: 2.245 Epoch 0 Batch 370/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.554, Loss: 2.164 Epoch 0 Batch 371/1077 - Train Accuracy: 0.534, Validation Accuracy: 0.558, Loss: 2.288 Epoch 0 Batch 372/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.550, Loss: 2.217 Epoch 0 Batch 373/1077 - Train Accuracy: 0.552, Validation Accuracy: 0.531, Loss: 2.193 Epoch 0 Batch 374/1077 - Train Accuracy: 0.439, Validation Accuracy: 0.543, Loss: 2.278 Epoch 0 Batch 375/1077 - Train Accuracy: 0.541, Validation Accuracy: 0.557, Loss: 2.150 Epoch 0 Batch 376/1077 - Train Accuracy: 0.544, Validation Accuracy: 0.537, Loss: 2.112 Epoch 0 Batch 377/1077 - Train Accuracy: 0.507, Validation Accuracy: 0.523, Loss: 2.180 Epoch 0 Batch 378/1077 - Train Accuracy: 0.507, Validation Accuracy: 0.542, Loss: 2.212 Epoch 0 Batch 379/1077 - Train Accuracy: 0.491, Validation Accuracy: 0.538, Loss: 2.274 Epoch 0 Batch 380/1077 - Train Accuracy: 0.511, Validation Accuracy: 0.547, Loss: 2.132 Epoch 0 Batch 381/1077 - Train Accuracy: 0.500, Validation Accuracy: 0.551, Loss: 2.150 Epoch 0 Batch 382/1077 - Train Accuracy: 0.505, Validation Accuracy: 0.549, Loss: 2.265 Epoch 0 Batch 383/1077 - Train Accuracy: 0.523, Validation Accuracy: 0.549, Loss: 2.174 Epoch 0 Batch 384/1077 - Train Accuracy: 0.515, Validation Accuracy: 0.553, Loss: 2.188 Epoch 0 Batch 385/1077 - Train Accuracy: 0.531, Validation Accuracy: 0.551, Loss: 2.206 Epoch 0 Batch 386/1077 - Train Accuracy: 0.500, Validation Accuracy: 0.540, Loss: 2.135 Epoch 0 Batch 387/1077 - Train Accuracy: 0.530, Validation Accuracy: 0.554, Loss: 2.195 Epoch 0 Batch 388/1077 - Train Accuracy: 0.530, Validation Accuracy: 0.553, Loss: 2.247 Epoch 0 Batch 389/1077 - Train Accuracy: 0.539, Validation Accuracy: 0.553, Loss: 2.228 Epoch 0 Batch 390/1077 - Train Accuracy: 0.464, Validation Accuracy: 0.555, Loss: 2.190 Epoch 0 Batch 391/1077 - Train Accuracy: 0.531, Validation Accuracy: 0.551, Loss: 2.212 Epoch 0 Batch 392/1077 - Train Accuracy: 0.457, Validation Accuracy: 0.556, Loss: 2.197 Epoch 0 Batch 393/1077 - Train Accuracy: 0.529, Validation Accuracy: 0.559, Loss: 2.222 Epoch 0 Batch 394/1077 - Train Accuracy: 0.477, Validation Accuracy: 0.544, Loss: 2.177 Epoch 0 Batch 395/1077 - Train Accuracy: 0.533, Validation Accuracy: 0.544, Loss: 2.170 Epoch 0 Batch 396/1077 - Train Accuracy: 0.514, Validation Accuracy: 0.552, Loss: 2.242 Epoch 0 Batch 397/1077 - Train Accuracy: 0.560, Validation Accuracy: 0.557, Loss: 2.166 Epoch 0 Batch 398/1077 - Train Accuracy: 0.515, Validation Accuracy: 0.547, Loss: 2.205 Epoch 0 Batch 399/1077 - Train Accuracy: 0.463, Validation Accuracy: 0.553, Loss: 2.229 Epoch 0 Batch 400/1077 - Train Accuracy: 0.520, Validation Accuracy: 0.554, Loss: 2.170 Epoch 0 Batch 401/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.555, Loss: 2.228 Epoch 0 Batch 402/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.528, Loss: 2.159 Epoch 0 Batch 403/1077 - Train Accuracy: 0.474, Validation Accuracy: 0.527, Loss: 2.200 Epoch 0 Batch 404/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.529, Loss: 2.165 Epoch 0 Batch 405/1077 - Train Accuracy: 0.462, Validation Accuracy: 0.535, Loss: 2.183 Epoch 0 Batch 406/1077 - Train Accuracy: 0.528, Validation Accuracy: 0.553, Loss: 2.143 Epoch 0 Batch 407/1077 - Train Accuracy: 0.520, Validation Accuracy: 0.559, Loss: 2.238 Epoch 0 Batch 408/1077 - Train Accuracy: 0.513, Validation Accuracy: 0.550, Loss: 2.210 Epoch 0 Batch 409/1077 - Train Accuracy: 0.514, Validation Accuracy: 0.554, Loss: 2.156 Epoch 0 Batch 410/1077 - Train Accuracy: 0.483, Validation Accuracy: 0.565, Loss: 2.255 Epoch 0 Batch 411/1077 - Train Accuracy: 0.544, Validation Accuracy: 0.566, Loss: 2.158 Epoch 0 Batch 412/1077 - Train Accuracy: 0.540, Validation Accuracy: 0.571, Loss: 2.127 Epoch 0 Batch 413/1077 - Train Accuracy: 0.556, Validation Accuracy: 0.571, Loss: 2.174 Epoch 0 Batch 414/1077 - Train Accuracy: 0.506, Validation Accuracy: 0.562, Loss: 2.203 Epoch 0 Batch 415/1077 - Train Accuracy: 0.530, Validation Accuracy: 0.564, Loss: 2.221 Epoch 0 Batch 416/1077 - Train Accuracy: 0.511, Validation Accuracy: 0.557, Loss: 2.133 Epoch 0 Batch 417/1077 - Train Accuracy: 0.510, Validation Accuracy: 0.556, Loss: 2.099 Epoch 0 Batch 418/1077 - Train Accuracy: 0.486, Validation Accuracy: 0.548, Loss: 2.120 Epoch 0 Batch 419/1077 - Train Accuracy: 0.504, Validation Accuracy: 0.562, Loss: 2.142 Epoch 0 Batch 420/1077 - Train Accuracy: 0.479, Validation Accuracy: 0.553, Loss: 2.110 Epoch 0 Batch 421/1077 - Train Accuracy: 0.506, Validation Accuracy: 0.556, Loss: 2.139 Epoch 0 Batch 422/1077 - Train Accuracy: 0.503, Validation Accuracy: 0.548, Loss: 2.130 Epoch 0 Batch 423/1077 - Train Accuracy: 0.512, Validation Accuracy: 0.543, Loss: 2.096 Epoch 0 Batch 424/1077 - Train Accuracy: 0.459, Validation Accuracy: 0.556, Loss: 2.185 Epoch 0 Batch 425/1077 - Train Accuracy: 0.561, Validation Accuracy: 0.554, Loss: 2.184 Epoch 0 Batch 426/1077 - Train Accuracy: 0.525, Validation Accuracy: 0.558, Loss: 2.142 Epoch 0 Batch 427/1077 - Train Accuracy: 0.496, Validation Accuracy: 0.565, Loss: 2.161 Epoch 0 Batch 428/1077 - Train Accuracy: 0.555, Validation Accuracy: 0.569, Loss: 2.037 Epoch 0 Batch 429/1077 - Train Accuracy: 0.529, Validation Accuracy: 0.574, Loss: 2.160 Epoch 0 Batch 430/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.567, Loss: 2.159 Epoch 0 Batch 431/1077 - Train Accuracy: 0.506, Validation Accuracy: 0.562, Loss: 2.226 Epoch 0 Batch 432/1077 - Train Accuracy: 0.540, Validation Accuracy: 0.582, Loss: 2.166 Epoch 0 Batch 433/1077 - Train Accuracy: 0.515, Validation Accuracy: 0.567, Loss: 2.175 Epoch 0 Batch 434/1077 - Train Accuracy: 0.508, Validation Accuracy: 0.572, Loss: 2.168 Epoch 0 Batch 435/1077 - Train Accuracy: 0.548, Validation Accuracy: 0.569, Loss: 2.124 Epoch 0 Batch 436/1077 - Train Accuracy: 0.542, Validation Accuracy: 0.575, Loss: 2.146 Epoch 0 Batch 437/1077 - Train Accuracy: 0.517, Validation Accuracy: 0.588, Loss: 2.149 Epoch 0 Batch 438/1077 - Train Accuracy: 0.543, Validation Accuracy: 0.582, Loss: 2.138 Epoch 0 Batch 439/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.580, Loss: 2.161 Epoch 0 Batch 440/1077 - Train Accuracy: 0.536, Validation Accuracy: 0.581, Loss: 2.128 Epoch 0 Batch 441/1077 - Train Accuracy: 0.504, Validation Accuracy: 0.581, Loss: 2.185 Epoch 0 Batch 442/1077 - Train Accuracy: 0.501, Validation Accuracy: 0.578, Loss: 2.166 Epoch 0 Batch 443/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.569, Loss: 2.092 Epoch 0 Batch 444/1077 - Train Accuracy: 0.538, Validation Accuracy: 0.560, Loss: 2.196 Epoch 0 Batch 445/1077 - Train Accuracy: 0.502, Validation Accuracy: 0.545, Loss: 2.273 Epoch 0 Batch 446/1077 - Train Accuracy: 0.527, Validation Accuracy: 0.542, Loss: 2.082 Epoch 0 Batch 447/1077 - Train Accuracy: 0.504, Validation Accuracy: 0.523, Loss: 2.085 Epoch 0 Batch 448/1077 - Train Accuracy: 0.523, Validation Accuracy: 0.527, Loss: 2.149 Epoch 0 Batch 449/1077 - Train Accuracy: 0.489, Validation Accuracy: 0.526, Loss: 2.148 Epoch 0 Batch 450/1077 - Train Accuracy: 0.496, Validation Accuracy: 0.530, Loss: 2.198 Epoch 0 Batch 451/1077 - Train Accuracy: 0.524, Validation Accuracy: 0.520, Loss: 2.057 Epoch 0 Batch 452/1077 - Train Accuracy: 0.519, Validation Accuracy: 0.534, Loss: 2.167 Epoch 0 Batch 453/1077 - Train Accuracy: 0.563, Validation Accuracy: 0.553, Loss: 2.099 Epoch 0 Batch 454/1077 - Train Accuracy: 0.548, Validation Accuracy: 0.562, Loss: 2.150 Epoch 0 Batch 455/1077 - Train Accuracy: 0.549, Validation Accuracy: 0.558, Loss: 2.114 Epoch 0 Batch 456/1077 - Train Accuracy: 0.514, Validation Accuracy: 0.554, Loss: 2.141 Epoch 0 Batch 457/1077 - Train Accuracy: 0.531, Validation Accuracy: 0.550, Loss: 2.110 Epoch 0 Batch 458/1077 - Train Accuracy: 0.489, Validation Accuracy: 0.570, Loss: 2.151 Epoch 0 Batch 459/1077 - Train Accuracy: 0.570, Validation Accuracy: 0.571, Loss: 2.081 Epoch 0 Batch 460/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.572, Loss: 2.107 Epoch 0 Batch 461/1077 - Train Accuracy: 0.537, Validation Accuracy: 0.583, Loss: 2.085 Epoch 0 Batch 462/1077 - Train Accuracy: 0.524, Validation Accuracy: 0.581, Loss: 2.070 Epoch 0 Batch 463/1077 - Train Accuracy: 0.503, Validation Accuracy: 0.583, Loss: 2.159 Epoch 0 Batch 464/1077 - Train Accuracy: 0.569, Validation Accuracy: 0.584, Loss: 2.106 Epoch 0 Batch 465/1077 - Train Accuracy: 0.532, Validation Accuracy: 0.581, Loss: 2.158 Epoch 0 Batch 466/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.580, Loss: 2.169 Epoch 0 Batch 467/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.569, Loss: 2.080 Epoch 0 Batch 468/1077 - Train Accuracy: 0.522, Validation Accuracy: 0.559, Loss: 2.160 Epoch 0 Batch 469/1077 - Train Accuracy: 0.482, Validation Accuracy: 0.548, Loss: 2.177 Epoch 0 Batch 470/1077 - Train Accuracy: 0.498, Validation Accuracy: 0.561, Loss: 2.082 Epoch 0 Batch 471/1077 - Train Accuracy: 0.561, Validation Accuracy: 0.569, Loss: 2.100 Epoch 0 Batch 472/1077 - Train Accuracy: 0.531, Validation Accuracy: 0.582, Loss: 2.192 Epoch 0 Batch 473/1077 - Train Accuracy: 0.534, Validation Accuracy: 0.582, Loss: 2.207 Epoch 0 Batch 474/1077 - Train Accuracy: 0.543, Validation Accuracy: 0.580, Loss: 2.151 Epoch 0 Batch 475/1077 - Train Accuracy: 0.544, Validation Accuracy: 0.575, Loss: 2.046 Epoch 0 Batch 476/1077 - Train Accuracy: 0.530, Validation Accuracy: 0.582, Loss: 2.122 Epoch 0 Batch 477/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.582, Loss: 2.046 Epoch 0 Batch 478/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.585, Loss: 2.135 Epoch 0 Batch 479/1077 - Train Accuracy: 0.535, Validation Accuracy: 0.588, Loss: 2.150 Epoch 0 Batch 480/1077 - Train Accuracy: 0.543, Validation Accuracy: 0.589, Loss: 2.163 Epoch 0 Batch 481/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.591, Loss: 2.089 Epoch 0 Batch 482/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.590, Loss: 2.180 Epoch 0 Batch 483/1077 - Train Accuracy: 0.536, Validation Accuracy: 0.591, Loss: 2.126 Epoch 0 Batch 484/1077 - Train Accuracy: 0.560, Validation Accuracy: 0.589, Loss: 2.107 Epoch 0 Batch 485/1077 - Train Accuracy: 0.577, Validation Accuracy: 0.575, Loss: 2.115 Epoch 0 Batch 486/1077 - Train Accuracy: 0.542, Validation Accuracy: 0.574, Loss: 2.156 Epoch 0 Batch 487/1077 - Train Accuracy: 0.529, Validation Accuracy: 0.591, Loss: 2.153 Epoch 0 Batch 488/1077 - Train Accuracy: 0.524, Validation Accuracy: 0.582, Loss: 2.163 Epoch 0 Batch 489/1077 - Train Accuracy: 0.584, Validation Accuracy: 0.583, Loss: 2.060 Epoch 0 Batch 490/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.587, Loss: 2.153 Epoch 0 Batch 491/1077 - Train Accuracy: 0.579, Validation Accuracy: 0.572, Loss: 2.153 Epoch 0 Batch 492/1077 - Train Accuracy: 0.539, Validation Accuracy: 0.572, Loss: 2.121 Epoch 0 Batch 493/1077 - Train Accuracy: 0.574, Validation Accuracy: 0.567, Loss: 2.067 Epoch 0 Batch 494/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.579, Loss: 2.118 Epoch 0 Batch 495/1077 - Train Accuracy: 0.534, Validation Accuracy: 0.577, Loss: 2.110 Epoch 0 Batch 496/1077 - Train Accuracy: 0.561, Validation Accuracy: 0.579, Loss: 2.163 Epoch 0 Batch 497/1077 - Train Accuracy: 0.513, Validation Accuracy: 0.580, Loss: 2.175 Epoch 0 Batch 498/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.578, Loss: 2.026 Epoch 0 Batch 499/1077 - Train Accuracy: 0.552, Validation Accuracy: 0.584, Loss: 2.059 Epoch 0 Batch 500/1077 - Train Accuracy: 0.561, Validation Accuracy: 0.582, Loss: 2.045 Epoch 0 Batch 501/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.585, Loss: 2.027 Epoch 0 Batch 502/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.582, Loss: 2.121 Epoch 0 Batch 503/1077 - Train Accuracy: 0.572, Validation Accuracy: 0.584, Loss: 2.185 Epoch 0 Batch 504/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.590, Loss: 2.151 Epoch 0 Batch 505/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.597, Loss: 1.995 Epoch 0 Batch 506/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.593, Loss: 2.108 Epoch 0 Batch 507/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.589, Loss: 2.076 Epoch 0 Batch 508/1077 - Train Accuracy: 0.584, Validation Accuracy: 0.594, Loss: 2.039 Epoch 0 Batch 509/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.584, Loss: 2.075 Epoch 0 Batch 510/1077 - Train Accuracy: 0.564, Validation Accuracy: 0.571, Loss: 2.151 Epoch 0 Batch 511/1077 - Train Accuracy: 0.533, Validation Accuracy: 0.575, Loss: 2.188 Epoch 0 Batch 512/1077 - Train Accuracy: 0.560, Validation Accuracy: 0.581, Loss: 2.146 Epoch 0 Batch 513/1077 - Train Accuracy: 0.580, Validation Accuracy: 0.576, Loss: 2.152 Epoch 0 Batch 514/1077 - Train Accuracy: 0.535, Validation Accuracy: 0.568, Loss: 2.109 Epoch 0 Batch 515/1077 - Train Accuracy: 0.540, Validation Accuracy: 0.567, Loss: 2.081 Epoch 0 Batch 516/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.567, Loss: 2.066 Epoch 0 Batch 517/1077 - Train Accuracy: 0.599, Validation Accuracy: 0.571, Loss: 2.058 Epoch 0 Batch 518/1077 - Train Accuracy: 0.575, Validation Accuracy: 0.577, Loss: 2.153 Epoch 0 Batch 519/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.579, Loss: 2.167 Epoch 0 Batch 520/1077 - Train Accuracy: 0.597, Validation Accuracy: 0.584, Loss: 2.027 Epoch 0 Batch 521/1077 - Train Accuracy: 0.571, Validation Accuracy: 0.591, Loss: 2.084 Epoch 0 Batch 522/1077 - Train Accuracy: 0.547, Validation Accuracy: 0.587, Loss: 2.165 Epoch 0 Batch 523/1077 - Train Accuracy: 0.555, Validation Accuracy: 0.585, Loss: 2.202 Epoch 0 Batch 524/1077 - Train Accuracy: 0.555, Validation Accuracy: 0.591, Loss: 2.189 Epoch 0 Batch 525/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.590, Loss: 2.069 Epoch 0 Batch 526/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.594, Loss: 2.025 Epoch 0 Batch 527/1077 - Train Accuracy: 0.550, Validation Accuracy: 0.596, Loss: 2.127 Epoch 0 Batch 528/1077 - Train Accuracy: 0.562, Validation Accuracy: 0.583, Loss: 2.090 Epoch 0 Batch 529/1077 - Train Accuracy: 0.545, Validation Accuracy: 0.581, Loss: 2.063 Epoch 0 Batch 530/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.587, Loss: 2.017 Epoch 0 Batch 531/1077 - Train Accuracy: 0.543, Validation Accuracy: 0.586, Loss: 2.059 Epoch 0 Batch 532/1077 - Train Accuracy: 0.496, Validation Accuracy: 0.576, Loss: 2.085 Epoch 0 Batch 533/1077 - Train Accuracy: 0.567, Validation Accuracy: 0.580, Loss: 2.125 Epoch 0 Batch 534/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.578, Loss: 2.091 Epoch 0 Batch 535/1077 - Train Accuracy: 0.548, Validation Accuracy: 0.573, Loss: 2.139 Epoch 0 Batch 536/1077 - Train Accuracy: 0.557, Validation Accuracy: 0.583, Loss: 2.044 Epoch 0 Batch 537/1077 - Train Accuracy: 0.555, Validation Accuracy: 0.582, Loss: 2.043 Epoch 0 Batch 538/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.575, Loss: 2.051 Epoch 0 Batch 539/1077 - Train Accuracy: 0.528, Validation Accuracy: 0.565, Loss: 2.124 Epoch 0 Batch 540/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.570, Loss: 2.080 Epoch 0 Batch 541/1077 - Train Accuracy: 0.550, Validation Accuracy: 0.577, Loss: 2.081 Epoch 0 Batch 542/1077 - Train Accuracy: 0.561, Validation Accuracy: 0.583, Loss: 2.099 Epoch 0 Batch 543/1077 - Train Accuracy: 0.543, Validation Accuracy: 0.582, Loss: 2.082 Epoch 0 Batch 544/1077 - Train Accuracy: 0.567, Validation Accuracy: 0.583, Loss: 2.052 Epoch 0 Batch 545/1077 - Train Accuracy: 0.565, Validation Accuracy: 0.589, Loss: 2.169 Epoch 0 Batch 546/1077 - Train Accuracy: 0.535, Validation Accuracy: 0.589, Loss: 2.086 Epoch 0 Batch 547/1077 - Train Accuracy: 0.573, Validation Accuracy: 0.586, Loss: 2.052 Epoch 0 Batch 548/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.575, Loss: 2.094 Epoch 0 Batch 549/1077 - Train Accuracy: 0.521, Validation Accuracy: 0.573, Loss: 2.115 Epoch 0 Batch 550/1077 - Train Accuracy: 0.526, Validation Accuracy: 0.577, Loss: 2.000 Epoch 0 Batch 551/1077 - Train Accuracy: 0.555, Validation Accuracy: 0.592, Loss: 2.059 Epoch 0 Batch 552/1077 - Train Accuracy: 0.558, Validation Accuracy: 0.591, Loss: 2.094 Epoch 0 Batch 553/1077 - Train Accuracy: 0.594, Validation Accuracy: 0.593, Loss: 2.119 Epoch 0 Batch 554/1077 - Train Accuracy: 0.570, Validation Accuracy: 0.593, Loss: 2.106 Epoch 0 Batch 555/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.585, Loss: 2.122 Epoch 0 Batch 556/1077 - Train Accuracy: 0.547, Validation Accuracy: 0.593, Loss: 2.023 Epoch 0 Batch 557/1077 - Train Accuracy: 0.576, Validation Accuracy: 0.600, Loss: 2.107 Epoch 0 Batch 558/1077 - Train Accuracy: 0.566, Validation Accuracy: 0.600, Loss: 1.993 Epoch 0 Batch 559/1077 - Train Accuracy: 0.577, Validation Accuracy: 0.591, Loss: 2.043 Epoch 0 Batch 560/1077 - Train Accuracy: 0.534, Validation Accuracy: 0.586, Loss: 2.054 Epoch 0 Batch 561/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.596, Loss: 2.113 Epoch 0 Batch 562/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.592, Loss: 2.099 Epoch 0 Batch 563/1077 - Train Accuracy: 0.565, Validation Accuracy: 0.599, Loss: 2.114 Epoch 0 Batch 564/1077 - Train Accuracy: 0.574, Validation Accuracy: 0.596, Loss: 2.107 Epoch 0 Batch 565/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.583, Loss: 2.034 Epoch 0 Batch 566/1077 - Train Accuracy: 0.563, Validation Accuracy: 0.580, Loss: 2.104 Epoch 0 Batch 567/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.574, Loss: 2.014 Epoch 0 Batch 568/1077 - Train Accuracy: 0.574, Validation Accuracy: 0.579, Loss: 2.074 Epoch 0 Batch 569/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.575, Loss: 2.087 Epoch 0 Batch 570/1077 - Train Accuracy: 0.564, Validation Accuracy: 0.577, Loss: 2.130 Epoch 0 Batch 571/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.571, Loss: 2.047 Epoch 0 Batch 572/1077 - Train Accuracy: 0.562, Validation Accuracy: 0.568, Loss: 2.096 Epoch 0 Batch 573/1077 - Train Accuracy: 0.558, Validation Accuracy: 0.565, Loss: 2.081 Epoch 0 Batch 574/1077 - Train Accuracy: 0.537, Validation Accuracy: 0.570, Loss: 2.142 Epoch 0 Batch 575/1077 - Train Accuracy: 0.581, Validation Accuracy: 0.585, Loss: 2.120 Epoch 0 Batch 576/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.595, Loss: 2.038 Epoch 0 Batch 577/1077 - Train Accuracy: 0.539, Validation Accuracy: 0.596, Loss: 2.167 Epoch 0 Batch 578/1077 - Train Accuracy: 0.534, Validation Accuracy: 0.593, Loss: 2.077 Epoch 0 Batch 579/1077 - Train Accuracy: 0.599, Validation Accuracy: 0.591, Loss: 2.025 Epoch 0 Batch 580/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.602, Loss: 1.969 Epoch 0 Batch 581/1077 - Train Accuracy: 0.563, Validation Accuracy: 0.596, Loss: 2.008 Epoch 0 Batch 582/1077 - Train Accuracy: 0.550, Validation Accuracy: 0.600, Loss: 2.101 Epoch 0 Batch 583/1077 - Train Accuracy: 0.532, Validation Accuracy: 0.599, Loss: 2.079 Epoch 0 Batch 584/1077 - Train Accuracy: 0.582, Validation Accuracy: 0.592, Loss: 2.105 Epoch 0 Batch 585/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.595, Loss: 2.032 Epoch 0 Batch 586/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.597, Loss: 2.097 Epoch 0 Batch 587/1077 - Train Accuracy: 0.537, Validation Accuracy: 0.587, Loss: 2.084 Epoch 0 Batch 588/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.582, Loss: 2.046 Epoch 0 Batch 589/1077 - Train Accuracy: 0.516, Validation Accuracy: 0.544, Loss: 2.115 Epoch 0 Batch 590/1077 - Train Accuracy: 0.505, Validation Accuracy: 0.555, Loss: 2.104 Epoch 0 Batch 591/1077 - Train Accuracy: 0.558, Validation Accuracy: 0.564, Loss: 2.060 Epoch 0 Batch 592/1077 - Train Accuracy: 0.574, Validation Accuracy: 0.579, Loss: 2.100 Epoch 0 Batch 593/1077 - Train Accuracy: 0.570, Validation Accuracy: 0.581, Loss: 2.053 Epoch 0 Batch 594/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.588, Loss: 2.117 Epoch 0 Batch 595/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.584, Loss: 2.052 Epoch 0 Batch 596/1077 - Train Accuracy: 0.560, Validation Accuracy: 0.594, Loss: 2.056 Epoch 0 Batch 597/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.601, Loss: 2.088 Epoch 0 Batch 598/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.598, Loss: 2.010 Epoch 0 Batch 599/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.606, Loss: 2.071 Epoch 0 Batch 600/1077 - Train Accuracy: 0.599, Validation Accuracy: 0.599, Loss: 2.023 Epoch 0 Batch 601/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.598, Loss: 2.022 Epoch 0 Batch 602/1077 - Train Accuracy: 0.597, Validation Accuracy: 0.588, Loss: 2.070 Epoch 0 Batch 603/1077 - Train Accuracy: 0.591, Validation Accuracy: 0.584, Loss: 2.040 Epoch 0 Batch 604/1077 - Train Accuracy: 0.530, Validation Accuracy: 0.589, Loss: 2.085 Epoch 0 Batch 605/1077 - Train Accuracy: 0.545, Validation Accuracy: 0.572, Loss: 2.105 Epoch 0 Batch 606/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.571, Loss: 1.968 Epoch 0 Batch 607/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.568, Loss: 1.973 Epoch 0 Batch 608/1077 - Train Accuracy: 0.540, Validation Accuracy: 0.566, Loss: 2.129 Epoch 0 Batch 609/1077 - Train Accuracy: 0.550, Validation Accuracy: 0.572, Loss: 2.071 Epoch 0 Batch 610/1077 - Train Accuracy: 0.536, Validation Accuracy: 0.575, Loss: 2.102 Epoch 0 Batch 611/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.600, Loss: 2.024 Epoch 0 Batch 612/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.602, Loss: 2.023 Epoch 0 Batch 613/1077 - Train Accuracy: 0.563, Validation Accuracy: 0.604, Loss: 2.147 Epoch 0 Batch 614/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.601, Loss: 2.024 Epoch 0 Batch 615/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.601, Loss: 2.114 Epoch 0 Batch 616/1077 - Train Accuracy: 0.547, Validation Accuracy: 0.603, Loss: 2.086 Epoch 0 Batch 617/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.610, Loss: 2.014 Epoch 0 Batch 618/1077 - Train Accuracy: 0.579, Validation Accuracy: 0.613, Loss: 2.011 Epoch 0 Batch 619/1077 - Train Accuracy: 0.558, Validation Accuracy: 0.605, Loss: 2.056 Epoch 0 Batch 620/1077 - Train Accuracy: 0.573, Validation Accuracy: 0.609, Loss: 2.067 Epoch 0 Batch 621/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.612, Loss: 2.096 Epoch 0 Batch 622/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.614, Loss: 2.094 Epoch 0 Batch 623/1077 - Train Accuracy: 0.564, Validation Accuracy: 0.617, Loss: 2.044 Epoch 0 Batch 624/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.610, Loss: 2.039 Epoch 0 Batch 625/1077 - Train Accuracy: 0.594, Validation Accuracy: 0.608, Loss: 2.047 Epoch 0 Batch 626/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.611, Loss: 1.959 Epoch 0 Batch 627/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.605, Loss: 2.089 Epoch 0 Batch 628/1077 - Train Accuracy: 0.557, Validation Accuracy: 0.599, Loss: 2.069 Epoch 0 Batch 629/1077 - Train Accuracy: 0.558, Validation Accuracy: 0.609, Loss: 2.041 Epoch 0 Batch 630/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.610, Loss: 2.039 Epoch 0 Batch 631/1077 - Train Accuracy: 0.578, Validation Accuracy: 0.611, Loss: 2.014 Epoch 0 Batch 632/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.613, Loss: 2.080 Epoch 0 Batch 633/1077 - Train Accuracy: 0.591, Validation Accuracy: 0.614, Loss: 1.969 Epoch 0 Batch 634/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.609, Loss: 1.960 Epoch 0 Batch 635/1077 - Train Accuracy: 0.562, Validation Accuracy: 0.610, Loss: 2.038 Epoch 0 Batch 636/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.614, Loss: 1.974 Epoch 0 Batch 637/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.606, Loss: 2.107 Epoch 0 Batch 638/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.601, Loss: 1.991 Epoch 0 Batch 639/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.601, Loss: 2.007 Epoch 0 Batch 640/1077 - Train Accuracy: 0.562, Validation Accuracy: 0.599, Loss: 1.958 Epoch 0 Batch 641/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.597, Loss: 1.989 Epoch 0 Batch 642/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.594, Loss: 1.999 Epoch 0 Batch 643/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.602, Loss: 2.050 Epoch 0 Batch 644/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.602, Loss: 2.063 Epoch 0 Batch 645/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.603, Loss: 1.971 Epoch 0 Batch 646/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.603, Loss: 2.058 Epoch 0 Batch 647/1077 - Train Accuracy: 0.592, Validation Accuracy: 0.598, Loss: 1.987 Epoch 0 Batch 648/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.585, Loss: 2.003 Epoch 0 Batch 649/1077 - Train Accuracy: 0.543, Validation Accuracy: 0.572, Loss: 2.169 Epoch 0 Batch 650/1077 - Train Accuracy: 0.542, Validation Accuracy: 0.577, Loss: 2.095 Epoch 0 Batch 651/1077 - Train Accuracy: 0.579, Validation Accuracy: 0.575, Loss: 1.993 Epoch 0 Batch 652/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.584, Loss: 2.103 Epoch 0 Batch 653/1077 - Train Accuracy: 0.608, Validation Accuracy: 0.599, Loss: 2.041 Epoch 0 Batch 654/1077 - Train Accuracy: 0.579, Validation Accuracy: 0.612, Loss: 2.041 Epoch 0 Batch 655/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.617, Loss: 2.012 Epoch 0 Batch 656/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.618, Loss: 2.066 Epoch 0 Batch 657/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.619, Loss: 2.065 Epoch 0 Batch 658/1077 - Train Accuracy: 0.594, Validation Accuracy: 0.621, Loss: 1.904 Epoch 0 Batch 659/1077 - Train Accuracy: 0.635, Validation Accuracy: 0.618, Loss: 2.021 Epoch 0 Batch 660/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.614, Loss: 2.073 Epoch 0 Batch 661/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.613, Loss: 1.984 Epoch 0 Batch 662/1077 - Train Accuracy: 0.608, Validation Accuracy: 0.616, Loss: 2.005 Epoch 0 Batch 663/1077 - Train Accuracy: 0.571, Validation Accuracy: 0.615, Loss: 1.985 Epoch 0 Batch 664/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.602, Loss: 2.127 Epoch 0 Batch 665/1077 - Train Accuracy: 0.573, Validation Accuracy: 0.601, Loss: 2.071 Epoch 0 Batch 666/1077 - Train Accuracy: 0.542, Validation Accuracy: 0.606, Loss: 2.058 Epoch 0 Batch 667/1077 - Train Accuracy: 0.569, Validation Accuracy: 0.613, Loss: 2.069 Epoch 0 Batch 668/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.617, Loss: 2.003 Epoch 0 Batch 669/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.616, Loss: 2.027 Epoch 0 Batch 670/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.616, Loss: 2.003 Epoch 0 Batch 671/1077 - Train Accuracy: 0.561, Validation Accuracy: 0.613, Loss: 2.078 Epoch 0 Batch 672/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.597, Loss: 1.994 Epoch 0 Batch 673/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.585, Loss: 2.049 Epoch 0 Batch 674/1077 - Train Accuracy: 0.566, Validation Accuracy: 0.581, Loss: 2.091 Epoch 0 Batch 675/1077 - Train Accuracy: 0.548, Validation Accuracy: 0.582, Loss: 2.094 Epoch 0 Batch 676/1077 - Train Accuracy: 0.583, Validation Accuracy: 0.600, Loss: 2.021 Epoch 0 Batch 677/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.593, Loss: 2.042 Epoch 0 Batch 678/1077 - Train Accuracy: 0.578, Validation Accuracy: 0.613, Loss: 2.058 Epoch 0 Batch 679/1077 - Train Accuracy: 0.558, Validation Accuracy: 0.615, Loss: 2.022 Epoch 0 Batch 680/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.609, Loss: 2.016 Epoch 0 Batch 681/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.602, Loss: 2.049 Epoch 0 Batch 682/1077 - Train Accuracy: 0.535, Validation Accuracy: 0.608, Loss: 1.949 Epoch 0 Batch 683/1077 - Train Accuracy: 0.568, Validation Accuracy: 0.619, Loss: 2.022 Epoch 0 Batch 684/1077 - Train Accuracy: 0.580, Validation Accuracy: 0.600, Loss: 2.068 Epoch 0 Batch 685/1077 - Train Accuracy: 0.555, Validation Accuracy: 0.592, Loss: 2.022 Epoch 0 Batch 686/1077 - Train Accuracy: 0.567, Validation Accuracy: 0.589, Loss: 2.035 Epoch 0 Batch 687/1077 - Train Accuracy: 0.580, Validation Accuracy: 0.600, Loss: 2.035 Epoch 0 Batch 688/1077 - Train Accuracy: 0.568, Validation Accuracy: 0.606, Loss: 2.044 Epoch 0 Batch 689/1077 - Train Accuracy: 0.584, Validation Accuracy: 0.615, Loss: 1.971 Epoch 0 Batch 690/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.600, Loss: 2.050 Epoch 0 Batch 691/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.607, Loss: 2.001 Epoch 0 Batch 692/1077 - Train Accuracy: 0.583, Validation Accuracy: 0.610, Loss: 1.979 Epoch 0 Batch 693/1077 - Train Accuracy: 0.538, Validation Accuracy: 0.618, Loss: 2.053 Epoch 0 Batch 694/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.621, Loss: 2.005 Epoch 0 Batch 695/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.625, Loss: 2.021 Epoch 0 Batch 696/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.624, Loss: 2.038 Epoch 0 Batch 697/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.623, Loss: 1.988 Epoch 0 Batch 698/1077 - Train Accuracy: 0.567, Validation Accuracy: 0.608, Loss: 2.044 Epoch 0 Batch 699/1077 - Train Accuracy: 0.514, Validation Accuracy: 0.612, Loss: 2.093 Epoch 0 Batch 700/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.624, Loss: 1.990 Epoch 0 Batch 701/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.621, Loss: 2.020 Epoch 0 Batch 702/1077 - Train Accuracy: 0.574, Validation Accuracy: 0.607, Loss: 1.989 Epoch 0 Batch 703/1077 - Train Accuracy: 0.569, Validation Accuracy: 0.604, Loss: 2.056 Epoch 0 Batch 704/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.601, Loss: 2.059 Epoch 0 Batch 705/1077 - Train Accuracy: 0.566, Validation Accuracy: 0.619, Loss: 2.061 Epoch 0 Batch 706/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.609, Loss: 1.978 Epoch 0 Batch 707/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.610, Loss: 2.023 Epoch 0 Batch 708/1077 - Train Accuracy: 0.592, Validation Accuracy: 0.621, Loss: 2.035 Epoch 0 Batch 709/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.618, Loss: 1.967 Epoch 0 Batch 710/1077 - Train Accuracy: 0.562, Validation Accuracy: 0.615, Loss: 2.013 Epoch 0 Batch 711/1077 - Train Accuracy: 0.579, Validation Accuracy: 0.588, Loss: 1.998 Epoch 0 Batch 712/1077 - Train Accuracy: 0.571, Validation Accuracy: 0.586, Loss: 2.015 Epoch 0 Batch 713/1077 - Train Accuracy: 0.608, Validation Accuracy: 0.588, Loss: 1.962 Epoch 0 Batch 714/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.599, Loss: 2.050 Epoch 0 Batch 715/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.612, Loss: 2.024 Epoch 0 Batch 716/1077 - Train Accuracy: 0.581, Validation Accuracy: 0.612, Loss: 1.973 Epoch 0 Batch 717/1077 - Train Accuracy: 0.577, Validation Accuracy: 0.618, Loss: 1.986 Epoch 0 Batch 718/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.613, Loss: 2.009 Epoch 0 Batch 719/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.614, Loss: 1.980 Epoch 0 Batch 720/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.607, Loss: 2.072 Epoch 0 Batch 721/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.610, Loss: 1.949 Epoch 0 Batch 722/1077 - Train Accuracy: 0.568, Validation Accuracy: 0.612, Loss: 1.989 Epoch 0 Batch 723/1077 - Train Accuracy: 0.557, Validation Accuracy: 0.602, Loss: 1.982 Epoch 0 Batch 724/1077 - Train Accuracy: 0.566, Validation Accuracy: 0.613, Loss: 1.983 Epoch 0 Batch 725/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.626, Loss: 1.951 Epoch 0 Batch 726/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.626, Loss: 1.968 Epoch 0 Batch 727/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.620, Loss: 1.973 Epoch 0 Batch 728/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.622, Loss: 1.971 Epoch 0 Batch 729/1077 - Train Accuracy: 0.557, Validation Accuracy: 0.626, Loss: 2.003 Epoch 0 Batch 730/1077 - Train Accuracy: 0.582, Validation Accuracy: 0.614, Loss: 1.954 Epoch 0 Batch 731/1077 - Train Accuracy: 0.582, Validation Accuracy: 0.613, Loss: 1.961 Epoch 0 Batch 732/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.611, Loss: 2.028 Epoch 0 Batch 733/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.606, Loss: 2.026 Epoch 0 Batch 734/1077 - Train Accuracy: 0.583, Validation Accuracy: 0.614, Loss: 2.013 Epoch 0 Batch 735/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.613, Loss: 2.007 Epoch 0 Batch 736/1077 - Train Accuracy: 0.608, Validation Accuracy: 0.616, Loss: 2.029 Epoch 0 Batch 737/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.612, Loss: 1.967 Epoch 0 Batch 738/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.602, Loss: 1.916 Epoch 0 Batch 739/1077 - Train Accuracy: 0.582, Validation Accuracy: 0.604, Loss: 2.023 Epoch 0 Batch 740/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.608, Loss: 1.972 Epoch 0 Batch 741/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.615, Loss: 2.020 Epoch 0 Batch 742/1077 - Train Accuracy: 0.579, Validation Accuracy: 0.615, Loss: 2.027 Epoch 0 Batch 743/1077 - Train Accuracy: 0.584, Validation Accuracy: 0.621, Loss: 1.953 Epoch 0 Batch 744/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.624, Loss: 1.947 Epoch 0 Batch 745/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.623, Loss: 2.007 Epoch 0 Batch 746/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.605, Loss: 2.009 Epoch 0 Batch 747/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.601, Loss: 1.969 Epoch 0 Batch 748/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.604, Loss: 1.927 Epoch 0 Batch 749/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.600, Loss: 2.023 Epoch 0 Batch 750/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.599, Loss: 2.044 Epoch 0 Batch 751/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.601, Loss: 1.969 Epoch 0 Batch 752/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.604, Loss: 1.889 Epoch 0 Batch 753/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.623, Loss: 1.967 Epoch 0 Batch 754/1077 - Train Accuracy: 0.574, Validation Accuracy: 0.618, Loss: 2.048 Epoch 0 Batch 755/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.613, Loss: 1.971 Epoch 0 Batch 756/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.613, Loss: 1.927 Epoch 0 Batch 757/1077 - Train Accuracy: 0.592, Validation Accuracy: 0.613, Loss: 2.004 Epoch 0 Batch 758/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.608, Loss: 1.902 Epoch 0 Batch 759/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.621, Loss: 1.923 Epoch 0 Batch 760/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.630, Loss: 2.042 Epoch 0 Batch 761/1077 - Train Accuracy: 0.559, Validation Accuracy: 0.637, Loss: 1.962 Epoch 0 Batch 762/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.634, Loss: 1.969 Epoch 0 Batch 763/1077 - Train Accuracy: 0.585, Validation Accuracy: 0.629, Loss: 1.930 Epoch 0 Batch 764/1077 - Train Accuracy: 0.565, Validation Accuracy: 0.629, Loss: 2.080 Epoch 0 Batch 765/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.629, Loss: 1.981 Epoch 0 Batch 766/1077 - Train Accuracy: 0.582, Validation Accuracy: 0.622, Loss: 2.083 Epoch 0 Batch 767/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.630, Loss: 2.004 Epoch 0 Batch 768/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.625, Loss: 1.986 Epoch 0 Batch 769/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.617, Loss: 1.956 Epoch 0 Batch 770/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.603, Loss: 1.960 Epoch 0 Batch 771/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.606, Loss: 2.014 Epoch 0 Batch 772/1077 - Train Accuracy: 0.574, Validation Accuracy: 0.603, Loss: 1.982 Epoch 0 Batch 773/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.609, Loss: 2.018 Epoch 0 Batch 774/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.618, Loss: 2.001 Epoch 0 Batch 775/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.620, Loss: 1.991 Epoch 0 Batch 776/1077 - Train Accuracy: 0.585, Validation Accuracy: 0.616, Loss: 1.930 Epoch 0 Batch 777/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.615, Loss: 1.957 Epoch 0 Batch 778/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.618, Loss: 1.962 Epoch 0 Batch 779/1077 - Train Accuracy: 0.594, Validation Accuracy: 0.624, Loss: 1.911 Epoch 0 Batch 780/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.614, Loss: 1.979 Epoch 0 Batch 781/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.614, Loss: 1.931 Epoch 0 Batch 782/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.626, Loss: 2.027 Epoch 0 Batch 783/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.623, Loss: 1.949 Epoch 0 Batch 784/1077 - Train Accuracy: 0.612, Validation Accuracy: 0.625, Loss: 1.938 Epoch 0 Batch 785/1077 - Train Accuracy: 0.650, Validation Accuracy: 0.617, Loss: 1.925 Epoch 0 Batch 786/1077 - Train Accuracy: 0.568, Validation Accuracy: 0.612, Loss: 2.030 Epoch 0 Batch 787/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.608, Loss: 1.980 Epoch 0 Batch 788/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.616, Loss: 1.976 Epoch 0 Batch 789/1077 - Train Accuracy: 0.556, Validation Accuracy: 0.623, Loss: 1.977 Epoch 0 Batch 790/1077 - Train Accuracy: 0.562, Validation Accuracy: 0.626, Loss: 2.077 Epoch 0 Batch 791/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.621, Loss: 2.000 Epoch 0 Batch 792/1077 - Train Accuracy: 0.571, Validation Accuracy: 0.613, Loss: 2.009 Epoch 0 Batch 793/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.617, Loss: 1.992 Epoch 0 Batch 794/1077 - Train Accuracy: 0.573, Validation Accuracy: 0.615, Loss: 1.947 Epoch 0 Batch 795/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.617, Loss: 2.045 Epoch 0 Batch 796/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.615, Loss: 1.897 Epoch 0 Batch 797/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.613, Loss: 1.939 Epoch 0 Batch 798/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.614, Loss: 1.963 Epoch 0 Batch 799/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.616, Loss: 2.002 Epoch 0 Batch 800/1077 - Train Accuracy: 0.584, Validation Accuracy: 0.619, Loss: 1.914 Epoch 0 Batch 801/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.615, Loss: 2.008 Epoch 0 Batch 802/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.618, Loss: 2.015 Epoch 0 Batch 803/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.619, Loss: 1.916 Epoch 0 Batch 804/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.609, Loss: 2.032 Epoch 0 Batch 805/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.617, Loss: 1.934 Epoch 0 Batch 806/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.625, Loss: 1.964 Epoch 0 Batch 807/1077 - Train Accuracy: 0.570, Validation Accuracy: 0.633, Loss: 1.991 Epoch 0 Batch 808/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.627, Loss: 1.924 Epoch 0 Batch 809/1077 - Train Accuracy: 0.581, Validation Accuracy: 0.609, Loss: 2.009 Epoch 0 Batch 810/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.608, Loss: 1.947 Epoch 0 Batch 811/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.611, Loss: 1.958 Epoch 0 Batch 812/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.626, Loss: 2.004 Epoch 0 Batch 813/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.627, Loss: 2.063 Epoch 0 Batch 814/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.618, Loss: 1.948 Epoch 0 Batch 815/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.626, Loss: 2.024 Epoch 0 Batch 816/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.621, Loss: 1.962 Epoch 0 Batch 817/1077 - Train Accuracy: 0.568, Validation Accuracy: 0.625, Loss: 1.952 Epoch 0 Batch 818/1077 - Train Accuracy: 0.573, Validation Accuracy: 0.626, Loss: 1.957 Epoch 0 Batch 819/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.620, Loss: 1.954 Epoch 0 Batch 820/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.629, Loss: 1.949 Epoch 0 Batch 821/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.631, Loss: 1.877 Epoch 0 Batch 822/1077 - Train Accuracy: 0.612, Validation Accuracy: 0.631, Loss: 2.014 Epoch 0 Batch 823/1077 - Train Accuracy: 0.580, Validation Accuracy: 0.615, Loss: 2.004 Epoch 0 Batch 824/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.615, Loss: 1.948 Epoch 0 Batch 825/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.616, Loss: 1.978 Epoch 0 Batch 826/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.619, Loss: 1.983 Epoch 0 Batch 827/1077 - Train Accuracy: 0.577, Validation Accuracy: 0.617, Loss: 1.966 Epoch 0 Batch 828/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.624, Loss: 2.014 Epoch 0 Batch 829/1077 - Train Accuracy: 0.592, Validation Accuracy: 0.625, Loss: 1.962 Epoch 0 Batch 830/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.626, Loss: 2.005 Epoch 0 Batch 831/1077 - Train Accuracy: 0.591, Validation Accuracy: 0.639, Loss: 1.982 Epoch 0 Batch 832/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.632, Loss: 2.023 Epoch 0 Batch 833/1077 - Train Accuracy: 0.612, Validation Accuracy: 0.632, Loss: 1.966 Epoch 0 Batch 834/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.635, Loss: 1.928 Epoch 0 Batch 835/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.632, Loss: 1.949 Epoch 0 Batch 836/1077 - Train Accuracy: 0.568, Validation Accuracy: 0.632, Loss: 1.994 Epoch 0 Batch 837/1077 - Train Accuracy: 0.576, Validation Accuracy: 0.634, Loss: 2.035 Epoch 0 Batch 838/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.620, Loss: 2.006 Epoch 0 Batch 839/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.619, Loss: 1.907 Epoch 0 Batch 840/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.616, Loss: 2.008 Epoch 0 Batch 841/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.605, Loss: 1.989 Epoch 0 Batch 842/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.620, Loss: 1.963 Epoch 0 Batch 843/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.620, Loss: 1.919 Epoch 0 Batch 844/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.623, Loss: 2.029 Epoch 0 Batch 845/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.629, Loss: 2.064 Epoch 0 Batch 846/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.618, Loss: 1.955 Epoch 0 Batch 847/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.620, Loss: 2.034 Epoch 0 Batch 848/1077 - Train Accuracy: 0.612, Validation Accuracy: 0.632, Loss: 1.881 Epoch 0 Batch 849/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.630, Loss: 1.925 Epoch 0 Batch 850/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.629, Loss: 1.954 Epoch 0 Batch 851/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.631, Loss: 1.903 Epoch 0 Batch 852/1077 - Train Accuracy: 0.597, Validation Accuracy: 0.624, Loss: 2.033 Epoch 0 Batch 853/1077 - Train Accuracy: 0.640, Validation Accuracy: 0.629, Loss: 1.995 Epoch 0 Batch 854/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.632, Loss: 1.906 Epoch 0 Batch 855/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.630, Loss: 1.898 Epoch 0 Batch 856/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.623, Loss: 1.940 Epoch 0 Batch 857/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.620, Loss: 1.963 Epoch 0 Batch 858/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.625, Loss: 1.962 Epoch 0 Batch 859/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.622, Loss: 2.053 Epoch 0 Batch 860/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.621, Loss: 1.915 Epoch 0 Batch 861/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.615, Loss: 1.905 Epoch 0 Batch 862/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.611, Loss: 1.960 Epoch 0 Batch 863/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.621, Loss: 1.957 Epoch 0 Batch 864/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.623, Loss: 1.952 Epoch 0 Batch 865/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.620, Loss: 1.891 Epoch 0 Batch 866/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.612, Loss: 1.963 Epoch 0 Batch 867/1077 - Train Accuracy: 0.566, Validation Accuracy: 0.612, Loss: 1.974 Epoch 0 Batch 868/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.612, Loss: 1.977 Epoch 0 Batch 869/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.627, Loss: 1.979 Epoch 0 Batch 870/1077 - Train Accuracy: 0.584, Validation Accuracy: 0.629, Loss: 2.062 Epoch 0 Batch 871/1077 - Train Accuracy: 0.583, Validation Accuracy: 0.634, Loss: 1.970 Epoch 0 Batch 872/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.630, Loss: 1.922 Epoch 0 Batch 873/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.631, Loss: 1.898 Epoch 0 Batch 874/1077 - Train Accuracy: 0.577, Validation Accuracy: 0.622, Loss: 2.007 Epoch 0 Batch 875/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.631, Loss: 2.016 Epoch 0 Batch 876/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.628, Loss: 1.949 Epoch 0 Batch 877/1077 - Train Accuracy: 0.592, Validation Accuracy: 0.627, Loss: 1.929 Epoch 0 Batch 878/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.638, Loss: 1.862 Epoch 0 Batch 879/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.640, Loss: 1.939 Epoch 0 Batch 880/1077 - Train Accuracy: 0.628, Validation Accuracy: 0.637, Loss: 1.947 Epoch 0 Batch 881/1077 - Train Accuracy: 0.569, Validation Accuracy: 0.627, Loss: 2.040 Epoch 0 Batch 882/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.629, Loss: 2.043 Epoch 0 Batch 883/1077 - Train Accuracy: 0.556, Validation Accuracy: 0.636, Loss: 2.033 Epoch 0 Batch 884/1077 - Train Accuracy: 0.577, Validation Accuracy: 0.651, Loss: 1.930 Epoch 0 Batch 885/1077 - Train Accuracy: 0.653, Validation Accuracy: 0.641, Loss: 1.845 Epoch 0 Batch 886/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.639, Loss: 2.011 Epoch 0 Batch 887/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.645, Loss: 2.036 Epoch 0 Batch 888/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.619, Loss: 2.009 Epoch 0 Batch 889/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.629, Loss: 1.979 Epoch 0 Batch 890/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.626, Loss: 1.952 Epoch 0 Batch 891/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.626, Loss: 1.931 Epoch 0 Batch 892/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.645, Loss: 1.892 Epoch 0 Batch 893/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.636, Loss: 1.923 Epoch 0 Batch 894/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.622, Loss: 1.911 Epoch 0 Batch 895/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.627, Loss: 1.961 Epoch 0 Batch 896/1077 - Train Accuracy: 0.556, Validation Accuracy: 0.624, Loss: 1.940 Epoch 0 Batch 897/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.621, Loss: 1.937 Epoch 0 Batch 898/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.640, Loss: 1.910 Epoch 0 Batch 899/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.641, Loss: 1.988 Epoch 0 Batch 900/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.638, Loss: 1.870 Epoch 0 Batch 901/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.642, Loss: 1.876 Epoch 0 Batch 902/1077 - Train Accuracy: 0.641, Validation Accuracy: 0.635, Loss: 1.928 Epoch 0 Batch 903/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.636, Loss: 1.838 Epoch 0 Batch 904/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.638, Loss: 1.973 Epoch 0 Batch 905/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.635, Loss: 1.957 Epoch 0 Batch 906/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.638, Loss: 1.878 Epoch 0 Batch 907/1077 - Train Accuracy: 0.608, Validation Accuracy: 0.643, Loss: 1.902 Epoch 0 Batch 908/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.638, Loss: 1.917 Epoch 0 Batch 909/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.641, Loss: 1.911 Epoch 0 Batch 910/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.639, Loss: 1.945 Epoch 0 Batch 911/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.637, Loss: 1.926 Epoch 0 Batch 912/1077 - Train Accuracy: 0.572, Validation Accuracy: 0.636, Loss: 1.970 Epoch 0 Batch 913/1077 - Train Accuracy: 0.580, Validation Accuracy: 0.635, Loss: 2.009 Epoch 0 Batch 914/1077 - Train Accuracy: 0.681, Validation Accuracy: 0.643, Loss: 1.932 Epoch 0 Batch 915/1077 - Train Accuracy: 0.582, Validation Accuracy: 0.637, Loss: 2.037 Epoch 0 Batch 916/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.633, Loss: 2.001 Epoch 0 Batch 917/1077 - Train Accuracy: 0.569, Validation Accuracy: 0.635, Loss: 1.898 Epoch 0 Batch 918/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.637, Loss: 1.879 Epoch 0 Batch 919/1077 - Train Accuracy: 0.654, Validation Accuracy: 0.637, Loss: 1.954 Epoch 0 Batch 920/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.642, Loss: 1.919 Epoch 0 Batch 921/1077 - Train Accuracy: 0.587, Validation Accuracy: 0.638, Loss: 1.919 Epoch 0 Batch 922/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.644, Loss: 1.982 Epoch 0 Batch 923/1077 - Train Accuracy: 0.547, Validation Accuracy: 0.646, Loss: 1.962 Epoch 0 Batch 924/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.640, Loss: 1.934 Epoch 0 Batch 925/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.638, Loss: 1.903 Epoch 0 Batch 926/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.620, Loss: 1.981 Epoch 0 Batch 927/1077 - Train Accuracy: 0.550, Validation Accuracy: 0.620, Loss: 1.992 Epoch 0 Batch 928/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.607, Loss: 1.947 Epoch 0 Batch 929/1077 - Train Accuracy: 0.568, Validation Accuracy: 0.613, Loss: 2.081 Epoch 0 Batch 930/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.637, Loss: 2.003 Epoch 0 Batch 931/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.649, Loss: 1.943 Epoch 0 Batch 932/1077 - Train Accuracy: 0.554, Validation Accuracy: 0.652, Loss: 1.984 Epoch 0 Batch 933/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.647, Loss: 1.923 Epoch 0 Batch 934/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.629, Loss: 1.951 Epoch 0 Batch 935/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.630, Loss: 1.997 Epoch 0 Batch 936/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.633, Loss: 1.948 Epoch 0 Batch 937/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.636, Loss: 1.967 Epoch 0 Batch 938/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.642, Loss: 1.898 Epoch 0 Batch 939/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.642, Loss: 1.903 Epoch 0 Batch 940/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.650, Loss: 1.957 Epoch 0 Batch 941/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.646, Loss: 2.016 Epoch 0 Batch 942/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.641, Loss: 2.012 Epoch 0 Batch 943/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.649, Loss: 2.000 Epoch 0 Batch 944/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.637, Loss: 1.917 Epoch 0 Batch 945/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.631, Loss: 1.901 Epoch 0 Batch 946/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.635, Loss: 1.908 Epoch 0 Batch 947/1077 - Train Accuracy: 0.564, Validation Accuracy: 0.636, Loss: 1.939 Epoch 0 Batch 948/1077 - Train Accuracy: 0.575, Validation Accuracy: 0.634, Loss: 1.885 Epoch 0 Batch 949/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.649, Loss: 1.901 Epoch 0 Batch 950/1077 - Train Accuracy: 0.584, Validation Accuracy: 0.656, Loss: 1.892 Epoch 0 Batch 951/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.650, Loss: 1.871 Epoch 0 Batch 952/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.650, Loss: 1.915 Epoch 0 Batch 953/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.645, Loss: 1.866 Epoch 0 Batch 954/1077 - Train Accuracy: 0.551, Validation Accuracy: 0.647, Loss: 1.933 Epoch 0 Batch 955/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.641, Loss: 1.879 Epoch 0 Batch 956/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.636, Loss: 1.861 Epoch 0 Batch 957/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.631, Loss: 1.911 Epoch 0 Batch 958/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.631, Loss: 1.916 Epoch 0 Batch 959/1077 - Train Accuracy: 0.652, Validation Accuracy: 0.632, Loss: 1.885 Epoch 0 Batch 960/1077 - Train Accuracy: 0.622, Validation Accuracy: 0.632, Loss: 1.959 Epoch 0 Batch 961/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.620, Loss: 1.803 Epoch 0 Batch 962/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.613, Loss: 1.881 Epoch 0 Batch 963/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.627, Loss: 1.934 Epoch 0 Batch 964/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.631, Loss: 1.902 Epoch 0 Batch 965/1077 - Train Accuracy: 0.546, Validation Accuracy: 0.637, Loss: 1.974 Epoch 0 Batch 966/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.623, Loss: 1.874 Epoch 0 Batch 967/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.617, Loss: 1.943 Epoch 0 Batch 968/1077 - Train Accuracy: 0.580, Validation Accuracy: 0.632, Loss: 1.937 Epoch 0 Batch 969/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.643, Loss: 1.920 Epoch 0 Batch 970/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.650, Loss: 1.895 Epoch 0 Batch 971/1077 - Train Accuracy: 0.633, Validation Accuracy: 0.651, Loss: 1.923 Epoch 0 Batch 972/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.642, Loss: 1.961 Epoch 0 Batch 973/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.635, Loss: 1.893 Epoch 0 Batch 974/1077 - Train Accuracy: 0.581, Validation Accuracy: 0.635, Loss: 1.884 Epoch 0 Batch 975/1077 - Train Accuracy: 0.669, Validation Accuracy: 0.627, Loss: 1.942 Epoch 0 Batch 976/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.635, Loss: 1.941 Epoch 0 Batch 977/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.645, Loss: 1.884 Epoch 0 Batch 978/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.651, Loss: 1.927 Epoch 0 Batch 979/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.646, Loss: 2.018 Epoch 0 Batch 980/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.644, Loss: 2.002 Epoch 0 Batch 981/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.654, Loss: 1.936 Epoch 0 Batch 982/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.657, Loss: 1.888 Epoch 0 Batch 983/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.660, Loss: 1.974 Epoch 0 Batch 984/1077 - Train Accuracy: 0.599, Validation Accuracy: 0.660, Loss: 1.880 Epoch 0 Batch 985/1077 - Train Accuracy: 0.661, Validation Accuracy: 0.655, Loss: 1.836 Epoch 0 Batch 986/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.660, Loss: 1.925 Epoch 0 Batch 987/1077 - Train Accuracy: 0.628, Validation Accuracy: 0.659, Loss: 1.866 Epoch 0 Batch 988/1077 - Train Accuracy: 0.578, Validation Accuracy: 0.660, Loss: 1.934 Epoch 0 Batch 989/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.646, Loss: 1.945 Epoch 0 Batch 990/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.643, Loss: 1.856 Epoch 0 Batch 991/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.642, Loss: 1.891 Epoch 0 Batch 992/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.643, Loss: 1.943 Epoch 0 Batch 993/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.642, Loss: 1.884 Epoch 0 Batch 994/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.647, Loss: 1.935 Epoch 0 Batch 995/1077 - Train Accuracy: 0.658, Validation Accuracy: 0.654, Loss: 1.870 Epoch 0 Batch 996/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.656, Loss: 1.843 Epoch 0 Batch 997/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.647, Loss: 1.883 Epoch 0 Batch 998/1077 - Train Accuracy: 0.596, Validation Accuracy: 0.646, Loss: 1.912 Epoch 0 Batch 999/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.639, Loss: 1.976 Epoch 0 Batch 1000/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.639, Loss: 1.872 Epoch 0 Batch 1001/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.637, Loss: 1.876 Epoch 0 Batch 1002/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.631, Loss: 1.856 Epoch 0 Batch 1003/1077 - Train Accuracy: 0.635, Validation Accuracy: 0.640, Loss: 1.973 Epoch 0 Batch 1004/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.638, Loss: 1.994 Epoch 0 Batch 1005/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.638, Loss: 1.951 Epoch 0 Batch 1006/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.643, Loss: 1.872 Epoch 0 Batch 1007/1077 - Train Accuracy: 0.661, Validation Accuracy: 0.635, Loss: 1.871 Epoch 0 Batch 1008/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.638, Loss: 1.910 Epoch 0 Batch 1009/1077 - Train Accuracy: 0.627, Validation Accuracy: 0.644, Loss: 1.914 Epoch 0 Batch 1010/1077 - Train Accuracy: 0.633, Validation Accuracy: 0.653, Loss: 1.921 Epoch 0 Batch 1011/1077 - Train Accuracy: 0.627, Validation Accuracy: 0.650, Loss: 1.927 Epoch 0 Batch 1012/1077 - Train Accuracy: 0.628, Validation Accuracy: 0.632, Loss: 1.901 Epoch 0 Batch 1013/1077 - Train Accuracy: 0.658, Validation Accuracy: 0.625, Loss: 1.910 Epoch 0 Batch 1014/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.633, Loss: 1.909 Epoch 0 Batch 1015/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.645, Loss: 1.964 Epoch 0 Batch 1016/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.647, Loss: 1.931 Epoch 0 Batch 1017/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.652, Loss: 1.853 Epoch 0 Batch 1018/1077 - Train Accuracy: 0.637, Validation Accuracy: 0.659, Loss: 1.882 Epoch 0 Batch 1019/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.661, Loss: 1.908 Epoch 0 Batch 1020/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.656, Loss: 1.874 Epoch 0 Batch 1021/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.658, Loss: 1.881 Epoch 0 Batch 1022/1077 - Train Accuracy: 0.666, Validation Accuracy: 0.651, Loss: 1.902 Epoch 0 Batch 1023/1077 - Train Accuracy: 0.653, Validation Accuracy: 0.662, Loss: 1.868 Epoch 0 Batch 1024/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.650, Loss: 1.937 Epoch 0 Batch 1025/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.648, Loss: 1.874 Epoch 0 Batch 1026/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.642, Loss: 1.883 Epoch 0 Batch 1027/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.632, Loss: 1.881 Epoch 0 Batch 1028/1077 - Train Accuracy: 0.597, Validation Accuracy: 0.634, Loss: 1.886 Epoch 0 Batch 1029/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.640, Loss: 1.878 Epoch 0 Batch 1030/1077 - Train Accuracy: 0.612, Validation Accuracy: 0.649, Loss: 1.938 Epoch 0 Batch 1031/1077 - Train Accuracy: 0.583, Validation Accuracy: 0.642, Loss: 1.911 Epoch 0 Batch 1032/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.639, Loss: 1.896 Epoch 0 Batch 1033/1077 - Train Accuracy: 0.580, Validation Accuracy: 0.637, Loss: 1.933 Epoch 0 Batch 1034/1077 - Train Accuracy: 0.576, Validation Accuracy: 0.636, Loss: 1.996 Epoch 0 Batch 1035/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.641, Loss: 1.904 Epoch 0 Batch 1036/1077 - Train Accuracy: 0.622, Validation Accuracy: 0.641, Loss: 1.880 Epoch 0 Batch 1037/1077 - Train Accuracy: 0.598, Validation Accuracy: 0.645, Loss: 1.911 Epoch 0 Batch 1038/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.645, Loss: 1.868 Epoch 0 Batch 1039/1077 - Train Accuracy: 0.669, Validation Accuracy: 0.651, Loss: 1.848 Epoch 0 Batch 1040/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.647, Loss: 1.899 Epoch 0 Batch 1041/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.636, Loss: 1.946 Epoch 0 Batch 1042/1077 - Train Accuracy: 0.665, Validation Accuracy: 0.647, Loss: 1.949 Epoch 0 Batch 1043/1077 - Train Accuracy: 0.652, Validation Accuracy: 0.653, Loss: 1.840 Epoch 0 Batch 1044/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.654, Loss: 1.938 Epoch 0 Batch 1045/1077 - Train Accuracy: 0.583, Validation Accuracy: 0.653, Loss: 1.860 Epoch 0 Batch 1046/1077 - Train Accuracy: 0.647, Validation Accuracy: 0.650, Loss: 1.965 Epoch 0 Batch 1047/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.643, Loss: 1.951 Epoch 0 Batch 1048/1077 - Train Accuracy: 0.599, Validation Accuracy: 0.641, Loss: 1.890 Epoch 0 Batch 1049/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.646, Loss: 1.945 Epoch 0 Batch 1050/1077 - Train Accuracy: 0.575, Validation Accuracy: 0.643, Loss: 1.837 Epoch 0 Batch 1051/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.647, Loss: 1.866 Epoch 0 Batch 1052/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.651, Loss: 1.975 Epoch 0 Batch 1053/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.642, Loss: 1.866 Epoch 0 Batch 1054/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.647, Loss: 1.909 Epoch 0 Batch 1055/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.645, Loss: 1.976 Epoch 0 Batch 1056/1077 - Train Accuracy: 0.640, Validation Accuracy: 0.640, Loss: 1.865 Epoch 0 Batch 1057/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.642, Loss: 1.836 Epoch 0 Batch 1058/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.640, Loss: 2.008 Epoch 0 Batch 1059/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.657, Loss: 1.932 Epoch 0 Batch 1060/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.650, Loss: 1.920 Epoch 0 Batch 1061/1077 - Train Accuracy: 0.656, Validation Accuracy: 0.650, Loss: 1.947 Epoch 0 Batch 1062/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.653, Loss: 1.896 Epoch 0 Batch 1063/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.647, Loss: 1.923 Epoch 0 Batch 1064/1077 - Train Accuracy: 0.654, Validation Accuracy: 0.656, Loss: 1.868 Epoch 0 Batch 1065/1077 - Train Accuracy: 0.622, Validation Accuracy: 0.651, Loss: 1.893 Epoch 0 Batch 1066/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.648, Loss: 1.869 Epoch 0 Batch 1067/1077 - Train Accuracy: 0.663, Validation Accuracy: 0.644, Loss: 1.842 Epoch 0 Batch 1068/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.640, Loss: 1.907 Epoch 0 Batch 1069/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.639, Loss: 1.854 Epoch 0 Batch 1070/1077 - Train Accuracy: 0.576, Validation Accuracy: 0.643, Loss: 1.930 Epoch 0 Batch 1071/1077 - Train Accuracy: 0.628, Validation Accuracy: 0.648, Loss: 1.908 Epoch 0 Batch 1072/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.647, Loss: 1.820 Epoch 0 Batch 1073/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.649, Loss: 1.886 Epoch 0 Batch 1074/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.635, Loss: 1.956 Epoch 0 Batch 1075/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.625, Loss: 1.960 Epoch 1 Batch 0/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.611, Loss: 1.839 Epoch 1 Batch 1/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.624, Loss: 1.906 Epoch 1 Batch 2/1077 - Train Accuracy: 0.536, Validation Accuracy: 0.634, Loss: 1.928 Epoch 1 Batch 3/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.650, Loss: 1.872 Epoch 1 Batch 4/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.646, Loss: 1.937 Epoch 1 Batch 5/1077 - Train Accuracy: 0.602, Validation Accuracy: 0.640, Loss: 1.965 Epoch 1 Batch 6/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.641, Loss: 1.941 Epoch 1 Batch 7/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.641, Loss: 1.950 Epoch 1 Batch 8/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.639, Loss: 1.858 Epoch 1 Batch 9/1077 - Train Accuracy: 0.666, Validation Accuracy: 0.642, Loss: 1.886 Epoch 1 Batch 10/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.647, Loss: 1.944 Epoch 1 Batch 11/1077 - Train Accuracy: 0.588, Validation Accuracy: 0.657, Loss: 1.888 Epoch 1 Batch 12/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.652, Loss: 1.838 Epoch 1 Batch 13/1077 - Train Accuracy: 0.664, Validation Accuracy: 0.645, Loss: 1.867 Epoch 1 Batch 14/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.641, Loss: 1.823 Epoch 1 Batch 15/1077 - Train Accuracy: 0.627, Validation Accuracy: 0.630, Loss: 1.833 Epoch 1 Batch 16/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.626, Loss: 1.943 Epoch 1 Batch 17/1077 - Train Accuracy: 0.656, Validation Accuracy: 0.638, Loss: 1.885 Epoch 1 Batch 18/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.634, Loss: 2.027 Epoch 1 Batch 19/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.633, Loss: 1.883 Epoch 1 Batch 20/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.629, Loss: 1.928 Epoch 1 Batch 21/1077 - Train Accuracy: 0.561, Validation Accuracy: 0.627, Loss: 1.908 Epoch 1 Batch 22/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.619, Loss: 1.927 Epoch 1 Batch 23/1077 - Train Accuracy: 0.622, Validation Accuracy: 0.625, Loss: 1.981 Epoch 1 Batch 24/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.634, Loss: 1.884 Epoch 1 Batch 25/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.625, Loss: 1.845 Epoch 1 Batch 26/1077 - Train Accuracy: 0.556, Validation Accuracy: 0.625, Loss: 1.919 Epoch 1 Batch 27/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.628, Loss: 1.826 Epoch 1 Batch 28/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.619, Loss: 1.955 Epoch 1 Batch 29/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.628, Loss: 1.844 Epoch 1 Batch 30/1077 - Train Accuracy: 0.612, Validation Accuracy: 0.635, Loss: 1.853 Epoch 1 Batch 31/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.644, Loss: 1.837 Epoch 1 Batch 32/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.647, Loss: 1.860 Epoch 1 Batch 33/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.651, Loss: 1.794 Epoch 1 Batch 34/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.651, Loss: 1.917 Epoch 1 Batch 35/1077 - Train Accuracy: 0.661, Validation Accuracy: 0.646, Loss: 1.773 Epoch 1 Batch 36/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.650, Loss: 1.931 Epoch 1 Batch 37/1077 - Train Accuracy: 0.635, Validation Accuracy: 0.648, Loss: 1.872 Epoch 1 Batch 38/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.647, Loss: 1.939 Epoch 1 Batch 39/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.650, Loss: 1.905 Epoch 1 Batch 40/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.650, Loss: 1.900 Epoch 1 Batch 41/1077 - Train Accuracy: 0.627, Validation Accuracy: 0.649, Loss: 1.957 Epoch 1 Batch 42/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.657, Loss: 1.896 Epoch 1 Batch 43/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.657, Loss: 1.838 Epoch 1 Batch 44/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.646, Loss: 1.965 Epoch 1 Batch 45/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.636, Loss: 1.903 Epoch 1 Batch 46/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.631, Loss: 1.920 Epoch 1 Batch 47/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.628, Loss: 1.920 Epoch 1 Batch 48/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.625, Loss: 1.967 Epoch 1 Batch 49/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.626, Loss: 1.838 Epoch 1 Batch 50/1077 - Train Accuracy: 0.589, Validation Accuracy: 0.617, Loss: 1.851 Epoch 1 Batch 51/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.630, Loss: 1.838 Epoch 1 Batch 52/1077 - Train Accuracy: 0.616, Validation Accuracy: 0.612, Loss: 1.965 Epoch 1 Batch 53/1077 - Train Accuracy: 0.586, Validation Accuracy: 0.632, Loss: 1.940 Epoch 1 Batch 54/1077 - Train Accuracy: 0.576, Validation Accuracy: 0.637, Loss: 1.908 Epoch 1 Batch 55/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.634, Loss: 1.829 Epoch 1 Batch 56/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.630, Loss: 1.874 Epoch 1 Batch 57/1077 - Train Accuracy: 0.652, Validation Accuracy: 0.626, Loss: 1.911 Epoch 1 Batch 58/1077 - Train Accuracy: 0.627, Validation Accuracy: 0.633, Loss: 1.856 Epoch 1 Batch 59/1077 - Train Accuracy: 0.635, Validation Accuracy: 0.634, Loss: 1.955 Epoch 1 Batch 60/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.633, Loss: 1.841 Epoch 1 Batch 61/1077 - Train Accuracy: 0.595, Validation Accuracy: 0.629, Loss: 1.870 Epoch 1 Batch 62/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.645, Loss: 1.920 Epoch 1 Batch 63/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.651, Loss: 1.813 Epoch 1 Batch 64/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.645, Loss: 1.886 Epoch 1 Batch 65/1077 - Train Accuracy: 0.593, Validation Accuracy: 0.640, Loss: 1.942 Epoch 1 Batch 66/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.630, Loss: 1.848 Epoch 1 Batch 67/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.632, Loss: 1.845 Epoch 1 Batch 68/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.635, Loss: 1.871 Epoch 1 Batch 69/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.641, Loss: 1.906 Epoch 1 Batch 70/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.640, Loss: 1.801 Epoch 1 Batch 71/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.640, Loss: 1.875 Epoch 1 Batch 72/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.641, Loss: 1.924 Epoch 1 Batch 73/1077 - Train Accuracy: 0.628, Validation Accuracy: 0.644, Loss: 1.916 Epoch 1 Batch 74/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.646, Loss: 1.826 Epoch 1 Batch 75/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.646, Loss: 1.906 Epoch 1 Batch 76/1077 - Train Accuracy: 0.650, Validation Accuracy: 0.650, Loss: 1.880 Epoch 1 Batch 77/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.649, Loss: 1.887 Epoch 1 Batch 78/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.642, Loss: 1.895 Epoch 1 Batch 79/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.649, Loss: 1.914 Epoch 1 Batch 80/1077 - Train Accuracy: 0.650, Validation Accuracy: 0.646, Loss: 1.888 Epoch 1 Batch 81/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.639, Loss: 1.867 Epoch 1 Batch 82/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.650, Loss: 1.832 Epoch 1 Batch 83/1077 - Train Accuracy: 0.600, Validation Accuracy: 0.635, Loss: 1.850 Epoch 1 Batch 84/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.627, Loss: 1.821 Epoch 1 Batch 85/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.635, Loss: 1.911 Epoch 1 Batch 86/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.642, Loss: 1.940 Epoch 1 Batch 87/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.650, Loss: 1.945 Epoch 1 Batch 88/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.659, Loss: 1.901 Epoch 1 Batch 89/1077 - Train Accuracy: 0.633, Validation Accuracy: 0.653, Loss: 1.902 Epoch 1 Batch 90/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.651, Loss: 1.889 Epoch 1 Batch 91/1077 - Train Accuracy: 0.669, Validation Accuracy: 0.651, Loss: 1.801 Epoch 1 Batch 92/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.646, Loss: 1.836 Epoch 1 Batch 93/1077 - Train Accuracy: 0.622, Validation Accuracy: 0.649, Loss: 1.906 Epoch 1 Batch 94/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.661, Loss: 1.905 Epoch 1 Batch 95/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.657, Loss: 1.900 Epoch 1 Batch 96/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.646, Loss: 1.895 Epoch 1 Batch 97/1077 - Train Accuracy: 0.640, Validation Accuracy: 0.640, Loss: 1.912 Epoch 1 Batch 98/1077 - Train Accuracy: 0.683, Validation Accuracy: 0.642, Loss: 1.837 Epoch 1 Batch 99/1077 - Train Accuracy: 0.637, Validation Accuracy: 0.646, Loss: 1.872 Epoch 1 Batch 100/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.637, Loss: 1.845 Epoch 1 Batch 101/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.637, Loss: 1.885 Epoch 1 Batch 102/1077 - Train Accuracy: 0.661, Validation Accuracy: 0.640, Loss: 1.901 Epoch 1 Batch 103/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.635, Loss: 1.917 Epoch 1 Batch 104/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.636, Loss: 1.971 Epoch 1 Batch 105/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.622, Loss: 1.874 Epoch 1 Batch 106/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.635, Loss: 1.894 Epoch 1 Batch 107/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.627, Loss: 1.864 Epoch 1 Batch 108/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.639, Loss: 1.847 Epoch 1 Batch 109/1077 - Train Accuracy: 0.637, Validation Accuracy: 0.648, Loss: 1.936 Epoch 1 Batch 110/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.647, Loss: 1.830 Epoch 1 Batch 111/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.631, Loss: 1.944 Epoch 1 Batch 112/1077 - Train Accuracy: 0.637, Validation Accuracy: 0.629, Loss: 1.938 Epoch 1 Batch 113/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.634, Loss: 1.901 Epoch 1 Batch 114/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.638, Loss: 1.890 Epoch 1 Batch 115/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.644, Loss: 1.914 Epoch 1 Batch 116/1077 - Train Accuracy: 0.590, Validation Accuracy: 0.640, Loss: 1.940 Epoch 1 Batch 117/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.642, Loss: 1.897 Epoch 1 Batch 118/1077 - Train Accuracy: 0.615, Validation Accuracy: 0.637, Loss: 1.840 Epoch 1 Batch 119/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.643, Loss: 1.897 Epoch 1 Batch 120/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.634, Loss: 1.853 Epoch 1 Batch 121/1077 - Train Accuracy: 0.649, Validation Accuracy: 0.643, Loss: 1.845 Epoch 1 Batch 122/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.637, Loss: 1.902 Epoch 1 Batch 123/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.632, Loss: 1.885 Epoch 1 Batch 124/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.647, Loss: 1.905 Epoch 1 Batch 125/1077 - Train Accuracy: 0.653, Validation Accuracy: 0.643, Loss: 1.824 Epoch 1 Batch 126/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.646, Loss: 1.846 Epoch 1 Batch 127/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.642, Loss: 1.899 Epoch 1 Batch 128/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.642, Loss: 1.846 Epoch 1 Batch 129/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.643, Loss: 1.857 Epoch 1 Batch 130/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.632, Loss: 1.839 Epoch 1 Batch 131/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.627, Loss: 1.918 Epoch 1 Batch 132/1077 - Train Accuracy: 0.633, Validation Accuracy: 0.625, Loss: 1.947 Epoch 1 Batch 133/1077 - Train Accuracy: 0.606, Validation Accuracy: 0.621, Loss: 1.842 Epoch 1 Batch 134/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.618, Loss: 1.848 Epoch 1 Batch 135/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.627, Loss: 1.838 Epoch 1 Batch 136/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.625, Loss: 1.900 Epoch 1 Batch 137/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.637, Loss: 1.780 Epoch 1 Batch 138/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.641, Loss: 1.895 Epoch 1 Batch 139/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.658, Loss: 1.803 Epoch 1 Batch 140/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.658, Loss: 1.803 Epoch 1 Batch 141/1077 - Train Accuracy: 0.647, Validation Accuracy: 0.656, Loss: 1.804 Epoch 1 Batch 142/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.659, Loss: 1.889 Epoch 1 Batch 143/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.661, Loss: 1.854 Epoch 1 Batch 144/1077 - Train Accuracy: 0.604, Validation Accuracy: 0.662, Loss: 1.868 Epoch 1 Batch 145/1077 - Train Accuracy: 0.686, Validation Accuracy: 0.664, Loss: 1.949 Epoch 1 Batch 146/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.655, Loss: 1.877 Epoch 1 Batch 147/1077 - Train Accuracy: 0.611, Validation Accuracy: 0.655, Loss: 1.861 Epoch 1 Batch 148/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.657, Loss: 1.803 Epoch 1 Batch 149/1077 - Train Accuracy: 0.621, Validation Accuracy: 0.642, Loss: 1.814 Epoch 1 Batch 150/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.646, Loss: 1.911 Epoch 1 Batch 151/1077 - Train Accuracy: 0.674, Validation Accuracy: 0.638, Loss: 1.836 Epoch 1 Batch 152/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.633, Loss: 1.844 Epoch 1 Batch 153/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.615, Loss: 1.901 Epoch 1 Batch 154/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.613, Loss: 1.952 Epoch 1 Batch 155/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.608, Loss: 1.912 Epoch 1 Batch 156/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.623, Loss: 1.869 Epoch 1 Batch 157/1077 - Train Accuracy: 0.660, Validation Accuracy: 0.643, Loss: 1.937 Epoch 1 Batch 158/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.653, Loss: 1.908 Epoch 1 Batch 159/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.656, Loss: 1.840 Epoch 1 Batch 160/1077 - Train Accuracy: 0.660, Validation Accuracy: 0.658, Loss: 1.845 Epoch 1 Batch 161/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.662, Loss: 1.798 Epoch 1 Batch 162/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.663, Loss: 1.975 Epoch 1 Batch 163/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.665, Loss: 1.864 Epoch 1 Batch 164/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.662, Loss: 1.911 Epoch 1 Batch 165/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.668, Loss: 1.836 Epoch 1 Batch 166/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.656, Loss: 1.864 Epoch 1 Batch 167/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.662, Loss: 1.837 Epoch 1 Batch 168/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.659, Loss: 1.872 Epoch 1 Batch 169/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.650, Loss: 1.894 Epoch 1 Batch 170/1077 - Train Accuracy: 0.579, Validation Accuracy: 0.642, Loss: 1.811 Epoch 1 Batch 171/1077 - Train Accuracy: 0.650, Validation Accuracy: 0.635, Loss: 1.864 Epoch 1 Batch 172/1077 - Train Accuracy: 0.658, Validation Accuracy: 0.630, Loss: 1.869 Epoch 1 Batch 173/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.635, Loss: 1.941 Epoch 1 Batch 174/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.635, Loss: 1.892 Epoch 1 Batch 175/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.646, Loss: 1.868 Epoch 1 Batch 176/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.645, Loss: 1.835 Epoch 1 Batch 177/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.652, Loss: 1.850 Epoch 1 Batch 178/1077 - Train Accuracy: 0.663, Validation Accuracy: 0.656, Loss: 1.828 Epoch 1 Batch 179/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.661, Loss: 1.845 Epoch 1 Batch 180/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.656, Loss: 1.836 Epoch 1 Batch 181/1077 - Train Accuracy: 0.634, Validation Accuracy: 0.658, Loss: 1.941 Epoch 1 Batch 182/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.657, Loss: 1.826 Epoch 1 Batch 183/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.641, Loss: 1.873 Epoch 1 Batch 184/1077 - Train Accuracy: 0.682, Validation Accuracy: 0.637, Loss: 1.982 Epoch 1 Batch 185/1077 - Train Accuracy: 0.610, Validation Accuracy: 0.648, Loss: 1.786 Epoch 1 Batch 186/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.652, Loss: 1.821 Epoch 1 Batch 187/1077 - Train Accuracy: 0.640, Validation Accuracy: 0.652, Loss: 1.843 Epoch 1 Batch 188/1077 - Train Accuracy: 0.614, Validation Accuracy: 0.638, Loss: 1.827 Epoch 1 Batch 189/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.614, Loss: 1.828 Epoch 1 Batch 190/1077 - Train Accuracy: 0.666, Validation Accuracy: 0.620, Loss: 1.902 Epoch 1 Batch 191/1077 - Train Accuracy: 0.656, Validation Accuracy: 0.630, Loss: 1.794 Epoch 1 Batch 192/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.646, Loss: 1.894 Epoch 1 Batch 193/1077 - Train Accuracy: 0.681, Validation Accuracy: 0.648, Loss: 1.796 Epoch 1 Batch 194/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.643, Loss: 1.800 Epoch 1 Batch 195/1077 - Train Accuracy: 0.622, Validation Accuracy: 0.646, Loss: 1.817 Epoch 1 Batch 196/1077 - Train Accuracy: 0.688, Validation Accuracy: 0.653, Loss: 1.894 Epoch 1 Batch 197/1077 - Train Accuracy: 0.664, Validation Accuracy: 0.658, Loss: 1.851 Epoch 1 Batch 198/1077 - Train Accuracy: 0.674, Validation Accuracy: 0.646, Loss: 1.857 Epoch 1 Batch 199/1077 - Train Accuracy: 0.641, Validation Accuracy: 0.647, Loss: 1.827 Epoch 1 Batch 200/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.644, Loss: 1.886 Epoch 1 Batch 201/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.646, Loss: 1.815 Epoch 1 Batch 202/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.643, Loss: 1.848 Epoch 1 Batch 203/1077 - Train Accuracy: 0.624, Validation Accuracy: 0.630, Loss: 1.797 Epoch 1 Batch 204/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.624, Loss: 1.815 Epoch 1 Batch 205/1077 - Train Accuracy: 0.641, Validation Accuracy: 0.619, Loss: 1.797 Epoch 1 Batch 206/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.644, Loss: 1.894 Epoch 1 Batch 207/1077 - Train Accuracy: 0.603, Validation Accuracy: 0.645, Loss: 1.846 Epoch 1 Batch 208/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.642, Loss: 1.796 Epoch 1 Batch 209/1077 - Train Accuracy: 0.651, Validation Accuracy: 0.648, Loss: 1.749 Epoch 1 Batch 210/1077 - Train Accuracy: 0.653, Validation Accuracy: 0.650, Loss: 1.788 Epoch 1 Batch 211/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.659, Loss: 1.862 Epoch 1 Batch 212/1077 - Train Accuracy: 0.663, Validation Accuracy: 0.663, Loss: 1.890 Epoch 1 Batch 213/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.664, Loss: 1.793 Epoch 1 Batch 214/1077 - Train Accuracy: 0.627, Validation Accuracy: 0.655, Loss: 1.810 Epoch 1 Batch 215/1077 - Train Accuracy: 0.607, Validation Accuracy: 0.653, Loss: 1.810 Epoch 1 Batch 216/1077 - Train Accuracy: 0.605, Validation Accuracy: 0.647, Loss: 1.847 Epoch 1 Batch 217/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.642, Loss: 1.860 Epoch 1 Batch 218/1077 - Train Accuracy: 0.653, Validation Accuracy: 0.650, Loss: 1.931 Epoch 1 Batch 219/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.649, Loss: 1.853 Epoch 1 Batch 220/1077 - Train Accuracy: 0.653, Validation Accuracy: 0.650, Loss: 1.902 Epoch 1 Batch 221/1077 - Train Accuracy: 0.695, Validation Accuracy: 0.656, Loss: 1.846 Epoch 1 Batch 222/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.661, Loss: 1.813 Epoch 1 Batch 223/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.658, Loss: 1.851 Epoch 1 Batch 224/1077 - Train Accuracy: 0.666, Validation Accuracy: 0.664, Loss: 1.864 Epoch 1 Batch 225/1077 - Train Accuracy: 0.656, Validation Accuracy: 0.665, Loss: 1.855 Epoch 1 Batch 226/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.660, Loss: 1.814 Epoch 1 Batch 227/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.668, Loss: 1.868 Epoch 1 Batch 228/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.660, Loss: 1.834 Epoch 1 Batch 229/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.659, Loss: 1.763 Epoch 1 Batch 230/1077 - Train Accuracy: 0.672, Validation Accuracy: 0.650, Loss: 1.857 Epoch 1 Batch 231/1077 - Train Accuracy: 0.650, Validation Accuracy: 0.653, Loss: 1.932 Epoch 1 Batch 232/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.662, Loss: 1.760 Epoch 1 Batch 233/1077 - Train Accuracy: 0.637, Validation Accuracy: 0.656, Loss: 1.842 Epoch 1 Batch 234/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.662, Loss: 1.877 Epoch 1 Batch 235/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.672, Loss: 1.819 Epoch 1 Batch 236/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.653, Loss: 1.893 Epoch 1 Batch 237/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.646, Loss: 1.831 Epoch 1 Batch 238/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.640, Loss: 1.785 Epoch 1 Batch 239/1077 - Train Accuracy: 0.661, Validation Accuracy: 0.650, Loss: 1.811 Epoch 1 Batch 240/1077 - Train Accuracy: 0.682, Validation Accuracy: 0.653, Loss: 1.800 Epoch 1 Batch 241/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.663, Loss: 1.806 Epoch 1 Batch 242/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.667, Loss: 1.739 Epoch 1 Batch 243/1077 - Train Accuracy: 0.609, Validation Accuracy: 0.662, Loss: 1.800 Epoch 1 Batch 244/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.662, Loss: 1.880 Epoch 1 Batch 245/1077 - Train Accuracy: 0.660, Validation Accuracy: 0.658, Loss: 1.816 Epoch 1 Batch 246/1077 - Train Accuracy: 0.647, Validation Accuracy: 0.641, Loss: 1.831 Epoch 1 Batch 247/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.632, Loss: 1.809 Epoch 1 Batch 248/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.633, Loss: 1.773 Epoch 1 Batch 249/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.623, Loss: 1.826 Epoch 1 Batch 250/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.623, Loss: 1.813 Epoch 1 Batch 251/1077 - Train Accuracy: 0.635, Validation Accuracy: 0.618, Loss: 1.831 Epoch 1 Batch 252/1077 - Train Accuracy: 0.633, Validation Accuracy: 0.620, Loss: 1.836 Epoch 1 Batch 253/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.642, Loss: 1.844 Epoch 1 Batch 254/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.631, Loss: 1.881 Epoch 1 Batch 255/1077 - Train Accuracy: 0.660, Validation Accuracy: 0.636, Loss: 1.813 Epoch 1 Batch 256/1077 - Train Accuracy: 0.647, Validation Accuracy: 0.629, Loss: 1.859 Epoch 1 Batch 257/1077 - Train Accuracy: 0.665, Validation Accuracy: 0.648, Loss: 1.845 Epoch 1 Batch 258/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.644, Loss: 1.782 Epoch 1 Batch 259/1077 - Train Accuracy: 0.643, Validation Accuracy: 0.643, Loss: 1.823 Epoch 1 Batch 260/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.635, Loss: 1.778 Epoch 1 Batch 261/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.643, Loss: 1.805 Epoch 1 Batch 262/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.638, Loss: 1.828 Epoch 1 Batch 263/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.649, Loss: 1.852 Epoch 1 Batch 264/1077 - Train Accuracy: 0.650, Validation Accuracy: 0.659, Loss: 1.750 Epoch 1 Batch 265/1077 - Train Accuracy: 0.618, Validation Accuracy: 0.643, Loss: 1.746 Epoch 1 Batch 266/1077 - Train Accuracy: 0.649, Validation Accuracy: 0.640, Loss: 1.876 Epoch 1 Batch 267/1077 - Train Accuracy: 0.633, Validation Accuracy: 0.640, Loss: 1.785 Epoch 1 Batch 268/1077 - Train Accuracy: 0.654, Validation Accuracy: 0.639, Loss: 1.780 Epoch 1 Batch 269/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.652, Loss: 1.861 Epoch 1 Batch 270/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.650, Loss: 1.813 Epoch 1 Batch 271/1077 - Train Accuracy: 0.666, Validation Accuracy: 0.626, Loss: 1.862 Epoch 1 Batch 272/1077 - Train Accuracy: 0.640, Validation Accuracy: 0.622, Loss: 1.841 Epoch 1 Batch 273/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.620, Loss: 1.854 Epoch 1 Batch 274/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.625, Loss: 1.742 Epoch 1 Batch 275/1077 - Train Accuracy: 0.641, Validation Accuracy: 0.631, Loss: 1.850 Epoch 1 Batch 276/1077 - Train Accuracy: 0.608, Validation Accuracy: 0.632, Loss: 1.934 Epoch 1 Batch 277/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.643, Loss: 1.861 Epoch 1 Batch 278/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.661, Loss: 1.918 Epoch 1 Batch 279/1077 - Train Accuracy: 0.640, Validation Accuracy: 0.661, Loss: 1.854 Epoch 1 Batch 280/1077 - Train Accuracy: 0.686, Validation Accuracy: 0.664, Loss: 1.786 Epoch 1 Batch 281/1077 - Train Accuracy: 0.628, Validation Accuracy: 0.663, Loss: 1.844 Epoch 1 Batch 282/1077 - Train Accuracy: 0.630, Validation Accuracy: 0.667, Loss: 1.798 Epoch 1 Batch 283/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.663, Loss: 1.884 Epoch 1 Batch 284/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.675, Loss: 1.827 Epoch 1 Batch 285/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.661, Loss: 1.819 Epoch 1 Batch 286/1077 - Train Accuracy: 0.719, Validation Accuracy: 0.670, Loss: 1.796 Epoch 1 Batch 287/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.661, Loss: 1.749 Epoch 1 Batch 288/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.661, Loss: 1.820 Epoch 1 Batch 289/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.668, Loss: 1.884 Epoch 1 Batch 290/1077 - Train Accuracy: 0.661, Validation Accuracy: 0.672, Loss: 1.861 Epoch 1 Batch 291/1077 - Train Accuracy: 0.619, Validation Accuracy: 0.657, Loss: 1.876 Epoch 1 Batch 292/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.651, Loss: 1.864 Epoch 1 Batch 293/1077 - Train Accuracy: 0.626, Validation Accuracy: 0.636, Loss: 1.839 Epoch 1 Batch 294/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.637, Loss: 1.787 Epoch 1 Batch 295/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.659, Loss: 1.904 Epoch 1 Batch 296/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.670, Loss: 1.824 Epoch 1 Batch 297/1077 - Train Accuracy: 0.649, Validation Accuracy: 0.672, Loss: 1.825 Epoch 1 Batch 298/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.678, Loss: 1.822 Epoch 1 Batch 299/1077 - Train Accuracy: 0.656, Validation Accuracy: 0.676, Loss: 1.824 Epoch 1 Batch 300/1077 - Train Accuracy: 0.617, Validation Accuracy: 0.679, Loss: 1.815 Epoch 1 Batch 301/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.679, Loss: 1.836 Epoch 1 Batch 302/1077 - Train Accuracy: 0.678, Validation Accuracy: 0.667, Loss: 1.861 Epoch 1 Batch 303/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.651, Loss: 1.823 Epoch 1 Batch 304/1077 - Train Accuracy: 0.635, Validation Accuracy: 0.673, Loss: 1.867 Epoch 1 Batch 305/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.670, Loss: 1.806 Epoch 1 Batch 306/1077 - Train Accuracy: 0.639, Validation Accuracy: 0.664, Loss: 1.769 Epoch 1 Batch 307/1077 - Train Accuracy: 0.638, Validation Accuracy: 0.658, Loss: 1.803 Epoch 1 Batch 308/1077 - Train Accuracy: 0.620, Validation Accuracy: 0.654, Loss: 1.824 Epoch 1 Batch 309/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.640, Loss: 1.745 Epoch 1 Batch 310/1077 - Train Accuracy: 0.633, Validation Accuracy: 0.638, Loss: 1.817 Epoch 1 Batch 311/1077 - Train Accuracy: 0.703, Validation Accuracy: 0.648, Loss: 1.819 Epoch 1 Batch 312/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.657, Loss: 1.846 Epoch 1 Batch 313/1077 - Train Accuracy: 0.652, Validation Accuracy: 0.667, Loss: 1.842 Epoch 1 Batch 314/1077 - Train Accuracy: 0.680, Validation Accuracy: 0.673, Loss: 1.796 Epoch 1 Batch 315/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.669, Loss: 1.754 Epoch 1 Batch 316/1077 - Train Accuracy: 0.659, Validation Accuracy: 0.670, Loss: 1.801 Epoch 1 Batch 317/1077 - Train Accuracy: 0.678, Validation Accuracy: 0.678, Loss: 1.816 Epoch 1 Batch 318/1077 - Train Accuracy: 0.663, Validation Accuracy: 0.667, Loss: 1.815 Epoch 1 Batch 319/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.657, Loss: 1.802 Epoch 1 Batch 320/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.660, Loss: 1.782 Epoch 1 Batch 321/1077 - Train Accuracy: 0.668, Validation Accuracy: 0.676, Loss: 1.902 Epoch 1 Batch 322/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.672, Loss: 1.737 Epoch 1 Batch 323/1077 - Train Accuracy: 0.629, Validation Accuracy: 0.670, Loss: 1.755 Epoch 1 Batch 324/1077 - Train Accuracy: 0.665, Validation Accuracy: 0.668, Loss: 1.828 Epoch 1 Batch 325/1077 - Train Accuracy: 0.703, Validation Accuracy: 0.673, Loss: 1.790 Epoch 1 Batch 326/1077 - Train Accuracy: 0.688, Validation Accuracy: 0.678, Loss: 1.806 Epoch 1 Batch 327/1077 - Train Accuracy: 0.655, Validation Accuracy: 0.667, Loss: 1.845 Epoch 1 Batch 328/1077 - Train Accuracy: 0.677, Validation Accuracy: 0.657, Loss: 1.844 Epoch 1 Batch 329/1077 - Train Accuracy: 0.659, Validation Accuracy: 0.666, Loss: 1.870 Epoch 1 Batch 330/1077 - Train Accuracy: 0.710, Validation Accuracy: 0.663, Loss: 1.841 Epoch 1 Batch 331/1077 - Train Accuracy: 0.645, Validation Accuracy: 0.656, Loss: 1.771 Epoch 1 Batch 332/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.651, Loss: 1.791 Epoch 1 Batch 333/1077 - Train Accuracy: 0.717, Validation Accuracy: 0.656, Loss: 1.808 Epoch 1 Batch 334/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.662, Loss: 1.737 Epoch 1 Batch 335/1077 - Train Accuracy: 0.697, Validation Accuracy: 0.653, Loss: 1.721 Epoch 1 Batch 336/1077 - Train Accuracy: 0.631, Validation Accuracy: 0.651, Loss: 1.811 Epoch 1 Batch 337/1077 - Train Accuracy: 0.623, Validation Accuracy: 0.656, Loss: 1.824 Epoch 1 Batch 338/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.649, Loss: 1.741 Epoch 1 Batch 339/1077 - Train Accuracy: 0.684, Validation Accuracy: 0.647, Loss: 1.756 Epoch 1 Batch 340/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.652, Loss: 1.830 Epoch 1 Batch 341/1077 - Train Accuracy: 0.707, Validation Accuracy: 0.660, Loss: 1.874 Epoch 1 Batch 342/1077 - Train Accuracy: 0.674, Validation Accuracy: 0.654, Loss: 1.808 Epoch 1 Batch 343/1077 - Train Accuracy: 0.675, Validation Accuracy: 0.650, Loss: 1.854 Epoch 1 Batch 344/1077 - Train Accuracy: 0.693, Validation Accuracy: 0.650, Loss: 1.817 Epoch 1 Batch 345/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.650, Loss: 1.847 Epoch 1 Batch 346/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.645, Loss: 1.796 Epoch 1 Batch 347/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.657, Loss: 1.755 Epoch 1 Batch 348/1077 - Train Accuracy: 0.658, Validation Accuracy: 0.652, Loss: 1.792 Epoch 1 Batch 349/1077 - Train Accuracy: 0.664, Validation Accuracy: 0.642, Loss: 1.832 Epoch 1 Batch 350/1077 - Train Accuracy: 0.681, Validation Accuracy: 0.649, Loss: 1.832 Epoch 1 Batch 351/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.646, Loss: 1.759 Epoch 1 Batch 352/1077 - Train Accuracy: 0.660, Validation Accuracy: 0.656, Loss: 1.769 Epoch 1 Batch 353/1077 - Train Accuracy: 0.641, Validation Accuracy: 0.651, Loss: 1.786 Epoch 1 Batch 354/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.654, Loss: 1.797 Epoch 1 Batch 355/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.672, Loss: 1.877 Epoch 1 Batch 356/1077 - Train Accuracy: 0.680, Validation Accuracy: 0.670, Loss: 1.815 Epoch 1 Batch 357/1077 - Train Accuracy: 0.675, Validation Accuracy: 0.677, Loss: 1.769 Epoch 1 Batch 358/1077 - Train Accuracy: 0.659, Validation Accuracy: 0.678, Loss: 1.814 Epoch 1 Batch 359/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.672, Loss: 1.809 Epoch 1 Batch 360/1077 - Train Accuracy: 0.693, Validation Accuracy: 0.680, Loss: 1.768 Epoch 1 Batch 361/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.656, Loss: 1.839 Epoch 1 Batch 362/1077 - Train Accuracy: 0.702, Validation Accuracy: 0.631, Loss: 1.822 Epoch 1 Batch 363/1077 - Train Accuracy: 0.647, Validation Accuracy: 0.634, Loss: 1.839 Epoch 1 Batch 364/1077 - Train Accuracy: 0.664, Validation Accuracy: 0.643, Loss: 1.803 Epoch 1 Batch 365/1077 - Train Accuracy: 0.632, Validation Accuracy: 0.652, Loss: 1.801 Epoch 1 Batch 366/1077 - Train Accuracy: 0.613, Validation Accuracy: 0.664, Loss: 1.812 Epoch 1 Batch 367/1077 - Train Accuracy: 0.722, Validation Accuracy: 0.662, Loss: 1.780 Epoch 1 Batch 368/1077 - Train Accuracy: 0.684, Validation Accuracy: 0.653, Loss: 1.825 Epoch 1 Batch 369/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.658, Loss: 1.900 Epoch 1 Batch 370/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.665, Loss: 1.831 Epoch 1 Batch 371/1077 - Train Accuracy: 0.700, Validation Accuracy: 0.665, Loss: 1.840 Epoch 1 Batch 372/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.661, Loss: 1.801 Epoch 1 Batch 373/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.664, Loss: 1.863 Epoch 1 Batch 374/1077 - Train Accuracy: 0.652, Validation Accuracy: 0.658, Loss: 1.829 Epoch 1 Batch 375/1077 - Train Accuracy: 0.684, Validation Accuracy: 0.651, Loss: 1.822 Epoch 1 Batch 376/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.637, Loss: 1.845 Epoch 1 Batch 377/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.643, Loss: 1.805 Epoch 1 Batch 378/1077 - Train Accuracy: 0.665, Validation Accuracy: 0.647, Loss: 1.765 Epoch 1 Batch 379/1077 - Train Accuracy: 0.686, Validation Accuracy: 0.653, Loss: 1.798 Epoch 1 Batch 380/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.662, Loss: 1.754 Epoch 1 Batch 381/1077 - Train Accuracy: 0.686, Validation Accuracy: 0.656, Loss: 1.802 Epoch 1 Batch 382/1077 - Train Accuracy: 0.680, Validation Accuracy: 0.656, Loss: 1.891 Epoch 1 Batch 383/1077 - Train Accuracy: 0.642, Validation Accuracy: 0.661, Loss: 1.821 Epoch 1 Batch 384/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.667, Loss: 1.905 Epoch 1 Batch 385/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.677, Loss: 1.814 Epoch 1 Batch 386/1077 - Train Accuracy: 0.654, Validation Accuracy: 0.667, Loss: 1.791 Epoch 1 Batch 387/1077 - Train Accuracy: 0.721, Validation Accuracy: 0.670, Loss: 1.777 Epoch 1 Batch 388/1077 - Train Accuracy: 0.664, Validation Accuracy: 0.671, Loss: 1.850 Epoch 1 Batch 389/1077 - Train Accuracy: 0.691, Validation Accuracy: 0.679, Loss: 1.809 Epoch 1 Batch 390/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.675, Loss: 1.815 Epoch 1 Batch 391/1077 - Train Accuracy: 0.701, Validation Accuracy: 0.672, Loss: 1.775 Epoch 1 Batch 392/1077 - Train Accuracy: 0.681, Validation Accuracy: 0.674, Loss: 1.756 Epoch 1 Batch 393/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.673, Loss: 1.743 Epoch 1 Batch 394/1077 - Train Accuracy: 0.648, Validation Accuracy: 0.678, Loss: 1.856 Epoch 1 Batch 395/1077 - Train Accuracy: 0.675, Validation Accuracy: 0.671, Loss: 1.815 Epoch 1 Batch 396/1077 - Train Accuracy: 0.660, Validation Accuracy: 0.667, Loss: 1.766 Epoch 1 Batch 397/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.662, Loss: 1.809 Epoch 1 Batch 398/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.671, Loss: 1.798 Epoch 1 Batch 399/1077 - Train Accuracy: 0.635, Validation Accuracy: 0.673, Loss: 1.832 Epoch 1 Batch 400/1077 - Train Accuracy: 0.707, Validation Accuracy: 0.672, Loss: 1.760 Epoch 1 Batch 401/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.678, Loss: 1.767 Epoch 1 Batch 402/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.687, Loss: 1.783 Epoch 1 Batch 403/1077 - Train Accuracy: 0.675, Validation Accuracy: 0.673, Loss: 1.809 Epoch 1 Batch 404/1077 - Train Accuracy: 0.698, Validation Accuracy: 0.679, Loss: 1.882 Epoch 1 Batch 405/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.663, Loss: 1.826 Epoch 1 Batch 406/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.658, Loss: 1.820 Epoch 1 Batch 407/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.660, Loss: 1.831 Epoch 1 Batch 408/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.668, Loss: 1.762 Epoch 1 Batch 409/1077 - Train Accuracy: 0.688, Validation Accuracy: 0.670, Loss: 1.836 Epoch 1 Batch 410/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.672, Loss: 1.861 Epoch 1 Batch 411/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.675, Loss: 1.729 Epoch 1 Batch 412/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.677, Loss: 1.730 Epoch 1 Batch 413/1077 - Train Accuracy: 0.674, Validation Accuracy: 0.677, Loss: 1.759 Epoch 1 Batch 414/1077 - Train Accuracy: 0.625, Validation Accuracy: 0.675, Loss: 1.888 Epoch 1 Batch 415/1077 - Train Accuracy: 0.699, Validation Accuracy: 0.673, Loss: 1.744 Epoch 1 Batch 416/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.670, Loss: 1.872 Epoch 1 Batch 417/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.667, Loss: 1.820 Epoch 1 Batch 418/1077 - Train Accuracy: 0.695, Validation Accuracy: 0.670, Loss: 1.879 Epoch 1 Batch 419/1077 - Train Accuracy: 0.675, Validation Accuracy: 0.671, Loss: 1.702 Epoch 1 Batch 420/1077 - Train Accuracy: 0.714, Validation Accuracy: 0.662, Loss: 1.767 Epoch 1 Batch 421/1077 - Train Accuracy: 0.665, Validation Accuracy: 0.680, Loss: 1.758 Epoch 1 Batch 422/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.668, Loss: 1.736 Epoch 1 Batch 423/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.656, Loss: 1.836 Epoch 1 Batch 424/1077 - Train Accuracy: 0.636, Validation Accuracy: 0.650, Loss: 1.793 Epoch 1 Batch 425/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.663, Loss: 1.777 Epoch 1 Batch 426/1077 - Train Accuracy: 0.699, Validation Accuracy: 0.669, Loss: 1.747 Epoch 1 Batch 427/1077 - Train Accuracy: 0.644, Validation Accuracy: 0.665, Loss: 1.787 Epoch 1 Batch 428/1077 - Train Accuracy: 0.688, Validation Accuracy: 0.650, Loss: 1.721 Epoch 1 Batch 429/1077 - Train Accuracy: 0.706, Validation Accuracy: 0.641, Loss: 1.750 Epoch 1 Batch 430/1077 - Train Accuracy: 0.652, Validation Accuracy: 0.661, Loss: 1.848 Epoch 1 Batch 431/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.661, Loss: 1.767 Epoch 1 Batch 432/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.657, Loss: 1.683 Epoch 1 Batch 433/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.664, Loss: 1.812 Epoch 1 Batch 434/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.654, Loss: 1.822 Epoch 1 Batch 435/1077 - Train Accuracy: 0.726, Validation Accuracy: 0.658, Loss: 1.737 Epoch 1 Batch 436/1077 - Train Accuracy: 0.695, Validation Accuracy: 0.663, Loss: 1.691 Epoch 1 Batch 437/1077 - Train Accuracy: 0.665, Validation Accuracy: 0.681, Loss: 1.843 Epoch 1 Batch 438/1077 - Train Accuracy: 0.667, Validation Accuracy: 0.670, Loss: 1.824 Epoch 1 Batch 439/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.657, Loss: 1.778 Epoch 1 Batch 440/1077 - Train Accuracy: 0.693, Validation Accuracy: 0.655, Loss: 1.658 Epoch 1 Batch 441/1077 - Train Accuracy: 0.662, Validation Accuracy: 0.650, Loss: 1.782 Epoch 1 Batch 442/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.642, Loss: 1.778 Epoch 1 Batch 443/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.657, Loss: 1.784 Epoch 1 Batch 444/1077 - Train Accuracy: 0.698, Validation Accuracy: 0.660, Loss: 1.712 Epoch 1 Batch 445/1077 - Train Accuracy: 0.669, Validation Accuracy: 0.644, Loss: 1.851 Epoch 1 Batch 446/1077 - Train Accuracy: 0.687, Validation Accuracy: 0.643, Loss: 1.749 Epoch 1 Batch 447/1077 - Train Accuracy: 0.701, Validation Accuracy: 0.635, Loss: 1.793 Epoch 1 Batch 448/1077 - Train Accuracy: 0.710, Validation Accuracy: 0.637, Loss: 1.777 Epoch 1 Batch 449/1077 - Train Accuracy: 0.669, Validation Accuracy: 0.652, Loss: 1.810 Epoch 1 Batch 450/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.654, Loss: 1.803 Epoch 1 Batch 451/1077 - Train Accuracy: 0.687, Validation Accuracy: 0.655, Loss: 1.717 Epoch 1 Batch 452/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.653, Loss: 1.903 Epoch 1 Batch 453/1077 - Train Accuracy: 0.698, Validation Accuracy: 0.650, Loss: 1.759 Epoch 1 Batch 454/1077 - Train Accuracy: 0.702, Validation Accuracy: 0.644, Loss: 1.837 Epoch 1 Batch 455/1077 - Train Accuracy: 0.693, Validation Accuracy: 0.642, Loss: 1.720 Epoch 1 Batch 456/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.654, Loss: 1.810 Epoch 1 Batch 457/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.667, Loss: 1.765 Epoch 1 Batch 458/1077 - Train Accuracy: 0.652, Validation Accuracy: 0.658, Loss: 1.856 Epoch 1 Batch 459/1077 - Train Accuracy: 0.706, Validation Accuracy: 0.670, Loss: 1.750 Epoch 1 Batch 460/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.675, Loss: 1.822 Epoch 1 Batch 461/1077 - Train Accuracy: 0.703, Validation Accuracy: 0.677, Loss: 1.807 Epoch 1 Batch 462/1077 - Train Accuracy: 0.691, Validation Accuracy: 0.661, Loss: 1.797 Epoch 1 Batch 463/1077 - Train Accuracy: 0.695, Validation Accuracy: 0.665, Loss: 1.830 Epoch 1 Batch 464/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.672, Loss: 1.786 Epoch 1 Batch 465/1077 - Train Accuracy: 0.693, Validation Accuracy: 0.669, Loss: 1.753 Epoch 1 Batch 466/1077 - Train Accuracy: 0.675, Validation Accuracy: 0.664, Loss: 1.843 Epoch 1 Batch 467/1077 - Train Accuracy: 0.739, Validation Accuracy: 0.670, Loss: 1.788 Epoch 1 Batch 468/1077 - Train Accuracy: 0.715, Validation Accuracy: 0.675, Loss: 1.848 Epoch 1 Batch 469/1077 - Train Accuracy: 0.665, Validation Accuracy: 0.667, Loss: 1.828 Epoch 1 Batch 470/1077 - Train Accuracy: 0.713, Validation Accuracy: 0.668, Loss: 1.850 Epoch 1 Batch 471/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.676, Loss: 1.803 Epoch 1 Batch 472/1077 - Train Accuracy: 0.712, Validation Accuracy: 0.677, Loss: 1.713 Epoch 1 Batch 473/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.677, Loss: 1.796 Epoch 1 Batch 474/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.659, Loss: 1.793 Epoch 1 Batch 475/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.667, Loss: 1.780 Epoch 1 Batch 476/1077 - Train Accuracy: 0.687, Validation Accuracy: 0.667, Loss: 1.726 Epoch 1 Batch 477/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.677, Loss: 1.795 Epoch 1 Batch 478/1077 - Train Accuracy: 0.697, Validation Accuracy: 0.670, Loss: 1.737 Epoch 1 Batch 479/1077 - Train Accuracy: 0.686, Validation Accuracy: 0.668, Loss: 1.689 Epoch 1 Batch 480/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.666, Loss: 1.762 Epoch 1 Batch 481/1077 - Train Accuracy: 0.678, Validation Accuracy: 0.675, Loss: 1.820 Epoch 1 Batch 482/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.667, Loss: 1.727 Epoch 1 Batch 483/1077 - Train Accuracy: 0.671, Validation Accuracy: 0.670, Loss: 1.815 Epoch 1 Batch 484/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.674, Loss: 1.742 Epoch 1 Batch 485/1077 - Train Accuracy: 0.730, Validation Accuracy: 0.670, Loss: 1.872 Epoch 1 Batch 486/1077 - Train Accuracy: 0.721, Validation Accuracy: 0.671, Loss: 1.821 Epoch 1 Batch 487/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.678, Loss: 1.887 Epoch 1 Batch 488/1077 - Train Accuracy: 0.687, Validation Accuracy: 0.674, Loss: 1.809 Epoch 1 Batch 489/1077 - Train Accuracy: 0.720, Validation Accuracy: 0.659, Loss: 1.794 Epoch 1 Batch 490/1077 - Train Accuracy: 0.720, Validation Accuracy: 0.665, Loss: 1.789 Epoch 1 Batch 491/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.676, Loss: 1.769 Epoch 1 Batch 492/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.674, Loss: 1.764 Epoch 1 Batch 493/1077 - Train Accuracy: 0.697, Validation Accuracy: 0.676, Loss: 1.692 Epoch 1 Batch 494/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.668, Loss: 1.761 Epoch 1 Batch 495/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.665, Loss: 1.756 Epoch 1 Batch 496/1077 - Train Accuracy: 0.707, Validation Accuracy: 0.662, Loss: 1.762 Epoch 1 Batch 497/1077 - Train Accuracy: 0.678, Validation Accuracy: 0.657, Loss: 1.822 Epoch 1 Batch 498/1077 - Train Accuracy: 0.721, Validation Accuracy: 0.656, Loss: 1.794 Epoch 1 Batch 499/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.659, Loss: 1.786 Epoch 1 Batch 500/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.664, Loss: 1.738 Epoch 1 Batch 501/1077 - Train Accuracy: 0.721, Validation Accuracy: 0.679, Loss: 1.720 Epoch 1 Batch 502/1077 - Train Accuracy: 0.735, Validation Accuracy: 0.673, Loss: 1.817 Epoch 1 Batch 503/1077 - Train Accuracy: 0.741, Validation Accuracy: 0.674, Loss: 1.740 Epoch 1 Batch 504/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.675, Loss: 1.809 Epoch 1 Batch 505/1077 - Train Accuracy: 0.737, Validation Accuracy: 0.676, Loss: 1.705 Epoch 1 Batch 506/1077 - Train Accuracy: 0.646, Validation Accuracy: 0.674, Loss: 1.724 Epoch 1 Batch 507/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.680, Loss: 1.731 Epoch 1 Batch 508/1077 - Train Accuracy: 0.710, Validation Accuracy: 0.679, Loss: 1.754 Epoch 1 Batch 509/1077 - Train Accuracy: 0.714, Validation Accuracy: 0.678, Loss: 1.810 Epoch 1 Batch 510/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.677, Loss: 1.756 Epoch 1 Batch 511/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.678, Loss: 1.804 Epoch 1 Batch 512/1077 - Train Accuracy: 0.753, Validation Accuracy: 0.676, Loss: 1.716 Epoch 1 Batch 513/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.670, Loss: 1.763 Epoch 1 Batch 514/1077 - Train Accuracy: 0.680, Validation Accuracy: 0.669, Loss: 1.711 Epoch 1 Batch 515/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.670, Loss: 1.786 Epoch 1 Batch 516/1077 - Train Accuracy: 0.752, Validation Accuracy: 0.673, Loss: 1.782 Epoch 1 Batch 517/1077 - Train Accuracy: 0.723, Validation Accuracy: 0.685, Loss: 1.784 Epoch 1 Batch 518/1077 - Train Accuracy: 0.709, Validation Accuracy: 0.688, Loss: 1.726 Epoch 1 Batch 519/1077 - Train Accuracy: 0.716, Validation Accuracy: 0.683, Loss: 1.728 Epoch 1 Batch 520/1077 - Train Accuracy: 0.774, Validation Accuracy: 0.682, Loss: 1.694 Epoch 1 Batch 521/1077 - Train Accuracy: 0.716, Validation Accuracy: 0.682, Loss: 1.816 Epoch 1 Batch 522/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.688, Loss: 1.817 Epoch 1 Batch 523/1077 - Train Accuracy: 0.716, Validation Accuracy: 0.686, Loss: 1.798 Epoch 1 Batch 524/1077 - Train Accuracy: 0.696, Validation Accuracy: 0.686, Loss: 1.802 Epoch 1 Batch 525/1077 - Train Accuracy: 0.707, Validation Accuracy: 0.685, Loss: 1.799 Epoch 1 Batch 526/1077 - Train Accuracy: 0.731, Validation Accuracy: 0.679, Loss: 1.774 Epoch 1 Batch 527/1077 - Train Accuracy: 0.691, Validation Accuracy: 0.688, Loss: 1.811 Epoch 1 Batch 528/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.695, Loss: 1.742 Epoch 1 Batch 529/1077 - Train Accuracy: 0.717, Validation Accuracy: 0.692, Loss: 1.778 Epoch 1 Batch 530/1077 - Train Accuracy: 0.694, Validation Accuracy: 0.683, Loss: 1.826 Epoch 1 Batch 531/1077 - Train Accuracy: 0.701, Validation Accuracy: 0.686, Loss: 1.808 Epoch 1 Batch 532/1077 - Train Accuracy: 0.682, Validation Accuracy: 0.678, Loss: 1.820 Epoch 1 Batch 533/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.675, Loss: 1.824 Epoch 1 Batch 534/1077 - Train Accuracy: 0.730, Validation Accuracy: 0.684, Loss: 1.718 Epoch 1 Batch 535/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.682, Loss: 1.794 Epoch 1 Batch 536/1077 - Train Accuracy: 0.742, Validation Accuracy: 0.673, Loss: 1.759 Epoch 1 Batch 537/1077 - Train Accuracy: 0.714, Validation Accuracy: 0.673, Loss: 1.763 Epoch 1 Batch 538/1077 - Train Accuracy: 0.730, Validation Accuracy: 0.677, Loss: 1.757 Epoch 1 Batch 539/1077 - Train Accuracy: 0.693, Validation Accuracy: 0.670, Loss: 1.800 Epoch 1 Batch 540/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.661, Loss: 1.771 Epoch 1 Batch 541/1077 - Train Accuracy: 0.679, Validation Accuracy: 0.679, Loss: 1.738 Epoch 1 Batch 542/1077 - Train Accuracy: 0.713, Validation Accuracy: 0.673, Loss: 1.824 Epoch 1 Batch 543/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.667, Loss: 1.744 Epoch 1 Batch 544/1077 - Train Accuracy: 0.702, Validation Accuracy: 0.672, Loss: 1.767 Epoch 1 Batch 545/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.681, Loss: 1.739 Epoch 1 Batch 546/1077 - Train Accuracy: 0.684, Validation Accuracy: 0.688, Loss: 1.776 Epoch 1 Batch 547/1077 - Train Accuracy: 0.733, Validation Accuracy: 0.694, Loss: 1.791 Epoch 1 Batch 548/1077 - Train Accuracy: 0.723, Validation Accuracy: 0.689, Loss: 1.746 Epoch 1 Batch 549/1077 - Train Accuracy: 0.661, Validation Accuracy: 0.692, Loss: 1.781 Epoch 1 Batch 550/1077 - Train Accuracy: 0.670, Validation Accuracy: 0.687, Loss: 1.804 Epoch 1 Batch 551/1077 - Train Accuracy: 0.673, Validation Accuracy: 0.691, Loss: 1.780 Epoch 1 Batch 552/1077 - Train Accuracy: 0.719, Validation Accuracy: 0.690, Loss: 1.802 Epoch 1 Batch 553/1077 - Train Accuracy: 0.737, Validation Accuracy: 0.699, Loss: 1.793 Epoch 1 Batch 554/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.690, Loss: 1.693 Epoch 1 Batch 555/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.688, Loss: 1.729 Epoch 1 Batch 556/1077 - Train Accuracy: 0.709, Validation Accuracy: 0.694, Loss: 1.728 Epoch 1 Batch 557/1077 - Train Accuracy: 0.726, Validation Accuracy: 0.678, Loss: 1.797 Epoch 1 Batch 558/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.672, Loss: 1.766 Epoch 1 Batch 559/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.673, Loss: 1.828 Epoch 1 Batch 560/1077 - Train Accuracy: 0.702, Validation Accuracy: 0.679, Loss: 1.802 Epoch 1 Batch 561/1077 - Train Accuracy: 0.724, Validation Accuracy: 0.685, Loss: 1.696 Epoch 1 Batch 562/1077 - Train Accuracy: 0.751, Validation Accuracy: 0.682, Loss: 1.712 Epoch 1 Batch 563/1077 - Train Accuracy: 0.682, Validation Accuracy: 0.673, Loss: 1.739 Epoch 1 Batch 564/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.670, Loss: 1.832 Epoch 1 Batch 565/1077 - Train Accuracy: 0.694, Validation Accuracy: 0.672, Loss: 1.787 Epoch 1 Batch 566/1077 - Train Accuracy: 0.714, Validation Accuracy: 0.685, Loss: 1.820 Epoch 1 Batch 567/1077 - Train Accuracy: 0.712, Validation Accuracy: 0.678, Loss: 1.757 Epoch 1 Batch 568/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.687, Loss: 1.801 Epoch 1 Batch 569/1077 - Train Accuracy: 0.733, Validation Accuracy: 0.687, Loss: 1.720 Epoch 1 Batch 570/1077 - Train Accuracy: 0.676, Validation Accuracy: 0.696, Loss: 1.741 Epoch 1 Batch 571/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.697, Loss: 1.713 Epoch 1 Batch 572/1077 - Train Accuracy: 0.728, Validation Accuracy: 0.702, Loss: 1.787 Epoch 1 Batch 573/1077 - Train Accuracy: 0.715, Validation Accuracy: 0.703, Loss: 1.768 Epoch 1 Batch 574/1077 - Train Accuracy: 0.678, Validation Accuracy: 0.688, Loss: 1.750 Epoch 1 Batch 575/1077 - Train Accuracy: 0.708, Validation Accuracy: 0.694, Loss: 1.819 Epoch 1 Batch 576/1077 - Train Accuracy: 0.756, Validation Accuracy: 0.686, Loss: 1.772 Epoch 1 Batch 577/1077 - Train Accuracy: 0.695, Validation Accuracy: 0.684, Loss: 1.810 Epoch 1 Batch 578/1077 - Train Accuracy: 0.698, Validation Accuracy: 0.688, Loss: 1.834 Epoch 1 Batch 579/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.690, Loss: 1.764 Epoch 1 Batch 580/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.692, Loss: 1.747 Epoch 1 Batch 581/1077 - Train Accuracy: 0.731, Validation Accuracy: 0.680, Loss: 1.744 Epoch 1 Batch 582/1077 - Train Accuracy: 0.733, Validation Accuracy: 0.681, Loss: 1.701 Epoch 1 Batch 583/1077 - Train Accuracy: 0.712, Validation Accuracy: 0.680, Loss: 1.798 Epoch 1 Batch 584/1077 - Train Accuracy: 0.713, Validation Accuracy: 0.694, Loss: 1.770 Epoch 1 Batch 585/1077 - Train Accuracy: 0.751, Validation Accuracy: 0.698, Loss: 1.711 Epoch 1 Batch 586/1077 - Train Accuracy: 0.697, Validation Accuracy: 0.705, Loss: 1.769 Epoch 1 Batch 587/1077 - Train Accuracy: 0.723, Validation Accuracy: 0.694, Loss: 1.707 Epoch 1 Batch 588/1077 - Train Accuracy: 0.716, Validation Accuracy: 0.688, Loss: 1.708 Epoch 1 Batch 589/1077 - Train Accuracy: 0.725, Validation Accuracy: 0.683, Loss: 1.780 Epoch 1 Batch 590/1077 - Train Accuracy: 0.698, Validation Accuracy: 0.683, Loss: 1.703 Epoch 1 Batch 591/1077 - Train Accuracy: 0.732, Validation Accuracy: 0.677, Loss: 1.749 Epoch 1 Batch 592/1077 - Train Accuracy: 0.739, Validation Accuracy: 0.673, Loss: 1.736 Epoch 1 Batch 593/1077 - Train Accuracy: 0.713, Validation Accuracy: 0.678, Loss: 1.757 Epoch 1 Batch 594/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.688, Loss: 1.752 Epoch 1 Batch 595/1077 - Train Accuracy: 0.701, Validation Accuracy: 0.689, Loss: 1.666 Epoch 1 Batch 596/1077 - Train Accuracy: 0.693, Validation Accuracy: 0.692, Loss: 1.794 Epoch 1 Batch 597/1077 - Train Accuracy: 0.659, Validation Accuracy: 0.697, Loss: 1.773 Epoch 1 Batch 598/1077 - Train Accuracy: 0.695, Validation Accuracy: 0.705, Loss: 1.700 Epoch 1 Batch 599/1077 - Train Accuracy: 0.698, Validation Accuracy: 0.704, Loss: 1.756 Epoch 1 Batch 600/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.709, Loss: 1.789 Epoch 1 Batch 601/1077 - Train Accuracy: 0.731, Validation Accuracy: 0.706, Loss: 1.755 Epoch 1 Batch 602/1077 - Train Accuracy: 0.720, Validation Accuracy: 0.705, Loss: 1.786 Epoch 1 Batch 603/1077 - Train Accuracy: 0.714, Validation Accuracy: 0.704, Loss: 1.716 Epoch 1 Batch 604/1077 - Train Accuracy: 0.700, Validation Accuracy: 0.704, Loss: 1.829 Epoch 1 Batch 605/1077 - Train Accuracy: 0.725, Validation Accuracy: 0.707, Loss: 1.705 Epoch 1 Batch 606/1077 - Train Accuracy: 0.734, Validation Accuracy: 0.702, Loss: 1.732 Epoch 1 Batch 607/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.700, Loss: 1.728 Epoch 1 Batch 608/1077 - Train Accuracy: 0.734, Validation Accuracy: 0.701, Loss: 1.785 Epoch 1 Batch 609/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.692, Loss: 1.661 Epoch 1 Batch 610/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.685, Loss: 1.800 Epoch 1 Batch 611/1077 - Train Accuracy: 0.717, Validation Accuracy: 0.691, Loss: 1.706 Epoch 1 Batch 612/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.690, Loss: 1.692 Epoch 1 Batch 613/1077 - Train Accuracy: 0.686, Validation Accuracy: 0.683, Loss: 1.836 Epoch 1 Batch 614/1077 - Train Accuracy: 0.717, Validation Accuracy: 0.681, Loss: 1.758 Epoch 1 Batch 615/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.686, Loss: 1.826 Epoch 1 Batch 616/1077 - Train Accuracy: 0.710, Validation Accuracy: 0.678, Loss: 1.758 Epoch 1 Batch 617/1077 - Train Accuracy: 0.709, Validation Accuracy: 0.697, Loss: 1.735 Epoch 1 Batch 618/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.707, Loss: 1.784 Epoch 1 Batch 619/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.698, Loss: 1.756 Epoch 1 Batch 620/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.701, Loss: 1.759 Epoch 1 Batch 621/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.697, Loss: 1.772 Epoch 1 Batch 622/1077 - Train Accuracy: 0.736, Validation Accuracy: 0.700, Loss: 1.682 Epoch 1 Batch 623/1077 - Train Accuracy: 0.691, Validation Accuracy: 0.687, Loss: 1.751 Epoch 1 Batch 624/1077 - Train Accuracy: 0.720, Validation Accuracy: 0.691, Loss: 1.771 Epoch 1 Batch 625/1077 - Train Accuracy: 0.722, Validation Accuracy: 0.695, Loss: 1.762 Epoch 1 Batch 626/1077 - Train Accuracy: 0.761, Validation Accuracy: 0.686, Loss: 1.749 Epoch 1 Batch 627/1077 - Train Accuracy: 0.724, Validation Accuracy: 0.693, Loss: 1.768 Epoch 1 Batch 628/1077 - Train Accuracy: 0.695, Validation Accuracy: 0.703, Loss: 1.739 Epoch 1 Batch 629/1077 - Train Accuracy: 0.690, Validation Accuracy: 0.715, Loss: 1.834 Epoch 1 Batch 630/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.723, Loss: 1.802 Epoch 1 Batch 631/1077 - Train Accuracy: 0.709, Validation Accuracy: 0.731, Loss: 1.706 Epoch 1 Batch 632/1077 - Train Accuracy: 0.733, Validation Accuracy: 0.727, Loss: 1.670 Epoch 1 Batch 633/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.718, Loss: 1.789 Epoch 1 Batch 634/1077 - Train Accuracy: 0.726, Validation Accuracy: 0.699, Loss: 1.825 Epoch 1 Batch 635/1077 - Train Accuracy: 0.712, Validation Accuracy: 0.698, Loss: 1.724 Epoch 1 Batch 636/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.701, Loss: 1.681 Epoch 1 Batch 637/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.702, Loss: 1.671 Epoch 1 Batch 638/1077 - Train Accuracy: 0.710, Validation Accuracy: 0.712, Loss: 1.734 Epoch 1 Batch 639/1077 - Train Accuracy: 0.739, Validation Accuracy: 0.695, Loss: 1.750 Epoch 1 Batch 640/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.697, Loss: 1.784 Epoch 1 Batch 641/1077 - Train Accuracy: 0.720, Validation Accuracy: 0.694, Loss: 1.776 Epoch 1 Batch 642/1077 - Train Accuracy: 0.701, Validation Accuracy: 0.692, Loss: 1.746 Epoch 1 Batch 643/1077 - Train Accuracy: 0.748, Validation Accuracy: 0.682, Loss: 1.746 Epoch 1 Batch 644/1077 - Train Accuracy: 0.732, Validation Accuracy: 0.683, Loss: 1.759 Epoch 1 Batch 645/1077 - Train Accuracy: 0.702, Validation Accuracy: 0.690, Loss: 1.733 Epoch 1 Batch 646/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.687, Loss: 1.704 Epoch 1 Batch 647/1077 - Train Accuracy: 0.723, Validation Accuracy: 0.699, Loss: 1.678 Epoch 1 Batch 648/1077 - Train Accuracy: 0.706, Validation Accuracy: 0.699, Loss: 1.751 Epoch 1 Batch 649/1077 - Train Accuracy: 0.715, Validation Accuracy: 0.694, Loss: 1.793 Epoch 1 Batch 650/1077 - Train Accuracy: 0.692, Validation Accuracy: 0.708, Loss: 1.723 Epoch 1 Batch 651/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.712, Loss: 1.663 Epoch 1 Batch 652/1077 - Train Accuracy: 0.740, Validation Accuracy: 0.717, Loss: 1.666 Epoch 1 Batch 653/1077 - Train Accuracy: 0.709, Validation Accuracy: 0.704, Loss: 1.729 Epoch 1 Batch 654/1077 - Train Accuracy: 0.736, Validation Accuracy: 0.700, Loss: 1.746 Epoch 1 Batch 655/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.705, Loss: 1.726 Epoch 1 Batch 656/1077 - Train Accuracy: 0.684, Validation Accuracy: 0.706, Loss: 1.724 Epoch 1 Batch 657/1077 - Train Accuracy: 0.740, Validation Accuracy: 0.700, Loss: 1.730 Epoch 1 Batch 658/1077 - Train Accuracy: 0.684, Validation Accuracy: 0.692, Loss: 1.771 Epoch 1 Batch 659/1077 - Train Accuracy: 0.737, Validation Accuracy: 0.693, Loss: 1.789 Epoch 1 Batch 660/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.695, Loss: 1.750 Epoch 1 Batch 661/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.695, Loss: 1.706 Epoch 1 Batch 662/1077 - Train Accuracy: 0.727, Validation Accuracy: 0.697, Loss: 1.745 Epoch 1 Batch 663/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.694, Loss: 1.725 Epoch 1 Batch 664/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.690, Loss: 1.809 Epoch 1 Batch 665/1077 - Train Accuracy: 0.709, Validation Accuracy: 0.691, Loss: 1.726 Epoch 1 Batch 666/1077 - Train Accuracy: 0.705, Validation Accuracy: 0.696, Loss: 1.863 Epoch 1 Batch 667/1077 - Train Accuracy: 0.702, Validation Accuracy: 0.699, Loss: 1.703 Epoch 1 Batch 668/1077 - Train Accuracy: 0.717, Validation Accuracy: 0.713, Loss: 1.741 Epoch 1 Batch 669/1077 - Train Accuracy: 0.712, Validation Accuracy: 0.701, Loss: 1.688 Epoch 1 Batch 670/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.700, Loss: 1.698 Epoch 1 Batch 671/1077 - Train Accuracy: 0.715, Validation Accuracy: 0.696, Loss: 1.767 Epoch 1 Batch 672/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.688, Loss: 1.635 Epoch 1 Batch 673/1077 - Train Accuracy: 0.736, Validation Accuracy: 0.692, Loss: 1.731 Epoch 1 Batch 674/1077 - Train Accuracy: 0.764, Validation Accuracy: 0.709, Loss: 1.763 Epoch 1 Batch 675/1077 - Train Accuracy: 0.751, Validation Accuracy: 0.703, Loss: 1.700 Epoch 1 Batch 676/1077 - Train Accuracy: 0.756, Validation Accuracy: 0.702, Loss: 1.766 Epoch 1 Batch 677/1077 - Train Accuracy: 0.685, Validation Accuracy: 0.700, Loss: 1.817 Epoch 1 Batch 678/1077 - Train Accuracy: 0.712, Validation Accuracy: 0.697, Loss: 1.709 Epoch 1 Batch 679/1077 - Train Accuracy: 0.731, Validation Accuracy: 0.699, Loss: 1.685 Epoch 1 Batch 680/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.696, Loss: 1.764 Epoch 1 Batch 681/1077 - Train Accuracy: 0.740, Validation Accuracy: 0.714, Loss: 1.772 Epoch 1 Batch 682/1077 - Train Accuracy: 0.737, Validation Accuracy: 0.714, Loss: 1.735 Epoch 1 Batch 683/1077 - Train Accuracy: 0.740, Validation Accuracy: 0.714, Loss: 1.646 Epoch 1 Batch 684/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.716, Loss: 1.707 Epoch 1 Batch 685/1077 - Train Accuracy: 0.725, Validation Accuracy: 0.720, Loss: 1.656 Epoch 1 Batch 686/1077 - Train Accuracy: 0.760, Validation Accuracy: 0.703, Loss: 1.791 Epoch 1 Batch 687/1077 - Train Accuracy: 0.761, Validation Accuracy: 0.707, Loss: 1.726 Epoch 1 Batch 688/1077 - Train Accuracy: 0.744, Validation Accuracy: 0.702, Loss: 1.682 Epoch 1 Batch 689/1077 - Train Accuracy: 0.719, Validation Accuracy: 0.708, Loss: 1.756 Epoch 1 Batch 690/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.716, Loss: 1.748 Epoch 1 Batch 691/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.716, Loss: 1.734 Epoch 1 Batch 692/1077 - Train Accuracy: 0.741, Validation Accuracy: 0.711, Loss: 1.783 Epoch 1 Batch 693/1077 - Train Accuracy: 0.669, Validation Accuracy: 0.704, Loss: 1.824 Epoch 1 Batch 694/1077 - Train Accuracy: 0.773, Validation Accuracy: 0.718, Loss: 1.764 Epoch 1 Batch 695/1077 - Train Accuracy: 0.740, Validation Accuracy: 0.711, Loss: 1.704 Epoch 1 Batch 696/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.705, Loss: 1.723 Epoch 1 Batch 697/1077 - Train Accuracy: 0.737, Validation Accuracy: 0.700, Loss: 1.719 Epoch 1 Batch 698/1077 - Train Accuracy: 0.747, Validation Accuracy: 0.701, Loss: 1.660 Epoch 1 Batch 699/1077 - Train Accuracy: 0.725, Validation Accuracy: 0.672, Loss: 1.787 Epoch 1 Batch 700/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.678, Loss: 1.717 Epoch 1 Batch 701/1077 - Train Accuracy: 0.730, Validation Accuracy: 0.686, Loss: 1.761 Epoch 1 Batch 702/1077 - Train Accuracy: 0.740, Validation Accuracy: 0.693, Loss: 1.687 Epoch 1 Batch 703/1077 - Train Accuracy: 0.758, Validation Accuracy: 0.706, Loss: 1.712 Epoch 1 Batch 704/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.702, Loss: 1.760 Epoch 1 Batch 705/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.696, Loss: 1.771 Epoch 1 Batch 706/1077 - Train Accuracy: 0.726, Validation Accuracy: 0.706, Loss: 1.726 Epoch 1 Batch 707/1077 - Train Accuracy: 0.752, Validation Accuracy: 0.700, Loss: 1.751 Epoch 1 Batch 708/1077 - Train Accuracy: 0.727, Validation Accuracy: 0.714, Loss: 1.702 Epoch 1 Batch 709/1077 - Train Accuracy: 0.709, Validation Accuracy: 0.713, Loss: 1.718 Epoch 1 Batch 710/1077 - Train Accuracy: 0.716, Validation Accuracy: 0.721, Loss: 1.742 Epoch 1 Batch 711/1077 - Train Accuracy: 0.707, Validation Accuracy: 0.714, Loss: 1.806 Epoch 1 Batch 712/1077 - Train Accuracy: 0.751, Validation Accuracy: 0.707, Loss: 1.684 Epoch 1 Batch 713/1077 - Train Accuracy: 0.740, Validation Accuracy: 0.703, Loss: 1.678 Epoch 1 Batch 714/1077 - Train Accuracy: 0.721, Validation Accuracy: 0.713, Loss: 1.809 Epoch 1 Batch 715/1077 - Train Accuracy: 0.694, Validation Accuracy: 0.734, Loss: 1.741 Epoch 1 Batch 716/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.723, Loss: 1.723 Epoch 1 Batch 717/1077 - Train Accuracy: 0.760, Validation Accuracy: 0.731, Loss: 1.699 Epoch 1 Batch 718/1077 - Train Accuracy: 0.721, Validation Accuracy: 0.704, Loss: 1.799 Epoch 1 Batch 719/1077 - Train Accuracy: 0.715, Validation Accuracy: 0.715, Loss: 1.715 Epoch 1 Batch 720/1077 - Train Accuracy: 0.752, Validation Accuracy: 0.718, Loss: 1.735 Epoch 1 Batch 721/1077 - Train Accuracy: 0.736, Validation Accuracy: 0.736, Loss: 1.696 Epoch 1 Batch 722/1077 - Train Accuracy: 0.727, Validation Accuracy: 0.723, Loss: 1.701 Epoch 1 Batch 723/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.712, Loss: 1.688 Epoch 1 Batch 724/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.714, Loss: 1.756 Epoch 1 Batch 725/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.734, Loss: 1.652 Epoch 1 Batch 726/1077 - Train Accuracy: 0.725, Validation Accuracy: 0.728, Loss: 1.727 Epoch 1 Batch 727/1077 - Train Accuracy: 0.753, Validation Accuracy: 0.738, Loss: 1.740 Epoch 1 Batch 728/1077 - Train Accuracy: 0.733, Validation Accuracy: 0.732, Loss: 1.693 Epoch 1 Batch 729/1077 - Train Accuracy: 0.727, Validation Accuracy: 0.713, Loss: 1.778 Epoch 1 Batch 730/1077 - Train Accuracy: 0.732, Validation Accuracy: 0.710, Loss: 1.843 Epoch 1 Batch 731/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.705, Loss: 1.698 Epoch 1 Batch 732/1077 - Train Accuracy: 0.735, Validation Accuracy: 0.703, Loss: 1.811 Epoch 1 Batch 733/1077 - Train Accuracy: 0.760, Validation Accuracy: 0.710, Loss: 1.784 Epoch 1 Batch 734/1077 - Train Accuracy: 0.724, Validation Accuracy: 0.713, Loss: 1.701 Epoch 1 Batch 735/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.711, Loss: 1.729 Epoch 1 Batch 736/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.704, Loss: 1.725 Epoch 1 Batch 737/1077 - Train Accuracy: 0.700, Validation Accuracy: 0.714, Loss: 1.699 Epoch 1 Batch 738/1077 - Train Accuracy: 0.766, Validation Accuracy: 0.711, Loss: 1.634 Epoch 1 Batch 739/1077 - Train Accuracy: 0.751, Validation Accuracy: 0.713, Loss: 1.755 Epoch 1 Batch 740/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.728, Loss: 1.750 Epoch 1 Batch 741/1077 - Train Accuracy: 0.755, Validation Accuracy: 0.730, Loss: 1.677 Epoch 1 Batch 742/1077 - Train Accuracy: 0.735, Validation Accuracy: 0.719, Loss: 1.775 Epoch 1 Batch 743/1077 - Train Accuracy: 0.722, Validation Accuracy: 0.708, Loss: 1.825 Epoch 1 Batch 744/1077 - Train Accuracy: 0.737, Validation Accuracy: 0.730, Loss: 1.663 Epoch 1 Batch 745/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.723, Loss: 1.699 Epoch 1 Batch 746/1077 - Train Accuracy: 0.758, Validation Accuracy: 0.714, Loss: 1.756 Epoch 1 Batch 747/1077 - Train Accuracy: 0.779, Validation Accuracy: 0.733, Loss: 1.677 Epoch 1 Batch 748/1077 - Train Accuracy: 0.726, Validation Accuracy: 0.734, Loss: 1.725 Epoch 1 Batch 749/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.744, Loss: 1.806 Epoch 1 Batch 750/1077 - Train Accuracy: 0.735, Validation Accuracy: 0.735, Loss: 1.720 Epoch 1 Batch 751/1077 - Train Accuracy: 0.755, Validation Accuracy: 0.741, Loss: 1.846 Epoch 1 Batch 752/1077 - Train Accuracy: 0.726, Validation Accuracy: 0.738, Loss: 1.745 Epoch 1 Batch 753/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.750, Loss: 1.689 Epoch 1 Batch 754/1077 - Train Accuracy: 0.723, Validation Accuracy: 0.739, Loss: 1.830 Epoch 1 Batch 755/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.728, Loss: 1.752 Epoch 1 Batch 756/1077 - Train Accuracy: 0.744, Validation Accuracy: 0.722, Loss: 1.724 Epoch 1 Batch 757/1077 - Train Accuracy: 0.748, Validation Accuracy: 0.707, Loss: 1.712 Epoch 1 Batch 758/1077 - Train Accuracy: 0.747, Validation Accuracy: 0.716, Loss: 1.689 Epoch 1 Batch 759/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.714, Loss: 1.689 Epoch 1 Batch 760/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.721, Loss: 1.698 Epoch 1 Batch 761/1077 - Train Accuracy: 0.694, Validation Accuracy: 0.724, Loss: 1.821 Epoch 1 Batch 762/1077 - Train Accuracy: 0.767, Validation Accuracy: 0.718, Loss: 1.683 Epoch 1 Batch 763/1077 - Train Accuracy: 0.739, Validation Accuracy: 0.723, Loss: 1.701 Epoch 1 Batch 764/1077 - Train Accuracy: 0.750, Validation Accuracy: 0.717, Loss: 1.753 Epoch 1 Batch 765/1077 - Train Accuracy: 0.759, Validation Accuracy: 0.732, Loss: 1.716 Epoch 1 Batch 766/1077 - Train Accuracy: 0.721, Validation Accuracy: 0.733, Loss: 1.704 Epoch 1 Batch 767/1077 - Train Accuracy: 0.781, Validation Accuracy: 0.743, Loss: 1.730 Epoch 1 Batch 768/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.747, Loss: 1.672 Epoch 1 Batch 769/1077 - Train Accuracy: 0.799, Validation Accuracy: 0.756, Loss: 1.664 Epoch 1 Batch 770/1077 - Train Accuracy: 0.747, Validation Accuracy: 0.750, Loss: 1.679 Epoch 1 Batch 771/1077 - Train Accuracy: 0.771, Validation Accuracy: 0.735, Loss: 1.681 Epoch 1 Batch 772/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.721, Loss: 1.692 Epoch 1 Batch 773/1077 - Train Accuracy: 0.758, Validation Accuracy: 0.725, Loss: 1.678 Epoch 1 Batch 774/1077 - Train Accuracy: 0.718, Validation Accuracy: 0.713, Loss: 1.677 Epoch 1 Batch 775/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.730, Loss: 1.772 Epoch 1 Batch 776/1077 - Train Accuracy: 0.741, Validation Accuracy: 0.730, Loss: 1.674 Epoch 1 Batch 777/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.723, Loss: 1.758 Epoch 1 Batch 778/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.717, Loss: 1.672 Epoch 1 Batch 779/1077 - Train Accuracy: 0.736, Validation Accuracy: 0.714, Loss: 1.650 Epoch 1 Batch 780/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.739, Loss: 1.793 Epoch 1 Batch 781/1077 - Train Accuracy: 0.779, Validation Accuracy: 0.731, Loss: 1.648 Epoch 1 Batch 782/1077 - Train Accuracy: 0.772, Validation Accuracy: 0.734, Loss: 1.740 Epoch 1 Batch 783/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.733, Loss: 1.648 Epoch 1 Batch 784/1077 - Train Accuracy: 0.756, Validation Accuracy: 0.732, Loss: 1.660 Epoch 1 Batch 785/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.740, Loss: 1.580 Epoch 1 Batch 786/1077 - Train Accuracy: 0.704, Validation Accuracy: 0.745, Loss: 1.685 Epoch 1 Batch 787/1077 - Train Accuracy: 0.769, Validation Accuracy: 0.748, Loss: 1.726 Epoch 1 Batch 788/1077 - Train Accuracy: 0.799, Validation Accuracy: 0.750, Loss: 1.677 Epoch 1 Batch 789/1077 - Train Accuracy: 0.687, Validation Accuracy: 0.748, Loss: 1.766 Epoch 1 Batch 790/1077 - Train Accuracy: 0.657, Validation Accuracy: 0.740, Loss: 1.764 Epoch 1 Batch 791/1077 - Train Accuracy: 0.756, Validation Accuracy: 0.739, Loss: 1.743 Epoch 1 Batch 792/1077 - Train Accuracy: 0.711, Validation Accuracy: 0.754, Loss: 1.756 Epoch 1 Batch 793/1077 - Train Accuracy: 0.769, Validation Accuracy: 0.732, Loss: 1.702 Epoch 1 Batch 794/1077 - Train Accuracy: 0.723, Validation Accuracy: 0.733, Loss: 1.729 Epoch 1 Batch 795/1077 - Train Accuracy: 0.760, Validation Accuracy: 0.737, Loss: 1.685 Epoch 1 Batch 796/1077 - Train Accuracy: 0.755, Validation Accuracy: 0.743, Loss: 1.710 Epoch 1 Batch 797/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.758, Loss: 1.695 Epoch 1 Batch 798/1077 - Train Accuracy: 0.713, Validation Accuracy: 0.754, Loss: 1.691 Epoch 1 Batch 799/1077 - Train Accuracy: 0.722, Validation Accuracy: 0.762, Loss: 1.736 Epoch 1 Batch 800/1077 - Train Accuracy: 0.776, Validation Accuracy: 0.750, Loss: 1.704 Epoch 1 Batch 801/1077 - Train Accuracy: 0.727, Validation Accuracy: 0.752, Loss: 1.694 Epoch 1 Batch 802/1077 - Train Accuracy: 0.766, Validation Accuracy: 0.753, Loss: 1.730 Epoch 1 Batch 803/1077 - Train Accuracy: 0.741, Validation Accuracy: 0.736, Loss: 1.668 Epoch 1 Batch 804/1077 - Train Accuracy: 0.766, Validation Accuracy: 0.732, Loss: 1.724 Epoch 1 Batch 805/1077 - Train Accuracy: 0.730, Validation Accuracy: 0.737, Loss: 1.737 Epoch 1 Batch 806/1077 - Train Accuracy: 0.749, Validation Accuracy: 0.739, Loss: 1.740 Epoch 1 Batch 807/1077 - Train Accuracy: 0.732, Validation Accuracy: 0.734, Loss: 1.654 Epoch 1 Batch 808/1077 - Train Accuracy: 0.750, Validation Accuracy: 0.723, Loss: 1.719 Epoch 1 Batch 809/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.727, Loss: 1.762 Epoch 1 Batch 810/1077 - Train Accuracy: 0.759, Validation Accuracy: 0.731, Loss: 1.634 Epoch 1 Batch 811/1077 - Train Accuracy: 0.788, Validation Accuracy: 0.721, Loss: 1.703 Epoch 1 Batch 812/1077 - Train Accuracy: 0.717, Validation Accuracy: 0.711, Loss: 1.678 Epoch 1 Batch 813/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.723, Loss: 1.742 Epoch 1 Batch 814/1077 - Train Accuracy: 0.734, Validation Accuracy: 0.741, Loss: 1.752 Epoch 1 Batch 815/1077 - Train Accuracy: 0.762, Validation Accuracy: 0.734, Loss: 1.695 Epoch 1 Batch 816/1077 - Train Accuracy: 0.762, Validation Accuracy: 0.735, Loss: 1.685 Epoch 1 Batch 817/1077 - Train Accuracy: 0.749, Validation Accuracy: 0.739, Loss: 1.696 Epoch 1 Batch 818/1077 - Train Accuracy: 0.719, Validation Accuracy: 0.747, Loss: 1.720 Epoch 1 Batch 819/1077 - Train Accuracy: 0.791, Validation Accuracy: 0.741, Loss: 1.723 Epoch 1 Batch 820/1077 - Train Accuracy: 0.732, Validation Accuracy: 0.730, Loss: 1.653 Epoch 1 Batch 821/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.731, Loss: 1.718 Epoch 1 Batch 822/1077 - Train Accuracy: 0.751, Validation Accuracy: 0.730, Loss: 1.718 Epoch 1 Batch 823/1077 - Train Accuracy: 0.757, Validation Accuracy: 0.735, Loss: 1.621 Epoch 1 Batch 824/1077 - Train Accuracy: 0.764, Validation Accuracy: 0.748, Loss: 1.694 Epoch 1 Batch 825/1077 - Train Accuracy: 0.764, Validation Accuracy: 0.749, Loss: 1.725 Epoch 1 Batch 826/1077 - Train Accuracy: 0.760, Validation Accuracy: 0.747, Loss: 1.664 Epoch 1 Batch 827/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.740, Loss: 1.646 Epoch 1 Batch 828/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.747, Loss: 1.697 Epoch 1 Batch 829/1077 - Train Accuracy: 0.748, Validation Accuracy: 0.748, Loss: 1.680 Epoch 1 Batch 830/1077 - Train Accuracy: 0.746, Validation Accuracy: 0.738, Loss: 1.708 Epoch 1 Batch 831/1077 - Train Accuracy: 0.729, Validation Accuracy: 0.739, Loss: 1.744 Epoch 1 Batch 832/1077 - Train Accuracy: 0.775, Validation Accuracy: 0.729, Loss: 1.712 Epoch 1 Batch 833/1077 - Train Accuracy: 0.735, Validation Accuracy: 0.729, Loss: 1.690 Epoch 1 Batch 834/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.732, Loss: 1.650 Epoch 1 Batch 835/1077 - Train Accuracy: 0.781, Validation Accuracy: 0.725, Loss: 1.721 Epoch 1 Batch 836/1077 - Train Accuracy: 0.759, Validation Accuracy: 0.719, Loss: 1.665 Epoch 1 Batch 837/1077 - Train Accuracy: 0.744, Validation Accuracy: 0.731, Loss: 1.665 Epoch 1 Batch 838/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.734, Loss: 1.686 Epoch 1 Batch 839/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.725, Loss: 1.723 Epoch 1 Batch 840/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.722, Loss: 1.722 Epoch 1 Batch 841/1077 - Train Accuracy: 0.788, Validation Accuracy: 0.723, Loss: 1.694 Epoch 1 Batch 842/1077 - Train Accuracy: 0.795, Validation Accuracy: 0.728, Loss: 1.700 Epoch 1 Batch 843/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.731, Loss: 1.719 Epoch 1 Batch 844/1077 - Train Accuracy: 0.763, Validation Accuracy: 0.743, Loss: 1.731 Epoch 1 Batch 845/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.734, Loss: 1.728 Epoch 1 Batch 846/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.732, Loss: 1.655 Epoch 1 Batch 847/1077 - Train Accuracy: 0.781, Validation Accuracy: 0.748, Loss: 1.712 Epoch 1 Batch 848/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.740, Loss: 1.635 Epoch 1 Batch 849/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.724, Loss: 1.725 Epoch 1 Batch 850/1077 - Train Accuracy: 0.762, Validation Accuracy: 0.730, Loss: 1.804 Epoch 1 Batch 851/1077 - Train Accuracy: 0.791, Validation Accuracy: 0.733, Loss: 1.696 Epoch 1 Batch 852/1077 - Train Accuracy: 0.752, Validation Accuracy: 0.745, Loss: 1.718 Epoch 1 Batch 853/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.746, Loss: 1.717 Epoch 1 Batch 854/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.748, Loss: 1.691 Epoch 1 Batch 855/1077 - Train Accuracy: 0.727, Validation Accuracy: 0.740, Loss: 1.658 Epoch 1 Batch 856/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.748, Loss: 1.708 Epoch 1 Batch 857/1077 - Train Accuracy: 0.787, Validation Accuracy: 0.742, Loss: 1.707 Epoch 1 Batch 858/1077 - Train Accuracy: 0.757, Validation Accuracy: 0.743, Loss: 1.741 Epoch 1 Batch 859/1077 - Train Accuracy: 0.769, Validation Accuracy: 0.747, Loss: 1.742 Epoch 1 Batch 860/1077 - Train Accuracy: 0.749, Validation Accuracy: 0.745, Loss: 1.740 Epoch 1 Batch 861/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.734, Loss: 1.621 Epoch 1 Batch 862/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.740, Loss: 1.691 Epoch 1 Batch 863/1077 - Train Accuracy: 0.750, Validation Accuracy: 0.736, Loss: 1.716 Epoch 1 Batch 864/1077 - Train Accuracy: 0.752, Validation Accuracy: 0.743, Loss: 1.691 Epoch 1 Batch 865/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.749, Loss: 1.637 Epoch 1 Batch 866/1077 - Train Accuracy: 0.794, Validation Accuracy: 0.752, Loss: 1.770 Epoch 1 Batch 867/1077 - Train Accuracy: 0.731, Validation Accuracy: 0.748, Loss: 1.751 Epoch 1 Batch 868/1077 - Train Accuracy: 0.803, Validation Accuracy: 0.746, Loss: 1.688 Epoch 1 Batch 869/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.746, Loss: 1.679 Epoch 1 Batch 870/1077 - Train Accuracy: 0.747, Validation Accuracy: 0.775, Loss: 1.717 Epoch 1 Batch 871/1077 - Train Accuracy: 0.759, Validation Accuracy: 0.778, Loss: 1.729 Epoch 1 Batch 872/1077 - Train Accuracy: 0.786, Validation Accuracy: 0.782, Loss: 1.654 Epoch 1 Batch 873/1077 - Train Accuracy: 0.763, Validation Accuracy: 0.793, Loss: 1.675 Epoch 1 Batch 874/1077 - Train Accuracy: 0.752, Validation Accuracy: 0.792, Loss: 1.659 Epoch 1 Batch 875/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.773, Loss: 1.701 Epoch 1 Batch 876/1077 - Train Accuracy: 0.775, Validation Accuracy: 0.766, Loss: 1.716 Epoch 1 Batch 877/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.776, Loss: 1.608 Epoch 1 Batch 878/1077 - Train Accuracy: 0.808, Validation Accuracy: 0.757, Loss: 1.722 Epoch 1 Batch 879/1077 - Train Accuracy: 0.800, Validation Accuracy: 0.740, Loss: 1.582 Epoch 1 Batch 880/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.744, Loss: 1.724 Epoch 1 Batch 881/1077 - Train Accuracy: 0.727, Validation Accuracy: 0.746, Loss: 1.654 Epoch 1 Batch 882/1077 - Train Accuracy: 0.786, Validation Accuracy: 0.744, Loss: 1.741 Epoch 1 Batch 883/1077 - Train Accuracy: 0.698, Validation Accuracy: 0.751, Loss: 1.709 Epoch 1 Batch 884/1077 - Train Accuracy: 0.767, Validation Accuracy: 0.742, Loss: 1.715 Epoch 1 Batch 885/1077 - Train Accuracy: 0.809, Validation Accuracy: 0.747, Loss: 1.647 Epoch 1 Batch 886/1077 - Train Accuracy: 0.760, Validation Accuracy: 0.744, Loss: 1.739 Epoch 1 Batch 887/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.727, Loss: 1.639 Epoch 1 Batch 888/1077 - Train Accuracy: 0.772, Validation Accuracy: 0.731, Loss: 1.718 Epoch 1 Batch 889/1077 - Train Accuracy: 0.772, Validation Accuracy: 0.732, Loss: 1.699 Epoch 1 Batch 890/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.738, Loss: 1.634 Epoch 1 Batch 891/1077 - Train Accuracy: 0.795, Validation Accuracy: 0.749, Loss: 1.662 Epoch 1 Batch 892/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.746, Loss: 1.685 Epoch 1 Batch 893/1077 - Train Accuracy: 0.774, Validation Accuracy: 0.746, Loss: 1.631 Epoch 1 Batch 894/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.764, Loss: 1.678 Epoch 1 Batch 895/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.758, Loss: 1.623 Epoch 1 Batch 896/1077 - Train Accuracy: 0.731, Validation Accuracy: 0.767, Loss: 1.628 Epoch 1 Batch 897/1077 - Train Accuracy: 0.795, Validation Accuracy: 0.771, Loss: 1.636 Epoch 1 Batch 898/1077 - Train Accuracy: 0.788, Validation Accuracy: 0.752, Loss: 1.681 Epoch 1 Batch 899/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.755, Loss: 1.649 Epoch 1 Batch 900/1077 - Train Accuracy: 0.775, Validation Accuracy: 0.750, Loss: 1.660 Epoch 1 Batch 901/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.756, Loss: 1.688 Epoch 1 Batch 902/1077 - Train Accuracy: 0.786, Validation Accuracy: 0.752, Loss: 1.710 Epoch 1 Batch 903/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.767, Loss: 1.707 Epoch 1 Batch 904/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.759, Loss: 1.657 Epoch 1 Batch 905/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.753, Loss: 1.600 Epoch 1 Batch 906/1077 - Train Accuracy: 0.791, Validation Accuracy: 0.756, Loss: 1.718 Epoch 1 Batch 907/1077 - Train Accuracy: 0.803, Validation Accuracy: 0.751, Loss: 1.694 Epoch 1 Batch 908/1077 - Train Accuracy: 0.777, Validation Accuracy: 0.752, Loss: 1.738 Epoch 1 Batch 909/1077 - Train Accuracy: 0.774, Validation Accuracy: 0.749, Loss: 1.706 Epoch 1 Batch 910/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.750, Loss: 1.609 Epoch 1 Batch 911/1077 - Train Accuracy: 0.813, Validation Accuracy: 0.735, Loss: 1.661 Epoch 1 Batch 912/1077 - Train Accuracy: 0.761, Validation Accuracy: 0.743, Loss: 1.703 Epoch 1 Batch 913/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.751, Loss: 1.679 Epoch 1 Batch 914/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.746, Loss: 1.635 Epoch 1 Batch 915/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.757, Loss: 1.654 Epoch 1 Batch 916/1077 - Train Accuracy: 0.774, Validation Accuracy: 0.763, Loss: 1.690 Epoch 1 Batch 917/1077 - Train Accuracy: 0.773, Validation Accuracy: 0.785, Loss: 1.578 Epoch 1 Batch 918/1077 - Train Accuracy: 0.795, Validation Accuracy: 0.773, Loss: 1.652 Epoch 1 Batch 919/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.786, Loss: 1.667 Epoch 1 Batch 920/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.784, Loss: 1.659 Epoch 1 Batch 921/1077 - Train Accuracy: 0.771, Validation Accuracy: 0.770, Loss: 1.665 Epoch 1 Batch 922/1077 - Train Accuracy: 0.737, Validation Accuracy: 0.774, Loss: 1.634 Epoch 1 Batch 923/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.775, Loss: 1.601 Epoch 1 Batch 924/1077 - Train Accuracy: 0.767, Validation Accuracy: 0.763, Loss: 1.638 Epoch 1 Batch 925/1077 - Train Accuracy: 0.753, Validation Accuracy: 0.770, Loss: 1.641 Epoch 1 Batch 926/1077 - Train Accuracy: 0.754, Validation Accuracy: 0.756, Loss: 1.594 Epoch 1 Batch 927/1077 - Train Accuracy: 0.753, Validation Accuracy: 0.757, Loss: 1.730 Epoch 1 Batch 928/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.759, Loss: 1.662 Epoch 1 Batch 929/1077 - Train Accuracy: 0.748, Validation Accuracy: 0.758, Loss: 1.660 Epoch 1 Batch 930/1077 - Train Accuracy: 0.763, Validation Accuracy: 0.755, Loss: 1.659 Epoch 1 Batch 931/1077 - Train Accuracy: 0.769, Validation Accuracy: 0.758, Loss: 1.728 Epoch 1 Batch 932/1077 - Train Accuracy: 0.744, Validation Accuracy: 0.755, Loss: 1.684 Epoch 1 Batch 933/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.750, Loss: 1.676 Epoch 1 Batch 934/1077 - Train Accuracy: 0.771, Validation Accuracy: 0.740, Loss: 1.646 Epoch 1 Batch 935/1077 - Train Accuracy: 0.786, Validation Accuracy: 0.737, Loss: 1.697 Epoch 1 Batch 936/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.750, Loss: 1.681 Epoch 1 Batch 937/1077 - Train Accuracy: 0.807, Validation Accuracy: 0.766, Loss: 1.737 Epoch 1 Batch 938/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.760, Loss: 1.674 Epoch 1 Batch 939/1077 - Train Accuracy: 0.773, Validation Accuracy: 0.771, Loss: 1.671 Epoch 1 Batch 940/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.759, Loss: 1.608 Epoch 1 Batch 941/1077 - Train Accuracy: 0.778, Validation Accuracy: 0.745, Loss: 1.662 Epoch 1 Batch 942/1077 - Train Accuracy: 0.779, Validation Accuracy: 0.759, Loss: 1.678 Epoch 1 Batch 943/1077 - Train Accuracy: 0.764, Validation Accuracy: 0.756, Loss: 1.681 Epoch 1 Batch 944/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.780, Loss: 1.666 Epoch 1 Batch 945/1077 - Train Accuracy: 0.821, Validation Accuracy: 0.773, Loss: 1.623 Epoch 1 Batch 946/1077 - Train Accuracy: 0.778, Validation Accuracy: 0.776, Loss: 1.696 Epoch 1 Batch 947/1077 - Train Accuracy: 0.689, Validation Accuracy: 0.768, Loss: 1.698 Epoch 1 Batch 948/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.769, Loss: 1.795 Epoch 1 Batch 949/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.761, Loss: 1.712 Epoch 1 Batch 950/1077 - Train Accuracy: 0.795, Validation Accuracy: 0.773, Loss: 1.667 Epoch 1 Batch 951/1077 - Train Accuracy: 0.775, Validation Accuracy: 0.762, Loss: 1.660 Epoch 1 Batch 952/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.768, Loss: 1.642 Epoch 1 Batch 953/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.756, Loss: 1.655 Epoch 1 Batch 954/1077 - Train Accuracy: 0.716, Validation Accuracy: 0.755, Loss: 1.746 Epoch 1 Batch 955/1077 - Train Accuracy: 0.735, Validation Accuracy: 0.757, Loss: 1.640 Epoch 1 Batch 956/1077 - Train Accuracy: 0.793, Validation Accuracy: 0.772, Loss: 1.638 Epoch 1 Batch 957/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.772, Loss: 1.662 Epoch 1 Batch 958/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.776, Loss: 1.658 Epoch 1 Batch 959/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.773, Loss: 1.638 Epoch 1 Batch 960/1077 - Train Accuracy: 0.797, Validation Accuracy: 0.767, Loss: 1.708 Epoch 1 Batch 961/1077 - Train Accuracy: 0.777, Validation Accuracy: 0.776, Loss: 1.613 Epoch 1 Batch 962/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.786, Loss: 1.650 Epoch 1 Batch 963/1077 - Train Accuracy: 0.794, Validation Accuracy: 0.783, Loss: 1.715 Epoch 1 Batch 964/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.779, Loss: 1.659 Epoch 1 Batch 965/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.781, Loss: 1.640 Epoch 1 Batch 966/1077 - Train Accuracy: 0.813, Validation Accuracy: 0.784, Loss: 1.676 Epoch 1 Batch 967/1077 - Train Accuracy: 0.795, Validation Accuracy: 0.767, Loss: 1.656 Epoch 1 Batch 968/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.773, Loss: 1.731 Epoch 1 Batch 969/1077 - Train Accuracy: 0.785, Validation Accuracy: 0.764, Loss: 1.650 Epoch 1 Batch 970/1077 - Train Accuracy: 0.786, Validation Accuracy: 0.770, Loss: 1.709 Epoch 1 Batch 971/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.754, Loss: 1.729 Epoch 1 Batch 972/1077 - Train Accuracy: 0.745, Validation Accuracy: 0.760, Loss: 1.661 Epoch 1 Batch 973/1077 - Train Accuracy: 0.809, Validation Accuracy: 0.765, Loss: 1.597 Epoch 1 Batch 974/1077 - Train Accuracy: 0.756, Validation Accuracy: 0.784, Loss: 1.736 Epoch 1 Batch 975/1077 - Train Accuracy: 0.769, Validation Accuracy: 0.785, Loss: 1.671 Epoch 1 Batch 976/1077 - Train Accuracy: 0.769, Validation Accuracy: 0.777, Loss: 1.616 Epoch 1 Batch 977/1077 - Train Accuracy: 0.796, Validation Accuracy: 0.772, Loss: 1.670 Epoch 1 Batch 978/1077 - Train Accuracy: 0.822, Validation Accuracy: 0.783, Loss: 1.719 Epoch 1 Batch 979/1077 - Train Accuracy: 0.769, Validation Accuracy: 0.791, Loss: 1.682 Epoch 1 Batch 980/1077 - Train Accuracy: 0.781, Validation Accuracy: 0.792, Loss: 1.702 Epoch 1 Batch 981/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.807, Loss: 1.705 Epoch 1 Batch 982/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.800, Loss: 1.662 Epoch 1 Batch 983/1077 - Train Accuracy: 0.756, Validation Accuracy: 0.801, Loss: 1.736 Epoch 1 Batch 984/1077 - Train Accuracy: 0.748, Validation Accuracy: 0.812, Loss: 1.651 Epoch 1 Batch 985/1077 - Train Accuracy: 0.810, Validation Accuracy: 0.808, Loss: 1.619 Epoch 1 Batch 986/1077 - Train Accuracy: 0.766, Validation Accuracy: 0.803, Loss: 1.611 Epoch 1 Batch 987/1077 - Train Accuracy: 0.743, Validation Accuracy: 0.782, Loss: 1.657 Epoch 1 Batch 988/1077 - Train Accuracy: 0.779, Validation Accuracy: 0.784, Loss: 1.708 Epoch 1 Batch 989/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.773, Loss: 1.681 Epoch 1 Batch 990/1077 - Train Accuracy: 0.761, Validation Accuracy: 0.776, Loss: 1.668 Epoch 1 Batch 991/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.769, Loss: 1.648 Epoch 1 Batch 992/1077 - Train Accuracy: 0.793, Validation Accuracy: 0.762, Loss: 1.606 Epoch 1 Batch 993/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.765, Loss: 1.608 Epoch 1 Batch 994/1077 - Train Accuracy: 0.779, Validation Accuracy: 0.773, Loss: 1.636 Epoch 1 Batch 995/1077 - Train Accuracy: 0.807, Validation Accuracy: 0.772, Loss: 1.606 Epoch 1 Batch 996/1077 - Train Accuracy: 0.809, Validation Accuracy: 0.781, Loss: 1.676 Epoch 1 Batch 997/1077 - Train Accuracy: 0.763, Validation Accuracy: 0.783, Loss: 1.713 Epoch 1 Batch 998/1077 - Train Accuracy: 0.742, Validation Accuracy: 0.788, Loss: 1.704 Epoch 1 Batch 999/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.795, Loss: 1.638 Epoch 1 Batch 1000/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.784, Loss: 1.631 Epoch 1 Batch 1001/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.788, Loss: 1.628 Epoch 1 Batch 1002/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.784, Loss: 1.665 Epoch 1 Batch 1003/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.769, Loss: 1.660 Epoch 1 Batch 1004/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.773, Loss: 1.670 Epoch 1 Batch 1005/1077 - Train Accuracy: 0.797, Validation Accuracy: 0.788, Loss: 1.659 Epoch 1 Batch 1006/1077 - Train Accuracy: 0.809, Validation Accuracy: 0.784, Loss: 1.612 Epoch 1 Batch 1007/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.778, Loss: 1.628 Epoch 1 Batch 1008/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.772, Loss: 1.683 Epoch 1 Batch 1009/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.771, Loss: 1.617 Epoch 1 Batch 1010/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.785, Loss: 1.681 Epoch 1 Batch 1011/1077 - Train Accuracy: 0.771, Validation Accuracy: 0.782, Loss: 1.644 Epoch 1 Batch 1012/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.779, Loss: 1.618 Epoch 1 Batch 1013/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.780, Loss: 1.617 Epoch 1 Batch 1014/1077 - Train Accuracy: 0.772, Validation Accuracy: 0.778, Loss: 1.750 Epoch 1 Batch 1015/1077 - Train Accuracy: 0.734, Validation Accuracy: 0.779, Loss: 1.663 Epoch 1 Batch 1016/1077 - Train Accuracy: 0.738, Validation Accuracy: 0.782, Loss: 1.675 Epoch 1 Batch 1017/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.784, Loss: 1.696 Epoch 1 Batch 1018/1077 - Train Accuracy: 0.747, Validation Accuracy: 0.780, Loss: 1.672 Epoch 1 Batch 1019/1077 - Train Accuracy: 0.749, Validation Accuracy: 0.783, Loss: 1.680 Epoch 1 Batch 1020/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.788, Loss: 1.694 Epoch 1 Batch 1021/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.804, Loss: 1.681 Epoch 1 Batch 1022/1077 - Train Accuracy: 0.815, Validation Accuracy: 0.800, Loss: 1.636 Epoch 1 Batch 1023/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.805, Loss: 1.675 Epoch 1 Batch 1024/1077 - Train Accuracy: 0.796, Validation Accuracy: 0.792, Loss: 1.615 Epoch 1 Batch 1025/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.794, Loss: 1.674 Epoch 1 Batch 1026/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.796, Loss: 1.667 Epoch 1 Batch 1027/1077 - Train Accuracy: 0.753, Validation Accuracy: 0.792, Loss: 1.714 Epoch 1 Batch 1028/1077 - Train Accuracy: 0.749, Validation Accuracy: 0.799, Loss: 1.687 Epoch 1 Batch 1029/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.802, Loss: 1.647 Epoch 1 Batch 1030/1077 - Train Accuracy: 0.759, Validation Accuracy: 0.783, Loss: 1.735 Epoch 1 Batch 1031/1077 - Train Accuracy: 0.763, Validation Accuracy: 0.778, Loss: 1.674 Epoch 1 Batch 1032/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.773, Loss: 1.719 Epoch 1 Batch 1033/1077 - Train Accuracy: 0.774, Validation Accuracy: 0.769, Loss: 1.609 Epoch 1 Batch 1034/1077 - Train Accuracy: 0.766, Validation Accuracy: 0.771, Loss: 1.640 Epoch 1 Batch 1035/1077 - Train Accuracy: 0.822, Validation Accuracy: 0.763, Loss: 1.698 Epoch 1 Batch 1036/1077 - Train Accuracy: 0.763, Validation Accuracy: 0.752, Loss: 1.627 Epoch 1 Batch 1037/1077 - Train Accuracy: 0.778, Validation Accuracy: 0.763, Loss: 1.646 Epoch 1 Batch 1038/1077 - Train Accuracy: 0.813, Validation Accuracy: 0.776, Loss: 1.633 Epoch 1 Batch 1039/1077 - Train Accuracy: 0.785, Validation Accuracy: 0.783, Loss: 1.695 Epoch 1 Batch 1040/1077 - Train Accuracy: 0.784, Validation Accuracy: 0.798, Loss: 1.655 Epoch 1 Batch 1041/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.801, Loss: 1.625 Epoch 1 Batch 1042/1077 - Train Accuracy: 0.793, Validation Accuracy: 0.801, Loss: 1.638 Epoch 1 Batch 1043/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.809, Loss: 1.767 Epoch 1 Batch 1044/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.799, Loss: 1.670 Epoch 1 Batch 1045/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.793, Loss: 1.748 Epoch 1 Batch 1046/1077 - Train Accuracy: 0.800, Validation Accuracy: 0.787, Loss: 1.655 Epoch 1 Batch 1047/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.784, Loss: 1.676 Epoch 1 Batch 1048/1077 - Train Accuracy: 0.766, Validation Accuracy: 0.784, Loss: 1.645 Epoch 1 Batch 1049/1077 - Train Accuracy: 0.778, Validation Accuracy: 0.797, Loss: 1.607 Epoch 1 Batch 1050/1077 - Train Accuracy: 0.778, Validation Accuracy: 0.801, Loss: 1.665 Epoch 1 Batch 1051/1077 - Train Accuracy: 0.815, Validation Accuracy: 0.798, Loss: 1.645 Epoch 1 Batch 1052/1077 - Train Accuracy: 0.826, Validation Accuracy: 0.784, Loss: 1.638 Epoch 1 Batch 1053/1077 - Train Accuracy: 0.813, Validation Accuracy: 0.797, Loss: 1.593 Epoch 1 Batch 1054/1077 - Train Accuracy: 0.799, Validation Accuracy: 0.793, Loss: 1.612 Epoch 1 Batch 1055/1077 - Train Accuracy: 0.803, Validation Accuracy: 0.796, Loss: 1.673 Epoch 1 Batch 1056/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.784, Loss: 1.681 Epoch 1 Batch 1057/1077 - Train Accuracy: 0.791, Validation Accuracy: 0.785, Loss: 1.618 Epoch 1 Batch 1058/1077 - Train Accuracy: 0.796, Validation Accuracy: 0.793, Loss: 1.680 Epoch 1 Batch 1059/1077 - Train Accuracy: 0.771, Validation Accuracy: 0.784, Loss: 1.668 Epoch 1 Batch 1060/1077 - Train Accuracy: 0.810, Validation Accuracy: 0.786, Loss: 1.617 Epoch 1 Batch 1061/1077 - Train Accuracy: 0.742, Validation Accuracy: 0.776, Loss: 1.578 Epoch 1 Batch 1062/1077 - Train Accuracy: 0.770, Validation Accuracy: 0.782, Loss: 1.663 Epoch 1 Batch 1063/1077 - Train Accuracy: 0.762, Validation Accuracy: 0.787, Loss: 1.630 Epoch 1 Batch 1064/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.798, Loss: 1.670 Epoch 1 Batch 1065/1077 - Train Accuracy: 0.820, Validation Accuracy: 0.800, Loss: 1.655 Epoch 1 Batch 1066/1077 - Train Accuracy: 0.807, Validation Accuracy: 0.800, Loss: 1.767 Epoch 1 Batch 1067/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.799, Loss: 1.725 Epoch 1 Batch 1068/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.798, Loss: 1.678 Epoch 1 Batch 1069/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.801, Loss: 1.662 Epoch 1 Batch 1070/1077 - Train Accuracy: 0.752, Validation Accuracy: 0.797, Loss: 1.674 Epoch 1 Batch 1071/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.797, Loss: 1.662 Epoch 1 Batch 1072/1077 - Train Accuracy: 0.793, Validation Accuracy: 0.796, Loss: 1.611 Epoch 1 Batch 1073/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.795, Loss: 1.641 Epoch 1 Batch 1074/1077 - Train Accuracy: 0.810, Validation Accuracy: 0.783, Loss: 1.622 Epoch 1 Batch 1075/1077 - Train Accuracy: 0.785, Validation Accuracy: 0.778, Loss: 1.662 Epoch 2 Batch 0/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.791, Loss: 1.564 Epoch 2 Batch 1/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.790, Loss: 1.646 Epoch 2 Batch 2/1077 - Train Accuracy: 0.762, Validation Accuracy: 0.794, Loss: 1.639 Epoch 2 Batch 3/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.796, Loss: 1.661 Epoch 2 Batch 4/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.794, Loss: 1.712 Epoch 2 Batch 5/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.790, Loss: 1.669 Epoch 2 Batch 6/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.800, Loss: 1.584 Epoch 2 Batch 7/1077 - Train Accuracy: 0.813, Validation Accuracy: 0.811, Loss: 1.666 Epoch 2 Batch 8/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.805, Loss: 1.697 Epoch 2 Batch 9/1077 - Train Accuracy: 0.807, Validation Accuracy: 0.790, Loss: 1.630 Epoch 2 Batch 10/1077 - Train Accuracy: 0.803, Validation Accuracy: 0.778, Loss: 1.651 Epoch 2 Batch 11/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.790, Loss: 1.623 Epoch 2 Batch 12/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.809, Loss: 1.645 Epoch 2 Batch 13/1077 - Train Accuracy: 0.800, Validation Accuracy: 0.808, Loss: 1.551 Epoch 2 Batch 14/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.823, Loss: 1.581 Epoch 2 Batch 15/1077 - Train Accuracy: 0.820, Validation Accuracy: 0.818, Loss: 1.646 Epoch 2 Batch 16/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.805, Loss: 1.601 Epoch 2 Batch 17/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.791, Loss: 1.615 Epoch 2 Batch 18/1077 - Train Accuracy: 0.775, Validation Accuracy: 0.796, Loss: 1.686 Epoch 2 Batch 19/1077 - Train Accuracy: 0.806, Validation Accuracy: 0.795, Loss: 1.624 Epoch 2 Batch 20/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.791, Loss: 1.549 Epoch 2 Batch 21/1077 - Train Accuracy: 0.755, Validation Accuracy: 0.789, Loss: 1.598 Epoch 2 Batch 22/1077 - Train Accuracy: 0.803, Validation Accuracy: 0.788, Loss: 1.563 Epoch 2 Batch 23/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.785, Loss: 1.688 Epoch 2 Batch 24/1077 - Train Accuracy: 0.819, Validation Accuracy: 0.794, Loss: 1.656 Epoch 2 Batch 25/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.771, Loss: 1.584 Epoch 2 Batch 26/1077 - Train Accuracy: 0.768, Validation Accuracy: 0.778, Loss: 1.731 Epoch 2 Batch 27/1077 - Train Accuracy: 0.826, Validation Accuracy: 0.775, Loss: 1.651 Epoch 2 Batch 28/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.787, Loss: 1.618 Epoch 2 Batch 29/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.798, Loss: 1.636 Epoch 2 Batch 30/1077 - Train Accuracy: 0.793, Validation Accuracy: 0.788, Loss: 1.620 Epoch 2 Batch 31/1077 - Train Accuracy: 0.781, Validation Accuracy: 0.790, Loss: 1.609 Epoch 2 Batch 32/1077 - Train Accuracy: 0.796, Validation Accuracy: 0.792, Loss: 1.683 Epoch 2 Batch 33/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.793, Loss: 1.572 Epoch 2 Batch 34/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.802, Loss: 1.625 Epoch 2 Batch 35/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.793, Loss: 1.573 Epoch 2 Batch 36/1077 - Train Accuracy: 0.773, Validation Accuracy: 0.800, Loss: 1.655 Epoch 2 Batch 37/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.807, Loss: 1.633 Epoch 2 Batch 38/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.817, Loss: 1.738 Epoch 2 Batch 39/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.829, Loss: 1.645 Epoch 2 Batch 40/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.836, Loss: 1.692 Epoch 2 Batch 41/1077 - Train Accuracy: 0.807, Validation Accuracy: 0.835, Loss: 1.653 Epoch 2 Batch 42/1077 - Train Accuracy: 0.795, Validation Accuracy: 0.825, Loss: 1.593 Epoch 2 Batch 43/1077 - Train Accuracy: 0.837, Validation Accuracy: 0.821, Loss: 1.639 Epoch 2 Batch 44/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.815, Loss: 1.764 Epoch 2 Batch 45/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.816, Loss: 1.659 Epoch 2 Batch 46/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.811, Loss: 1.668 Epoch 2 Batch 47/1077 - Train Accuracy: 0.834, Validation Accuracy: 0.805, Loss: 1.683 Epoch 2 Batch 48/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.806, Loss: 1.684 Epoch 2 Batch 49/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.809, Loss: 1.621 Epoch 2 Batch 50/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.809, Loss: 1.639 Epoch 2 Batch 51/1077 - Train Accuracy: 0.806, Validation Accuracy: 0.805, Loss: 1.560 Epoch 2 Batch 52/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.814, Loss: 1.589 Epoch 2 Batch 53/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.814, Loss: 1.612 Epoch 2 Batch 54/1077 - Train Accuracy: 0.757, Validation Accuracy: 0.806, Loss: 1.631 Epoch 2 Batch 55/1077 - Train Accuracy: 0.797, Validation Accuracy: 0.817, Loss: 1.587 Epoch 2 Batch 56/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.815, Loss: 1.672 Epoch 2 Batch 57/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.797, Loss: 1.729 Epoch 2 Batch 58/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.788, Loss: 1.625 Epoch 2 Batch 59/1077 - Train Accuracy: 0.766, Validation Accuracy: 0.803, Loss: 1.566 Epoch 2 Batch 60/1077 - Train Accuracy: 0.796, Validation Accuracy: 0.802, Loss: 1.650 Epoch 2 Batch 61/1077 - Train Accuracy: 0.788, Validation Accuracy: 0.799, Loss: 1.662 Epoch 2 Batch 62/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.811, Loss: 1.656 Epoch 2 Batch 63/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.815, Loss: 1.654 Epoch 2 Batch 64/1077 - Train Accuracy: 0.814, Validation Accuracy: 0.773, Loss: 1.684 Epoch 2 Batch 65/1077 - Train Accuracy: 0.788, Validation Accuracy: 0.759, Loss: 1.601 Epoch 2 Batch 66/1077 - Train Accuracy: 0.786, Validation Accuracy: 0.756, Loss: 1.586 Epoch 2 Batch 67/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.774, Loss: 1.680 Epoch 2 Batch 68/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.778, Loss: 1.633 Epoch 2 Batch 69/1077 - Train Accuracy: 0.803, Validation Accuracy: 0.781, Loss: 1.688 Epoch 2 Batch 70/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.788, Loss: 1.657 Epoch 2 Batch 71/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.794, Loss: 1.552 Epoch 2 Batch 72/1077 - Train Accuracy: 0.797, Validation Accuracy: 0.782, Loss: 1.642 Epoch 2 Batch 73/1077 - Train Accuracy: 0.832, Validation Accuracy: 0.776, Loss: 1.679 Epoch 2 Batch 74/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.776, Loss: 1.545 Epoch 2 Batch 75/1077 - Train Accuracy: 0.799, Validation Accuracy: 0.782, Loss: 1.635 Epoch 2 Batch 76/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.805, Loss: 1.598 Epoch 2 Batch 77/1077 - Train Accuracy: 0.791, Validation Accuracy: 0.799, Loss: 1.697 Epoch 2 Batch 78/1077 - Train Accuracy: 0.810, Validation Accuracy: 0.807, Loss: 1.721 Epoch 2 Batch 79/1077 - Train Accuracy: 0.837, Validation Accuracy: 0.807, Loss: 1.584 Epoch 2 Batch 80/1077 - Train Accuracy: 0.790, Validation Accuracy: 0.800, Loss: 1.619 Epoch 2 Batch 81/1077 - Train Accuracy: 0.821, Validation Accuracy: 0.793, Loss: 1.610 Epoch 2 Batch 82/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.800, Loss: 1.653 Epoch 2 Batch 83/1077 - Train Accuracy: 0.803, Validation Accuracy: 0.805, Loss: 1.568 Epoch 2 Batch 84/1077 - Train Accuracy: 0.806, Validation Accuracy: 0.812, Loss: 1.642 Epoch 2 Batch 85/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.814, Loss: 1.618 Epoch 2 Batch 86/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.827, Loss: 1.653 Epoch 2 Batch 87/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.818, Loss: 1.622 Epoch 2 Batch 88/1077 - Train Accuracy: 0.827, Validation Accuracy: 0.820, Loss: 1.590 Epoch 2 Batch 89/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.815, Loss: 1.651 Epoch 2 Batch 90/1077 - Train Accuracy: 0.789, Validation Accuracy: 0.816, Loss: 1.605 Epoch 2 Batch 91/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.816, Loss: 1.651 Epoch 2 Batch 92/1077 - Train Accuracy: 0.817, Validation Accuracy: 0.810, Loss: 1.603 Epoch 2 Batch 93/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.808, Loss: 1.634 Epoch 2 Batch 94/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.822, Loss: 1.589 Epoch 2 Batch 95/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.834, Loss: 1.647 Epoch 2 Batch 96/1077 - Train Accuracy: 0.808, Validation Accuracy: 0.833, Loss: 1.670 Epoch 2 Batch 97/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.834, Loss: 1.595 Epoch 2 Batch 98/1077 - Train Accuracy: 0.826, Validation Accuracy: 0.826, Loss: 1.680 Epoch 2 Batch 99/1077 - Train Accuracy: 0.809, Validation Accuracy: 0.822, Loss: 1.619 Epoch 2 Batch 100/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.812, Loss: 1.669 Epoch 2 Batch 101/1077 - Train Accuracy: 0.798, Validation Accuracy: 0.809, Loss: 1.590 Epoch 2 Batch 102/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.798, Loss: 1.705 Epoch 2 Batch 103/1077 - Train Accuracy: 0.777, Validation Accuracy: 0.801, Loss: 1.637 Epoch 2 Batch 104/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.777, Loss: 1.725 Epoch 2 Batch 105/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.756, Loss: 1.570 Epoch 2 Batch 106/1077 - Train Accuracy: 0.799, Validation Accuracy: 0.756, Loss: 1.635 Epoch 2 Batch 107/1077 - Train Accuracy: 0.765, Validation Accuracy: 0.759, Loss: 1.494 Epoch 2 Batch 108/1077 - Train Accuracy: 0.826, Validation Accuracy: 0.789, Loss: 1.629 Epoch 2 Batch 109/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.801, Loss: 1.626 Epoch 2 Batch 110/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.816, Loss: 1.621 Epoch 2 Batch 111/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.814, Loss: 1.567 Epoch 2 Batch 112/1077 - Train Accuracy: 0.806, Validation Accuracy: 0.810, Loss: 1.600 Epoch 2 Batch 113/1077 - Train Accuracy: 0.791, Validation Accuracy: 0.809, Loss: 1.655 Epoch 2 Batch 114/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.810, Loss: 1.542 Epoch 2 Batch 115/1077 - Train Accuracy: 0.793, Validation Accuracy: 0.814, Loss: 1.681 Epoch 2 Batch 116/1077 - Train Accuracy: 0.773, Validation Accuracy: 0.822, Loss: 1.661 Epoch 2 Batch 117/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.833, Loss: 1.656 Epoch 2 Batch 118/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.836, Loss: 1.651 Epoch 2 Batch 119/1077 - Train Accuracy: 0.813, Validation Accuracy: 0.834, Loss: 1.596 Epoch 2 Batch 120/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.828, Loss: 1.692 Epoch 2 Batch 121/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.829, Loss: 1.595 Epoch 2 Batch 122/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.835, Loss: 1.652 Epoch 2 Batch 123/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.839, Loss: 1.605 Epoch 2 Batch 124/1077 - Train Accuracy: 0.844, Validation Accuracy: 0.846, Loss: 1.650 Epoch 2 Batch 125/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.853, Loss: 1.667 Epoch 2 Batch 126/1077 - Train Accuracy: 0.858, Validation Accuracy: 0.841, Loss: 1.552 Epoch 2 Batch 127/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.844, Loss: 1.579 Epoch 2 Batch 128/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.839, Loss: 1.590 Epoch 2 Batch 129/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.831, Loss: 1.546 Epoch 2 Batch 130/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.817, Loss: 1.631 Epoch 2 Batch 131/1077 - Train Accuracy: 0.820, Validation Accuracy: 0.804, Loss: 1.627 Epoch 2 Batch 132/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.797, Loss: 1.693 Epoch 2 Batch 133/1077 - Train Accuracy: 0.781, Validation Accuracy: 0.797, Loss: 1.628 Epoch 2 Batch 134/1077 - Train Accuracy: 0.843, Validation Accuracy: 0.792, Loss: 1.567 Epoch 2 Batch 135/1077 - Train Accuracy: 0.785, Validation Accuracy: 0.798, Loss: 1.640 Epoch 2 Batch 136/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.812, Loss: 1.583 Epoch 2 Batch 137/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.816, Loss: 1.626 Epoch 2 Batch 138/1077 - Train Accuracy: 0.814, Validation Accuracy: 0.819, Loss: 1.667 Epoch 2 Batch 139/1077 - Train Accuracy: 0.793, Validation Accuracy: 0.808, Loss: 1.585 Epoch 2 Batch 140/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.810, Loss: 1.589 Epoch 2 Batch 141/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.811, Loss: 1.557 Epoch 2 Batch 142/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.817, Loss: 1.602 Epoch 2 Batch 143/1077 - Train Accuracy: 0.827, Validation Accuracy: 0.821, Loss: 1.631 Epoch 2 Batch 144/1077 - Train Accuracy: 0.834, Validation Accuracy: 0.821, Loss: 1.546 Epoch 2 Batch 145/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.818, Loss: 1.682 Epoch 2 Batch 146/1077 - Train Accuracy: 0.839, Validation Accuracy: 0.822, Loss: 1.723 Epoch 2 Batch 147/1077 - Train Accuracy: 0.783, Validation Accuracy: 0.815, Loss: 1.590 Epoch 2 Batch 148/1077 - Train Accuracy: 0.832, Validation Accuracy: 0.824, Loss: 1.657 Epoch 2 Batch 149/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.835, Loss: 1.598 Epoch 2 Batch 150/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.830, Loss: 1.665 Epoch 2 Batch 151/1077 - Train Accuracy: 0.801, Validation Accuracy: 0.824, Loss: 1.619 Epoch 2 Batch 152/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.822, Loss: 1.692 Epoch 2 Batch 153/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.823, Loss: 1.597 Epoch 2 Batch 154/1077 - Train Accuracy: 0.829, Validation Accuracy: 0.827, Loss: 1.662 Epoch 2 Batch 155/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.826, Loss: 1.629 Epoch 2 Batch 156/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.813, Loss: 1.691 Epoch 2 Batch 157/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.807, Loss: 1.587 Epoch 2 Batch 158/1077 - Train Accuracy: 0.820, Validation Accuracy: 0.806, Loss: 1.607 Epoch 2 Batch 159/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.808, Loss: 1.566 Epoch 2 Batch 160/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.794, Loss: 1.613 Epoch 2 Batch 161/1077 - Train Accuracy: 0.814, Validation Accuracy: 0.804, Loss: 1.589 Epoch 2 Batch 162/1077 - Train Accuracy: 0.827, Validation Accuracy: 0.820, Loss: 1.636 Epoch 2 Batch 163/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.831, Loss: 1.674 Epoch 2 Batch 164/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.831, Loss: 1.573 Epoch 2 Batch 165/1077 - Train Accuracy: 0.785, Validation Accuracy: 0.823, Loss: 1.603 Epoch 2 Batch 166/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.822, Loss: 1.650 Epoch 2 Batch 167/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.823, Loss: 1.613 Epoch 2 Batch 168/1077 - Train Accuracy: 0.810, Validation Accuracy: 0.830, Loss: 1.630 Epoch 2 Batch 169/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.826, Loss: 1.671 Epoch 2 Batch 170/1077 - Train Accuracy: 0.802, Validation Accuracy: 0.814, Loss: 1.615 Epoch 2 Batch 171/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.807, Loss: 1.552 Epoch 2 Batch 172/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.809, Loss: 1.630 Epoch 2 Batch 173/1077 - Train Accuracy: 0.822, Validation Accuracy: 0.808, Loss: 1.725 Epoch 2 Batch 174/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.813, Loss: 1.614 Epoch 2 Batch 175/1077 - Train Accuracy: 0.843, Validation Accuracy: 0.823, Loss: 1.551 Epoch 2 Batch 176/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.829, Loss: 1.628 Epoch 2 Batch 177/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.828, Loss: 1.607 Epoch 2 Batch 178/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.834, Loss: 1.596 Epoch 2 Batch 179/1077 - Train Accuracy: 0.808, Validation Accuracy: 0.832, Loss: 1.676 Epoch 2 Batch 180/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.830, Loss: 1.543 Epoch 2 Batch 181/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.832, Loss: 1.609 Epoch 2 Batch 182/1077 - Train Accuracy: 0.839, Validation Accuracy: 0.832, Loss: 1.595 Epoch 2 Batch 183/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.833, Loss: 1.667 Epoch 2 Batch 184/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.817, Loss: 1.645 Epoch 2 Batch 185/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.813, Loss: 1.610 Epoch 2 Batch 186/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.812, Loss: 1.645 Epoch 2 Batch 187/1077 - Train Accuracy: 0.834, Validation Accuracy: 0.806, Loss: 1.634 Epoch 2 Batch 188/1077 - Train Accuracy: 0.839, Validation Accuracy: 0.817, Loss: 1.546 Epoch 2 Batch 189/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.821, Loss: 1.615 Epoch 2 Batch 190/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.822, Loss: 1.593 Epoch 2 Batch 191/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.810, Loss: 1.588 Epoch 2 Batch 192/1077 - Train Accuracy: 0.817, Validation Accuracy: 0.789, Loss: 1.592 Epoch 2 Batch 193/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.781, Loss: 1.531 Epoch 2 Batch 194/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.792, Loss: 1.525 Epoch 2 Batch 195/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.812, Loss: 1.559 Epoch 2 Batch 196/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.824, Loss: 1.569 Epoch 2 Batch 197/1077 - Train Accuracy: 0.837, Validation Accuracy: 0.821, Loss: 1.602 Epoch 2 Batch 198/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.832, Loss: 1.670 Epoch 2 Batch 199/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.842, Loss: 1.645 Epoch 2 Batch 200/1077 - Train Accuracy: 0.806, Validation Accuracy: 0.840, Loss: 1.549 Epoch 2 Batch 201/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.834, Loss: 1.539 Epoch 2 Batch 202/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.827, Loss: 1.500 Epoch 2 Batch 203/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.826, Loss: 1.629 Epoch 2 Batch 204/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.833, Loss: 1.645 Epoch 2 Batch 205/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.827, Loss: 1.607 Epoch 2 Batch 206/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.830, Loss: 1.595 Epoch 2 Batch 207/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.826, Loss: 1.628 Epoch 2 Batch 208/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.825, Loss: 1.573 Epoch 2 Batch 209/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.806, Loss: 1.639 Epoch 2 Batch 210/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.805, Loss: 1.655 Epoch 2 Batch 211/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.799, Loss: 1.615 Epoch 2 Batch 212/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.793, Loss: 1.572 Epoch 2 Batch 213/1077 - Train Accuracy: 0.814, Validation Accuracy: 0.801, Loss: 1.573 Epoch 2 Batch 214/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.808, Loss: 1.653 Epoch 2 Batch 215/1077 - Train Accuracy: 0.782, Validation Accuracy: 0.815, Loss: 1.618 Epoch 2 Batch 216/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.828, Loss: 1.592 Epoch 2 Batch 217/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.835, Loss: 1.656 Epoch 2 Batch 218/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.838, Loss: 1.641 Epoch 2 Batch 219/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.834, Loss: 1.648 Epoch 2 Batch 220/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.833, Loss: 1.715 Epoch 2 Batch 221/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.833, Loss: 1.553 Epoch 2 Batch 222/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.836, Loss: 1.627 Epoch 2 Batch 223/1077 - Train Accuracy: 0.843, Validation Accuracy: 0.835, Loss: 1.621 Epoch 2 Batch 224/1077 - Train Accuracy: 0.837, Validation Accuracy: 0.835, Loss: 1.614 Epoch 2 Batch 225/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.834, Loss: 1.634 Epoch 2 Batch 226/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.839, Loss: 1.616 Epoch 2 Batch 227/1077 - Train Accuracy: 0.794, Validation Accuracy: 0.833, Loss: 1.632 Epoch 2 Batch 228/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.806, Loss: 1.570 Epoch 2 Batch 229/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.805, Loss: 1.560 Epoch 2 Batch 230/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.817, Loss: 1.606 Epoch 2 Batch 231/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.823, Loss: 1.689 Epoch 2 Batch 232/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.841, Loss: 1.620 Epoch 2 Batch 233/1077 - Train Accuracy: 0.854, Validation Accuracy: 0.855, Loss: 1.647 Epoch 2 Batch 234/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.847, Loss: 1.666 Epoch 2 Batch 235/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.846, Loss: 1.513 Epoch 2 Batch 236/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.847, Loss: 1.609 Epoch 2 Batch 237/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.846, Loss: 1.575 Epoch 2 Batch 238/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.846, Loss: 1.683 Epoch 2 Batch 239/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.840, Loss: 1.588 Epoch 2 Batch 240/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.836, Loss: 1.655 Epoch 2 Batch 241/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.828, Loss: 1.623 Epoch 2 Batch 242/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.825, Loss: 1.603 Epoch 2 Batch 243/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.837, Loss: 1.662 Epoch 2 Batch 244/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.839, Loss: 1.622 Epoch 2 Batch 245/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.836, Loss: 1.637 Epoch 2 Batch 246/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.826, Loss: 1.607 Epoch 2 Batch 247/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.821, Loss: 1.582 Epoch 2 Batch 248/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.831, Loss: 1.615 Epoch 2 Batch 249/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.829, Loss: 1.647 Epoch 2 Batch 250/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.832, Loss: 1.617 Epoch 2 Batch 251/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.831, Loss: 1.573 Epoch 2 Batch 252/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.833, Loss: 1.641 Epoch 2 Batch 253/1077 - Train Accuracy: 0.827, Validation Accuracy: 0.847, Loss: 1.616 Epoch 2 Batch 254/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.854, Loss: 1.612 Epoch 2 Batch 255/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.852, Loss: 1.548 Epoch 2 Batch 256/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.853, Loss: 1.554 Epoch 2 Batch 257/1077 - Train Accuracy: 0.832, Validation Accuracy: 0.849, Loss: 1.588 Epoch 2 Batch 258/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.849, Loss: 1.601 Epoch 2 Batch 259/1077 - Train Accuracy: 0.807, Validation Accuracy: 0.846, Loss: 1.598 Epoch 2 Batch 260/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.850, Loss: 1.588 Epoch 2 Batch 261/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.850, Loss: 1.591 Epoch 2 Batch 262/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.865, Loss: 1.579 Epoch 2 Batch 263/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.860, Loss: 1.594 Epoch 2 Batch 264/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.862, Loss: 1.691 Epoch 2 Batch 265/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.862, Loss: 1.572 Epoch 2 Batch 266/1077 - Train Accuracy: 0.827, Validation Accuracy: 0.851, Loss: 1.566 Epoch 2 Batch 267/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.849, Loss: 1.606 Epoch 2 Batch 268/1077 - Train Accuracy: 0.811, Validation Accuracy: 0.847, Loss: 1.561 Epoch 2 Batch 269/1077 - Train Accuracy: 0.822, Validation Accuracy: 0.830, Loss: 1.568 Epoch 2 Batch 270/1077 - Train Accuracy: 0.791, Validation Accuracy: 0.825, Loss: 1.547 Epoch 2 Batch 271/1077 - Train Accuracy: 0.876, Validation Accuracy: 0.825, Loss: 1.645 Epoch 2 Batch 272/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.833, Loss: 1.587 Epoch 2 Batch 273/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.828, Loss: 1.618 Epoch 2 Batch 274/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.825, Loss: 1.593 Epoch 2 Batch 275/1077 - Train Accuracy: 0.827, Validation Accuracy: 0.829, Loss: 1.603 Epoch 2 Batch 276/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.844, Loss: 1.650 Epoch 2 Batch 277/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.841, Loss: 1.603 Epoch 2 Batch 278/1077 - Train Accuracy: 0.805, Validation Accuracy: 0.835, Loss: 1.521 Epoch 2 Batch 279/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.832, Loss: 1.582 Epoch 2 Batch 280/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.835, Loss: 1.636 Epoch 2 Batch 281/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.843, Loss: 1.644 Epoch 2 Batch 282/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.848, Loss: 1.697 Epoch 2 Batch 283/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.860, Loss: 1.605 Epoch 2 Batch 284/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.864, Loss: 1.594 Epoch 2 Batch 285/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.859, Loss: 1.557 Epoch 2 Batch 286/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.858, Loss: 1.598 Epoch 2 Batch 287/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.851, Loss: 1.549 Epoch 2 Batch 288/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.829, Loss: 1.624 Epoch 2 Batch 289/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.831, Loss: 1.641 Epoch 2 Batch 290/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.815, Loss: 1.533 Epoch 2 Batch 291/1077 - Train Accuracy: 0.792, Validation Accuracy: 0.805, Loss: 1.601 Epoch 2 Batch 292/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.799, Loss: 1.648 Epoch 2 Batch 293/1077 - Train Accuracy: 0.804, Validation Accuracy: 0.816, Loss: 1.571 Epoch 2 Batch 294/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.836, Loss: 1.574 Epoch 2 Batch 295/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.841, Loss: 1.630 Epoch 2 Batch 296/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.841, Loss: 1.670 Epoch 2 Batch 297/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.831, Loss: 1.594 Epoch 2 Batch 298/1077 - Train Accuracy: 0.814, Validation Accuracy: 0.833, Loss: 1.564 Epoch 2 Batch 299/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.838, Loss: 1.615 Epoch 2 Batch 300/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.837, Loss: 1.628 Epoch 2 Batch 301/1077 - Train Accuracy: 0.825, Validation Accuracy: 0.828, Loss: 1.569 Epoch 2 Batch 302/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.834, Loss: 1.682 Epoch 2 Batch 303/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.832, Loss: 1.541 Epoch 2 Batch 304/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.833, Loss: 1.564 Epoch 2 Batch 305/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.844, Loss: 1.550 Epoch 2 Batch 306/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.846, Loss: 1.600 Epoch 2 Batch 307/1077 - Train Accuracy: 0.829, Validation Accuracy: 0.843, Loss: 1.650 Epoch 2 Batch 308/1077 - Train Accuracy: 0.844, Validation Accuracy: 0.836, Loss: 1.532 Epoch 2 Batch 309/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.831, Loss: 1.551 Epoch 2 Batch 310/1077 - Train Accuracy: 0.843, Validation Accuracy: 0.837, Loss: 1.603 Epoch 2 Batch 311/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.844, Loss: 1.538 Epoch 2 Batch 312/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.847, Loss: 1.627 Epoch 2 Batch 313/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.846, Loss: 1.520 Epoch 2 Batch 314/1077 - Train Accuracy: 0.876, Validation Accuracy: 0.851, Loss: 1.564 Epoch 2 Batch 315/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.843, Loss: 1.549 Epoch 2 Batch 316/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.842, Loss: 1.612 Epoch 2 Batch 317/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.842, Loss: 1.581 Epoch 2 Batch 318/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.853, Loss: 1.617 Epoch 2 Batch 319/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.855, Loss: 1.617 Epoch 2 Batch 320/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.866, Loss: 1.608 Epoch 2 Batch 321/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.866, Loss: 1.609 Epoch 2 Batch 322/1077 - Train Accuracy: 0.829, Validation Accuracy: 0.869, Loss: 1.541 Epoch 2 Batch 323/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.860, Loss: 1.597 Epoch 2 Batch 324/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.849, Loss: 1.667 Epoch 2 Batch 325/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.856, Loss: 1.558 Epoch 2 Batch 326/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.839, Loss: 1.540 Epoch 2 Batch 327/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.840, Loss: 1.629 Epoch 2 Batch 328/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.848, Loss: 1.576 Epoch 2 Batch 329/1077 - Train Accuracy: 0.829, Validation Accuracy: 0.846, Loss: 1.682 Epoch 2 Batch 330/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.832, Loss: 1.597 Epoch 2 Batch 331/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.837, Loss: 1.644 Epoch 2 Batch 332/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.836, Loss: 1.536 Epoch 2 Batch 333/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.837, Loss: 1.588 Epoch 2 Batch 334/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.845, Loss: 1.660 Epoch 2 Batch 335/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.840, Loss: 1.562 Epoch 2 Batch 336/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.835, Loss: 1.587 Epoch 2 Batch 337/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.833, Loss: 1.611 Epoch 2 Batch 338/1077 - Train Accuracy: 0.835, Validation Accuracy: 0.840, Loss: 1.632 Epoch 2 Batch 339/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.836, Loss: 1.524 Epoch 2 Batch 340/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.841, Loss: 1.575 Epoch 2 Batch 341/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.847, Loss: 1.683 Epoch 2 Batch 342/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.852, Loss: 1.601 Epoch 2 Batch 343/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.860, Loss: 1.588 Epoch 2 Batch 344/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.861, Loss: 1.552 Epoch 2 Batch 345/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.839, Loss: 1.617 Epoch 2 Batch 346/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.843, Loss: 1.570 Epoch 2 Batch 347/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.840, Loss: 1.606 Epoch 2 Batch 348/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.837, Loss: 1.505 Epoch 2 Batch 349/1077 - Train Accuracy: 0.854, Validation Accuracy: 0.840, Loss: 1.582 Epoch 2 Batch 350/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.837, Loss: 1.590 Epoch 2 Batch 351/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.835, Loss: 1.536 Epoch 2 Batch 352/1077 - Train Accuracy: 0.851, Validation Accuracy: 0.822, Loss: 1.587 Epoch 2 Batch 353/1077 - Train Accuracy: 0.810, Validation Accuracy: 0.826, Loss: 1.690 Epoch 2 Batch 354/1077 - Train Accuracy: 0.834, Validation Accuracy: 0.824, Loss: 1.662 Epoch 2 Batch 355/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.824, Loss: 1.586 Epoch 2 Batch 356/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.842, Loss: 1.608 Epoch 2 Batch 357/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.830, Loss: 1.646 Epoch 2 Batch 358/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.838, Loss: 1.635 Epoch 2 Batch 359/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.843, Loss: 1.635 Epoch 2 Batch 360/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.849, Loss: 1.551 Epoch 2 Batch 361/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.845, Loss: 1.536 Epoch 2 Batch 362/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.846, Loss: 1.627 Epoch 2 Batch 363/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.851, Loss: 1.485 Epoch 2 Batch 364/1077 - Train Accuracy: 0.876, Validation Accuracy: 0.847, Loss: 1.643 Epoch 2 Batch 365/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.846, Loss: 1.624 Epoch 2 Batch 366/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.859, Loss: 1.611 Epoch 2 Batch 367/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.864, Loss: 1.517 Epoch 2 Batch 368/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.864, Loss: 1.514 Epoch 2 Batch 369/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.859, Loss: 1.616 Epoch 2 Batch 370/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.837, Loss: 1.623 Epoch 2 Batch 371/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.843, Loss: 1.601 Epoch 2 Batch 372/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.832, Loss: 1.585 Epoch 2 Batch 373/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.833, Loss: 1.553 Epoch 2 Batch 374/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.830, Loss: 1.594 Epoch 2 Batch 375/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.829, Loss: 1.569 Epoch 2 Batch 376/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.838, Loss: 1.563 Epoch 2 Batch 377/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.852, Loss: 1.614 Epoch 2 Batch 378/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.848, Loss: 1.597 Epoch 2 Batch 379/1077 - Train Accuracy: 0.829, Validation Accuracy: 0.844, Loss: 1.622 Epoch 2 Batch 380/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.851, Loss: 1.597 Epoch 2 Batch 381/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.862, Loss: 1.601 Epoch 2 Batch 382/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.857, Loss: 1.644 Epoch 2 Batch 383/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.849, Loss: 1.560 Epoch 2 Batch 384/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.870, Loss: 1.568 Epoch 2 Batch 385/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.869, Loss: 1.513 Epoch 2 Batch 386/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.878, Loss: 1.561 Epoch 2 Batch 387/1077 - Train Accuracy: 0.858, Validation Accuracy: 0.870, Loss: 1.584 Epoch 2 Batch 388/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.863, Loss: 1.618 Epoch 2 Batch 389/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.856, Loss: 1.584 Epoch 2 Batch 390/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.846, Loss: 1.485 Epoch 2 Batch 391/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.847, Loss: 1.548 Epoch 2 Batch 392/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.841, Loss: 1.586 Epoch 2 Batch 393/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.832, Loss: 1.539 Epoch 2 Batch 394/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.825, Loss: 1.613 Epoch 2 Batch 395/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.821, Loss: 1.620 Epoch 2 Batch 396/1077 - Train Accuracy: 0.844, Validation Accuracy: 0.825, Loss: 1.556 Epoch 2 Batch 397/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.830, Loss: 1.611 Epoch 2 Batch 398/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.832, Loss: 1.589 Epoch 2 Batch 399/1077 - Train Accuracy: 0.826, Validation Accuracy: 0.840, Loss: 1.635 Epoch 2 Batch 400/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.845, Loss: 1.565 Epoch 2 Batch 401/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.843, Loss: 1.586 Epoch 2 Batch 402/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.847, Loss: 1.634 Epoch 2 Batch 403/1077 - Train Accuracy: 0.832, Validation Accuracy: 0.843, Loss: 1.578 Epoch 2 Batch 404/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.847, Loss: 1.497 Epoch 2 Batch 405/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.855, Loss: 1.657 Epoch 2 Batch 406/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.854, Loss: 1.497 Epoch 2 Batch 407/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.861, Loss: 1.588 Epoch 2 Batch 408/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.857, Loss: 1.630 Epoch 2 Batch 409/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.850, Loss: 1.637 Epoch 2 Batch 410/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.852, Loss: 1.512 Epoch 2 Batch 411/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.857, Loss: 1.647 Epoch 2 Batch 412/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.850, Loss: 1.598 Epoch 2 Batch 413/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.846, Loss: 1.593 Epoch 2 Batch 414/1077 - Train Accuracy: 0.837, Validation Accuracy: 0.864, Loss: 1.629 Epoch 2 Batch 415/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.867, Loss: 1.607 Epoch 2 Batch 416/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.859, Loss: 1.519 Epoch 2 Batch 417/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.854, Loss: 1.667 Epoch 2 Batch 418/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.849, Loss: 1.554 Epoch 2 Batch 419/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.851, Loss: 1.491 Epoch 2 Batch 420/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.841, Loss: 1.567 Epoch 2 Batch 421/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.840, Loss: 1.585 Epoch 2 Batch 422/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.830, Loss: 1.566 Epoch 2 Batch 423/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.836, Loss: 1.618 Epoch 2 Batch 424/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.824, Loss: 1.589 Epoch 2 Batch 425/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.840, Loss: 1.497 Epoch 2 Batch 426/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.848, Loss: 1.619 Epoch 2 Batch 427/1077 - Train Accuracy: 0.823, Validation Accuracy: 0.847, Loss: 1.617 Epoch 2 Batch 428/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.848, Loss: 1.515 Epoch 2 Batch 429/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.852, Loss: 1.533 Epoch 2 Batch 430/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.852, Loss: 1.578 Epoch 2 Batch 431/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.850, Loss: 1.573 Epoch 2 Batch 432/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.852, Loss: 1.558 Epoch 2 Batch 433/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.848, Loss: 1.586 Epoch 2 Batch 434/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.849, Loss: 1.536 Epoch 2 Batch 435/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.849, Loss: 1.611 Epoch 2 Batch 436/1077 - Train Accuracy: 0.843, Validation Accuracy: 0.850, Loss: 1.612 Epoch 2 Batch 437/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.852, Loss: 1.603 Epoch 2 Batch 438/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.842, Loss: 1.540 Epoch 2 Batch 439/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.836, Loss: 1.628 Epoch 2 Batch 440/1077 - Train Accuracy: 0.832, Validation Accuracy: 0.837, Loss: 1.564 Epoch 2 Batch 441/1077 - Train Accuracy: 0.844, Validation Accuracy: 0.831, Loss: 1.639 Epoch 2 Batch 442/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.830, Loss: 1.578 Epoch 2 Batch 443/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.834, Loss: 1.607 Epoch 2 Batch 444/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.825, Loss: 1.564 Epoch 2 Batch 445/1077 - Train Accuracy: 0.812, Validation Accuracy: 0.826, Loss: 1.610 Epoch 2 Batch 446/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.821, Loss: 1.526 Epoch 2 Batch 447/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.824, Loss: 1.600 Epoch 2 Batch 448/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.827, Loss: 1.527 Epoch 2 Batch 449/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.839, Loss: 1.614 Epoch 2 Batch 450/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.842, Loss: 1.628 Epoch 2 Batch 451/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.839, Loss: 1.580 Epoch 2 Batch 452/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.836, Loss: 1.621 Epoch 2 Batch 453/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.837, Loss: 1.649 Epoch 2 Batch 454/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.838, Loss: 1.575 Epoch 2 Batch 455/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.836, Loss: 1.594 Epoch 2 Batch 456/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.844, Loss: 1.607 Epoch 2 Batch 457/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.843, Loss: 1.549 Epoch 2 Batch 458/1077 - Train Accuracy: 0.824, Validation Accuracy: 0.843, Loss: 1.703 Epoch 2 Batch 459/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.849, Loss: 1.550 Epoch 2 Batch 460/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.856, Loss: 1.604 Epoch 2 Batch 461/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.859, Loss: 1.555 Epoch 2 Batch 462/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.872, Loss: 1.623 Epoch 2 Batch 463/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.867, Loss: 1.577 Epoch 2 Batch 464/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.858, Loss: 1.593 Epoch 2 Batch 465/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.858, Loss: 1.551 Epoch 2 Batch 466/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.858, Loss: 1.548 Epoch 2 Batch 467/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.860, Loss: 1.557 Epoch 2 Batch 468/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.852, Loss: 1.582 Epoch 2 Batch 469/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.848, Loss: 1.591 Epoch 2 Batch 470/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.854, Loss: 1.553 Epoch 2 Batch 471/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.860, Loss: 1.576 Epoch 2 Batch 472/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.856, Loss: 1.524 Epoch 2 Batch 473/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.845, Loss: 1.534 Epoch 2 Batch 474/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.845, Loss: 1.554 Epoch 2 Batch 475/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.849, Loss: 1.531 Epoch 2 Batch 476/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.843, Loss: 1.516 Epoch 2 Batch 477/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.839, Loss: 1.609 Epoch 2 Batch 478/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.838, Loss: 1.643 Epoch 2 Batch 479/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.843, Loss: 1.575 Epoch 2 Batch 480/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.859, Loss: 1.626 Epoch 2 Batch 481/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.865, Loss: 1.597 Epoch 2 Batch 482/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.872, Loss: 1.658 Epoch 2 Batch 483/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.863, Loss: 1.576 Epoch 2 Batch 484/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.864, Loss: 1.509 Epoch 2 Batch 485/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.863, Loss: 1.490 Epoch 2 Batch 486/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.869, Loss: 1.570 Epoch 2 Batch 487/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.864, Loss: 1.475 Epoch 2 Batch 488/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.853, Loss: 1.636 Epoch 2 Batch 489/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.851, Loss: 1.506 Epoch 2 Batch 490/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.849, Loss: 1.596 Epoch 2 Batch 491/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.835, Loss: 1.641 Epoch 2 Batch 492/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.844, Loss: 1.565 Epoch 2 Batch 493/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.852, Loss: 1.616 Epoch 2 Batch 494/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.858, Loss: 1.591 Epoch 2 Batch 495/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.861, Loss: 1.556 Epoch 2 Batch 496/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.870, Loss: 1.494 Epoch 2 Batch 497/1077 - Train Accuracy: 0.854, Validation Accuracy: 0.863, Loss: 1.598 Epoch 2 Batch 498/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.857, Loss: 1.516 Epoch 2 Batch 499/1077 - Train Accuracy: 0.839, Validation Accuracy: 0.854, Loss: 1.501 Epoch 2 Batch 500/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.849, Loss: 1.634 Epoch 2 Batch 501/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.850, Loss: 1.631 Epoch 2 Batch 502/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.845, Loss: 1.493 Epoch 2 Batch 503/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.856, Loss: 1.525 Epoch 2 Batch 504/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.839, Loss: 1.561 Epoch 2 Batch 505/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.840, Loss: 1.516 Epoch 2 Batch 506/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.841, Loss: 1.546 Epoch 2 Batch 507/1077 - Train Accuracy: 0.820, Validation Accuracy: 0.840, Loss: 1.609 Epoch 2 Batch 508/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.844, Loss: 1.596 Epoch 2 Batch 509/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.848, Loss: 1.498 Epoch 2 Batch 510/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.850, Loss: 1.602 Epoch 2 Batch 511/1077 - Train Accuracy: 0.854, Validation Accuracy: 0.849, Loss: 1.607 Epoch 2 Batch 512/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.856, Loss: 1.560 Epoch 2 Batch 513/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.849, Loss: 1.568 Epoch 2 Batch 514/1077 - Train Accuracy: 0.848, Validation Accuracy: 0.839, Loss: 1.657 Epoch 2 Batch 515/1077 - Train Accuracy: 0.843, Validation Accuracy: 0.843, Loss: 1.627 Epoch 2 Batch 516/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.841, Loss: 1.554 Epoch 2 Batch 517/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.847, Loss: 1.537 Epoch 2 Batch 518/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.852, Loss: 1.556 Epoch 2 Batch 519/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.857, Loss: 1.593 Epoch 2 Batch 520/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.856, Loss: 1.525 Epoch 2 Batch 521/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.850, Loss: 1.577 Epoch 2 Batch 522/1077 - Train Accuracy: 0.818, Validation Accuracy: 0.853, Loss: 1.584 Epoch 2 Batch 523/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.858, Loss: 1.580 Epoch 2 Batch 524/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.856, Loss: 1.570 Epoch 2 Batch 525/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.858, Loss: 1.554 Epoch 2 Batch 526/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.864, Loss: 1.542 Epoch 2 Batch 527/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.854, Loss: 1.527 Epoch 2 Batch 528/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.850, Loss: 1.626 Epoch 2 Batch 529/1077 - Train Accuracy: 0.851, Validation Accuracy: 0.851, Loss: 1.580 Epoch 2 Batch 530/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.849, Loss: 1.606 Epoch 2 Batch 531/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.849, Loss: 1.543 Epoch 2 Batch 532/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.853, Loss: 1.595 Epoch 2 Batch 533/1077 - Train Accuracy: 0.839, Validation Accuracy: 0.858, Loss: 1.530 Epoch 2 Batch 534/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.864, Loss: 1.532 Epoch 2 Batch 535/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.859, Loss: 1.560 Epoch 2 Batch 536/1077 - Train Accuracy: 0.855, Validation Accuracy: 0.856, Loss: 1.556 Epoch 2 Batch 537/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.861, Loss: 1.524 Epoch 2 Batch 538/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.842, Loss: 1.564 Epoch 2 Batch 539/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.854, Loss: 1.574 Epoch 2 Batch 540/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.854, Loss: 1.630 Epoch 2 Batch 541/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.848, Loss: 1.501 Epoch 2 Batch 542/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.847, Loss: 1.577 Epoch 2 Batch 543/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.842, Loss: 1.545 Epoch 2 Batch 544/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.848, Loss: 1.597 Epoch 2 Batch 545/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.853, Loss: 1.556 Epoch 2 Batch 546/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.850, Loss: 1.562 Epoch 2 Batch 547/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.850, Loss: 1.442 Epoch 2 Batch 548/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.864, Loss: 1.497 Epoch 2 Batch 549/1077 - Train Accuracy: 0.820, Validation Accuracy: 0.871, Loss: 1.574 Epoch 2 Batch 550/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.867, Loss: 1.544 Epoch 2 Batch 551/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.869, Loss: 1.443 Epoch 2 Batch 552/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.874, Loss: 1.568 Epoch 2 Batch 553/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.862, Loss: 1.515 Epoch 2 Batch 554/1077 - Train Accuracy: 0.851, Validation Accuracy: 0.866, Loss: 1.544 Epoch 2 Batch 555/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.868, Loss: 1.522 Epoch 2 Batch 556/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.868, Loss: 1.586 Epoch 2 Batch 557/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.871, Loss: 1.475 Epoch 2 Batch 558/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.864, Loss: 1.500 Epoch 2 Batch 559/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.849, Loss: 1.678 Epoch 2 Batch 560/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.846, Loss: 1.602 Epoch 2 Batch 561/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.847, Loss: 1.524 Epoch 2 Batch 562/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.844, Loss: 1.622 Epoch 2 Batch 563/1077 - Train Accuracy: 0.854, Validation Accuracy: 0.865, Loss: 1.590 Epoch 2 Batch 564/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.850, Loss: 1.586 Epoch 2 Batch 565/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.859, Loss: 1.602 Epoch 2 Batch 566/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.847, Loss: 1.553 Epoch 2 Batch 567/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.842, Loss: 1.533 Epoch 2 Batch 568/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.838, Loss: 1.631 Epoch 2 Batch 569/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.839, Loss: 1.533 Epoch 2 Batch 570/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.851, Loss: 1.566 Epoch 2 Batch 571/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.859, Loss: 1.547 Epoch 2 Batch 572/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.865, Loss: 1.499 Epoch 2 Batch 573/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.869, Loss: 1.538 Epoch 2 Batch 574/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.864, Loss: 1.595 Epoch 2 Batch 575/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.855, Loss: 1.613 Epoch 2 Batch 576/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.858, Loss: 1.566 Epoch 2 Batch 577/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.855, Loss: 1.525 Epoch 2 Batch 578/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.837, Loss: 1.551 Epoch 2 Batch 579/1077 - Train Accuracy: 0.851, Validation Accuracy: 0.837, Loss: 1.509 Epoch 2 Batch 580/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.835, Loss: 1.606 Epoch 2 Batch 581/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.828, Loss: 1.571 Epoch 2 Batch 582/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.843, Loss: 1.636 Epoch 2 Batch 583/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.855, Loss: 1.642 Epoch 2 Batch 584/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.851, Loss: 1.606 Epoch 2 Batch 585/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.845, Loss: 1.522 Epoch 2 Batch 586/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.839, Loss: 1.601 Epoch 2 Batch 587/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.841, Loss: 1.465 Epoch 2 Batch 588/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.847, Loss: 1.541 Epoch 2 Batch 589/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.847, Loss: 1.629 Epoch 2 Batch 590/1077 - Train Accuracy: 0.836, Validation Accuracy: 0.851, Loss: 1.614 Epoch 2 Batch 591/1077 - Train Accuracy: 0.876, Validation Accuracy: 0.855, Loss: 1.539 Epoch 2 Batch 592/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.860, Loss: 1.518 Epoch 2 Batch 593/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.862, Loss: 1.577 Epoch 2 Batch 594/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.868, Loss: 1.551 Epoch 2 Batch 595/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.869, Loss: 1.570 Epoch 2 Batch 596/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.855, Loss: 1.484 Epoch 2 Batch 597/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.854, Loss: 1.558 Epoch 2 Batch 598/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.859, Loss: 1.618 Epoch 2 Batch 599/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.856, Loss: 1.600 Epoch 2 Batch 600/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.865, Loss: 1.515 Epoch 2 Batch 601/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.854, Loss: 1.457 Epoch 2 Batch 602/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.848, Loss: 1.524 Epoch 2 Batch 603/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.846, Loss: 1.600 Epoch 2 Batch 604/1077 - Train Accuracy: 0.841, Validation Accuracy: 0.848, Loss: 1.511 Epoch 2 Batch 605/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.858, Loss: 1.627 Epoch 2 Batch 606/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.854, Loss: 1.596 Epoch 2 Batch 607/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.858, Loss: 1.616 Epoch 2 Batch 608/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.873, Loss: 1.619 Epoch 2 Batch 609/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.876, Loss: 1.552 Epoch 2 Batch 610/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.872, Loss: 1.563 Epoch 2 Batch 611/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.868, Loss: 1.555 Epoch 2 Batch 612/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.865, Loss: 1.558 Epoch 2 Batch 613/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.863, Loss: 1.495 Epoch 2 Batch 614/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.863, Loss: 1.500 Epoch 2 Batch 615/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.866, Loss: 1.581 Epoch 2 Batch 616/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.868, Loss: 1.571 Epoch 2 Batch 617/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.866, Loss: 1.540 Epoch 2 Batch 618/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.863, Loss: 1.553 Epoch 2 Batch 619/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.864, Loss: 1.612 Epoch 2 Batch 620/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.865, Loss: 1.600 Epoch 2 Batch 621/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.869, Loss: 1.545 Epoch 2 Batch 622/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.866, Loss: 1.613 Epoch 2 Batch 623/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.860, Loss: 1.488 Epoch 2 Batch 624/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.862, Loss: 1.525 Epoch 2 Batch 625/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.866, Loss: 1.641 Epoch 2 Batch 626/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.867, Loss: 1.530 Epoch 2 Batch 627/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.864, Loss: 1.575 Epoch 2 Batch 628/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.864, Loss: 1.587 Epoch 2 Batch 629/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.874, Loss: 1.548 Epoch 2 Batch 630/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.876, Loss: 1.596 Epoch 2 Batch 631/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.880, Loss: 1.574 Epoch 2 Batch 632/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.880, Loss: 1.516 Epoch 2 Batch 633/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.876, Loss: 1.585 Epoch 2 Batch 634/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.867, Loss: 1.569 Epoch 2 Batch 635/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.867, Loss: 1.550 Epoch 2 Batch 636/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.862, Loss: 1.570 Epoch 2 Batch 637/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.870, Loss: 1.534 Epoch 2 Batch 638/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.878, Loss: 1.554 Epoch 2 Batch 639/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.868, Loss: 1.577 Epoch 2 Batch 640/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.857, Loss: 1.558 Epoch 2 Batch 641/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.853, Loss: 1.588 Epoch 2 Batch 642/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.857, Loss: 1.572 Epoch 2 Batch 643/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.863, Loss: 1.507 Epoch 2 Batch 644/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.865, Loss: 1.469 Epoch 2 Batch 645/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.870, Loss: 1.526 Epoch 2 Batch 646/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.877, Loss: 1.583 Epoch 2 Batch 647/1077 - Train Accuracy: 0.867, Validation Accuracy: 0.870, Loss: 1.612 Epoch 2 Batch 648/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.869, Loss: 1.557 Epoch 2 Batch 649/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.879, Loss: 1.495 Epoch 2 Batch 650/1077 - Train Accuracy: 0.854, Validation Accuracy: 0.882, Loss: 1.556 Epoch 2 Batch 651/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.873, Loss: 1.513 Epoch 2 Batch 652/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.874, Loss: 1.592 Epoch 2 Batch 653/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.866, Loss: 1.459 Epoch 2 Batch 654/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.870, Loss: 1.562 Epoch 2 Batch 655/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.876, Loss: 1.572 Epoch 2 Batch 656/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.874, Loss: 1.642 Epoch 2 Batch 657/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.868, Loss: 1.477 Epoch 2 Batch 658/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.866, Loss: 1.636 Epoch 2 Batch 659/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.869, Loss: 1.553 Epoch 2 Batch 660/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.865, Loss: 1.598 Epoch 2 Batch 661/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.871, Loss: 1.513 Epoch 2 Batch 662/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.873, Loss: 1.468 Epoch 2 Batch 663/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.879, Loss: 1.538 Epoch 2 Batch 664/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.876, Loss: 1.517 Epoch 2 Batch 665/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.868, Loss: 1.565 Epoch 2 Batch 666/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.866, Loss: 1.555 Epoch 2 Batch 667/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.867, Loss: 1.490 Epoch 2 Batch 668/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.869, Loss: 1.614 Epoch 2 Batch 669/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.863, Loss: 1.555 Epoch 2 Batch 670/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.856, Loss: 1.546 Epoch 2 Batch 671/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.855, Loss: 1.539 Epoch 2 Batch 672/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.867, Loss: 1.540 Epoch 2 Batch 673/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.875, Loss: 1.570 Epoch 2 Batch 674/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.867, Loss: 1.497 Epoch 2 Batch 675/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.869, Loss: 1.503 Epoch 2 Batch 676/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.867, Loss: 1.501 Epoch 2 Batch 677/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.861, Loss: 1.539 Epoch 2 Batch 678/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.867, Loss: 1.622 Epoch 2 Batch 679/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.875, Loss: 1.540 Epoch 2 Batch 680/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.873, Loss: 1.518 Epoch 2 Batch 681/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.875, Loss: 1.598 Epoch 2 Batch 682/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.875, Loss: 1.529 Epoch 2 Batch 683/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.874, Loss: 1.556 Epoch 2 Batch 684/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.873, Loss: 1.497 Epoch 2 Batch 685/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.871, Loss: 1.579 Epoch 2 Batch 686/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.868, Loss: 1.601 Epoch 2 Batch 687/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.856, Loss: 1.614 Epoch 2 Batch 688/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.862, Loss: 1.499 Epoch 2 Batch 689/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.857, Loss: 1.486 Epoch 2 Batch 690/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.857, Loss: 1.523 Epoch 2 Batch 691/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.860, Loss: 1.533 Epoch 2 Batch 692/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.869, Loss: 1.542 Epoch 2 Batch 693/1077 - Train Accuracy: 0.833, Validation Accuracy: 0.868, Loss: 1.620 Epoch 2 Batch 694/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.868, Loss: 1.546 Epoch 2 Batch 695/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.863, Loss: 1.574 Epoch 2 Batch 696/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.865, Loss: 1.524 Epoch 2 Batch 697/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.865, Loss: 1.486 Epoch 2 Batch 698/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.862, Loss: 1.572 Epoch 2 Batch 699/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.861, Loss: 1.537 Epoch 2 Batch 700/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.860, Loss: 1.574 Epoch 2 Batch 701/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.875, Loss: 1.558 Epoch 2 Batch 702/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.865, Loss: 1.522 Epoch 2 Batch 703/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.864, Loss: 1.597 Epoch 2 Batch 704/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.863, Loss: 1.476 Epoch 2 Batch 705/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.859, Loss: 1.585 Epoch 2 Batch 706/1077 - Train Accuracy: 0.816, Validation Accuracy: 0.863, Loss: 1.525 Epoch 2 Batch 707/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.865, Loss: 1.574 Epoch 2 Batch 708/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.864, Loss: 1.519 Epoch 2 Batch 709/1077 - Train Accuracy: 0.845, Validation Accuracy: 0.853, Loss: 1.595 Epoch 2 Batch 710/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.867, Loss: 1.493 Epoch 2 Batch 711/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.863, Loss: 1.644 Epoch 2 Batch 712/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.871, Loss: 1.503 Epoch 2 Batch 713/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.857, Loss: 1.561 Epoch 2 Batch 714/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.849, Loss: 1.535 Epoch 2 Batch 715/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.851, Loss: 1.638 Epoch 2 Batch 716/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.860, Loss: 1.543 Epoch 2 Batch 717/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.863, Loss: 1.514 Epoch 2 Batch 718/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.864, Loss: 1.552 Epoch 2 Batch 719/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.873, Loss: 1.474 Epoch 2 Batch 720/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.865, Loss: 1.638 Epoch 2 Batch 721/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.875, Loss: 1.596 Epoch 2 Batch 722/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.881, Loss: 1.555 Epoch 2 Batch 723/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.880, Loss: 1.523 Epoch 2 Batch 724/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.870, Loss: 1.519 Epoch 2 Batch 725/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.866, Loss: 1.547 Epoch 2 Batch 726/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.869, Loss: 1.563 Epoch 2 Batch 727/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.874, Loss: 1.497 Epoch 2 Batch 728/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.865, Loss: 1.472 Epoch 2 Batch 729/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.871, Loss: 1.454 Epoch 2 Batch 730/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.870, Loss: 1.568 Epoch 2 Batch 731/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.868, Loss: 1.489 Epoch 2 Batch 732/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.866, Loss: 1.564 Epoch 2 Batch 733/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.860, Loss: 1.597 Epoch 2 Batch 734/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.856, Loss: 1.566 Epoch 2 Batch 735/1077 - Train Accuracy: 0.876, Validation Accuracy: 0.863, Loss: 1.577 Epoch 2 Batch 736/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.865, Loss: 1.549 Epoch 2 Batch 737/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.875, Loss: 1.496 Epoch 2 Batch 738/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.870, Loss: 1.521 Epoch 2 Batch 739/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.865, Loss: 1.514 Epoch 2 Batch 740/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.866, Loss: 1.578 Epoch 2 Batch 741/1077 - Train Accuracy: 0.840, Validation Accuracy: 0.866, Loss: 1.571 Epoch 2 Batch 742/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.874, Loss: 1.577 Epoch 2 Batch 743/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.867, Loss: 1.575 Epoch 2 Batch 744/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.871, Loss: 1.481 Epoch 2 Batch 745/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.880, Loss: 1.574 Epoch 2 Batch 746/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.889, Loss: 1.585 Epoch 2 Batch 747/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.887, Loss: 1.496 Epoch 2 Batch 748/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.885, Loss: 1.514 Epoch 2 Batch 749/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.885, Loss: 1.582 Epoch 2 Batch 750/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.873, Loss: 1.569 Epoch 2 Batch 751/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.879, Loss: 1.594 Epoch 2 Batch 752/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.874, Loss: 1.532 Epoch 2 Batch 753/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.879, Loss: 1.555 Epoch 2 Batch 754/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.880, Loss: 1.567 Epoch 2 Batch 755/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.885, Loss: 1.516 Epoch 2 Batch 756/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.884, Loss: 1.526 Epoch 2 Batch 757/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.884, Loss: 1.511 Epoch 2 Batch 758/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.880, Loss: 1.503 Epoch 2 Batch 759/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.880, Loss: 1.549 Epoch 2 Batch 760/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.881, Loss: 1.573 Epoch 2 Batch 761/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.876, Loss: 1.542 Epoch 2 Batch 762/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.873, Loss: 1.614 Epoch 2 Batch 763/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.866, Loss: 1.525 Epoch 2 Batch 764/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.868, Loss: 1.524 Epoch 2 Batch 765/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.868, Loss: 1.616 Epoch 2 Batch 766/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.873, Loss: 1.571 Epoch 2 Batch 767/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.868, Loss: 1.496 Epoch 2 Batch 768/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.872, Loss: 1.593 Epoch 2 Batch 769/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.871, Loss: 1.489 Epoch 2 Batch 770/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.880, Loss: 1.480 Epoch 2 Batch 771/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.880, Loss: 1.519 Epoch 2 Batch 772/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.885, Loss: 1.549 Epoch 2 Batch 773/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.885, Loss: 1.539 Epoch 2 Batch 774/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.886, Loss: 1.524 Epoch 2 Batch 775/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.883, Loss: 1.561 Epoch 2 Batch 776/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.877, Loss: 1.525 Epoch 2 Batch 777/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.888, Loss: 1.520 Epoch 2 Batch 778/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.879, Loss: 1.538 Epoch 2 Batch 779/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.884, Loss: 1.485 Epoch 2 Batch 780/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.884, Loss: 1.643 Epoch 2 Batch 781/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.875, Loss: 1.524 Epoch 2 Batch 782/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.879, Loss: 1.515 Epoch 2 Batch 783/1077 - Train Accuracy: 0.862, Validation Accuracy: 0.868, Loss: 1.510 Epoch 2 Batch 784/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.862, Loss: 1.516 Epoch 2 Batch 785/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.860, Loss: 1.540 Epoch 2 Batch 786/1077 - Train Accuracy: 0.846, Validation Accuracy: 0.851, Loss: 1.552 Epoch 2 Batch 787/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.857, Loss: 1.471 Epoch 2 Batch 788/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.857, Loss: 1.537 Epoch 2 Batch 789/1077 - Train Accuracy: 0.858, Validation Accuracy: 0.862, Loss: 1.581 Epoch 2 Batch 790/1077 - Train Accuracy: 0.814, Validation Accuracy: 0.858, Loss: 1.501 Epoch 2 Batch 791/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.870, Loss: 1.562 Epoch 2 Batch 792/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.874, Loss: 1.507 Epoch 2 Batch 793/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.864, Loss: 1.571 Epoch 2 Batch 794/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.861, Loss: 1.508 Epoch 2 Batch 795/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.864, Loss: 1.545 Epoch 2 Batch 796/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.871, Loss: 1.478 Epoch 2 Batch 797/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.882, Loss: 1.534 Epoch 2 Batch 798/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.878, Loss: 1.559 Epoch 2 Batch 799/1077 - Train Accuracy: 0.847, Validation Accuracy: 0.872, Loss: 1.535 Epoch 2 Batch 800/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.875, Loss: 1.528 Epoch 2 Batch 801/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.868, Loss: 1.506 Epoch 2 Batch 802/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.866, Loss: 1.546 Epoch 2 Batch 803/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.874, Loss: 1.512 Epoch 2 Batch 804/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.874, Loss: 1.558 Epoch 2 Batch 805/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.876, Loss: 1.543 Epoch 2 Batch 806/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.881, Loss: 1.498 Epoch 2 Batch 807/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.884, Loss: 1.532 Epoch 2 Batch 808/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.870, Loss: 1.505 Epoch 2 Batch 809/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.881, Loss: 1.556 Epoch 2 Batch 810/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.866, Loss: 1.500 Epoch 2 Batch 811/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.857, Loss: 1.531 Epoch 2 Batch 812/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.852, Loss: 1.543 Epoch 2 Batch 813/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.854, Loss: 1.494 Epoch 2 Batch 814/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.857, Loss: 1.509 Epoch 2 Batch 815/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.859, Loss: 1.521 Epoch 2 Batch 816/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.869, Loss: 1.539 Epoch 2 Batch 817/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.863, Loss: 1.630 Epoch 2 Batch 818/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.877, Loss: 1.549 Epoch 2 Batch 819/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.882, Loss: 1.573 Epoch 2 Batch 820/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.874, Loss: 1.549 Epoch 2 Batch 821/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.875, Loss: 1.567 Epoch 2 Batch 822/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.877, Loss: 1.445 Epoch 2 Batch 823/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.871, Loss: 1.495 Epoch 2 Batch 824/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.867, Loss: 1.541 Epoch 2 Batch 825/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.868, Loss: 1.566 Epoch 2 Batch 826/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.865, Loss: 1.493 Epoch 2 Batch 827/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.864, Loss: 1.579 Epoch 2 Batch 828/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.858, Loss: 1.424 Epoch 2 Batch 829/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.864, Loss: 1.495 Epoch 2 Batch 830/1077 - Train Accuracy: 0.856, Validation Accuracy: 0.862, Loss: 1.518 Epoch 2 Batch 831/1077 - Train Accuracy: 0.844, Validation Accuracy: 0.866, Loss: 1.531 Epoch 2 Batch 832/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.870, Loss: 1.548 Epoch 2 Batch 833/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.872, Loss: 1.562 Epoch 2 Batch 834/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.868, Loss: 1.464 Epoch 2 Batch 835/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.869, Loss: 1.509 Epoch 2 Batch 836/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.872, Loss: 1.519 Epoch 2 Batch 837/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.876, Loss: 1.519 Epoch 2 Batch 838/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.876, Loss: 1.554 Epoch 2 Batch 839/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.881, Loss: 1.474 Epoch 2 Batch 840/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.878, Loss: 1.502 Epoch 2 Batch 841/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.878, Loss: 1.624 Epoch 2 Batch 842/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.876, Loss: 1.534 Epoch 2 Batch 843/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.879, Loss: 1.517 Epoch 2 Batch 844/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.880, Loss: 1.480 Epoch 2 Batch 845/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.873, Loss: 1.499 Epoch 2 Batch 846/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.870, Loss: 1.578 Epoch 2 Batch 847/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.872, Loss: 1.556 Epoch 2 Batch 848/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.877, Loss: 1.494 Epoch 2 Batch 849/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.870, Loss: 1.515 Epoch 2 Batch 850/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.871, Loss: 1.595 Epoch 2 Batch 851/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.870, Loss: 1.571 Epoch 2 Batch 852/1077 - Train Accuracy: 0.876, Validation Accuracy: 0.874, Loss: 1.524 Epoch 2 Batch 853/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.888, Loss: 1.468 Epoch 2 Batch 854/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.886, Loss: 1.539 Epoch 2 Batch 855/1077 - Train Accuracy: 0.858, Validation Accuracy: 0.886, Loss: 1.564 Epoch 2 Batch 856/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.890, Loss: 1.558 Epoch 2 Batch 857/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.884, Loss: 1.499 Epoch 2 Batch 858/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.890, Loss: 1.452 Epoch 2 Batch 859/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.893, Loss: 1.532 Epoch 2 Batch 860/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.886, Loss: 1.556 Epoch 2 Batch 861/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.887, Loss: 1.557 Epoch 2 Batch 862/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.884, Loss: 1.501 Epoch 2 Batch 863/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.883, Loss: 1.543 Epoch 2 Batch 864/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.882, Loss: 1.559 Epoch 2 Batch 865/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.883, Loss: 1.528 Epoch 2 Batch 866/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.881, Loss: 1.547 Epoch 2 Batch 867/1077 - Train Accuracy: 0.844, Validation Accuracy: 0.877, Loss: 1.607 Epoch 2 Batch 868/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.872, Loss: 1.558 Epoch 2 Batch 869/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.877, Loss: 1.594 Epoch 2 Batch 870/1077 - Train Accuracy: 0.828, Validation Accuracy: 0.869, Loss: 1.549 Epoch 2 Batch 871/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.867, Loss: 1.480 Epoch 2 Batch 872/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.874, Loss: 1.492 Epoch 2 Batch 873/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.869, Loss: 1.514 Epoch 2 Batch 874/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.874, Loss: 1.483 Epoch 2 Batch 875/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.870, Loss: 1.552 Epoch 2 Batch 876/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.870, Loss: 1.568 Epoch 2 Batch 877/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.878, Loss: 1.529 Epoch 2 Batch 878/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.873, Loss: 1.505 Epoch 2 Batch 879/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.888, Loss: 1.579 Epoch 2 Batch 880/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.886, Loss: 1.492 Epoch 2 Batch 881/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.883, Loss: 1.613 Epoch 2 Batch 882/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.879, Loss: 1.488 Epoch 2 Batch 883/1077 - Train Accuracy: 0.851, Validation Accuracy: 0.874, Loss: 1.561 Epoch 2 Batch 884/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.875, Loss: 1.467 Epoch 2 Batch 885/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.873, Loss: 1.391 Epoch 2 Batch 886/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.875, Loss: 1.528 Epoch 2 Batch 887/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.878, Loss: 1.623 Epoch 2 Batch 888/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.873, Loss: 1.535 Epoch 2 Batch 889/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.852, Loss: 1.474 Epoch 2 Batch 890/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.853, Loss: 1.508 Epoch 2 Batch 891/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.851, Loss: 1.503 Epoch 2 Batch 892/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.861, Loss: 1.508 Epoch 2 Batch 893/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.860, Loss: 1.527 Epoch 2 Batch 894/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.865, Loss: 1.533 Epoch 2 Batch 895/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.868, Loss: 1.489 Epoch 2 Batch 896/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.866, Loss: 1.521 Epoch 2 Batch 897/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.872, Loss: 1.534 Epoch 2 Batch 898/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.879, Loss: 1.526 Epoch 2 Batch 899/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.880, Loss: 1.459 Epoch 2 Batch 900/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.876, Loss: 1.483 Epoch 2 Batch 901/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.874, Loss: 1.540 Epoch 2 Batch 902/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.875, Loss: 1.607 Epoch 2 Batch 903/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.873, Loss: 1.595 Epoch 2 Batch 904/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.879, Loss: 1.561 Epoch 2 Batch 905/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.876, Loss: 1.570 Epoch 2 Batch 906/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.881, Loss: 1.539 Epoch 2 Batch 907/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.886, Loss: 1.590 Epoch 2 Batch 908/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.890, Loss: 1.516 Epoch 2 Batch 909/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.887, Loss: 1.538 Epoch 2 Batch 910/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.883, Loss: 1.520 Epoch 2 Batch 911/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.882, Loss: 1.533 Epoch 2 Batch 912/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.886, Loss: 1.444 Epoch 2 Batch 913/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.866, Loss: 1.592 Epoch 2 Batch 914/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.860, Loss: 1.479 Epoch 2 Batch 915/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.860, Loss: 1.466 Epoch 2 Batch 916/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.852, Loss: 1.452 Epoch 2 Batch 917/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.852, Loss: 1.508 Epoch 2 Batch 918/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.857, Loss: 1.539 Epoch 2 Batch 919/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.856, Loss: 1.582 Epoch 2 Batch 920/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.858, Loss: 1.550 Epoch 2 Batch 921/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.853, Loss: 1.518 Epoch 2 Batch 922/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.854, Loss: 1.537 Epoch 2 Batch 923/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.864, Loss: 1.534 Epoch 2 Batch 924/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.877, Loss: 1.508 Epoch 2 Batch 925/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.882, Loss: 1.465 Epoch 2 Batch 926/1077 - Train Accuracy: 0.839, Validation Accuracy: 0.881, Loss: 1.564 Epoch 2 Batch 927/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.876, Loss: 1.598 Epoch 2 Batch 928/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.880, Loss: 1.501 Epoch 2 Batch 929/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.877, Loss: 1.566 Epoch 2 Batch 930/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.879, Loss: 1.515 Epoch 2 Batch 931/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.877, Loss: 1.571 Epoch 2 Batch 932/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.877, Loss: 1.536 Epoch 2 Batch 933/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.880, Loss: 1.446 Epoch 2 Batch 934/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.866, Loss: 1.523 Epoch 2 Batch 935/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.862, Loss: 1.526 Epoch 2 Batch 936/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.862, Loss: 1.575 Epoch 2 Batch 937/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.855, Loss: 1.586 Epoch 2 Batch 938/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.859, Loss: 1.578 Epoch 2 Batch 939/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.854, Loss: 1.511 Epoch 2 Batch 940/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.858, Loss: 1.452 Epoch 2 Batch 941/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.862, Loss: 1.530 Epoch 2 Batch 942/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.852, Loss: 1.528 Epoch 2 Batch 943/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.849, Loss: 1.509 Epoch 2 Batch 944/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.852, Loss: 1.508 Epoch 2 Batch 945/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.855, Loss: 1.509 Epoch 2 Batch 946/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.864, Loss: 1.500 Epoch 2 Batch 947/1077 - Train Accuracy: 0.830, Validation Accuracy: 0.870, Loss: 1.529 Epoch 2 Batch 948/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.865, Loss: 1.583 Epoch 2 Batch 949/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.882, Loss: 1.595 Epoch 2 Batch 950/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.871, Loss: 1.597 Epoch 2 Batch 951/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.875, Loss: 1.489 Epoch 2 Batch 952/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.874, Loss: 1.587 Epoch 2 Batch 953/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.879, Loss: 1.588 Epoch 2 Batch 954/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.882, Loss: 1.519 Epoch 2 Batch 955/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.889, Loss: 1.557 Epoch 2 Batch 956/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.884, Loss: 1.483 Epoch 2 Batch 957/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.882, Loss: 1.519 Epoch 2 Batch 958/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.878, Loss: 1.422 Epoch 2 Batch 959/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.870, Loss: 1.476 Epoch 2 Batch 960/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.853, Loss: 1.526 Epoch 2 Batch 961/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.853, Loss: 1.508 Epoch 2 Batch 962/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.851, Loss: 1.517 Epoch 2 Batch 963/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.862, Loss: 1.547 Epoch 2 Batch 964/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.864, Loss: 1.452 Epoch 2 Batch 965/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.872, Loss: 1.483 Epoch 2 Batch 966/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.873, Loss: 1.537 Epoch 2 Batch 967/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.871, Loss: 1.646 Epoch 2 Batch 968/1077 - Train Accuracy: 0.852, Validation Accuracy: 0.867, Loss: 1.536 Epoch 2 Batch 969/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.865, Loss: 1.568 Epoch 2 Batch 970/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.879, Loss: 1.574 Epoch 2 Batch 971/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.874, Loss: 1.566 Epoch 2 Batch 972/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.879, Loss: 1.507 Epoch 2 Batch 973/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.878, Loss: 1.517 Epoch 2 Batch 974/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.869, Loss: 1.550 Epoch 2 Batch 975/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.872, Loss: 1.474 Epoch 2 Batch 976/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.864, Loss: 1.524 Epoch 2 Batch 977/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.865, Loss: 1.544 Epoch 2 Batch 978/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.871, Loss: 1.604 Epoch 2 Batch 979/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.875, Loss: 1.570 Epoch 2 Batch 980/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.874, Loss: 1.498 Epoch 2 Batch 981/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.880, Loss: 1.469 Epoch 2 Batch 982/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.871, Loss: 1.591 Epoch 2 Batch 983/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.874, Loss: 1.545 Epoch 2 Batch 984/1077 - Train Accuracy: 0.850, Validation Accuracy: 0.874, Loss: 1.593 Epoch 2 Batch 985/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.878, Loss: 1.522 Epoch 2 Batch 986/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.865, Loss: 1.541 Epoch 2 Batch 987/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.869, Loss: 1.472 Epoch 2 Batch 988/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.879, Loss: 1.498 Epoch 2 Batch 989/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.870, Loss: 1.559 Epoch 2 Batch 990/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.873, Loss: 1.554 Epoch 2 Batch 991/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.864, Loss: 1.498 Epoch 2 Batch 992/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.874, Loss: 1.489 Epoch 2 Batch 993/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.876, Loss: 1.515 Epoch 2 Batch 994/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.876, Loss: 1.528 Epoch 2 Batch 995/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.869, Loss: 1.526 Epoch 2 Batch 996/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.872, Loss: 1.437 Epoch 2 Batch 997/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.872, Loss: 1.484 Epoch 2 Batch 998/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.886, Loss: 1.606 Epoch 2 Batch 999/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.882, Loss: 1.529 Epoch 2 Batch 1000/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.888, Loss: 1.539 Epoch 2 Batch 1001/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.891, Loss: 1.499 Epoch 2 Batch 1002/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.893, Loss: 1.522 Epoch 2 Batch 1003/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.890, Loss: 1.568 Epoch 2 Batch 1004/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.891, Loss: 1.521 Epoch 2 Batch 1005/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.881, Loss: 1.434 Epoch 2 Batch 1006/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.886, Loss: 1.481 Epoch 2 Batch 1007/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.893, Loss: 1.522 Epoch 2 Batch 1008/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.894, Loss: 1.497 Epoch 2 Batch 1009/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.896, Loss: 1.525 Epoch 2 Batch 1010/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.891, Loss: 1.496 Epoch 2 Batch 1011/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.889, Loss: 1.514 Epoch 2 Batch 1012/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.888, Loss: 1.487 Epoch 2 Batch 1013/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.888, Loss: 1.490 Epoch 2 Batch 1014/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.887, Loss: 1.560 Epoch 2 Batch 1015/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.888, Loss: 1.576 Epoch 2 Batch 1016/1077 - Train Accuracy: 0.838, Validation Accuracy: 0.884, Loss: 1.569 Epoch 2 Batch 1017/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.882, Loss: 1.469 Epoch 2 Batch 1018/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.877, Loss: 1.523 Epoch 2 Batch 1019/1077 - Train Accuracy: 0.858, Validation Accuracy: 0.878, Loss: 1.511 Epoch 2 Batch 1020/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.867, Loss: 1.508 Epoch 2 Batch 1021/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.878, Loss: 1.481 Epoch 2 Batch 1022/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.869, Loss: 1.537 Epoch 2 Batch 1023/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.874, Loss: 1.552 Epoch 2 Batch 1024/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.878, Loss: 1.520 Epoch 2 Batch 1025/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.875, Loss: 1.568 Epoch 2 Batch 1026/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.879, Loss: 1.541 Epoch 2 Batch 1027/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.879, Loss: 1.567 Epoch 2 Batch 1028/1077 - Train Accuracy: 0.844, Validation Accuracy: 0.897, Loss: 1.529 Epoch 2 Batch 1029/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.898, Loss: 1.596 Epoch 2 Batch 1030/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.905, Loss: 1.478 Epoch 2 Batch 1031/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.898, Loss: 1.488 Epoch 2 Batch 1032/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.884, Loss: 1.472 Epoch 2 Batch 1033/1077 - Train Accuracy: 0.853, Validation Accuracy: 0.883, Loss: 1.562 Epoch 2 Batch 1034/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.879, Loss: 1.522 Epoch 2 Batch 1035/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.876, Loss: 1.501 Epoch 2 Batch 1036/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.871, Loss: 1.527 Epoch 2 Batch 1037/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.870, Loss: 1.509 Epoch 2 Batch 1038/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.870, Loss: 1.408 Epoch 2 Batch 1039/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.870, Loss: 1.499 Epoch 2 Batch 1040/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.863, Loss: 1.560 Epoch 2 Batch 1041/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.871, Loss: 1.604 Epoch 2 Batch 1042/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.871, Loss: 1.569 Epoch 2 Batch 1043/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.870, Loss: 1.508 Epoch 2 Batch 1044/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.870, Loss: 1.507 Epoch 2 Batch 1045/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.876, Loss: 1.577 Epoch 2 Batch 1046/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.874, Loss: 1.585 Epoch 2 Batch 1047/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.870, Loss: 1.548 Epoch 2 Batch 1048/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.881, Loss: 1.454 Epoch 2 Batch 1049/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.890, Loss: 1.510 Epoch 2 Batch 1050/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.885, Loss: 1.422 Epoch 2 Batch 1051/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.882, Loss: 1.478 Epoch 2 Batch 1052/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.881, Loss: 1.446 Epoch 2 Batch 1053/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.876, Loss: 1.488 Epoch 2 Batch 1054/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.863, Loss: 1.545 Epoch 2 Batch 1055/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.869, Loss: 1.567 Epoch 2 Batch 1056/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.873, Loss: 1.537 Epoch 2 Batch 1057/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.874, Loss: 1.473 Epoch 2 Batch 1058/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.885, Loss: 1.487 Epoch 2 Batch 1059/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.880, Loss: 1.503 Epoch 2 Batch 1060/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.885, Loss: 1.523 Epoch 2 Batch 1061/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.890, Loss: 1.505 Epoch 2 Batch 1062/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.891, Loss: 1.503 Epoch 2 Batch 1063/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.886, Loss: 1.454 Epoch 2 Batch 1064/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.885, Loss: 1.508 Epoch 2 Batch 1065/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.867, Loss: 1.471 Epoch 2 Batch 1066/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.863, Loss: 1.515 Epoch 2 Batch 1067/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.856, Loss: 1.487 Epoch 2 Batch 1068/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.849, Loss: 1.500 Epoch 2 Batch 1069/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.862, Loss: 1.571 Epoch 2 Batch 1070/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.865, Loss: 1.466 Epoch 2 Batch 1071/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.869, Loss: 1.465 Epoch 2 Batch 1072/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.869, Loss: 1.503 Epoch 2 Batch 1073/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.876, Loss: 1.493 Epoch 2 Batch 1074/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.890, Loss: 1.469 Epoch 2 Batch 1075/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.892, Loss: 1.479 Epoch 3 Batch 0/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.893, Loss: 1.454 Epoch 3 Batch 1/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.885, Loss: 1.450 Epoch 3 Batch 2/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.881, Loss: 1.522 Epoch 3 Batch 3/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.874, Loss: 1.487 Epoch 3 Batch 4/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.881, Loss: 1.482 Epoch 3 Batch 5/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.880, Loss: 1.554 Epoch 3 Batch 6/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.882, Loss: 1.507 Epoch 3 Batch 7/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.885, Loss: 1.516 Epoch 3 Batch 8/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.887, Loss: 1.477 Epoch 3 Batch 9/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.889, Loss: 1.498 Epoch 3 Batch 10/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.889, Loss: 1.608 Epoch 3 Batch 11/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.880, Loss: 1.486 Epoch 3 Batch 12/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.867, Loss: 1.545 Epoch 3 Batch 13/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.878, Loss: 1.569 Epoch 3 Batch 14/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.869, Loss: 1.555 Epoch 3 Batch 15/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.869, Loss: 1.589 Epoch 3 Batch 16/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.871, Loss: 1.524 Epoch 3 Batch 17/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.866, Loss: 1.522 Epoch 3 Batch 18/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.861, Loss: 1.509 Epoch 3 Batch 19/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.861, Loss: 1.520 Epoch 3 Batch 20/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.856, Loss: 1.546 Epoch 3 Batch 21/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.851, Loss: 1.549 Epoch 3 Batch 22/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.860, Loss: 1.486 Epoch 3 Batch 23/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.864, Loss: 1.447 Epoch 3 Batch 24/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.869, Loss: 1.477 Epoch 3 Batch 25/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.868, Loss: 1.490 Epoch 3 Batch 26/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.866, Loss: 1.575 Epoch 3 Batch 27/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.863, Loss: 1.583 Epoch 3 Batch 28/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.851, Loss: 1.541 Epoch 3 Batch 29/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.849, Loss: 1.494 Epoch 3 Batch 30/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.846, Loss: 1.432 Epoch 3 Batch 31/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.863, Loss: 1.535 Epoch 3 Batch 32/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.863, Loss: 1.536 Epoch 3 Batch 33/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.870, Loss: 1.401 Epoch 3 Batch 34/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.872, Loss: 1.534 Epoch 3 Batch 35/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.870, Loss: 1.579 Epoch 3 Batch 36/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.875, Loss: 1.457 Epoch 3 Batch 37/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.873, Loss: 1.545 Epoch 3 Batch 38/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.873, Loss: 1.559 Epoch 3 Batch 39/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.873, Loss: 1.499 Epoch 3 Batch 40/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.879, Loss: 1.487 Epoch 3 Batch 41/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.872, Loss: 1.504 Epoch 3 Batch 42/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.879, Loss: 1.498 Epoch 3 Batch 43/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.876, Loss: 1.537 Epoch 3 Batch 44/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.879, Loss: 1.529 Epoch 3 Batch 45/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.864, Loss: 1.602 Epoch 3 Batch 46/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.869, Loss: 1.487 Epoch 3 Batch 47/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.867, Loss: 1.510 Epoch 3 Batch 48/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.867, Loss: 1.584 Epoch 3 Batch 49/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.862, Loss: 1.473 Epoch 3 Batch 50/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.879, Loss: 1.495 Epoch 3 Batch 51/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.877, Loss: 1.489 Epoch 3 Batch 52/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.877, Loss: 1.472 Epoch 3 Batch 53/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.882, Loss: 1.572 Epoch 3 Batch 54/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.879, Loss: 1.584 Epoch 3 Batch 55/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.876, Loss: 1.464 Epoch 3 Batch 56/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.880, Loss: 1.434 Epoch 3 Batch 57/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.875, Loss: 1.488 Epoch 3 Batch 58/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.878, Loss: 1.512 Epoch 3 Batch 59/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.875, Loss: 1.525 Epoch 3 Batch 60/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.863, Loss: 1.596 Epoch 3 Batch 61/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.854, Loss: 1.556 Epoch 3 Batch 62/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.858, Loss: 1.538 Epoch 3 Batch 63/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.855, Loss: 1.530 Epoch 3 Batch 64/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.860, Loss: 1.450 Epoch 3 Batch 65/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.873, Loss: 1.459 Epoch 3 Batch 66/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.870, Loss: 1.453 Epoch 3 Batch 67/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.874, Loss: 1.565 Epoch 3 Batch 68/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.879, Loss: 1.498 Epoch 3 Batch 69/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.871, Loss: 1.617 Epoch 3 Batch 70/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.882, Loss: 1.472 Epoch 3 Batch 71/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.875, Loss: 1.493 Epoch 3 Batch 72/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.868, Loss: 1.530 Epoch 3 Batch 73/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.867, Loss: 1.486 Epoch 3 Batch 74/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.871, Loss: 1.514 Epoch 3 Batch 75/1077 - Train Accuracy: 0.873, Validation Accuracy: 0.876, Loss: 1.528 Epoch 3 Batch 76/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.870, Loss: 1.540 Epoch 3 Batch 77/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.876, Loss: 1.466 Epoch 3 Batch 78/1077 - Train Accuracy: 0.868, Validation Accuracy: 0.873, Loss: 1.506 Epoch 3 Batch 79/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.862, Loss: 1.478 Epoch 3 Batch 80/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.867, Loss: 1.471 Epoch 3 Batch 81/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.872, Loss: 1.508 Epoch 3 Batch 82/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.878, Loss: 1.502 Epoch 3 Batch 83/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.879, Loss: 1.498 Epoch 3 Batch 84/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.887, Loss: 1.491 Epoch 3 Batch 85/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.891, Loss: 1.447 Epoch 3 Batch 86/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.887, Loss: 1.431 Epoch 3 Batch 87/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.877, Loss: 1.531 Epoch 3 Batch 88/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.877, Loss: 1.475 Epoch 3 Batch 89/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.876, Loss: 1.553 Epoch 3 Batch 90/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.884, Loss: 1.467 Epoch 3 Batch 91/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.881, Loss: 1.517 Epoch 3 Batch 92/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.890, Loss: 1.554 Epoch 3 Batch 93/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.891, Loss: 1.499 Epoch 3 Batch 94/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.885, Loss: 1.521 Epoch 3 Batch 95/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.880, Loss: 1.546 Epoch 3 Batch 96/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.882, Loss: 1.466 Epoch 3 Batch 97/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.887, Loss: 1.513 Epoch 3 Batch 98/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.872, Loss: 1.442 Epoch 3 Batch 99/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.873, Loss: 1.503 Epoch 3 Batch 100/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.883, Loss: 1.419 Epoch 3 Batch 101/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.897, Loss: 1.546 Epoch 3 Batch 102/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.889, Loss: 1.490 Epoch 3 Batch 103/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.893, Loss: 1.495 Epoch 3 Batch 104/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.887, Loss: 1.536 Epoch 3 Batch 105/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.881, Loss: 1.454 Epoch 3 Batch 106/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.871, Loss: 1.543 Epoch 3 Batch 107/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.875, Loss: 1.492 Epoch 3 Batch 108/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.871, Loss: 1.495 Epoch 3 Batch 109/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.865, Loss: 1.455 Epoch 3 Batch 110/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.871, Loss: 1.486 Epoch 3 Batch 111/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.864, Loss: 1.517 Epoch 3 Batch 112/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.863, Loss: 1.526 Epoch 3 Batch 113/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.851, Loss: 1.478 Epoch 3 Batch 114/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.857, Loss: 1.418 Epoch 3 Batch 115/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.866, Loss: 1.551 Epoch 3 Batch 116/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.870, Loss: 1.476 Epoch 3 Batch 117/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.881, Loss: 1.546 Epoch 3 Batch 118/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.892, Loss: 1.531 Epoch 3 Batch 119/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.886, Loss: 1.507 Epoch 3 Batch 120/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.889, Loss: 1.407 Epoch 3 Batch 121/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.891, Loss: 1.460 Epoch 3 Batch 122/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.891, Loss: 1.485 Epoch 3 Batch 123/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.888, Loss: 1.455 Epoch 3 Batch 124/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.887, Loss: 1.552 Epoch 3 Batch 125/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.891, Loss: 1.525 Epoch 3 Batch 126/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.888, Loss: 1.480 Epoch 3 Batch 127/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.892, Loss: 1.502 Epoch 3 Batch 128/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.891, Loss: 1.509 Epoch 3 Batch 129/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.890, Loss: 1.608 Epoch 3 Batch 130/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.878, Loss: 1.558 Epoch 3 Batch 131/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.873, Loss: 1.561 Epoch 3 Batch 132/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.857, Loss: 1.467 Epoch 3 Batch 133/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.862, Loss: 1.503 Epoch 3 Batch 134/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.857, Loss: 1.529 Epoch 3 Batch 135/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.862, Loss: 1.443 Epoch 3 Batch 136/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.863, Loss: 1.522 Epoch 3 Batch 137/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.863, Loss: 1.458 Epoch 3 Batch 138/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.870, Loss: 1.475 Epoch 3 Batch 139/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.879, Loss: 1.527 Epoch 3 Batch 140/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.875, Loss: 1.517 Epoch 3 Batch 141/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.875, Loss: 1.518 Epoch 3 Batch 142/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.885, Loss: 1.559 Epoch 3 Batch 143/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.883, Loss: 1.524 Epoch 3 Batch 144/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.885, Loss: 1.576 Epoch 3 Batch 145/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.879, Loss: 1.536 Epoch 3 Batch 146/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.879, Loss: 1.471 Epoch 3 Batch 147/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.884, Loss: 1.472 Epoch 3 Batch 148/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.886, Loss: 1.509 Epoch 3 Batch 149/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.884, Loss: 1.466 Epoch 3 Batch 150/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.886, Loss: 1.458 Epoch 3 Batch 151/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.883, Loss: 1.546 Epoch 3 Batch 152/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.876, Loss: 1.580 Epoch 3 Batch 153/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.876, Loss: 1.464 Epoch 3 Batch 154/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.871, Loss: 1.538 Epoch 3 Batch 155/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.868, Loss: 1.493 Epoch 3 Batch 156/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.868, Loss: 1.436 Epoch 3 Batch 157/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.870, Loss: 1.452 Epoch 3 Batch 158/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.875, Loss: 1.503 Epoch 3 Batch 159/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.877, Loss: 1.473 Epoch 3 Batch 160/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.873, Loss: 1.517 Epoch 3 Batch 161/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.881, Loss: 1.491 Epoch 3 Batch 162/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.885, Loss: 1.492 Epoch 3 Batch 163/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.876, Loss: 1.580 Epoch 3 Batch 164/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.880, Loss: 1.516 Epoch 3 Batch 165/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.891, Loss: 1.419 Epoch 3 Batch 166/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.876, Loss: 1.521 Epoch 3 Batch 167/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.874, Loss: 1.480 Epoch 3 Batch 168/1077 - Train Accuracy: 0.883, Validation Accuracy: 0.873, Loss: 1.498 Epoch 3 Batch 169/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.876, Loss: 1.489 Epoch 3 Batch 170/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.875, Loss: 1.435 Epoch 3 Batch 171/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.876, Loss: 1.429 Epoch 3 Batch 172/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.879, Loss: 1.450 Epoch 3 Batch 173/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.873, Loss: 1.558 Epoch 3 Batch 174/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.866, Loss: 1.469 Epoch 3 Batch 175/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.867, Loss: 1.574 Epoch 3 Batch 176/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.867, Loss: 1.522 Epoch 3 Batch 177/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.874, Loss: 1.573 Epoch 3 Batch 178/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.868, Loss: 1.505 Epoch 3 Batch 179/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.872, Loss: 1.518 Epoch 3 Batch 180/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.877, Loss: 1.528 Epoch 3 Batch 181/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.874, Loss: 1.498 Epoch 3 Batch 182/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.873, Loss: 1.553 Epoch 3 Batch 183/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.870, Loss: 1.442 Epoch 3 Batch 184/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.863, Loss: 1.504 Epoch 3 Batch 185/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.867, Loss: 1.506 Epoch 3 Batch 186/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.863, Loss: 1.464 Epoch 3 Batch 187/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.862, Loss: 1.505 Epoch 3 Batch 188/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.861, Loss: 1.551 Epoch 3 Batch 189/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.860, Loss: 1.449 Epoch 3 Batch 190/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.862, Loss: 1.422 Epoch 3 Batch 191/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.862, Loss: 1.476 Epoch 3 Batch 192/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.871, Loss: 1.558 Epoch 3 Batch 193/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.876, Loss: 1.475 Epoch 3 Batch 194/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.875, Loss: 1.530 Epoch 3 Batch 195/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.869, Loss: 1.585 Epoch 3 Batch 196/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.875, Loss: 1.520 Epoch 3 Batch 197/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.879, Loss: 1.450 Epoch 3 Batch 198/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.877, Loss: 1.558 Epoch 3 Batch 199/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.874, Loss: 1.469 Epoch 3 Batch 200/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.871, Loss: 1.482 Epoch 3 Batch 201/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.872, Loss: 1.640 Epoch 3 Batch 202/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.878, Loss: 1.427 Epoch 3 Batch 203/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.874, Loss: 1.517 Epoch 3 Batch 204/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.875, Loss: 1.554 Epoch 3 Batch 205/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.859, Loss: 1.553 Epoch 3 Batch 206/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.858, Loss: 1.494 Epoch 3 Batch 207/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.866, Loss: 1.497 Epoch 3 Batch 208/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.864, Loss: 1.540 Epoch 3 Batch 209/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.869, Loss: 1.467 Epoch 3 Batch 210/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.868, Loss: 1.450 Epoch 3 Batch 211/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.872, Loss: 1.484 Epoch 3 Batch 212/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.871, Loss: 1.452 Epoch 3 Batch 213/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.863, Loss: 1.484 Epoch 3 Batch 214/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.858, Loss: 1.511 Epoch 3 Batch 215/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.858, Loss: 1.481 Epoch 3 Batch 216/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.853, Loss: 1.423 Epoch 3 Batch 217/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.860, Loss: 1.485 Epoch 3 Batch 218/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.866, Loss: 1.580 Epoch 3 Batch 219/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.870, Loss: 1.470 Epoch 3 Batch 220/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.870, Loss: 1.511 Epoch 3 Batch 221/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.877, Loss: 1.452 Epoch 3 Batch 222/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.884, Loss: 1.412 Epoch 3 Batch 223/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.881, Loss: 1.519 Epoch 3 Batch 224/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.882, Loss: 1.541 Epoch 3 Batch 225/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.878, Loss: 1.469 Epoch 3 Batch 226/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.876, Loss: 1.538 Epoch 3 Batch 227/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.878, Loss: 1.549 Epoch 3 Batch 228/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.876, Loss: 1.571 Epoch 3 Batch 229/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.880, Loss: 1.513 Epoch 3 Batch 230/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.888, Loss: 1.446 Epoch 3 Batch 231/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.887, Loss: 1.512 Epoch 3 Batch 232/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.885, Loss: 1.490 Epoch 3 Batch 233/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.884, Loss: 1.540 Epoch 3 Batch 234/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.875, Loss: 1.603 Epoch 3 Batch 235/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.875, Loss: 1.506 Epoch 3 Batch 236/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.876, Loss: 1.483 Epoch 3 Batch 237/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.886, Loss: 1.529 Epoch 3 Batch 238/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.884, Loss: 1.433 Epoch 3 Batch 239/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.884, Loss: 1.467 Epoch 3 Batch 240/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.880, Loss: 1.460 Epoch 3 Batch 241/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.885, Loss: 1.457 Epoch 3 Batch 242/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.886, Loss: 1.533 Epoch 3 Batch 243/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.877, Loss: 1.491 Epoch 3 Batch 244/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.883, Loss: 1.503 Epoch 3 Batch 245/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.874, Loss: 1.523 Epoch 3 Batch 246/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.877, Loss: 1.466 Epoch 3 Batch 247/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.877, Loss: 1.500 Epoch 3 Batch 248/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.869, Loss: 1.480 Epoch 3 Batch 249/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.877, Loss: 1.446 Epoch 3 Batch 250/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.885, Loss: 1.514 Epoch 3 Batch 251/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.869, Loss: 1.428 Epoch 3 Batch 252/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.873, Loss: 1.469 Epoch 3 Batch 253/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.875, Loss: 1.500 Epoch 3 Batch 254/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.882, Loss: 1.548 Epoch 3 Batch 255/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.882, Loss: 1.488 Epoch 3 Batch 256/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.892, Loss: 1.470 Epoch 3 Batch 257/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.893, Loss: 1.503 Epoch 3 Batch 258/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.886, Loss: 1.429 Epoch 3 Batch 259/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.884, Loss: 1.460 Epoch 3 Batch 260/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.890, Loss: 1.527 Epoch 3 Batch 261/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.893, Loss: 1.497 Epoch 3 Batch 262/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.892, Loss: 1.533 Epoch 3 Batch 263/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.886, Loss: 1.469 Epoch 3 Batch 264/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.891, Loss: 1.534 Epoch 3 Batch 265/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.886, Loss: 1.520 Epoch 3 Batch 266/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.892, Loss: 1.546 Epoch 3 Batch 267/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.889, Loss: 1.521 Epoch 3 Batch 268/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.888, Loss: 1.536 Epoch 3 Batch 269/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.883, Loss: 1.538 Epoch 3 Batch 270/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.873, Loss: 1.502 Epoch 3 Batch 271/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.868, Loss: 1.552 Epoch 3 Batch 272/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.866, Loss: 1.520 Epoch 3 Batch 273/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.876, Loss: 1.500 Epoch 3 Batch 274/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.881, Loss: 1.469 Epoch 3 Batch 275/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.882, Loss: 1.455 Epoch 3 Batch 276/1077 - Train Accuracy: 0.866, Validation Accuracy: 0.873, Loss: 1.458 Epoch 3 Batch 277/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.864, Loss: 1.458 Epoch 3 Batch 278/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.871, Loss: 1.600 Epoch 3 Batch 279/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.889, Loss: 1.500 Epoch 3 Batch 280/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.891, Loss: 1.532 Epoch 3 Batch 281/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.892, Loss: 1.554 Epoch 3 Batch 282/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.892, Loss: 1.476 Epoch 3 Batch 283/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.903, Loss: 1.530 Epoch 3 Batch 284/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.895, Loss: 1.524 Epoch 3 Batch 285/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.891, Loss: 1.522 Epoch 3 Batch 286/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.891, Loss: 1.537 Epoch 3 Batch 287/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.894, Loss: 1.522 Epoch 3 Batch 288/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.892, Loss: 1.456 Epoch 3 Batch 289/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.888, Loss: 1.507 Epoch 3 Batch 290/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.894, Loss: 1.531 Epoch 3 Batch 291/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.894, Loss: 1.527 Epoch 3 Batch 292/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.894, Loss: 1.552 Epoch 3 Batch 293/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.885, Loss: 1.509 Epoch 3 Batch 294/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.888, Loss: 1.522 Epoch 3 Batch 295/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.887, Loss: 1.524 Epoch 3 Batch 296/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.890, Loss: 1.493 Epoch 3 Batch 297/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.887, Loss: 1.500 Epoch 3 Batch 298/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.889, Loss: 1.602 Epoch 3 Batch 299/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.891, Loss: 1.476 Epoch 3 Batch 300/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.891, Loss: 1.501 Epoch 3 Batch 301/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.885, Loss: 1.540 Epoch 3 Batch 302/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.880, Loss: 1.453 Epoch 3 Batch 303/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.885, Loss: 1.494 Epoch 3 Batch 304/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.882, Loss: 1.479 Epoch 3 Batch 305/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.875, Loss: 1.439 Epoch 3 Batch 306/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.877, Loss: 1.523 Epoch 3 Batch 307/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.884, Loss: 1.522 Epoch 3 Batch 308/1077 - Train Accuracy: 0.861, Validation Accuracy: 0.880, Loss: 1.515 Epoch 3 Batch 309/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.878, Loss: 1.495 Epoch 3 Batch 310/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.866, Loss: 1.513 Epoch 3 Batch 311/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.866, Loss: 1.445 Epoch 3 Batch 312/1077 - Train Accuracy: 0.860, Validation Accuracy: 0.866, Loss: 1.543 Epoch 3 Batch 313/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.866, Loss: 1.499 Epoch 3 Batch 314/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.871, Loss: 1.513 Epoch 3 Batch 315/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.866, Loss: 1.538 Epoch 3 Batch 316/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.867, Loss: 1.451 Epoch 3 Batch 317/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.872, Loss: 1.458 Epoch 3 Batch 318/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.868, Loss: 1.560 Epoch 3 Batch 319/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.865, Loss: 1.599 Epoch 3 Batch 320/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.859, Loss: 1.428 Epoch 3 Batch 321/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.868, Loss: 1.487 Epoch 3 Batch 322/1077 - Train Accuracy: 0.881, Validation Accuracy: 0.868, Loss: 1.482 Epoch 3 Batch 323/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.863, Loss: 1.510 Epoch 3 Batch 324/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.863, Loss: 1.472 Epoch 3 Batch 325/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.875, Loss: 1.524 Epoch 3 Batch 326/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.876, Loss: 1.379 Epoch 3 Batch 327/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.877, Loss: 1.475 Epoch 3 Batch 328/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.875, Loss: 1.501 Epoch 3 Batch 329/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.875, Loss: 1.399 Epoch 3 Batch 330/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.876, Loss: 1.427 Epoch 3 Batch 331/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.875, Loss: 1.565 Epoch 3 Batch 332/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.875, Loss: 1.462 Epoch 3 Batch 333/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.875, Loss: 1.431 Epoch 3 Batch 334/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.880, Loss: 1.514 Epoch 3 Batch 335/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.879, Loss: 1.433 Epoch 3 Batch 336/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.877, Loss: 1.533 Epoch 3 Batch 337/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.882, Loss: 1.505 Epoch 3 Batch 338/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.882, Loss: 1.521 Epoch 3 Batch 339/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.875, Loss: 1.452 Epoch 3 Batch 340/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.876, Loss: 1.507 Epoch 3 Batch 341/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.885, Loss: 1.427 Epoch 3 Batch 342/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.880, Loss: 1.559 Epoch 3 Batch 343/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.869, Loss: 1.537 Epoch 3 Batch 344/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.864, Loss: 1.501 Epoch 3 Batch 345/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.871, Loss: 1.478 Epoch 3 Batch 346/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.878, Loss: 1.453 Epoch 3 Batch 347/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.867, Loss: 1.493 Epoch 3 Batch 348/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.855, Loss: 1.388 Epoch 3 Batch 349/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.858, Loss: 1.451 Epoch 3 Batch 350/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.853, Loss: 1.468 Epoch 3 Batch 351/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.856, Loss: 1.496 Epoch 3 Batch 352/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.860, Loss: 1.451 Epoch 3 Batch 353/1077 - Train Accuracy: 0.871, Validation Accuracy: 0.864, Loss: 1.480 Epoch 3 Batch 354/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.870, Loss: 1.474 Epoch 3 Batch 355/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.875, Loss: 1.502 Epoch 3 Batch 356/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.879, Loss: 1.432 Epoch 3 Batch 357/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.869, Loss: 1.449 Epoch 3 Batch 358/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.862, Loss: 1.398 Epoch 3 Batch 359/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.863, Loss: 1.529 Epoch 3 Batch 360/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.869, Loss: 1.446 Epoch 3 Batch 361/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.879, Loss: 1.513 Epoch 3 Batch 362/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.877, Loss: 1.455 Epoch 3 Batch 363/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.886, Loss: 1.434 Epoch 3 Batch 364/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.886, Loss: 1.534 Epoch 3 Batch 365/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.889, Loss: 1.446 Epoch 3 Batch 366/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.891, Loss: 1.451 Epoch 3 Batch 367/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.893, Loss: 1.515 Epoch 3 Batch 368/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.886, Loss: 1.471 Epoch 3 Batch 369/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.874, Loss: 1.421 Epoch 3 Batch 370/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.870, Loss: 1.484 Epoch 3 Batch 371/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.877, Loss: 1.447 Epoch 3 Batch 372/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.872, Loss: 1.452 Epoch 3 Batch 373/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.879, Loss: 1.466 Epoch 3 Batch 374/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.891, Loss: 1.548 Epoch 3 Batch 375/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.889, Loss: 1.506 Epoch 3 Batch 376/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.888, Loss: 1.502 Epoch 3 Batch 377/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.895, Loss: 1.586 Epoch 3 Batch 378/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.895, Loss: 1.593 Epoch 3 Batch 379/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.885, Loss: 1.463 Epoch 3 Batch 380/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.880, Loss: 1.460 Epoch 3 Batch 381/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.884, Loss: 1.572 Epoch 3 Batch 382/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.882, Loss: 1.505 Epoch 3 Batch 383/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.883, Loss: 1.410 Epoch 3 Batch 384/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.889, Loss: 1.520 Epoch 3 Batch 385/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.894, Loss: 1.390 Epoch 3 Batch 386/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.887, Loss: 1.457 Epoch 3 Batch 387/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.889, Loss: 1.480 Epoch 3 Batch 388/1077 - Train Accuracy: 0.876, Validation Accuracy: 0.890, Loss: 1.457 Epoch 3 Batch 389/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.891, Loss: 1.477 Epoch 3 Batch 390/1077 - Train Accuracy: 0.872, Validation Accuracy: 0.896, Loss: 1.549 Epoch 3 Batch 391/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.891, Loss: 1.487 Epoch 3 Batch 392/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.891, Loss: 1.532 Epoch 3 Batch 393/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.890, Loss: 1.473 Epoch 3 Batch 394/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.901, Loss: 1.499 Epoch 3 Batch 395/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.897, Loss: 1.493 Epoch 3 Batch 396/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.890, Loss: 1.505 Epoch 3 Batch 397/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.891, Loss: 1.566 Epoch 3 Batch 398/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.876, Loss: 1.525 Epoch 3 Batch 399/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.867, Loss: 1.456 Epoch 3 Batch 400/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.875, Loss: 1.479 Epoch 3 Batch 401/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.886, Loss: 1.437 Epoch 3 Batch 402/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.887, Loss: 1.477 Epoch 3 Batch 403/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.884, Loss: 1.508 Epoch 3 Batch 404/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.886, Loss: 1.479 Epoch 3 Batch 405/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.901, Loss: 1.445 Epoch 3 Batch 406/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.902, Loss: 1.442 Epoch 3 Batch 407/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.895, Loss: 1.512 Epoch 3 Batch 408/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.897, Loss: 1.433 Epoch 3 Batch 409/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.897, Loss: 1.464 Epoch 3 Batch 410/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.885, Loss: 1.487 Epoch 3 Batch 411/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.890, Loss: 1.572 Epoch 3 Batch 412/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.897, Loss: 1.514 Epoch 3 Batch 413/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.903, Loss: 1.463 Epoch 3 Batch 414/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.893, Loss: 1.516 Epoch 3 Batch 415/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.893, Loss: 1.483 Epoch 3 Batch 416/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.894, Loss: 1.484 Epoch 3 Batch 417/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.893, Loss: 1.481 Epoch 3 Batch 418/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.891, Loss: 1.528 Epoch 3 Batch 419/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.882, Loss: 1.501 Epoch 3 Batch 420/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.877, Loss: 1.563 Epoch 3 Batch 421/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.877, Loss: 1.496 Epoch 3 Batch 422/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.877, Loss: 1.445 Epoch 3 Batch 423/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.878, Loss: 1.524 Epoch 3 Batch 424/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.888, Loss: 1.483 Epoch 3 Batch 425/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.900, Loss: 1.498 Epoch 3 Batch 426/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.902, Loss: 1.462 Epoch 3 Batch 427/1077 - Train Accuracy: 0.859, Validation Accuracy: 0.904, Loss: 1.508 Epoch 3 Batch 428/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.908, Loss: 1.453 Epoch 3 Batch 429/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.902, Loss: 1.436 Epoch 3 Batch 430/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.901, Loss: 1.520 Epoch 3 Batch 431/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.901, Loss: 1.449 Epoch 3 Batch 432/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.907, Loss: 1.476 Epoch 3 Batch 433/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.893, Loss: 1.444 Epoch 3 Batch 434/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.897, Loss: 1.495 Epoch 3 Batch 435/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.899, Loss: 1.434 Epoch 3 Batch 436/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.893, Loss: 1.491 Epoch 3 Batch 437/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.898, Loss: 1.503 Epoch 3 Batch 438/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.893, Loss: 1.468 Epoch 3 Batch 439/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.884, Loss: 1.451 Epoch 3 Batch 440/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.879, Loss: 1.464 Epoch 3 Batch 441/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.870, Loss: 1.512 Epoch 3 Batch 442/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.866, Loss: 1.434 Epoch 3 Batch 443/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.858, Loss: 1.511 Epoch 3 Batch 444/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.870, Loss: 1.537 Epoch 3 Batch 445/1077 - Train Accuracy: 0.858, Validation Accuracy: 0.876, Loss: 1.490 Epoch 3 Batch 446/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.871, Loss: 1.503 Epoch 3 Batch 447/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.871, Loss: 1.416 Epoch 3 Batch 448/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.876, Loss: 1.437 Epoch 3 Batch 449/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.876, Loss: 1.546 Epoch 3 Batch 450/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.881, Loss: 1.438 Epoch 3 Batch 451/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.884, Loss: 1.459 Epoch 3 Batch 452/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.887, Loss: 1.561 Epoch 3 Batch 453/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.887, Loss: 1.535 Epoch 3 Batch 454/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.881, Loss: 1.445 Epoch 3 Batch 455/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.879, Loss: 1.489 Epoch 3 Batch 456/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.878, Loss: 1.494 Epoch 3 Batch 457/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.872, Loss: 1.457 Epoch 3 Batch 458/1077 - Train Accuracy: 0.849, Validation Accuracy: 0.878, Loss: 1.504 Epoch 3 Batch 459/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.878, Loss: 1.427 Epoch 3 Batch 460/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.876, Loss: 1.500 Epoch 3 Batch 461/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.891, Loss: 1.515 Epoch 3 Batch 462/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.892, Loss: 1.549 Epoch 3 Batch 463/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.888, Loss: 1.523 Epoch 3 Batch 464/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.882, Loss: 1.505 Epoch 3 Batch 465/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.883, Loss: 1.488 Epoch 3 Batch 466/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.873, Loss: 1.580 Epoch 3 Batch 467/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.869, Loss: 1.421 Epoch 3 Batch 468/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.877, Loss: 1.416 Epoch 3 Batch 469/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.875, Loss: 1.487 Epoch 3 Batch 470/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.875, Loss: 1.407 Epoch 3 Batch 471/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.876, Loss: 1.474 Epoch 3 Batch 472/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.880, Loss: 1.485 Epoch 3 Batch 473/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.878, Loss: 1.527 Epoch 3 Batch 474/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.887, Loss: 1.428 Epoch 3 Batch 475/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.877, Loss: 1.499 Epoch 3 Batch 476/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.890, Loss: 1.478 Epoch 3 Batch 477/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.884, Loss: 1.551 Epoch 3 Batch 478/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.892, Loss: 1.473 Epoch 3 Batch 479/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.891, Loss: 1.479 Epoch 3 Batch 480/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.892, Loss: 1.477 Epoch 3 Batch 481/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.890, Loss: 1.564 Epoch 3 Batch 482/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.890, Loss: 1.529 Epoch 3 Batch 483/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.892, Loss: 1.517 Epoch 3 Batch 484/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.888, Loss: 1.511 Epoch 3 Batch 485/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.884, Loss: 1.468 Epoch 3 Batch 486/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.880, Loss: 1.436 Epoch 3 Batch 487/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.877, Loss: 1.507 Epoch 3 Batch 488/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.882, Loss: 1.529 Epoch 3 Batch 489/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.876, Loss: 1.488 Epoch 3 Batch 490/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.868, Loss: 1.577 Epoch 3 Batch 491/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.868, Loss: 1.578 Epoch 3 Batch 492/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.875, Loss: 1.497 Epoch 3 Batch 493/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.860, Loss: 1.401 Epoch 3 Batch 494/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.860, Loss: 1.461 Epoch 3 Batch 495/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.864, Loss: 1.416 Epoch 3 Batch 496/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.869, Loss: 1.579 Epoch 3 Batch 497/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.870, Loss: 1.533 Epoch 3 Batch 498/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.885, Loss: 1.477 Epoch 3 Batch 499/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.885, Loss: 1.479 Epoch 3 Batch 500/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.877, Loss: 1.490 Epoch 3 Batch 501/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.878, Loss: 1.481 Epoch 3 Batch 502/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.874, Loss: 1.511 Epoch 3 Batch 503/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.874, Loss: 1.528 Epoch 3 Batch 504/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.875, Loss: 1.408 Epoch 3 Batch 505/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.880, Loss: 1.513 Epoch 3 Batch 506/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.887, Loss: 1.443 Epoch 3 Batch 507/1077 - Train Accuracy: 0.869, Validation Accuracy: 0.890, Loss: 1.487 Epoch 3 Batch 508/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.887, Loss: 1.557 Epoch 3 Batch 509/1077 - Train Accuracy: 0.870, Validation Accuracy: 0.884, Loss: 1.483 Epoch 3 Batch 510/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.892, Loss: 1.512 Epoch 3 Batch 511/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.891, Loss: 1.497 Epoch 3 Batch 512/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.891, Loss: 1.511 Epoch 3 Batch 513/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.885, Loss: 1.514 Epoch 3 Batch 514/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.881, Loss: 1.502 Epoch 3 Batch 515/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.887, Loss: 1.436 Epoch 3 Batch 516/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.880, Loss: 1.463 Epoch 3 Batch 517/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.869, Loss: 1.473 Epoch 3 Batch 518/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.875, Loss: 1.423 Epoch 3 Batch 519/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.870, Loss: 1.418 Epoch 3 Batch 520/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.862, Loss: 1.453 Epoch 3 Batch 521/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.865, Loss: 1.495 Epoch 3 Batch 522/1077 - Train Accuracy: 0.842, Validation Accuracy: 0.864, Loss: 1.465 Epoch 3 Batch 523/1077 - Train Accuracy: 0.874, Validation Accuracy: 0.866, Loss: 1.443 Epoch 3 Batch 524/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.875, Loss: 1.515 Epoch 3 Batch 525/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.886, Loss: 1.565 Epoch 3 Batch 526/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.881, Loss: 1.471 Epoch 3 Batch 527/1077 - Train Accuracy: 0.878, Validation Accuracy: 0.880, Loss: 1.492 Epoch 3 Batch 528/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.887, Loss: 1.451 Epoch 3 Batch 529/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.900, Loss: 1.443 Epoch 3 Batch 530/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.909, Loss: 1.490 Epoch 3 Batch 531/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.901, Loss: 1.446 Epoch 3 Batch 532/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.903, Loss: 1.478 Epoch 3 Batch 533/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.904, Loss: 1.505 Epoch 3 Batch 534/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.898, Loss: 1.472 Epoch 3 Batch 535/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.891, Loss: 1.535 Epoch 3 Batch 536/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.892, Loss: 1.477 Epoch 3 Batch 537/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.890, Loss: 1.451 Epoch 3 Batch 538/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.891, Loss: 1.544 Epoch 3 Batch 539/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.890, Loss: 1.430 Epoch 3 Batch 540/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.880, Loss: 1.482 Epoch 3 Batch 541/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.876, Loss: 1.497 Epoch 3 Batch 542/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.876, Loss: 1.462 Epoch 3 Batch 543/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.885, Loss: 1.466 Epoch 3 Batch 544/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.896, Loss: 1.540 Epoch 3 Batch 545/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.896, Loss: 1.535 Epoch 3 Batch 546/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.894, Loss: 1.446 Epoch 3 Batch 547/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.884, Loss: 1.429 Epoch 3 Batch 548/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.879, Loss: 1.509 Epoch 3 Batch 549/1077 - Train Accuracy: 0.865, Validation Accuracy: 0.885, Loss: 1.516 Epoch 3 Batch 550/1077 - Train Accuracy: 0.877, Validation Accuracy: 0.887, Loss: 1.522 Epoch 3 Batch 551/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.885, Loss: 1.413 Epoch 3 Batch 552/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.887, Loss: 1.486 Epoch 3 Batch 553/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.897, Loss: 1.510 Epoch 3 Batch 554/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.898, Loss: 1.379 Epoch 3 Batch 555/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.899, Loss: 1.471 Epoch 3 Batch 556/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.893, Loss: 1.447 Epoch 3 Batch 557/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.896, Loss: 1.470 Epoch 3 Batch 558/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.898, Loss: 1.473 Epoch 3 Batch 559/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.889, Loss: 1.467 Epoch 3 Batch 560/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.882, Loss: 1.503 Epoch 3 Batch 561/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.887, Loss: 1.471 Epoch 3 Batch 562/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.900, Loss: 1.402 Epoch 3 Batch 563/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.899, Loss: 1.534 Epoch 3 Batch 564/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.906, Loss: 1.476 Epoch 3 Batch 565/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.897, Loss: 1.570 Epoch 3 Batch 566/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.898, Loss: 1.451 Epoch 3 Batch 567/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.898, Loss: 1.469 Epoch 3 Batch 568/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.902, Loss: 1.504 Epoch 3 Batch 569/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.904, Loss: 1.474 Epoch 3 Batch 570/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.904, Loss: 1.466 Epoch 3 Batch 571/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.888, Loss: 1.579 Epoch 3 Batch 572/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.881, Loss: 1.507 Epoch 3 Batch 573/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.879, Loss: 1.432 Epoch 3 Batch 574/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.879, Loss: 1.502 Epoch 3 Batch 575/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.885, Loss: 1.576 Epoch 3 Batch 576/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.881, Loss: 1.435 Epoch 3 Batch 577/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.870, Loss: 1.416 Epoch 3 Batch 578/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.870, Loss: 1.441 Epoch 3 Batch 579/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.874, Loss: 1.456 Epoch 3 Batch 580/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.870, Loss: 1.453 Epoch 3 Batch 581/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.870, Loss: 1.466 Epoch 3 Batch 582/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.868, Loss: 1.437 Epoch 3 Batch 583/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.882, Loss: 1.505 Epoch 3 Batch 584/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.875, Loss: 1.482 Epoch 3 Batch 585/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.874, Loss: 1.475 Epoch 3 Batch 586/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.873, Loss: 1.491 Epoch 3 Batch 587/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.877, Loss: 1.502 Epoch 3 Batch 588/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.875, Loss: 1.497 Epoch 3 Batch 589/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.880, Loss: 1.471 Epoch 3 Batch 590/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.884, Loss: 1.554 Epoch 3 Batch 591/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.887, Loss: 1.520 Epoch 3 Batch 592/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.887, Loss: 1.507 Epoch 3 Batch 593/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.887, Loss: 1.433 Epoch 3 Batch 594/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.887, Loss: 1.438 Epoch 3 Batch 595/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.886, Loss: 1.420 Epoch 3 Batch 596/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.887, Loss: 1.500 Epoch 3 Batch 597/1077 - Train Accuracy: 0.890, Validation Accuracy: 0.887, Loss: 1.446 Epoch 3 Batch 598/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.886, Loss: 1.473 Epoch 3 Batch 599/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.887, Loss: 1.496 Epoch 3 Batch 600/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.882, Loss: 1.486 Epoch 3 Batch 601/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.886, Loss: 1.477 Epoch 3 Batch 602/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.900, Loss: 1.450 Epoch 3 Batch 603/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.894, Loss: 1.541 Epoch 3 Batch 604/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.890, Loss: 1.554 Epoch 3 Batch 605/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.890, Loss: 1.502 Epoch 3 Batch 606/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.889, Loss: 1.517 Epoch 3 Batch 607/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.889, Loss: 1.499 Epoch 3 Batch 608/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.883, Loss: 1.479 Epoch 3 Batch 609/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.884, Loss: 1.508 Epoch 3 Batch 610/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.889, Loss: 1.445 Epoch 3 Batch 611/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.889, Loss: 1.488 Epoch 3 Batch 612/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.889, Loss: 1.435 Epoch 3 Batch 613/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.895, Loss: 1.425 Epoch 3 Batch 614/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.887, Loss: 1.504 Epoch 3 Batch 615/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.891, Loss: 1.475 Epoch 3 Batch 616/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.888, Loss: 1.515 Epoch 3 Batch 617/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.888, Loss: 1.398 Epoch 3 Batch 618/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.889, Loss: 1.531 Epoch 3 Batch 619/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.890, Loss: 1.476 Epoch 3 Batch 620/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.898, Loss: 1.469 Epoch 3 Batch 621/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.908, Loss: 1.418 Epoch 3 Batch 622/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.901, Loss: 1.441 Epoch 3 Batch 623/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.898, Loss: 1.581 Epoch 3 Batch 624/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.900, Loss: 1.485 Epoch 3 Batch 625/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.895, Loss: 1.350 Epoch 3 Batch 626/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.896, Loss: 1.513 Epoch 3 Batch 627/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.898, Loss: 1.452 Epoch 3 Batch 628/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.903, Loss: 1.439 Epoch 3 Batch 629/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.893, Loss: 1.512 Epoch 3 Batch 630/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.893, Loss: 1.480 Epoch 3 Batch 631/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.890, Loss: 1.485 Epoch 3 Batch 632/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.885, Loss: 1.413 Epoch 3 Batch 633/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.893, Loss: 1.544 Epoch 3 Batch 634/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.893, Loss: 1.454 Epoch 3 Batch 635/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.897, Loss: 1.546 Epoch 3 Batch 636/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.877, Loss: 1.465 Epoch 3 Batch 637/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.881, Loss: 1.444 Epoch 3 Batch 638/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.888, Loss: 1.481 Epoch 3 Batch 639/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.878, Loss: 1.561 Epoch 3 Batch 640/1077 - Train Accuracy: 0.885, Validation Accuracy: 0.874, Loss: 1.497 Epoch 3 Batch 641/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.876, Loss: 1.438 Epoch 3 Batch 642/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.876, Loss: 1.498 Epoch 3 Batch 643/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.881, Loss: 1.476 Epoch 3 Batch 644/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.890, Loss: 1.459 Epoch 3 Batch 645/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.893, Loss: 1.424 Epoch 3 Batch 646/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.895, Loss: 1.440 Epoch 3 Batch 647/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.888, Loss: 1.536 Epoch 3 Batch 648/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.891, Loss: 1.526 Epoch 3 Batch 649/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.886, Loss: 1.512 Epoch 3 Batch 650/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.881, Loss: 1.530 Epoch 3 Batch 651/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.883, Loss: 1.434 Epoch 3 Batch 652/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.877, Loss: 1.570 Epoch 3 Batch 653/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.887, Loss: 1.485 Epoch 3 Batch 654/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.895, Loss: 1.516 Epoch 3 Batch 655/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.888, Loss: 1.543 Epoch 3 Batch 656/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.895, Loss: 1.446 Epoch 3 Batch 657/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.900, Loss: 1.410 Epoch 3 Batch 658/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.904, Loss: 1.499 Epoch 3 Batch 659/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.900, Loss: 1.466 Epoch 3 Batch 660/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.900, Loss: 1.462 Epoch 3 Batch 661/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.891, Loss: 1.490 Epoch 3 Batch 662/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.888, Loss: 1.465 Epoch 3 Batch 663/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.885, Loss: 1.452 Epoch 3 Batch 664/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.878, Loss: 1.488 Epoch 3 Batch 665/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.882, Loss: 1.477 Epoch 3 Batch 666/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.891, Loss: 1.467 Epoch 3 Batch 667/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.895, Loss: 1.557 Epoch 3 Batch 668/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.895, Loss: 1.542 Epoch 3 Batch 669/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.891, Loss: 1.460 Epoch 3 Batch 670/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.890, Loss: 1.529 Epoch 3 Batch 671/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.902, Loss: 1.450 Epoch 3 Batch 672/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.891, Loss: 1.428 Epoch 3 Batch 673/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.890, Loss: 1.428 Epoch 3 Batch 674/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.884, Loss: 1.518 Epoch 3 Batch 675/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.893, Loss: 1.456 Epoch 3 Batch 676/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.895, Loss: 1.511 Epoch 3 Batch 677/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.895, Loss: 1.544 Epoch 3 Batch 678/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.895, Loss: 1.474 Epoch 3 Batch 679/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.895, Loss: 1.443 Epoch 3 Batch 680/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.895, Loss: 1.445 Epoch 3 Batch 681/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.901, Loss: 1.484 Epoch 3 Batch 682/1077 - Train Accuracy: 0.880, Validation Accuracy: 0.901, Loss: 1.438 Epoch 3 Batch 683/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.899, Loss: 1.484 Epoch 3 Batch 684/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.900, Loss: 1.442 Epoch 3 Batch 685/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.896, Loss: 1.508 Epoch 3 Batch 686/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.909, Loss: 1.416 Epoch 3 Batch 687/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.909, Loss: 1.536 Epoch 3 Batch 688/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.907, Loss: 1.442 Epoch 3 Batch 689/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.901, Loss: 1.486 Epoch 3 Batch 690/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.901, Loss: 1.479 Epoch 3 Batch 691/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.898, Loss: 1.434 Epoch 3 Batch 692/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.896, Loss: 1.487 Epoch 3 Batch 693/1077 - Train Accuracy: 0.858, Validation Accuracy: 0.893, Loss: 1.509 Epoch 3 Batch 694/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.886, Loss: 1.526 Epoch 3 Batch 695/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.885, Loss: 1.554 Epoch 3 Batch 696/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.877, Loss: 1.477 Epoch 3 Batch 697/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.877, Loss: 1.443 Epoch 3 Batch 698/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.876, Loss: 1.426 Epoch 3 Batch 699/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.874, Loss: 1.455 Epoch 3 Batch 700/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.871, Loss: 1.480 Epoch 3 Batch 701/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.873, Loss: 1.500 Epoch 3 Batch 702/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.878, Loss: 1.519 Epoch 3 Batch 703/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.895, Loss: 1.593 Epoch 3 Batch 704/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.899, Loss: 1.501 Epoch 3 Batch 705/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.900, Loss: 1.488 Epoch 3 Batch 706/1077 - Train Accuracy: 0.863, Validation Accuracy: 0.906, Loss: 1.464 Epoch 3 Batch 707/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.893, Loss: 1.482 Epoch 3 Batch 708/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.897, Loss: 1.408 Epoch 3 Batch 709/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.893, Loss: 1.539 Epoch 3 Batch 710/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.907, Loss: 1.410 Epoch 3 Batch 711/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.904, Loss: 1.542 Epoch 3 Batch 712/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.901, Loss: 1.475 Epoch 3 Batch 713/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.893, Loss: 1.433 Epoch 3 Batch 714/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.887, Loss: 1.530 Epoch 3 Batch 715/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.896, Loss: 1.510 Epoch 3 Batch 716/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.891, Loss: 1.409 Epoch 3 Batch 717/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.898, Loss: 1.504 Epoch 3 Batch 718/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.897, Loss: 1.543 Epoch 3 Batch 719/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.890, Loss: 1.485 Epoch 3 Batch 720/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.874, Loss: 1.548 Epoch 3 Batch 721/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.884, Loss: 1.575 Epoch 3 Batch 722/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.886, Loss: 1.490 Epoch 3 Batch 723/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.894, Loss: 1.438 Epoch 3 Batch 724/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.891, Loss: 1.486 Epoch 3 Batch 725/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.890, Loss: 1.452 Epoch 3 Batch 726/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.902, Loss: 1.407 Epoch 3 Batch 727/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.908, Loss: 1.514 Epoch 3 Batch 728/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.914, Loss: 1.461 Epoch 3 Batch 729/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.904, Loss: 1.533 Epoch 3 Batch 730/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.901, Loss: 1.487 Epoch 3 Batch 731/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.902, Loss: 1.565 Epoch 3 Batch 732/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.894, Loss: 1.468 Epoch 3 Batch 733/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.889, Loss: 1.528 Epoch 3 Batch 734/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.882, Loss: 1.469 Epoch 3 Batch 735/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.880, Loss: 1.449 Epoch 3 Batch 736/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.880, Loss: 1.459 Epoch 3 Batch 737/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.880, Loss: 1.405 Epoch 3 Batch 738/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.884, Loss: 1.517 Epoch 3 Batch 739/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.882, Loss: 1.414 Epoch 3 Batch 740/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.897, Loss: 1.529 Epoch 3 Batch 741/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.907, Loss: 1.536 Epoch 3 Batch 742/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.901, Loss: 1.394 Epoch 3 Batch 743/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.906, Loss: 1.461 Epoch 3 Batch 744/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.903, Loss: 1.528 Epoch 3 Batch 745/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.898, Loss: 1.548 Epoch 3 Batch 746/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.900, Loss: 1.486 Epoch 3 Batch 747/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.909, Loss: 1.463 Epoch 3 Batch 748/1077 - Train Accuracy: 0.898, Validation Accuracy: 0.909, Loss: 1.500 Epoch 3 Batch 749/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.902, Loss: 1.450 Epoch 3 Batch 750/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.892, Loss: 1.411 Epoch 3 Batch 751/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.892, Loss: 1.469 Epoch 3 Batch 752/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.910, Loss: 1.496 Epoch 3 Batch 753/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.914, Loss: 1.486 Epoch 3 Batch 754/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.930, Loss: 1.528 Epoch 3 Batch 755/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.942, Loss: 1.428 Epoch 3 Batch 756/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.923, Loss: 1.435 Epoch 3 Batch 757/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.915, Loss: 1.433 Epoch 3 Batch 758/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.913, Loss: 1.456 Epoch 3 Batch 759/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.914, Loss: 1.552 Epoch 3 Batch 760/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.911, Loss: 1.468 Epoch 3 Batch 761/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.909, Loss: 1.427 Epoch 3 Batch 762/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.904, Loss: 1.421 Epoch 3 Batch 763/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.908, Loss: 1.466 Epoch 3 Batch 764/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.907, Loss: 1.456 Epoch 3 Batch 765/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.906, Loss: 1.514 Epoch 3 Batch 766/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.916, Loss: 1.441 Epoch 3 Batch 767/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.917, Loss: 1.470 Epoch 3 Batch 768/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.906, Loss: 1.466 Epoch 3 Batch 769/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.900, Loss: 1.437 Epoch 3 Batch 770/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.908, Loss: 1.500 Epoch 3 Batch 771/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.905, Loss: 1.485 Epoch 3 Batch 772/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.905, Loss: 1.470 Epoch 3 Batch 773/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.907, Loss: 1.464 Epoch 3 Batch 774/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.908, Loss: 1.450 Epoch 3 Batch 775/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.907, Loss: 1.421 Epoch 3 Batch 776/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.904, Loss: 1.528 Epoch 3 Batch 777/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.912, Loss: 1.516 Epoch 3 Batch 778/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.908, Loss: 1.464 Epoch 3 Batch 779/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.901, Loss: 1.488 Epoch 3 Batch 780/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.900, Loss: 1.507 Epoch 3 Batch 781/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.904, Loss: 1.372 Epoch 3 Batch 782/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.904, Loss: 1.459 Epoch 3 Batch 783/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.909, Loss: 1.535 Epoch 3 Batch 784/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.903, Loss: 1.423 Epoch 3 Batch 785/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.897, Loss: 1.437 Epoch 3 Batch 786/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.893, Loss: 1.517 Epoch 3 Batch 787/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.885, Loss: 1.532 Epoch 3 Batch 788/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.879, Loss: 1.405 Epoch 3 Batch 789/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.880, Loss: 1.441 Epoch 3 Batch 790/1077 - Train Accuracy: 0.875, Validation Accuracy: 0.885, Loss: 1.471 Epoch 3 Batch 791/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.896, Loss: 1.459 Epoch 3 Batch 792/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.895, Loss: 1.419 Epoch 3 Batch 793/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.894, Loss: 1.433 Epoch 3 Batch 794/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.886, Loss: 1.448 Epoch 3 Batch 795/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.891, Loss: 1.436 Epoch 3 Batch 796/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.903, Loss: 1.500 Epoch 3 Batch 797/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.907, Loss: 1.403 Epoch 3 Batch 798/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.901, Loss: 1.488 Epoch 3 Batch 799/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.911, Loss: 1.516 Epoch 3 Batch 800/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.921, Loss: 1.476 Epoch 3 Batch 801/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.917, Loss: 1.440 Epoch 3 Batch 802/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.913, Loss: 1.473 Epoch 3 Batch 803/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.912, Loss: 1.499 Epoch 3 Batch 804/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.896, Loss: 1.425 Epoch 3 Batch 805/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.900, Loss: 1.370 Epoch 3 Batch 806/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.886, Loss: 1.476 Epoch 3 Batch 807/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.891, Loss: 1.523 Epoch 3 Batch 808/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.885, Loss: 1.456 Epoch 3 Batch 809/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.890, Loss: 1.472 Epoch 3 Batch 810/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.899, Loss: 1.552 Epoch 3 Batch 811/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.893, Loss: 1.508 Epoch 3 Batch 812/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.906, Loss: 1.474 Epoch 3 Batch 813/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.911, Loss: 1.477 Epoch 3 Batch 814/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.910, Loss: 1.518 Epoch 3 Batch 815/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.904, Loss: 1.514 Epoch 3 Batch 816/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.901, Loss: 1.442 Epoch 3 Batch 817/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.898, Loss: 1.514 Epoch 3 Batch 818/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.901, Loss: 1.485 Epoch 3 Batch 819/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.900, Loss: 1.587 Epoch 3 Batch 820/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.900, Loss: 1.442 Epoch 3 Batch 821/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.901, Loss: 1.448 Epoch 3 Batch 822/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.906, Loss: 1.434 Epoch 3 Batch 823/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.901, Loss: 1.479 Epoch 3 Batch 824/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.903, Loss: 1.451 Epoch 3 Batch 825/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.899, Loss: 1.410 Epoch 3 Batch 826/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.899, Loss: 1.483 Epoch 3 Batch 827/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.896, Loss: 1.482 Epoch 3 Batch 828/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.900, Loss: 1.449 Epoch 3 Batch 829/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.896, Loss: 1.474 Epoch 3 Batch 830/1077 - Train Accuracy: 0.857, Validation Accuracy: 0.900, Loss: 1.529 Epoch 3 Batch 831/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.891, Loss: 1.401 Epoch 3 Batch 832/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.884, Loss: 1.472 Epoch 3 Batch 833/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.883, Loss: 1.541 Epoch 3 Batch 834/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.887, Loss: 1.443 Epoch 3 Batch 835/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.882, Loss: 1.397 Epoch 3 Batch 836/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.887, Loss: 1.552 Epoch 3 Batch 837/1077 - Train Accuracy: 0.889, Validation Accuracy: 0.890, Loss: 1.444 Epoch 3 Batch 838/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.898, Loss: 1.485 Epoch 3 Batch 839/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.899, Loss: 1.445 Epoch 3 Batch 840/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.900, Loss: 1.442 Epoch 3 Batch 841/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.900, Loss: 1.538 Epoch 3 Batch 842/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.897, Loss: 1.489 Epoch 3 Batch 843/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.899, Loss: 1.411 Epoch 3 Batch 844/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.901, Loss: 1.492 Epoch 3 Batch 845/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.905, Loss: 1.452 Epoch 3 Batch 846/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.910, Loss: 1.478 Epoch 3 Batch 847/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.910, Loss: 1.508 Epoch 3 Batch 848/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.914, Loss: 1.386 Epoch 3 Batch 849/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.908, Loss: 1.445 Epoch 3 Batch 850/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.897, Loss: 1.490 Epoch 3 Batch 851/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.893, Loss: 1.506 Epoch 3 Batch 852/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.899, Loss: 1.423 Epoch 3 Batch 853/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.890, Loss: 1.486 Epoch 3 Batch 854/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.898, Loss: 1.496 Epoch 3 Batch 855/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.893, Loss: 1.497 Epoch 3 Batch 856/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.901, Loss: 1.464 Epoch 3 Batch 857/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.896, Loss: 1.443 Epoch 3 Batch 858/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.891, Loss: 1.440 Epoch 3 Batch 859/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.884, Loss: 1.525 Epoch 3 Batch 860/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.883, Loss: 1.483 Epoch 3 Batch 861/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.883, Loss: 1.438 Epoch 3 Batch 862/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.886, Loss: 1.519 Epoch 3 Batch 863/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.890, Loss: 1.411 Epoch 3 Batch 864/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.883, Loss: 1.443 Epoch 3 Batch 865/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.881, Loss: 1.466 Epoch 3 Batch 866/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.881, Loss: 1.458 Epoch 3 Batch 867/1077 - Train Accuracy: 0.882, Validation Accuracy: 0.888, Loss: 1.580 Epoch 3 Batch 868/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.888, Loss: 1.456 Epoch 3 Batch 869/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.887, Loss: 1.502 Epoch 3 Batch 870/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.883, Loss: 1.552 Epoch 3 Batch 871/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.892, Loss: 1.441 Epoch 3 Batch 872/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.897, Loss: 1.493 Epoch 3 Batch 873/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.899, Loss: 1.442 Epoch 3 Batch 874/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.904, Loss: 1.511 Epoch 3 Batch 875/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.907, Loss: 1.487 Epoch 3 Batch 876/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.911, Loss: 1.468 Epoch 3 Batch 877/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.905, Loss: 1.459 Epoch 3 Batch 878/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.899, Loss: 1.455 Epoch 3 Batch 879/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.901, Loss: 1.454 Epoch 3 Batch 880/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.906, Loss: 1.484 Epoch 3 Batch 881/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.904, Loss: 1.484 Epoch 3 Batch 882/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.907, Loss: 1.492 Epoch 3 Batch 883/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.902, Loss: 1.449 Epoch 3 Batch 884/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.902, Loss: 1.383 Epoch 3 Batch 885/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.900, Loss: 1.468 Epoch 3 Batch 886/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.897, Loss: 1.382 Epoch 3 Batch 887/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.892, Loss: 1.523 Epoch 3 Batch 888/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.885, Loss: 1.492 Epoch 3 Batch 889/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.889, Loss: 1.480 Epoch 3 Batch 890/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.891, Loss: 1.519 Epoch 3 Batch 891/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.889, Loss: 1.466 Epoch 3 Batch 892/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.897, Loss: 1.590 Epoch 3 Batch 893/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.897, Loss: 1.491 Epoch 3 Batch 894/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.885, Loss: 1.382 Epoch 3 Batch 895/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.879, Loss: 1.563 Epoch 3 Batch 896/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.883, Loss: 1.489 Epoch 3 Batch 897/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.885, Loss: 1.451 Epoch 3 Batch 898/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.893, Loss: 1.450 Epoch 3 Batch 899/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.903, Loss: 1.481 Epoch 3 Batch 900/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.910, Loss: 1.497 Epoch 3 Batch 901/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.912, Loss: 1.415 Epoch 3 Batch 902/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.913, Loss: 1.489 Epoch 3 Batch 903/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.915, Loss: 1.447 Epoch 3 Batch 904/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.914, Loss: 1.519 Epoch 3 Batch 905/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.918, Loss: 1.488 Epoch 3 Batch 906/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.918, Loss: 1.403 Epoch 3 Batch 907/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.904, Loss: 1.522 Epoch 3 Batch 908/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.905, Loss: 1.412 Epoch 3 Batch 909/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.904, Loss: 1.443 Epoch 3 Batch 910/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.893, Loss: 1.470 Epoch 3 Batch 911/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.879, Loss: 1.471 Epoch 3 Batch 912/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.867, Loss: 1.443 Epoch 3 Batch 913/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.864, Loss: 1.516 Epoch 3 Batch 914/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.876, Loss: 1.440 Epoch 3 Batch 915/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.901, Loss: 1.425 Epoch 3 Batch 916/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.896, Loss: 1.431 Epoch 3 Batch 917/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.896, Loss: 1.501 Epoch 3 Batch 918/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.900, Loss: 1.450 Epoch 3 Batch 919/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.897, Loss: 1.428 Epoch 3 Batch 920/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.886, Loss: 1.501 Epoch 3 Batch 921/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.886, Loss: 1.476 Epoch 3 Batch 922/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.893, Loss: 1.437 Epoch 3 Batch 923/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.888, Loss: 1.447 Epoch 3 Batch 924/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.891, Loss: 1.485 Epoch 3 Batch 925/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.896, Loss: 1.442 Epoch 3 Batch 926/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.906, Loss: 1.525 Epoch 3 Batch 927/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.916, Loss: 1.434 Epoch 3 Batch 928/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.911, Loss: 1.454 Epoch 3 Batch 929/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.918, Loss: 1.517 Epoch 3 Batch 930/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.911, Loss: 1.459 Epoch 3 Batch 931/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.916, Loss: 1.493 Epoch 3 Batch 932/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.907, Loss: 1.473 Epoch 3 Batch 933/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.908, Loss: 1.397 Epoch 3 Batch 934/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.898, Loss: 1.562 Epoch 3 Batch 935/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.901, Loss: 1.427 Epoch 3 Batch 936/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.894, Loss: 1.552 Epoch 3 Batch 937/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.885, Loss: 1.476 Epoch 3 Batch 938/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.881, Loss: 1.516 Epoch 3 Batch 939/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.885, Loss: 1.556 Epoch 3 Batch 940/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.890, Loss: 1.472 Epoch 3 Batch 941/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.885, Loss: 1.449 Epoch 3 Batch 942/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.882, Loss: 1.367 Epoch 3 Batch 943/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.878, Loss: 1.472 Epoch 3 Batch 944/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.885, Loss: 1.495 Epoch 3 Batch 945/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.887, Loss: 1.517 Epoch 3 Batch 946/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.891, Loss: 1.541 Epoch 3 Batch 947/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.891, Loss: 1.566 Epoch 3 Batch 948/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.893, Loss: 1.520 Epoch 3 Batch 949/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.895, Loss: 1.433 Epoch 3 Batch 950/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.906, Loss: 1.464 Epoch 3 Batch 951/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.905, Loss: 1.511 Epoch 3 Batch 952/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.908, Loss: 1.546 Epoch 3 Batch 953/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.905, Loss: 1.492 Epoch 3 Batch 954/1077 - Train Accuracy: 0.879, Validation Accuracy: 0.909, Loss: 1.484 Epoch 3 Batch 955/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.914, Loss: 1.395 Epoch 3 Batch 956/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.910, Loss: 1.432 Epoch 3 Batch 957/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.901, Loss: 1.364 Epoch 3 Batch 958/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.898, Loss: 1.473 Epoch 3 Batch 959/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.900, Loss: 1.464 Epoch 3 Batch 960/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.898, Loss: 1.523 Epoch 3 Batch 961/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.907, Loss: 1.505 Epoch 3 Batch 962/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.914, Loss: 1.458 Epoch 3 Batch 963/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.899, Loss: 1.405 Epoch 3 Batch 964/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.898, Loss: 1.538 Epoch 3 Batch 965/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.904, Loss: 1.524 Epoch 3 Batch 966/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.908, Loss: 1.434 Epoch 3 Batch 967/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.902, Loss: 1.391 Epoch 3 Batch 968/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.893, Loss: 1.566 Epoch 3 Batch 969/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.897, Loss: 1.480 Epoch 3 Batch 970/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.898, Loss: 1.431 Epoch 3 Batch 971/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.897, Loss: 1.452 Epoch 3 Batch 972/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.897, Loss: 1.474 Epoch 3 Batch 973/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.899, Loss: 1.479 Epoch 3 Batch 974/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.902, Loss: 1.461 Epoch 3 Batch 975/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.898, Loss: 1.451 Epoch 3 Batch 976/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.898, Loss: 1.478 Epoch 3 Batch 977/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.902, Loss: 1.440 Epoch 3 Batch 978/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.900, Loss: 1.527 Epoch 3 Batch 979/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.899, Loss: 1.431 Epoch 3 Batch 980/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.903, Loss: 1.428 Epoch 3 Batch 981/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.914, Loss: 1.469 Epoch 3 Batch 982/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.914, Loss: 1.495 Epoch 3 Batch 983/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.911, Loss: 1.521 Epoch 3 Batch 984/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.902, Loss: 1.462 Epoch 3 Batch 985/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.902, Loss: 1.519 Epoch 3 Batch 986/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.906, Loss: 1.423 Epoch 3 Batch 987/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.901, Loss: 1.470 Epoch 3 Batch 988/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.904, Loss: 1.414 Epoch 3 Batch 989/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.899, Loss: 1.488 Epoch 3 Batch 990/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.897, Loss: 1.456 Epoch 3 Batch 991/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.906, Loss: 1.432 Epoch 3 Batch 992/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.901, Loss: 1.521 Epoch 3 Batch 993/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.903, Loss: 1.547 Epoch 3 Batch 994/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.895, Loss: 1.509 Epoch 3 Batch 995/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.890, Loss: 1.498 Epoch 3 Batch 996/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.890, Loss: 1.472 Epoch 3 Batch 997/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.901, Loss: 1.412 Epoch 3 Batch 998/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.901, Loss: 1.459 Epoch 3 Batch 999/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.904, Loss: 1.564 Epoch 3 Batch 1000/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.909, Loss: 1.474 Epoch 3 Batch 1001/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.905, Loss: 1.423 Epoch 3 Batch 1002/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.902, Loss: 1.526 Epoch 3 Batch 1003/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.895, Loss: 1.505 Epoch 3 Batch 1004/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.896, Loss: 1.497 Epoch 3 Batch 1005/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.905, Loss: 1.459 Epoch 3 Batch 1006/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.906, Loss: 1.475 Epoch 3 Batch 1007/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.915, Loss: 1.546 Epoch 3 Batch 1008/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.915, Loss: 1.456 Epoch 3 Batch 1009/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.919, Loss: 1.474 Epoch 3 Batch 1010/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.919, Loss: 1.423 Epoch 3 Batch 1011/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.905, Loss: 1.454 Epoch 3 Batch 1012/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.904, Loss: 1.444 Epoch 3 Batch 1013/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.905, Loss: 1.357 Epoch 3 Batch 1014/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.894, Loss: 1.499 Epoch 3 Batch 1015/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.909, Loss: 1.469 Epoch 3 Batch 1016/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.909, Loss: 1.464 Epoch 3 Batch 1017/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.911, Loss: 1.485 Epoch 3 Batch 1018/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.911, Loss: 1.417 Epoch 3 Batch 1019/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.904, Loss: 1.522 Epoch 3 Batch 1020/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.904, Loss: 1.466 Epoch 3 Batch 1021/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.897, Loss: 1.486 Epoch 3 Batch 1022/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.901, Loss: 1.456 Epoch 3 Batch 1023/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.898, Loss: 1.458 Epoch 3 Batch 1024/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.889, Loss: 1.427 Epoch 3 Batch 1025/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.893, Loss: 1.455 Epoch 3 Batch 1026/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.898, Loss: 1.519 Epoch 3 Batch 1027/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.895, Loss: 1.432 Epoch 3 Batch 1028/1077 - Train Accuracy: 0.886, Validation Accuracy: 0.890, Loss: 1.469 Epoch 3 Batch 1029/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.890, Loss: 1.412 Epoch 3 Batch 1030/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.895, Loss: 1.455 Epoch 3 Batch 1031/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.900, Loss: 1.462 Epoch 3 Batch 1032/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.907, Loss: 1.517 Epoch 3 Batch 1033/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.910, Loss: 1.503 Epoch 3 Batch 1034/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.910, Loss: 1.463 Epoch 3 Batch 1035/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.909, Loss: 1.446 Epoch 3 Batch 1036/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.906, Loss: 1.488 Epoch 3 Batch 1037/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.900, Loss: 1.413 Epoch 3 Batch 1038/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.900, Loss: 1.474 Epoch 3 Batch 1039/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.905, Loss: 1.481 Epoch 3 Batch 1040/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.898, Loss: 1.444 Epoch 3 Batch 1041/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.916, Loss: 1.482 Epoch 3 Batch 1042/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.910, Loss: 1.459 Epoch 3 Batch 1043/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.916, Loss: 1.534 Epoch 3 Batch 1044/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.914, Loss: 1.533 Epoch 3 Batch 1045/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.919, Loss: 1.467 Epoch 3 Batch 1046/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.914, Loss: 1.526 Epoch 3 Batch 1047/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.915, Loss: 1.497 Epoch 3 Batch 1048/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.920, Loss: 1.514 Epoch 3 Batch 1049/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.909, Loss: 1.481 Epoch 3 Batch 1050/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.909, Loss: 1.442 Epoch 3 Batch 1051/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.911, Loss: 1.457 Epoch 3 Batch 1052/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.907, Loss: 1.505 Epoch 3 Batch 1053/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.911, Loss: 1.454 Epoch 3 Batch 1054/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.924, Loss: 1.516 Epoch 3 Batch 1055/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.924, Loss: 1.489 Epoch 3 Batch 1056/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.924, Loss: 1.433 Epoch 3 Batch 1057/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.925, Loss: 1.436 Epoch 3 Batch 1058/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.916, Loss: 1.494 Epoch 3 Batch 1059/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.915, Loss: 1.549 Epoch 3 Batch 1060/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.911, Loss: 1.463 Epoch 3 Batch 1061/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.906, Loss: 1.394 Epoch 3 Batch 1062/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.899, Loss: 1.518 Epoch 3 Batch 1063/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.903, Loss: 1.459 Epoch 3 Batch 1064/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.903, Loss: 1.483 Epoch 3 Batch 1065/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.907, Loss: 1.463 Epoch 3 Batch 1066/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.898, Loss: 1.468 Epoch 3 Batch 1067/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.891, Loss: 1.527 Epoch 3 Batch 1068/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.892, Loss: 1.492 Epoch 3 Batch 1069/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.893, Loss: 1.484 Epoch 3 Batch 1070/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.897, Loss: 1.478 Epoch 3 Batch 1071/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.902, Loss: 1.441 Epoch 3 Batch 1072/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.895, Loss: 1.431 Epoch 3 Batch 1073/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.895, Loss: 1.556 Epoch 3 Batch 1074/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.896, Loss: 1.514 Epoch 3 Batch 1075/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.891, Loss: 1.465 Epoch 4 Batch 0/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.891, Loss: 1.394 Epoch 4 Batch 1/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.907, Loss: 1.443 Epoch 4 Batch 2/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.903, Loss: 1.495 Epoch 4 Batch 3/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.896, Loss: 1.458 Epoch 4 Batch 4/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.905, Loss: 1.433 Epoch 4 Batch 5/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.909, Loss: 1.527 Epoch 4 Batch 6/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.919, Loss: 1.508 Epoch 4 Batch 7/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.927, Loss: 1.482 Epoch 4 Batch 8/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.929, Loss: 1.426 Epoch 4 Batch 9/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.938, Loss: 1.431 Epoch 4 Batch 10/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.938, Loss: 1.378 Epoch 4 Batch 11/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.932, Loss: 1.449 Epoch 4 Batch 12/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.935, Loss: 1.456 Epoch 4 Batch 13/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.919, Loss: 1.457 Epoch 4 Batch 14/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.924, Loss: 1.461 Epoch 4 Batch 15/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.928, Loss: 1.461 Epoch 4 Batch 16/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.930, Loss: 1.438 Epoch 4 Batch 17/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.926, Loss: 1.528 Epoch 4 Batch 18/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.921, Loss: 1.467 Epoch 4 Batch 19/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.907, Loss: 1.446 Epoch 4 Batch 20/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.921, Loss: 1.329 Epoch 4 Batch 21/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.917, Loss: 1.463 Epoch 4 Batch 22/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.911, Loss: 1.496 Epoch 4 Batch 23/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.907, Loss: 1.446 Epoch 4 Batch 24/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.903, Loss: 1.426 Epoch 4 Batch 25/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.898, Loss: 1.367 Epoch 4 Batch 26/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.903, Loss: 1.559 Epoch 4 Batch 27/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.898, Loss: 1.453 Epoch 4 Batch 28/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.891, Loss: 1.398 Epoch 4 Batch 29/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.904, Loss: 1.465 Epoch 4 Batch 30/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.908, Loss: 1.435 Epoch 4 Batch 31/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.908, Loss: 1.464 Epoch 4 Batch 32/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.903, Loss: 1.469 Epoch 4 Batch 33/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.886, Loss: 1.448 Epoch 4 Batch 34/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.892, Loss: 1.446 Epoch 4 Batch 35/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.898, Loss: 1.393 Epoch 4 Batch 36/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.920, Loss: 1.429 Epoch 4 Batch 37/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.920, Loss: 1.475 Epoch 4 Batch 38/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.926, Loss: 1.552 Epoch 4 Batch 39/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.928, Loss: 1.521 Epoch 4 Batch 40/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.923, Loss: 1.501 Epoch 4 Batch 41/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.917, Loss: 1.410 Epoch 4 Batch 42/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.922, Loss: 1.461 Epoch 4 Batch 43/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.914, Loss: 1.352 Epoch 4 Batch 44/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.916, Loss: 1.413 Epoch 4 Batch 45/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.920, Loss: 1.453 Epoch 4 Batch 46/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.928, Loss: 1.491 Epoch 4 Batch 47/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.924, Loss: 1.410 Epoch 4 Batch 48/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.912, Loss: 1.492 Epoch 4 Batch 49/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.912, Loss: 1.512 Epoch 4 Batch 50/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.912, Loss: 1.459 Epoch 4 Batch 51/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.912, Loss: 1.471 Epoch 4 Batch 52/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.914, Loss: 1.487 Epoch 4 Batch 53/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.909, Loss: 1.487 Epoch 4 Batch 54/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.908, Loss: 1.468 Epoch 4 Batch 55/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.908, Loss: 1.423 Epoch 4 Batch 56/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.908, Loss: 1.442 Epoch 4 Batch 57/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.902, Loss: 1.438 Epoch 4 Batch 58/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.898, Loss: 1.475 Epoch 4 Batch 59/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.907, Loss: 1.461 Epoch 4 Batch 60/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.907, Loss: 1.417 Epoch 4 Batch 61/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.907, Loss: 1.489 Epoch 4 Batch 62/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.905, Loss: 1.411 Epoch 4 Batch 63/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.898, Loss: 1.503 Epoch 4 Batch 64/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.892, Loss: 1.354 Epoch 4 Batch 65/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.892, Loss: 1.468 Epoch 4 Batch 66/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.887, Loss: 1.487 Epoch 4 Batch 67/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.886, Loss: 1.466 Epoch 4 Batch 68/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.887, Loss: 1.377 Epoch 4 Batch 69/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.891, Loss: 1.475 Epoch 4 Batch 70/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.893, Loss: 1.385 Epoch 4 Batch 71/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.897, Loss: 1.420 Epoch 4 Batch 72/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.897, Loss: 1.492 Epoch 4 Batch 73/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.902, Loss: 1.467 Epoch 4 Batch 74/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.906, Loss: 1.439 Epoch 4 Batch 75/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.916, Loss: 1.470 Epoch 4 Batch 76/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.912, Loss: 1.536 Epoch 4 Batch 77/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.918, Loss: 1.505 Epoch 4 Batch 78/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.914, Loss: 1.475 Epoch 4 Batch 79/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.918, Loss: 1.509 Epoch 4 Batch 80/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.915, Loss: 1.437 Epoch 4 Batch 81/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.915, Loss: 1.450 Epoch 4 Batch 82/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.917, Loss: 1.467 Epoch 4 Batch 83/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.923, Loss: 1.453 Epoch 4 Batch 84/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.927, Loss: 1.532 Epoch 4 Batch 85/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.928, Loss: 1.521 Epoch 4 Batch 86/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.928, Loss: 1.477 Epoch 4 Batch 87/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.928, Loss: 1.449 Epoch 4 Batch 88/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.928, Loss: 1.456 Epoch 4 Batch 89/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.913, Loss: 1.440 Epoch 4 Batch 90/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.901, Loss: 1.379 Epoch 4 Batch 91/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.904, Loss: 1.457 Epoch 4 Batch 92/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.902, Loss: 1.429 Epoch 4 Batch 93/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.900, Loss: 1.426 Epoch 4 Batch 94/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.904, Loss: 1.433 Epoch 4 Batch 95/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.904, Loss: 1.453 Epoch 4 Batch 96/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.898, Loss: 1.454 Epoch 4 Batch 97/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.903, Loss: 1.527 Epoch 4 Batch 98/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.915, Loss: 1.474 Epoch 4 Batch 99/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.915, Loss: 1.458 Epoch 4 Batch 100/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.918, Loss: 1.522 Epoch 4 Batch 101/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.922, Loss: 1.404 Epoch 4 Batch 102/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.912, Loss: 1.525 Epoch 4 Batch 103/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.907, Loss: 1.536 Epoch 4 Batch 104/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.907, Loss: 1.474 Epoch 4 Batch 105/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.906, Loss: 1.463 Epoch 4 Batch 106/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.907, Loss: 1.484 Epoch 4 Batch 107/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.911, Loss: 1.430 Epoch 4 Batch 108/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.914, Loss: 1.430 Epoch 4 Batch 109/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.915, Loss: 1.472 Epoch 4 Batch 110/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.910, Loss: 1.482 Epoch 4 Batch 111/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.906, Loss: 1.416 Epoch 4 Batch 112/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.906, Loss: 1.389 Epoch 4 Batch 113/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.913, Loss: 1.530 Epoch 4 Batch 114/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.912, Loss: 1.414 Epoch 4 Batch 115/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.909, Loss: 1.422 Epoch 4 Batch 116/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.911, Loss: 1.453 Epoch 4 Batch 117/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.913, Loss: 1.522 Epoch 4 Batch 118/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.916, Loss: 1.557 Epoch 4 Batch 119/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.907, Loss: 1.463 Epoch 4 Batch 120/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.912, Loss: 1.477 Epoch 4 Batch 121/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.908, Loss: 1.464 Epoch 4 Batch 122/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.909, Loss: 1.475 Epoch 4 Batch 123/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.909, Loss: 1.470 Epoch 4 Batch 124/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.915, Loss: 1.428 Epoch 4 Batch 125/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.917, Loss: 1.503 Epoch 4 Batch 126/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.916, Loss: 1.425 Epoch 4 Batch 127/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.920, Loss: 1.434 Epoch 4 Batch 128/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.922, Loss: 1.398 Epoch 4 Batch 129/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.910, Loss: 1.572 Epoch 4 Batch 130/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.910, Loss: 1.440 Epoch 4 Batch 131/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.897, Loss: 1.485 Epoch 4 Batch 132/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.898, Loss: 1.481 Epoch 4 Batch 133/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.894, Loss: 1.466 Epoch 4 Batch 134/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.890, Loss: 1.445 Epoch 4 Batch 135/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.902, Loss: 1.480 Epoch 4 Batch 136/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.902, Loss: 1.514 Epoch 4 Batch 137/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.902, Loss: 1.437 Epoch 4 Batch 138/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.910, Loss: 1.478 Epoch 4 Batch 139/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.910, Loss: 1.455 Epoch 4 Batch 140/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.922, Loss: 1.476 Epoch 4 Batch 141/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.928, Loss: 1.394 Epoch 4 Batch 142/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.925, Loss: 1.458 Epoch 4 Batch 143/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.917, Loss: 1.394 Epoch 4 Batch 144/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.912, Loss: 1.466 Epoch 4 Batch 145/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.912, Loss: 1.558 Epoch 4 Batch 146/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.912, Loss: 1.500 Epoch 4 Batch 147/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.907, Loss: 1.479 Epoch 4 Batch 148/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.908, Loss: 1.471 Epoch 4 Batch 149/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.915, Loss: 1.518 Epoch 4 Batch 150/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.915, Loss: 1.474 Epoch 4 Batch 151/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.925, Loss: 1.508 Epoch 4 Batch 152/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.919, Loss: 1.461 Epoch 4 Batch 153/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.919, Loss: 1.532 Epoch 4 Batch 154/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.919, Loss: 1.422 Epoch 4 Batch 155/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.914, Loss: 1.436 Epoch 4 Batch 156/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.918, Loss: 1.407 Epoch 4 Batch 157/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.918, Loss: 1.460 Epoch 4 Batch 158/1077 - Train Accuracy: 0.894, Validation Accuracy: 0.924, Loss: 1.440 Epoch 4 Batch 159/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.913, Loss: 1.453 Epoch 4 Batch 160/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.923, Loss: 1.479 Epoch 4 Batch 161/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.927, Loss: 1.550 Epoch 4 Batch 162/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.923, Loss: 1.565 Epoch 4 Batch 163/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.923, Loss: 1.514 Epoch 4 Batch 164/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.923, Loss: 1.428 Epoch 4 Batch 165/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.923, Loss: 1.496 Epoch 4 Batch 166/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.919, Loss: 1.503 Epoch 4 Batch 167/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.919, Loss: 1.428 Epoch 4 Batch 168/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.913, Loss: 1.344 Epoch 4 Batch 169/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.909, Loss: 1.524 Epoch 4 Batch 170/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.902, Loss: 1.367 Epoch 4 Batch 171/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.903, Loss: 1.409 Epoch 4 Batch 172/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.903, Loss: 1.424 Epoch 4 Batch 173/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.901, Loss: 1.530 Epoch 4 Batch 174/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.897, Loss: 1.431 Epoch 4 Batch 175/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.897, Loss: 1.527 Epoch 4 Batch 176/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.893, Loss: 1.433 Epoch 4 Batch 177/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.900, Loss: 1.484 Epoch 4 Batch 178/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.900, Loss: 1.526 Epoch 4 Batch 179/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.906, Loss: 1.462 Epoch 4 Batch 180/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.906, Loss: 1.473 Epoch 4 Batch 181/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.905, Loss: 1.471 Epoch 4 Batch 182/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.900, Loss: 1.539 Epoch 4 Batch 183/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.894, Loss: 1.452 Epoch 4 Batch 184/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.899, Loss: 1.436 Epoch 4 Batch 185/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.904, Loss: 1.486 Epoch 4 Batch 186/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.909, Loss: 1.476 Epoch 4 Batch 187/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.909, Loss: 1.393 Epoch 4 Batch 188/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.909, Loss: 1.447 Epoch 4 Batch 189/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.904, Loss: 1.480 Epoch 4 Batch 190/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.907, Loss: 1.499 Epoch 4 Batch 191/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.907, Loss: 1.472 Epoch 4 Batch 192/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.905, Loss: 1.516 Epoch 4 Batch 193/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.910, Loss: 1.465 Epoch 4 Batch 194/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.914, Loss: 1.448 Epoch 4 Batch 195/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.914, Loss: 1.452 Epoch 4 Batch 196/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.914, Loss: 1.399 Epoch 4 Batch 197/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.913, Loss: 1.476 Epoch 4 Batch 198/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.915, Loss: 1.577 Epoch 4 Batch 199/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.910, Loss: 1.427 Epoch 4 Batch 200/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.916, Loss: 1.491 Epoch 4 Batch 201/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.910, Loss: 1.500 Epoch 4 Batch 202/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.914, Loss: 1.453 Epoch 4 Batch 203/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.917, Loss: 1.451 Epoch 4 Batch 204/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.912, Loss: 1.494 Epoch 4 Batch 205/1077 - Train Accuracy: 0.888, Validation Accuracy: 0.913, Loss: 1.486 Epoch 4 Batch 206/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.915, Loss: 1.461 Epoch 4 Batch 207/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.910, Loss: 1.550 Epoch 4 Batch 208/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.917, Loss: 1.364 Epoch 4 Batch 209/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.921, Loss: 1.426 Epoch 4 Batch 210/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.921, Loss: 1.438 Epoch 4 Batch 211/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.934, Loss: 1.389 Epoch 4 Batch 212/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.929, Loss: 1.425 Epoch 4 Batch 213/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.908, Loss: 1.520 Epoch 4 Batch 214/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.901, Loss: 1.499 Epoch 4 Batch 215/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.903, Loss: 1.577 Epoch 4 Batch 216/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.900, Loss: 1.454 Epoch 4 Batch 217/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.895, Loss: 1.501 Epoch 4 Batch 218/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.901, Loss: 1.505 Epoch 4 Batch 219/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.896, Loss: 1.465 Epoch 4 Batch 220/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.902, Loss: 1.460 Epoch 4 Batch 221/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.896, Loss: 1.550 Epoch 4 Batch 222/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.895, Loss: 1.535 Epoch 4 Batch 223/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.910, Loss: 1.404 Epoch 4 Batch 224/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.911, Loss: 1.478 Epoch 4 Batch 225/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.904, Loss: 1.572 Epoch 4 Batch 226/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.918, Loss: 1.400 Epoch 4 Batch 227/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.917, Loss: 1.487 Epoch 4 Batch 228/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.911, Loss: 1.455 Epoch 4 Batch 229/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.911, Loss: 1.335 Epoch 4 Batch 230/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.907, Loss: 1.417 Epoch 4 Batch 231/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.910, Loss: 1.434 Epoch 4 Batch 232/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.909, Loss: 1.426 Epoch 4 Batch 233/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.901, Loss: 1.540 Epoch 4 Batch 234/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.895, Loss: 1.522 Epoch 4 Batch 235/1077 - Train Accuracy: 0.887, Validation Accuracy: 0.904, Loss: 1.476 Epoch 4 Batch 236/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.906, Loss: 1.526 Epoch 4 Batch 237/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.913, Loss: 1.452 Epoch 4 Batch 238/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.909, Loss: 1.451 Epoch 4 Batch 239/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.904, Loss: 1.502 Epoch 4 Batch 240/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.913, Loss: 1.461 Epoch 4 Batch 241/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.908, Loss: 1.438 Epoch 4 Batch 242/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.899, Loss: 1.498 Epoch 4 Batch 243/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.904, Loss: 1.427 Epoch 4 Batch 244/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.913, Loss: 1.509 Epoch 4 Batch 245/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.911, Loss: 1.461 Epoch 4 Batch 246/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.911, Loss: 1.485 Epoch 4 Batch 247/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.914, Loss: 1.474 Epoch 4 Batch 248/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.914, Loss: 1.424 Epoch 4 Batch 249/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.914, Loss: 1.508 Epoch 4 Batch 250/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.906, Loss: 1.500 Epoch 4 Batch 251/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.906, Loss: 1.430 Epoch 4 Batch 252/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.906, Loss: 1.480 Epoch 4 Batch 253/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.901, Loss: 1.451 Epoch 4 Batch 254/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.911, Loss: 1.505 Epoch 4 Batch 255/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.910, Loss: 1.471 Epoch 4 Batch 256/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.910, Loss: 1.428 Epoch 4 Batch 257/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.905, Loss: 1.500 Epoch 4 Batch 258/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.918, Loss: 1.390 Epoch 4 Batch 259/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.918, Loss: 1.510 Epoch 4 Batch 260/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.919, Loss: 1.404 Epoch 4 Batch 261/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.912, Loss: 1.522 Epoch 4 Batch 262/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.914, Loss: 1.452 Epoch 4 Batch 263/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.907, Loss: 1.379 Epoch 4 Batch 264/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.916, Loss: 1.480 Epoch 4 Batch 265/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.921, Loss: 1.419 Epoch 4 Batch 266/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.914, Loss: 1.503 Epoch 4 Batch 267/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.905, Loss: 1.468 Epoch 4 Batch 268/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.906, Loss: 1.460 Epoch 4 Batch 269/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.916, Loss: 1.492 Epoch 4 Batch 270/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.917, Loss: 1.539 Epoch 4 Batch 271/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.907, Loss: 1.482 Epoch 4 Batch 272/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.911, Loss: 1.459 Epoch 4 Batch 273/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.911, Loss: 1.462 Epoch 4 Batch 274/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.906, Loss: 1.449 Epoch 4 Batch 275/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.906, Loss: 1.510 Epoch 4 Batch 276/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.901, Loss: 1.474 Epoch 4 Batch 277/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.898, Loss: 1.436 Epoch 4 Batch 278/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.897, Loss: 1.446 Epoch 4 Batch 279/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.902, Loss: 1.454 Epoch 4 Batch 280/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.904, Loss: 1.473 Epoch 4 Batch 281/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.901, Loss: 1.517 Epoch 4 Batch 282/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.895, Loss: 1.415 Epoch 4 Batch 283/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.900, Loss: 1.507 Epoch 4 Batch 284/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.902, Loss: 1.450 Epoch 4 Batch 285/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.901, Loss: 1.394 Epoch 4 Batch 286/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.903, Loss: 1.463 Epoch 4 Batch 287/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.908, Loss: 1.411 Epoch 4 Batch 288/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.913, Loss: 1.494 Epoch 4 Batch 289/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.919, Loss: 1.524 Epoch 4 Batch 290/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.919, Loss: 1.443 Epoch 4 Batch 291/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.920, Loss: 1.459 Epoch 4 Batch 292/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.918, Loss: 1.432 Epoch 4 Batch 293/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.919, Loss: 1.446 Epoch 4 Batch 294/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.914, Loss: 1.428 Epoch 4 Batch 295/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.920, Loss: 1.427 Epoch 4 Batch 296/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.905, Loss: 1.548 Epoch 4 Batch 297/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.898, Loss: 1.457 Epoch 4 Batch 298/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.902, Loss: 1.510 Epoch 4 Batch 299/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.905, Loss: 1.424 Epoch 4 Batch 300/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.900, Loss: 1.450 Epoch 4 Batch 301/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.900, Loss: 1.461 Epoch 4 Batch 302/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.922, Loss: 1.463 Epoch 4 Batch 303/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.913, Loss: 1.487 Epoch 4 Batch 304/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.916, Loss: 1.437 Epoch 4 Batch 305/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.919, Loss: 1.456 Epoch 4 Batch 306/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.924, Loss: 1.427 Epoch 4 Batch 307/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.920, Loss: 1.476 Epoch 4 Batch 308/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.924, Loss: 1.501 Epoch 4 Batch 309/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.923, Loss: 1.441 Epoch 4 Batch 310/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.910, Loss: 1.364 Epoch 4 Batch 311/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.907, Loss: 1.369 Epoch 4 Batch 312/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.906, Loss: 1.544 Epoch 4 Batch 313/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.906, Loss: 1.506 Epoch 4 Batch 314/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.913, Loss: 1.391 Epoch 4 Batch 315/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.908, Loss: 1.500 Epoch 4 Batch 316/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.903, Loss: 1.444 Epoch 4 Batch 317/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.903, Loss: 1.499 Epoch 4 Batch 318/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.900, Loss: 1.488 Epoch 4 Batch 319/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.893, Loss: 1.460 Epoch 4 Batch 320/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.891, Loss: 1.531 Epoch 4 Batch 321/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.896, Loss: 1.489 Epoch 4 Batch 322/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.909, Loss: 1.556 Epoch 4 Batch 323/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.895, Loss: 1.449 Epoch 4 Batch 324/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.892, Loss: 1.517 Epoch 4 Batch 325/1077 - Train Accuracy: 0.897, Validation Accuracy: 0.892, Loss: 1.508 Epoch 4 Batch 326/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.897, Loss: 1.426 Epoch 4 Batch 327/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.898, Loss: 1.465 Epoch 4 Batch 328/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.905, Loss: 1.459 Epoch 4 Batch 329/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.911, Loss: 1.489 Epoch 4 Batch 330/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.911, Loss: 1.459 Epoch 4 Batch 331/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.906, Loss: 1.611 Epoch 4 Batch 332/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.909, Loss: 1.452 Epoch 4 Batch 333/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.910, Loss: 1.483 Epoch 4 Batch 334/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.905, Loss: 1.402 Epoch 4 Batch 335/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.907, Loss: 1.451 Epoch 4 Batch 336/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.897, Loss: 1.446 Epoch 4 Batch 337/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.892, Loss: 1.435 Epoch 4 Batch 338/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.902, Loss: 1.524 Epoch 4 Batch 339/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.899, Loss: 1.413 Epoch 4 Batch 340/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.897, Loss: 1.449 Epoch 4 Batch 341/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.900, Loss: 1.487 Epoch 4 Batch 342/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.904, Loss: 1.430 Epoch 4 Batch 343/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.898, Loss: 1.489 Epoch 4 Batch 344/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.904, Loss: 1.416 Epoch 4 Batch 345/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.904, Loss: 1.496 Epoch 4 Batch 346/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.906, Loss: 1.401 Epoch 4 Batch 347/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.911, Loss: 1.438 Epoch 4 Batch 348/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.907, Loss: 1.414 Epoch 4 Batch 349/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.901, Loss: 1.463 Epoch 4 Batch 350/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.900, Loss: 1.495 Epoch 4 Batch 351/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.899, Loss: 1.471 Epoch 4 Batch 352/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.889, Loss: 1.494 Epoch 4 Batch 353/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.887, Loss: 1.453 Epoch 4 Batch 354/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.889, Loss: 1.553 Epoch 4 Batch 355/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.896, Loss: 1.426 Epoch 4 Batch 356/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.892, Loss: 1.493 Epoch 4 Batch 357/1077 - Train Accuracy: 0.908, Validation Accuracy: 0.892, Loss: 1.575 Epoch 4 Batch 358/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.897, Loss: 1.455 Epoch 4 Batch 359/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.898, Loss: 1.468 Epoch 4 Batch 360/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.902, Loss: 1.439 Epoch 4 Batch 361/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.913, Loss: 1.449 Epoch 4 Batch 362/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.914, Loss: 1.458 Epoch 4 Batch 363/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.913, Loss: 1.502 Epoch 4 Batch 364/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.904, Loss: 1.447 Epoch 4 Batch 365/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.909, Loss: 1.513 Epoch 4 Batch 366/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.915, Loss: 1.420 Epoch 4 Batch 367/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.916, Loss: 1.436 Epoch 4 Batch 368/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.922, Loss: 1.457 Epoch 4 Batch 369/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.918, Loss: 1.479 Epoch 4 Batch 370/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.917, Loss: 1.391 Epoch 4 Batch 371/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.905, Loss: 1.443 Epoch 4 Batch 372/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.910, Loss: 1.481 Epoch 4 Batch 373/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.909, Loss: 1.367 Epoch 4 Batch 374/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.912, Loss: 1.451 Epoch 4 Batch 375/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.918, Loss: 1.508 Epoch 4 Batch 376/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.917, Loss: 1.481 Epoch 4 Batch 377/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.912, Loss: 1.433 Epoch 4 Batch 378/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.903, Loss: 1.461 Epoch 4 Batch 379/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.902, Loss: 1.530 Epoch 4 Batch 380/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.906, Loss: 1.367 Epoch 4 Batch 381/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.903, Loss: 1.467 Epoch 4 Batch 382/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.912, Loss: 1.457 Epoch 4 Batch 383/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.907, Loss: 1.476 Epoch 4 Batch 384/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.916, Loss: 1.417 Epoch 4 Batch 385/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.916, Loss: 1.418 Epoch 4 Batch 386/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.922, Loss: 1.455 Epoch 4 Batch 387/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.918, Loss: 1.448 Epoch 4 Batch 388/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.919, Loss: 1.500 Epoch 4 Batch 389/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.926, Loss: 1.476 Epoch 4 Batch 390/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.922, Loss: 1.468 Epoch 4 Batch 391/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.920, Loss: 1.458 Epoch 4 Batch 392/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.919, Loss: 1.433 Epoch 4 Batch 393/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.913, Loss: 1.429 Epoch 4 Batch 394/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.915, Loss: 1.430 Epoch 4 Batch 395/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.908, Loss: 1.426 Epoch 4 Batch 396/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.913, Loss: 1.489 Epoch 4 Batch 397/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.908, Loss: 1.388 Epoch 4 Batch 398/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.903, Loss: 1.433 Epoch 4 Batch 399/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.897, Loss: 1.468 Epoch 4 Batch 400/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.897, Loss: 1.449 Epoch 4 Batch 401/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.897, Loss: 1.489 Epoch 4 Batch 402/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.897, Loss: 1.426 Epoch 4 Batch 403/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.906, Loss: 1.522 Epoch 4 Batch 404/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.897, Loss: 1.427 Epoch 4 Batch 405/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.907, Loss: 1.431 Epoch 4 Batch 406/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.908, Loss: 1.416 Epoch 4 Batch 407/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.905, Loss: 1.503 Epoch 4 Batch 408/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.915, Loss: 1.507 Epoch 4 Batch 409/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.915, Loss: 1.480 Epoch 4 Batch 410/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.915, Loss: 1.507 Epoch 4 Batch 411/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.920, Loss: 1.465 Epoch 4 Batch 412/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.920, Loss: 1.419 Epoch 4 Batch 413/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.925, Loss: 1.405 Epoch 4 Batch 414/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.926, Loss: 1.449 Epoch 4 Batch 415/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.924, Loss: 1.422 Epoch 4 Batch 416/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.924, Loss: 1.447 Epoch 4 Batch 417/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.930, Loss: 1.453 Epoch 4 Batch 418/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.930, Loss: 1.494 Epoch 4 Batch 419/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.926, Loss: 1.386 Epoch 4 Batch 420/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.926, Loss: 1.467 Epoch 4 Batch 421/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.926, Loss: 1.451 Epoch 4 Batch 422/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.920, Loss: 1.468 Epoch 4 Batch 423/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.922, Loss: 1.489 Epoch 4 Batch 424/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.920, Loss: 1.447 Epoch 4 Batch 425/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.919, Loss: 1.409 Epoch 4 Batch 426/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.919, Loss: 1.490 Epoch 4 Batch 427/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.924, Loss: 1.446 Epoch 4 Batch 428/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.929, Loss: 1.417 Epoch 4 Batch 429/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.925, Loss: 1.447 Epoch 4 Batch 430/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.920, Loss: 1.466 Epoch 4 Batch 431/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.915, Loss: 1.394 Epoch 4 Batch 432/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.920, Loss: 1.427 Epoch 4 Batch 433/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.922, Loss: 1.503 Epoch 4 Batch 434/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.917, Loss: 1.428 Epoch 4 Batch 435/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.917, Loss: 1.572 Epoch 4 Batch 436/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.917, Loss: 1.525 Epoch 4 Batch 437/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.912, Loss: 1.360 Epoch 4 Batch 438/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.912, Loss: 1.439 Epoch 4 Batch 439/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.914, Loss: 1.467 Epoch 4 Batch 440/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.913, Loss: 1.427 Epoch 4 Batch 441/1077 - Train Accuracy: 0.912, Validation Accuracy: 0.911, Loss: 1.428 Epoch 4 Batch 442/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.911, Loss: 1.453 Epoch 4 Batch 443/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.905, Loss: 1.496 Epoch 4 Batch 444/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.900, Loss: 1.400 Epoch 4 Batch 445/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.900, Loss: 1.475 Epoch 4 Batch 446/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.889, Loss: 1.439 Epoch 4 Batch 447/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.883, Loss: 1.467 Epoch 4 Batch 448/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.890, Loss: 1.500 Epoch 4 Batch 449/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.895, Loss: 1.482 Epoch 4 Batch 450/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.894, Loss: 1.436 Epoch 4 Batch 451/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.914, Loss: 1.480 Epoch 4 Batch 452/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.907, Loss: 1.553 Epoch 4 Batch 453/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.903, Loss: 1.410 Epoch 4 Batch 454/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.892, Loss: 1.439 Epoch 4 Batch 455/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.871, Loss: 1.533 Epoch 4 Batch 456/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.877, Loss: 1.400 Epoch 4 Batch 457/1077 - Train Accuracy: 0.899, Validation Accuracy: 0.880, Loss: 1.516 Epoch 4 Batch 458/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.904, Loss: 1.505 Epoch 4 Batch 459/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.899, Loss: 1.479 Epoch 4 Batch 460/1077 - Train Accuracy: 0.902, Validation Accuracy: 0.892, Loss: 1.558 Epoch 4 Batch 461/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.887, Loss: 1.535 Epoch 4 Batch 462/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.898, Loss: 1.471 Epoch 4 Batch 463/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.900, Loss: 1.501 Epoch 4 Batch 464/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.904, Loss: 1.419 Epoch 4 Batch 465/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.902, Loss: 1.502 Epoch 4 Batch 466/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.915, Loss: 1.468 Epoch 4 Batch 467/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.911, Loss: 1.484 Epoch 4 Batch 468/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.904, Loss: 1.533 Epoch 4 Batch 469/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.910, Loss: 1.362 Epoch 4 Batch 470/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.910, Loss: 1.560 Epoch 4 Batch 471/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.919, Loss: 1.501 Epoch 4 Batch 472/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.914, Loss: 1.520 Epoch 4 Batch 473/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.917, Loss: 1.471 Epoch 4 Batch 474/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.917, Loss: 1.494 Epoch 4 Batch 475/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.918, Loss: 1.475 Epoch 4 Batch 476/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.908, Loss: 1.436 Epoch 4 Batch 477/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.924, Loss: 1.491 Epoch 4 Batch 478/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.923, Loss: 1.451 Epoch 4 Batch 479/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.917, Loss: 1.386 Epoch 4 Batch 480/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.922, Loss: 1.450 Epoch 4 Batch 481/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.918, Loss: 1.431 Epoch 4 Batch 482/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.921, Loss: 1.495 Epoch 4 Batch 483/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.899, Loss: 1.429 Epoch 4 Batch 484/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.900, Loss: 1.381 Epoch 4 Batch 485/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.903, Loss: 1.496 Epoch 4 Batch 486/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.912, Loss: 1.371 Epoch 4 Batch 487/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.917, Loss: 1.432 Epoch 4 Batch 488/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.916, Loss: 1.541 Epoch 4 Batch 489/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.915, Loss: 1.449 Epoch 4 Batch 490/1077 - Train Accuracy: 0.905, Validation Accuracy: 0.911, Loss: 1.553 Epoch 4 Batch 491/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.894, Loss: 1.434 Epoch 4 Batch 492/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.884, Loss: 1.554 Epoch 4 Batch 493/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.885, Loss: 1.476 Epoch 4 Batch 494/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.883, Loss: 1.455 Epoch 4 Batch 495/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.887, Loss: 1.447 Epoch 4 Batch 496/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.893, Loss: 1.485 Epoch 4 Batch 497/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.897, Loss: 1.416 Epoch 4 Batch 498/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.894, Loss: 1.500 Epoch 4 Batch 499/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.891, Loss: 1.418 Epoch 4 Batch 500/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.892, Loss: 1.432 Epoch 4 Batch 501/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.893, Loss: 1.507 Epoch 4 Batch 502/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.898, Loss: 1.444 Epoch 4 Batch 503/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.907, Loss: 1.473 Epoch 4 Batch 504/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.911, Loss: 1.401 Epoch 4 Batch 505/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.914, Loss: 1.419 Epoch 4 Batch 506/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.914, Loss: 1.554 Epoch 4 Batch 507/1077 - Train Accuracy: 0.892, Validation Accuracy: 0.919, Loss: 1.444 Epoch 4 Batch 508/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.925, Loss: 1.507 Epoch 4 Batch 509/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.918, Loss: 1.465 Epoch 4 Batch 510/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.915, Loss: 1.474 Epoch 4 Batch 511/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.919, Loss: 1.466 Epoch 4 Batch 512/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.898, Loss: 1.555 Epoch 4 Batch 513/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.898, Loss: 1.413 Epoch 4 Batch 514/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.901, Loss: 1.492 Epoch 4 Batch 515/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.904, Loss: 1.468 Epoch 4 Batch 516/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.898, Loss: 1.432 Epoch 4 Batch 517/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.895, Loss: 1.427 Epoch 4 Batch 518/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.897, Loss: 1.402 Epoch 4 Batch 519/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.909, Loss: 1.478 Epoch 4 Batch 520/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.900, Loss: 1.431 Epoch 4 Batch 521/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.907, Loss: 1.423 Epoch 4 Batch 522/1077 - Train Accuracy: 0.864, Validation Accuracy: 0.912, Loss: 1.498 Epoch 4 Batch 523/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.906, Loss: 1.422 Epoch 4 Batch 524/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.906, Loss: 1.379 Epoch 4 Batch 525/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.910, Loss: 1.502 Epoch 4 Batch 526/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.917, Loss: 1.446 Epoch 4 Batch 527/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.924, Loss: 1.487 Epoch 4 Batch 528/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.925, Loss: 1.551 Epoch 4 Batch 529/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.917, Loss: 1.411 Epoch 4 Batch 530/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.912, Loss: 1.456 Epoch 4 Batch 531/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.914, Loss: 1.441 Epoch 4 Batch 532/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.911, Loss: 1.502 Epoch 4 Batch 533/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.911, Loss: 1.524 Epoch 4 Batch 534/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.906, Loss: 1.412 Epoch 4 Batch 535/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.905, Loss: 1.433 Epoch 4 Batch 536/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.902, Loss: 1.462 Epoch 4 Batch 537/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.906, Loss: 1.417 Epoch 4 Batch 538/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.907, Loss: 1.428 Epoch 4 Batch 539/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.907, Loss: 1.435 Epoch 4 Batch 540/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.907, Loss: 1.388 Epoch 4 Batch 541/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.905, Loss: 1.439 Epoch 4 Batch 542/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.907, Loss: 1.425 Epoch 4 Batch 543/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.916, Loss: 1.501 Epoch 4 Batch 544/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.917, Loss: 1.403 Epoch 4 Batch 545/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.925, Loss: 1.486 Epoch 4 Batch 546/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.924, Loss: 1.483 Epoch 4 Batch 547/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.925, Loss: 1.410 Epoch 4 Batch 548/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.918, Loss: 1.474 Epoch 4 Batch 549/1077 - Train Accuracy: 0.893, Validation Accuracy: 0.914, Loss: 1.426 Epoch 4 Batch 550/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.896, Loss: 1.441 Epoch 4 Batch 551/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.896, Loss: 1.493 Epoch 4 Batch 552/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.898, Loss: 1.487 Epoch 4 Batch 553/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.901, Loss: 1.429 Epoch 4 Batch 554/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.906, Loss: 1.473 Epoch 4 Batch 555/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.922, Loss: 1.456 Epoch 4 Batch 556/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.928, Loss: 1.464 Epoch 4 Batch 557/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.935, Loss: 1.514 Epoch 4 Batch 558/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.931, Loss: 1.471 Epoch 4 Batch 559/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.932, Loss: 1.531 Epoch 4 Batch 560/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.930, Loss: 1.385 Epoch 4 Batch 561/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.930, Loss: 1.521 Epoch 4 Batch 562/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.924, Loss: 1.480 Epoch 4 Batch 563/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.920, Loss: 1.485 Epoch 4 Batch 564/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.908, Loss: 1.453 Epoch 4 Batch 565/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.907, Loss: 1.480 Epoch 4 Batch 566/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.902, Loss: 1.463 Epoch 4 Batch 567/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.902, Loss: 1.475 Epoch 4 Batch 568/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.916, Loss: 1.436 Epoch 4 Batch 569/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.911, Loss: 1.472 Epoch 4 Batch 570/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.922, Loss: 1.417 Epoch 4 Batch 571/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.918, Loss: 1.496 Epoch 4 Batch 572/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.911, Loss: 1.495 Epoch 4 Batch 573/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.900, Loss: 1.388 Epoch 4 Batch 574/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.910, Loss: 1.459 Epoch 4 Batch 575/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.911, Loss: 1.456 Epoch 4 Batch 576/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.906, Loss: 1.453 Epoch 4 Batch 577/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.905, Loss: 1.519 Epoch 4 Batch 578/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.902, Loss: 1.463 Epoch 4 Batch 579/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.900, Loss: 1.363 Epoch 4 Batch 580/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.902, Loss: 1.450 Epoch 4 Batch 581/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.902, Loss: 1.430 Epoch 4 Batch 582/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.897, Loss: 1.372 Epoch 4 Batch 583/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.902, Loss: 1.419 Epoch 4 Batch 584/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.897, Loss: 1.506 Epoch 4 Batch 585/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.897, Loss: 1.352 Epoch 4 Batch 586/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.902, Loss: 1.437 Epoch 4 Batch 587/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.899, Loss: 1.494 Epoch 4 Batch 588/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.913, Loss: 1.419 Epoch 4 Batch 589/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.908, Loss: 1.450 Epoch 4 Batch 590/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.907, Loss: 1.386 Epoch 4 Batch 591/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.913, Loss: 1.496 Epoch 4 Batch 592/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.913, Loss: 1.436 Epoch 4 Batch 593/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.922, Loss: 1.499 Epoch 4 Batch 594/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.903, Loss: 1.509 Epoch 4 Batch 595/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.903, Loss: 1.469 Epoch 4 Batch 596/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.914, Loss: 1.544 Epoch 4 Batch 597/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.926, Loss: 1.485 Epoch 4 Batch 598/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.921, Loss: 1.478 Epoch 4 Batch 599/1077 - Train Accuracy: 0.900, Validation Accuracy: 0.922, Loss: 1.556 Epoch 4 Batch 600/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.913, Loss: 1.468 Epoch 4 Batch 601/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.919, Loss: 1.442 Epoch 4 Batch 602/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.925, Loss: 1.548 Epoch 4 Batch 603/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.932, Loss: 1.484 Epoch 4 Batch 604/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.922, Loss: 1.509 Epoch 4 Batch 605/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.918, Loss: 1.544 Epoch 4 Batch 606/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.916, Loss: 1.434 Epoch 4 Batch 607/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.915, Loss: 1.456 Epoch 4 Batch 608/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.911, Loss: 1.453 Epoch 4 Batch 609/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.918, Loss: 1.558 Epoch 4 Batch 610/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.923, Loss: 1.527 Epoch 4 Batch 611/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.910, Loss: 1.437 Epoch 4 Batch 612/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.912, Loss: 1.414 Epoch 4 Batch 613/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.911, Loss: 1.476 Epoch 4 Batch 614/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.912, Loss: 1.471 Epoch 4 Batch 615/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.917, Loss: 1.462 Epoch 4 Batch 616/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.909, Loss: 1.422 Epoch 4 Batch 617/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.907, Loss: 1.483 Epoch 4 Batch 618/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.913, Loss: 1.490 Epoch 4 Batch 619/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.918, Loss: 1.477 Epoch 4 Batch 620/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.918, Loss: 1.464 Epoch 4 Batch 621/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.929, Loss: 1.442 Epoch 4 Batch 622/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.929, Loss: 1.452 Epoch 4 Batch 623/1077 - Train Accuracy: 0.903, Validation Accuracy: 0.935, Loss: 1.436 Epoch 4 Batch 624/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.934, Loss: 1.453 Epoch 4 Batch 625/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.940, Loss: 1.473 Epoch 4 Batch 626/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.938, Loss: 1.470 Epoch 4 Batch 627/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.922, Loss: 1.450 Epoch 4 Batch 628/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.920, Loss: 1.484 Epoch 4 Batch 629/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.924, Loss: 1.520 Epoch 4 Batch 630/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.924, Loss: 1.517 Epoch 4 Batch 631/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.924, Loss: 1.517 Epoch 4 Batch 632/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.925, Loss: 1.471 Epoch 4 Batch 633/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.935, Loss: 1.440 Epoch 4 Batch 634/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.930, Loss: 1.433 Epoch 4 Batch 635/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.927, Loss: 1.499 Epoch 4 Batch 636/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.921, Loss: 1.431 Epoch 4 Batch 637/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.920, Loss: 1.472 Epoch 4 Batch 638/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.912, Loss: 1.378 Epoch 4 Batch 639/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.912, Loss: 1.405 Epoch 4 Batch 640/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.912, Loss: 1.399 Epoch 4 Batch 641/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.903, Loss: 1.414 Epoch 4 Batch 642/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.894, Loss: 1.483 Epoch 4 Batch 643/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.899, Loss: 1.480 Epoch 4 Batch 644/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.907, Loss: 1.459 Epoch 4 Batch 645/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.907, Loss: 1.473 Epoch 4 Batch 646/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.895, Loss: 1.419 Epoch 4 Batch 647/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.895, Loss: 1.497 Epoch 4 Batch 648/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.894, Loss: 1.426 Epoch 4 Batch 649/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.889, Loss: 1.438 Epoch 4 Batch 650/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.902, Loss: 1.515 Epoch 4 Batch 651/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.906, Loss: 1.456 Epoch 4 Batch 652/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.906, Loss: 1.431 Epoch 4 Batch 653/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.913, Loss: 1.383 Epoch 4 Batch 654/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.913, Loss: 1.445 Epoch 4 Batch 655/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.923, Loss: 1.417 Epoch 4 Batch 656/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.935, Loss: 1.489 Epoch 4 Batch 657/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.930, Loss: 1.461 Epoch 4 Batch 658/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.929, Loss: 1.449 Epoch 4 Batch 659/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.929, Loss: 1.482 Epoch 4 Batch 660/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.928, Loss: 1.503 Epoch 4 Batch 661/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.924, Loss: 1.498 Epoch 4 Batch 662/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.923, Loss: 1.430 Epoch 4 Batch 663/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.923, Loss: 1.462 Epoch 4 Batch 664/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.917, Loss: 1.469 Epoch 4 Batch 665/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.913, Loss: 1.431 Epoch 4 Batch 666/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.912, Loss: 1.499 Epoch 4 Batch 667/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.917, Loss: 1.398 Epoch 4 Batch 668/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.914, Loss: 1.360 Epoch 4 Batch 669/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.916, Loss: 1.442 Epoch 4 Batch 670/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.919, Loss: 1.417 Epoch 4 Batch 671/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.924, Loss: 1.530 Epoch 4 Batch 672/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.917, Loss: 1.369 Epoch 4 Batch 673/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.922, Loss: 1.398 Epoch 4 Batch 674/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.921, Loss: 1.549 Epoch 4 Batch 675/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.935, Loss: 1.503 Epoch 4 Batch 676/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.935, Loss: 1.458 Epoch 4 Batch 677/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.936, Loss: 1.534 Epoch 4 Batch 678/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.935, Loss: 1.448 Epoch 4 Batch 679/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.935, Loss: 1.447 Epoch 4 Batch 680/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.931, Loss: 1.490 Epoch 4 Batch 681/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.926, Loss: 1.401 Epoch 4 Batch 682/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.937, Loss: 1.484 Epoch 4 Batch 683/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.942, Loss: 1.448 Epoch 4 Batch 684/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.941, Loss: 1.429 Epoch 4 Batch 685/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.941, Loss: 1.484 Epoch 4 Batch 686/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.941, Loss: 1.450 Epoch 4 Batch 687/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.935, Loss: 1.426 Epoch 4 Batch 688/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.931, Loss: 1.470 Epoch 4 Batch 689/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.925, Loss: 1.477 Epoch 4 Batch 690/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.930, Loss: 1.485 Epoch 4 Batch 691/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.924, Loss: 1.431 Epoch 4 Batch 692/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.924, Loss: 1.412 Epoch 4 Batch 693/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.918, Loss: 1.448 Epoch 4 Batch 694/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.912, Loss: 1.480 Epoch 4 Batch 695/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.904, Loss: 1.438 Epoch 4 Batch 696/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.902, Loss: 1.499 Epoch 4 Batch 697/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.902, Loss: 1.477 Epoch 4 Batch 698/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.904, Loss: 1.462 Epoch 4 Batch 699/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.899, Loss: 1.407 Epoch 4 Batch 700/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.895, Loss: 1.445 Epoch 4 Batch 701/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.897, Loss: 1.453 Epoch 4 Batch 702/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.903, Loss: 1.420 Epoch 4 Batch 703/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.912, Loss: 1.487 Epoch 4 Batch 704/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.906, Loss: 1.468 Epoch 4 Batch 705/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.912, Loss: 1.483 Epoch 4 Batch 706/1077 - Train Accuracy: 0.884, Validation Accuracy: 0.923, Loss: 1.523 Epoch 4 Batch 707/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.919, Loss: 1.414 Epoch 4 Batch 708/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.930, Loss: 1.392 Epoch 4 Batch 709/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.934, Loss: 1.536 Epoch 4 Batch 710/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.926, Loss: 1.415 Epoch 4 Batch 711/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.923, Loss: 1.432 Epoch 4 Batch 712/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.919, Loss: 1.498 Epoch 4 Batch 713/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.923, Loss: 1.415 Epoch 4 Batch 714/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.912, Loss: 1.489 Epoch 4 Batch 715/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.912, Loss: 1.445 Epoch 4 Batch 716/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.911, Loss: 1.475 Epoch 4 Batch 717/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.911, Loss: 1.424 Epoch 4 Batch 718/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.915, Loss: 1.427 Epoch 4 Batch 719/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.915, Loss: 1.423 Epoch 4 Batch 720/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.920, Loss: 1.453 Epoch 4 Batch 721/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.925, Loss: 1.423 Epoch 4 Batch 722/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.934, Loss: 1.529 Epoch 4 Batch 723/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.935, Loss: 1.462 Epoch 4 Batch 724/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.930, Loss: 1.396 Epoch 4 Batch 725/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.931, Loss: 1.491 Epoch 4 Batch 726/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.942, Loss: 1.428 Epoch 4 Batch 727/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.938, Loss: 1.411 Epoch 4 Batch 728/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.929, Loss: 1.494 Epoch 4 Batch 729/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.929, Loss: 1.421 Epoch 4 Batch 730/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.930, Loss: 1.476 Epoch 4 Batch 731/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.925, Loss: 1.456 Epoch 4 Batch 732/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.926, Loss: 1.418 Epoch 4 Batch 733/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.926, Loss: 1.437 Epoch 4 Batch 734/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.934, Loss: 1.462 Epoch 4 Batch 735/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.933, Loss: 1.443 Epoch 4 Batch 736/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.928, Loss: 1.479 Epoch 4 Batch 737/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.927, Loss: 1.475 Epoch 4 Batch 738/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.922, Loss: 1.412 Epoch 4 Batch 739/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.922, Loss: 1.503 Epoch 4 Batch 740/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.922, Loss: 1.512 Epoch 4 Batch 741/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.927, Loss: 1.484 Epoch 4 Batch 742/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.920, Loss: 1.439 Epoch 4 Batch 743/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.920, Loss: 1.505 Epoch 4 Batch 744/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.921, Loss: 1.521 Epoch 4 Batch 745/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.926, Loss: 1.458 Epoch 4 Batch 746/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.922, Loss: 1.446 Epoch 4 Batch 747/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.915, Loss: 1.481 Epoch 4 Batch 748/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.915, Loss: 1.454 Epoch 4 Batch 749/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.921, Loss: 1.517 Epoch 4 Batch 750/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.928, Loss: 1.480 Epoch 4 Batch 751/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.932, Loss: 1.500 Epoch 4 Batch 752/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.937, Loss: 1.450 Epoch 4 Batch 753/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.936, Loss: 1.384 Epoch 4 Batch 754/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.932, Loss: 1.382 Epoch 4 Batch 755/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.935, Loss: 1.526 Epoch 4 Batch 756/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.935, Loss: 1.386 Epoch 4 Batch 757/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.922, Loss: 1.442 Epoch 4 Batch 758/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.926, Loss: 1.430 Epoch 4 Batch 759/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.926, Loss: 1.431 Epoch 4 Batch 760/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.926, Loss: 1.475 Epoch 4 Batch 761/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.930, Loss: 1.480 Epoch 4 Batch 762/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.933, Loss: 1.438 Epoch 4 Batch 763/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.927, Loss: 1.453 Epoch 4 Batch 764/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.929, Loss: 1.516 Epoch 4 Batch 765/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.933, Loss: 1.384 Epoch 4 Batch 766/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.933, Loss: 1.457 Epoch 4 Batch 767/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.933, Loss: 1.420 Epoch 4 Batch 768/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.938, Loss: 1.451 Epoch 4 Batch 769/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.939, Loss: 1.519 Epoch 4 Batch 770/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.948, Loss: 1.435 Epoch 4 Batch 771/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.947, Loss: 1.484 Epoch 4 Batch 772/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.945, Loss: 1.453 Epoch 4 Batch 773/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.946, Loss: 1.442 Epoch 4 Batch 774/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.947, Loss: 1.413 Epoch 4 Batch 775/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.941, Loss: 1.438 Epoch 4 Batch 776/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.945, Loss: 1.500 Epoch 4 Batch 777/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.946, Loss: 1.412 Epoch 4 Batch 778/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.932, Loss: 1.434 Epoch 4 Batch 779/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.934, Loss: 1.412 Epoch 4 Batch 780/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.927, Loss: 1.389 Epoch 4 Batch 781/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.931, Loss: 1.441 Epoch 4 Batch 782/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.931, Loss: 1.452 Epoch 4 Batch 783/1077 - Train Accuracy: 0.901, Validation Accuracy: 0.924, Loss: 1.517 Epoch 4 Batch 784/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.924, Loss: 1.441 Epoch 4 Batch 785/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.923, Loss: 1.463 Epoch 4 Batch 786/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.923, Loss: 1.438 Epoch 4 Batch 787/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.929, Loss: 1.394 Epoch 4 Batch 788/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.931, Loss: 1.463 Epoch 4 Batch 789/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.926, Loss: 1.411 Epoch 4 Batch 790/1077 - Train Accuracy: 0.896, Validation Accuracy: 0.920, Loss: 1.450 Epoch 4 Batch 791/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.918, Loss: 1.481 Epoch 4 Batch 792/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.933, Loss: 1.402 Epoch 4 Batch 793/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.916, Loss: 1.447 Epoch 4 Batch 794/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.921, Loss: 1.423 Epoch 4 Batch 795/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.914, Loss: 1.402 Epoch 4 Batch 796/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.921, Loss: 1.384 Epoch 4 Batch 797/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.927, Loss: 1.509 Epoch 4 Batch 798/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.930, Loss: 1.384 Epoch 4 Batch 799/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.924, Loss: 1.460 Epoch 4 Batch 800/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.920, Loss: 1.446 Epoch 4 Batch 801/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.924, Loss: 1.481 Epoch 4 Batch 802/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.928, Loss: 1.400 Epoch 4 Batch 803/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.933, Loss: 1.476 Epoch 4 Batch 804/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.935, Loss: 1.416 Epoch 4 Batch 805/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.935, Loss: 1.468 Epoch 4 Batch 806/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.923, Loss: 1.415 Epoch 4 Batch 807/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.918, Loss: 1.420 Epoch 4 Batch 808/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.921, Loss: 1.499 Epoch 4 Batch 809/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.931, Loss: 1.517 Epoch 4 Batch 810/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.937, Loss: 1.418 Epoch 4 Batch 811/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.930, Loss: 1.500 Epoch 4 Batch 812/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.930, Loss: 1.517 Epoch 4 Batch 813/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.930, Loss: 1.476 Epoch 4 Batch 814/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.931, Loss: 1.447 Epoch 4 Batch 815/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.933, Loss: 1.429 Epoch 4 Batch 816/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.937, Loss: 1.470 Epoch 4 Batch 817/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.936, Loss: 1.442 Epoch 4 Batch 818/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.934, Loss: 1.411 Epoch 4 Batch 819/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.929, Loss: 1.470 Epoch 4 Batch 820/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.934, Loss: 1.463 Epoch 4 Batch 821/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.929, Loss: 1.442 Epoch 4 Batch 822/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.922, Loss: 1.420 Epoch 4 Batch 823/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.925, Loss: 1.448 Epoch 4 Batch 824/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.924, Loss: 1.429 Epoch 4 Batch 825/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.919, Loss: 1.352 Epoch 4 Batch 826/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.920, Loss: 1.447 Epoch 4 Batch 827/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.920, Loss: 1.460 Epoch 4 Batch 828/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.920, Loss: 1.407 Epoch 4 Batch 829/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.920, Loss: 1.484 Epoch 4 Batch 830/1077 - Train Accuracy: 0.891, Validation Accuracy: 0.915, Loss: 1.476 Epoch 4 Batch 831/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.920, Loss: 1.462 Epoch 4 Batch 832/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.919, Loss: 1.484 Epoch 4 Batch 833/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.922, Loss: 1.488 Epoch 4 Batch 834/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.918, Loss: 1.565 Epoch 4 Batch 835/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.924, Loss: 1.405 Epoch 4 Batch 836/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.913, Loss: 1.493 Epoch 4 Batch 837/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.919, Loss: 1.479 Epoch 4 Batch 838/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.919, Loss: 1.460 Epoch 4 Batch 839/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.925, Loss: 1.465 Epoch 4 Batch 840/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.931, Loss: 1.420 Epoch 4 Batch 841/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.927, Loss: 1.395 Epoch 4 Batch 842/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.927, Loss: 1.420 Epoch 4 Batch 843/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.931, Loss: 1.461 Epoch 4 Batch 844/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.936, Loss: 1.454 Epoch 4 Batch 845/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.936, Loss: 1.443 Epoch 4 Batch 846/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.939, Loss: 1.522 Epoch 4 Batch 847/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.939, Loss: 1.452 Epoch 4 Batch 848/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.945, Loss: 1.402 Epoch 4 Batch 849/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.949, Loss: 1.437 Epoch 4 Batch 850/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.949, Loss: 1.420 Epoch 4 Batch 851/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.949, Loss: 1.427 Epoch 4 Batch 852/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.949, Loss: 1.421 Epoch 4 Batch 853/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.946, Loss: 1.514 Epoch 4 Batch 854/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.941, Loss: 1.423 Epoch 4 Batch 855/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.945, Loss: 1.471 Epoch 4 Batch 856/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.946, Loss: 1.443 Epoch 4 Batch 857/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.943, Loss: 1.477 Epoch 4 Batch 858/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.932, Loss: 1.435 Epoch 4 Batch 859/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.944, Loss: 1.415 Epoch 4 Batch 860/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.940, Loss: 1.419 Epoch 4 Batch 861/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.936, Loss: 1.414 Epoch 4 Batch 862/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.940, Loss: 1.452 Epoch 4 Batch 863/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.933, Loss: 1.401 Epoch 4 Batch 864/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.933, Loss: 1.461 Epoch 4 Batch 865/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.923, Loss: 1.377 Epoch 4 Batch 866/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.923, Loss: 1.438 Epoch 4 Batch 867/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.923, Loss: 1.520 Epoch 4 Batch 868/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.931, Loss: 1.471 Epoch 4 Batch 869/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.927, Loss: 1.474 Epoch 4 Batch 870/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.927, Loss: 1.421 Epoch 4 Batch 871/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.933, Loss: 1.446 Epoch 4 Batch 872/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.937, Loss: 1.379 Epoch 4 Batch 873/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.937, Loss: 1.422 Epoch 4 Batch 874/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.939, Loss: 1.515 Epoch 4 Batch 875/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.943, Loss: 1.480 Epoch 4 Batch 876/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.947, Loss: 1.510 Epoch 4 Batch 877/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.948, Loss: 1.469 Epoch 4 Batch 878/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.948, Loss: 1.359 Epoch 4 Batch 879/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.948, Loss: 1.475 Epoch 4 Batch 880/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.944, Loss: 1.472 Epoch 4 Batch 881/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.939, Loss: 1.452 Epoch 4 Batch 882/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.936, Loss: 1.432 Epoch 4 Batch 883/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.937, Loss: 1.441 Epoch 4 Batch 884/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.936, Loss: 1.481 Epoch 4 Batch 885/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.941, Loss: 1.442 Epoch 4 Batch 886/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.947, Loss: 1.410 Epoch 4 Batch 887/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.943, Loss: 1.474 Epoch 4 Batch 888/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.936, Loss: 1.401 Epoch 4 Batch 889/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.936, Loss: 1.390 Epoch 4 Batch 890/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.936, Loss: 1.452 Epoch 4 Batch 891/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.931, Loss: 1.503 Epoch 4 Batch 892/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.931, Loss: 1.431 Epoch 4 Batch 893/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.943, Loss: 1.374 Epoch 4 Batch 894/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.942, Loss: 1.515 Epoch 4 Batch 895/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.937, Loss: 1.361 Epoch 4 Batch 896/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.934, Loss: 1.442 Epoch 4 Batch 897/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.934, Loss: 1.477 Epoch 4 Batch 898/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.934, Loss: 1.442 Epoch 4 Batch 899/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.930, Loss: 1.503 Epoch 4 Batch 900/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.924, Loss: 1.411 Epoch 4 Batch 901/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.929, Loss: 1.426 Epoch 4 Batch 902/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.930, Loss: 1.501 Epoch 4 Batch 903/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.933, Loss: 1.426 Epoch 4 Batch 904/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.931, Loss: 1.406 Epoch 4 Batch 905/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.931, Loss: 1.491 Epoch 4 Batch 906/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.926, Loss: 1.440 Epoch 4 Batch 907/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.931, Loss: 1.461 Epoch 4 Batch 908/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.936, Loss: 1.457 Epoch 4 Batch 909/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.926, Loss: 1.529 Epoch 4 Batch 910/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.933, Loss: 1.481 Epoch 4 Batch 911/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.938, Loss: 1.386 Epoch 4 Batch 912/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.939, Loss: 1.495 Epoch 4 Batch 913/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.939, Loss: 1.455 Epoch 4 Batch 914/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.930, Loss: 1.526 Epoch 4 Batch 915/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.934, Loss: 1.445 Epoch 4 Batch 916/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.927, Loss: 1.443 Epoch 4 Batch 917/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.938, Loss: 1.442 Epoch 4 Batch 918/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.942, Loss: 1.455 Epoch 4 Batch 919/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.942, Loss: 1.438 Epoch 4 Batch 920/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.942, Loss: 1.398 Epoch 4 Batch 921/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.946, Loss: 1.495 Epoch 4 Batch 922/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.940, Loss: 1.459 Epoch 4 Batch 923/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.948, Loss: 1.440 Epoch 4 Batch 924/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.945, Loss: 1.480 Epoch 4 Batch 925/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.945, Loss: 1.457 Epoch 4 Batch 926/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.946, Loss: 1.467 Epoch 4 Batch 927/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.942, Loss: 1.398 Epoch 4 Batch 928/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.941, Loss: 1.433 Epoch 4 Batch 929/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.933, Loss: 1.465 Epoch 4 Batch 930/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.927, Loss: 1.519 Epoch 4 Batch 931/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.919, Loss: 1.465 Epoch 4 Batch 932/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.919, Loss: 1.452 Epoch 4 Batch 933/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.914, Loss: 1.461 Epoch 4 Batch 934/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.919, Loss: 1.360 Epoch 4 Batch 935/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.924, Loss: 1.445 Epoch 4 Batch 936/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.930, Loss: 1.452 Epoch 4 Batch 937/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.930, Loss: 1.378 Epoch 4 Batch 938/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.931, Loss: 1.493 Epoch 4 Batch 939/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.924, Loss: 1.559 Epoch 4 Batch 940/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.926, Loss: 1.379 Epoch 4 Batch 941/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.927, Loss: 1.417 Epoch 4 Batch 942/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.927, Loss: 1.455 Epoch 4 Batch 943/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.932, Loss: 1.454 Epoch 4 Batch 944/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.923, Loss: 1.442 Epoch 4 Batch 945/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.923, Loss: 1.417 Epoch 4 Batch 946/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.923, Loss: 1.483 Epoch 4 Batch 947/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.921, Loss: 1.460 Epoch 4 Batch 948/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.924, Loss: 1.431 Epoch 4 Batch 949/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.918, Loss: 1.419 Epoch 4 Batch 950/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.914, Loss: 1.446 Epoch 4 Batch 951/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.914, Loss: 1.463 Epoch 4 Batch 952/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.914, Loss: 1.463 Epoch 4 Batch 953/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.919, Loss: 1.494 Epoch 4 Batch 954/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.932, Loss: 1.489 Epoch 4 Batch 955/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.939, Loss: 1.405 Epoch 4 Batch 956/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.933, Loss: 1.480 Epoch 4 Batch 957/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.933, Loss: 1.452 Epoch 4 Batch 958/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.941, Loss: 1.441 Epoch 4 Batch 959/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.942, Loss: 1.420 Epoch 4 Batch 960/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.935, Loss: 1.493 Epoch 4 Batch 961/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.930, Loss: 1.470 Epoch 4 Batch 962/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.930, Loss: 1.427 Epoch 4 Batch 963/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.934, Loss: 1.472 Epoch 4 Batch 964/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.933, Loss: 1.425 Epoch 4 Batch 965/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.930, Loss: 1.374 Epoch 4 Batch 966/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.930, Loss: 1.382 Epoch 4 Batch 967/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.925, Loss: 1.588 Epoch 4 Batch 968/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.919, Loss: 1.478 Epoch 4 Batch 969/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.919, Loss: 1.464 Epoch 4 Batch 970/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.912, Loss: 1.394 Epoch 4 Batch 971/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.906, Loss: 1.437 Epoch 4 Batch 972/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.908, Loss: 1.372 Epoch 4 Batch 973/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.908, Loss: 1.483 Epoch 4 Batch 974/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.919, Loss: 1.384 Epoch 4 Batch 975/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.912, Loss: 1.406 Epoch 4 Batch 976/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.920, Loss: 1.464 Epoch 4 Batch 977/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.928, Loss: 1.467 Epoch 4 Batch 978/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.923, Loss: 1.393 Epoch 4 Batch 979/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.923, Loss: 1.482 Epoch 4 Batch 980/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.928, Loss: 1.396 Epoch 4 Batch 981/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.928, Loss: 1.479 Epoch 4 Batch 982/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.923, Loss: 1.467 Epoch 4 Batch 983/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.927, Loss: 1.488 Epoch 4 Batch 984/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.936, Loss: 1.477 Epoch 4 Batch 985/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.932, Loss: 1.364 Epoch 4 Batch 986/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.928, Loss: 1.419 Epoch 4 Batch 987/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.918, Loss: 1.420 Epoch 4 Batch 988/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.908, Loss: 1.474 Epoch 4 Batch 989/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.908, Loss: 1.465 Epoch 4 Batch 990/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.912, Loss: 1.461 Epoch 4 Batch 991/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.912, Loss: 1.518 Epoch 4 Batch 992/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.911, Loss: 1.473 Epoch 4 Batch 993/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.922, Loss: 1.447 Epoch 4 Batch 994/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.922, Loss: 1.498 Epoch 4 Batch 995/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.925, Loss: 1.332 Epoch 4 Batch 996/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.930, Loss: 1.371 Epoch 4 Batch 997/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.930, Loss: 1.542 Epoch 4 Batch 998/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.924, Loss: 1.390 Epoch 4 Batch 999/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.929, Loss: 1.439 Epoch 4 Batch 1000/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.920, Loss: 1.462 Epoch 4 Batch 1001/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.929, Loss: 1.421 Epoch 4 Batch 1002/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.924, Loss: 1.429 Epoch 4 Batch 1003/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.929, Loss: 1.512 Epoch 4 Batch 1004/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.924, Loss: 1.411 Epoch 4 Batch 1005/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.923, Loss: 1.456 Epoch 4 Batch 1006/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.913, Loss: 1.460 Epoch 4 Batch 1007/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.923, Loss: 1.476 Epoch 4 Batch 1008/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.916, Loss: 1.495 Epoch 4 Batch 1009/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.917, Loss: 1.415 Epoch 4 Batch 1010/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.914, Loss: 1.446 Epoch 4 Batch 1011/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.914, Loss: 1.432 Epoch 4 Batch 1012/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.919, Loss: 1.441 Epoch 4 Batch 1013/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.918, Loss: 1.437 Epoch 4 Batch 1014/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.913, Loss: 1.399 Epoch 4 Batch 1015/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.911, Loss: 1.502 Epoch 4 Batch 1016/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.915, Loss: 1.475 Epoch 4 Batch 1017/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.907, Loss: 1.458 Epoch 4 Batch 1018/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.915, Loss: 1.440 Epoch 4 Batch 1019/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.907, Loss: 1.493 Epoch 4 Batch 1020/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.921, Loss: 1.453 Epoch 4 Batch 1021/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.925, Loss: 1.460 Epoch 4 Batch 1022/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.924, Loss: 1.497 Epoch 4 Batch 1023/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.924, Loss: 1.488 Epoch 4 Batch 1024/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.923, Loss: 1.377 Epoch 4 Batch 1025/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.915, Loss: 1.459 Epoch 4 Batch 1026/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.915, Loss: 1.378 Epoch 4 Batch 1027/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.917, Loss: 1.458 Epoch 4 Batch 1028/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.922, Loss: 1.479 Epoch 4 Batch 1029/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.921, Loss: 1.330 Epoch 4 Batch 1030/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.926, Loss: 1.423 Epoch 4 Batch 1031/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.928, Loss: 1.421 Epoch 4 Batch 1032/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.928, Loss: 1.442 Epoch 4 Batch 1033/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.923, Loss: 1.494 Epoch 4 Batch 1034/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.928, Loss: 1.475 Epoch 4 Batch 1035/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.911, Loss: 1.486 Epoch 4 Batch 1036/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.911, Loss: 1.513 Epoch 4 Batch 1037/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.910, Loss: 1.539 Epoch 4 Batch 1038/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.910, Loss: 1.476 Epoch 4 Batch 1039/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.923, Loss: 1.516 Epoch 4 Batch 1040/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.916, Loss: 1.423 Epoch 4 Batch 1041/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.916, Loss: 1.427 Epoch 4 Batch 1042/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.920, Loss: 1.418 Epoch 4 Batch 1043/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.921, Loss: 1.439 Epoch 4 Batch 1044/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.934, Loss: 1.475 Epoch 4 Batch 1045/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.939, Loss: 1.424 Epoch 4 Batch 1046/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.942, Loss: 1.490 Epoch 4 Batch 1047/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.941, Loss: 1.398 Epoch 4 Batch 1048/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.941, Loss: 1.488 Epoch 4 Batch 1049/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.922, Loss: 1.440 Epoch 4 Batch 1050/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.925, Loss: 1.440 Epoch 4 Batch 1051/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.925, Loss: 1.499 Epoch 4 Batch 1052/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.925, Loss: 1.480 Epoch 4 Batch 1053/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.930, Loss: 1.371 Epoch 4 Batch 1054/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.926, Loss: 1.392 Epoch 4 Batch 1055/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.933, Loss: 1.410 Epoch 4 Batch 1056/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.938, Loss: 1.456 Epoch 4 Batch 1057/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.940, Loss: 1.469 Epoch 4 Batch 1058/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.936, Loss: 1.486 Epoch 4 Batch 1059/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.935, Loss: 1.433 Epoch 4 Batch 1060/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.929, Loss: 1.450 Epoch 4 Batch 1061/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.931, Loss: 1.545 Epoch 4 Batch 1062/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.931, Loss: 1.507 Epoch 4 Batch 1063/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.933, Loss: 1.421 Epoch 4 Batch 1064/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.932, Loss: 1.439 Epoch 4 Batch 1065/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.932, Loss: 1.436 Epoch 4 Batch 1066/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.931, Loss: 1.435 Epoch 4 Batch 1067/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.934, Loss: 1.462 Epoch 4 Batch 1068/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.941, Loss: 1.409 Epoch 4 Batch 1069/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.941, Loss: 1.533 Epoch 4 Batch 1070/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.941, Loss: 1.459 Epoch 4 Batch 1071/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.948, Loss: 1.470 Epoch 4 Batch 1072/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.947, Loss: 1.479 Epoch 4 Batch 1073/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.945, Loss: 1.423 Epoch 4 Batch 1074/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.484 Epoch 4 Batch 1075/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.944, Loss: 1.446 Epoch 5 Batch 0/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.944, Loss: 1.428 Epoch 5 Batch 1/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.950, Loss: 1.493 Epoch 5 Batch 2/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.945, Loss: 1.507 Epoch 5 Batch 3/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.945, Loss: 1.467 Epoch 5 Batch 4/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.945, Loss: 1.444 Epoch 5 Batch 5/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.945, Loss: 1.480 Epoch 5 Batch 6/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.938, Loss: 1.363 Epoch 5 Batch 7/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.943, Loss: 1.510 Epoch 5 Batch 8/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.948, Loss: 1.443 Epoch 5 Batch 9/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.947, Loss: 1.392 Epoch 5 Batch 10/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.948, Loss: 1.477 Epoch 5 Batch 11/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.948, Loss: 1.439 Epoch 5 Batch 12/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.944, Loss: 1.472 Epoch 5 Batch 13/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.938, Loss: 1.433 Epoch 5 Batch 14/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.937, Loss: 1.481 Epoch 5 Batch 15/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.926, Loss: 1.494 Epoch 5 Batch 16/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.933, Loss: 1.510 Epoch 5 Batch 17/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.938, Loss: 1.389 Epoch 5 Batch 18/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.948, Loss: 1.442 Epoch 5 Batch 19/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.948, Loss: 1.423 Epoch 5 Batch 20/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.946, Loss: 1.509 Epoch 5 Batch 21/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.946, Loss: 1.496 Epoch 5 Batch 22/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.946, Loss: 1.374 Epoch 5 Batch 23/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.946, Loss: 1.449 Epoch 5 Batch 24/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.941, Loss: 1.443 Epoch 5 Batch 25/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.940, Loss: 1.452 Epoch 5 Batch 26/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.950, Loss: 1.479 Epoch 5 Batch 27/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.947, Loss: 1.458 Epoch 5 Batch 28/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.939, Loss: 1.434 Epoch 5 Batch 29/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.933, Loss: 1.360 Epoch 5 Batch 30/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.919, Loss: 1.422 Epoch 5 Batch 31/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.919, Loss: 1.481 Epoch 5 Batch 32/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.919, Loss: 1.460 Epoch 5 Batch 33/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.915, Loss: 1.405 Epoch 5 Batch 34/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.915, Loss: 1.501 Epoch 5 Batch 35/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.915, Loss: 1.485 Epoch 5 Batch 36/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.917, Loss: 1.418 Epoch 5 Batch 37/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.913, Loss: 1.555 Epoch 5 Batch 38/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.913, Loss: 1.469 Epoch 5 Batch 39/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.913, Loss: 1.440 Epoch 5 Batch 40/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.908, Loss: 1.408 Epoch 5 Batch 41/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.913, Loss: 1.393 Epoch 5 Batch 42/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.919, Loss: 1.495 Epoch 5 Batch 43/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.919, Loss: 1.423 Epoch 5 Batch 44/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.930, Loss: 1.408 Epoch 5 Batch 45/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.950, Loss: 1.391 Epoch 5 Batch 46/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.950, Loss: 1.519 Epoch 5 Batch 47/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.950, Loss: 1.563 Epoch 5 Batch 48/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.951, Loss: 1.426 Epoch 5 Batch 49/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.947, Loss: 1.464 Epoch 5 Batch 50/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.949, Loss: 1.484 Epoch 5 Batch 51/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.949, Loss: 1.453 Epoch 5 Batch 52/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.952, Loss: 1.374 Epoch 5 Batch 53/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.952, Loss: 1.431 Epoch 5 Batch 54/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.951, Loss: 1.442 Epoch 5 Batch 55/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.941, Loss: 1.412 Epoch 5 Batch 56/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.929, Loss: 1.401 Epoch 5 Batch 57/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.928, Loss: 1.475 Epoch 5 Batch 58/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.927, Loss: 1.374 Epoch 5 Batch 59/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.915, Loss: 1.415 Epoch 5 Batch 60/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.916, Loss: 1.454 Epoch 5 Batch 61/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.911, Loss: 1.400 Epoch 5 Batch 62/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.917, Loss: 1.386 Epoch 5 Batch 63/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.918, Loss: 1.476 Epoch 5 Batch 64/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.925, Loss: 1.493 Epoch 5 Batch 65/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.941, Loss: 1.451 Epoch 5 Batch 66/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.946, Loss: 1.433 Epoch 5 Batch 67/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.942, Loss: 1.532 Epoch 5 Batch 68/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.929, Loss: 1.406 Epoch 5 Batch 69/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.934, Loss: 1.469 Epoch 5 Batch 70/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.931, Loss: 1.478 Epoch 5 Batch 71/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.926, Loss: 1.406 Epoch 5 Batch 72/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.928, Loss: 1.442 Epoch 5 Batch 73/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.928, Loss: 1.455 Epoch 5 Batch 74/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.936, Loss: 1.417 Epoch 5 Batch 75/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.938, Loss: 1.418 Epoch 5 Batch 76/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.939, Loss: 1.446 Epoch 5 Batch 77/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.939, Loss: 1.447 Epoch 5 Batch 78/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.945, Loss: 1.420 Epoch 5 Batch 79/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.948, Loss: 1.414 Epoch 5 Batch 80/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.943, Loss: 1.484 Epoch 5 Batch 81/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.939, Loss: 1.458 Epoch 5 Batch 82/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.945, Loss: 1.459 Epoch 5 Batch 83/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.940, Loss: 1.447 Epoch 5 Batch 84/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.937, Loss: 1.498 Epoch 5 Batch 85/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.935, Loss: 1.368 Epoch 5 Batch 86/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.461 Epoch 5 Batch 87/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.934, Loss: 1.455 Epoch 5 Batch 88/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.935, Loss: 1.419 Epoch 5 Batch 89/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.935, Loss: 1.470 Epoch 5 Batch 90/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.932, Loss: 1.413 Epoch 5 Batch 91/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.926, Loss: 1.435 Epoch 5 Batch 92/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.924, Loss: 1.418 Epoch 5 Batch 93/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.926, Loss: 1.470 Epoch 5 Batch 94/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.927, Loss: 1.491 Epoch 5 Batch 95/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.913, Loss: 1.482 Epoch 5 Batch 96/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.919, Loss: 1.504 Epoch 5 Batch 97/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.922, Loss: 1.429 Epoch 5 Batch 98/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.914, Loss: 1.359 Epoch 5 Batch 99/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.922, Loss: 1.400 Epoch 5 Batch 100/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.918, Loss: 1.387 Epoch 5 Batch 101/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.913, Loss: 1.495 Epoch 5 Batch 102/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.913, Loss: 1.489 Epoch 5 Batch 103/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.920, Loss: 1.527 Epoch 5 Batch 104/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.918, Loss: 1.443 Epoch 5 Batch 105/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.919, Loss: 1.502 Epoch 5 Batch 106/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.928, Loss: 1.449 Epoch 5 Batch 107/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.926, Loss: 1.432 Epoch 5 Batch 108/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.939, Loss: 1.401 Epoch 5 Batch 109/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.939, Loss: 1.452 Epoch 5 Batch 110/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.940, Loss: 1.447 Epoch 5 Batch 111/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.935, Loss: 1.467 Epoch 5 Batch 112/1077 - Train Accuracy: 0.918, Validation Accuracy: 0.935, Loss: 1.453 Epoch 5 Batch 113/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.935, Loss: 1.354 Epoch 5 Batch 114/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.930, Loss: 1.396 Epoch 5 Batch 115/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.503 Epoch 5 Batch 116/1077 - Train Accuracy: 0.904, Validation Accuracy: 0.932, Loss: 1.457 Epoch 5 Batch 117/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.925, Loss: 1.473 Epoch 5 Batch 118/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.926, Loss: 1.461 Epoch 5 Batch 119/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.921, Loss: 1.451 Epoch 5 Batch 120/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.925, Loss: 1.495 Epoch 5 Batch 121/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.944, Loss: 1.423 Epoch 5 Batch 122/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.941, Loss: 1.404 Epoch 5 Batch 123/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.941, Loss: 1.396 Epoch 5 Batch 124/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.936, Loss: 1.442 Epoch 5 Batch 125/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.940, Loss: 1.416 Epoch 5 Batch 126/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.938, Loss: 1.367 Epoch 5 Batch 127/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.939, Loss: 1.449 Epoch 5 Batch 128/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.934, Loss: 1.433 Epoch 5 Batch 129/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.941, Loss: 1.410 Epoch 5 Batch 130/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.935, Loss: 1.436 Epoch 5 Batch 131/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.925, Loss: 1.470 Epoch 5 Batch 132/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.930, Loss: 1.477 Epoch 5 Batch 133/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.922, Loss: 1.369 Epoch 5 Batch 134/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.915, Loss: 1.441 Epoch 5 Batch 135/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.898, Loss: 1.431 Epoch 5 Batch 136/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.902, Loss: 1.418 Epoch 5 Batch 137/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.902, Loss: 1.382 Epoch 5 Batch 138/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.896, Loss: 1.438 Epoch 5 Batch 139/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.892, Loss: 1.530 Epoch 5 Batch 140/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.901, Loss: 1.415 Epoch 5 Batch 141/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.902, Loss: 1.475 Epoch 5 Batch 142/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.925, Loss: 1.473 Epoch 5 Batch 143/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.928, Loss: 1.459 Epoch 5 Batch 144/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.928, Loss: 1.470 Epoch 5 Batch 145/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.928, Loss: 1.427 Epoch 5 Batch 146/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.925, Loss: 1.439 Epoch 5 Batch 147/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.925, Loss: 1.499 Epoch 5 Batch 148/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.925, Loss: 1.506 Epoch 5 Batch 149/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.926, Loss: 1.522 Epoch 5 Batch 150/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.919, Loss: 1.396 Epoch 5 Batch 151/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.920, Loss: 1.450 Epoch 5 Batch 152/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.925, Loss: 1.429 Epoch 5 Batch 153/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.925, Loss: 1.498 Epoch 5 Batch 154/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.925, Loss: 1.354 Epoch 5 Batch 155/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.925, Loss: 1.383 Epoch 5 Batch 156/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.925, Loss: 1.390 Epoch 5 Batch 157/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.925, Loss: 1.459 Epoch 5 Batch 158/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.936, Loss: 1.336 Epoch 5 Batch 159/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.923, Loss: 1.440 Epoch 5 Batch 160/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.923, Loss: 1.423 Epoch 5 Batch 161/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.928, Loss: 1.440 Epoch 5 Batch 162/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.928, Loss: 1.503 Epoch 5 Batch 163/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.919, Loss: 1.447 Epoch 5 Batch 164/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.919, Loss: 1.415 Epoch 5 Batch 165/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.924, Loss: 1.418 Epoch 5 Batch 166/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.926, Loss: 1.440 Epoch 5 Batch 167/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.931, Loss: 1.462 Epoch 5 Batch 168/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.926, Loss: 1.417 Epoch 5 Batch 169/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.926, Loss: 1.361 Epoch 5 Batch 170/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.926, Loss: 1.441 Epoch 5 Batch 171/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.921, Loss: 1.397 Epoch 5 Batch 172/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.930, Loss: 1.426 Epoch 5 Batch 173/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.923, Loss: 1.423 Epoch 5 Batch 174/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.913, Loss: 1.416 Epoch 5 Batch 175/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.923, Loss: 1.486 Epoch 5 Batch 176/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.923, Loss: 1.486 Epoch 5 Batch 177/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.930, Loss: 1.457 Epoch 5 Batch 178/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.936, Loss: 1.439 Epoch 5 Batch 179/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.935, Loss: 1.392 Epoch 5 Batch 180/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.935, Loss: 1.459 Epoch 5 Batch 181/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.935, Loss: 1.390 Epoch 5 Batch 182/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.936, Loss: 1.476 Epoch 5 Batch 183/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.940, Loss: 1.480 Epoch 5 Batch 184/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.940, Loss: 1.518 Epoch 5 Batch 185/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.944, Loss: 1.393 Epoch 5 Batch 186/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.945, Loss: 1.400 Epoch 5 Batch 187/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.945, Loss: 1.479 Epoch 5 Batch 188/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.945, Loss: 1.489 Epoch 5 Batch 189/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.945, Loss: 1.460 Epoch 5 Batch 190/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.945, Loss: 1.472 Epoch 5 Batch 191/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.943, Loss: 1.462 Epoch 5 Batch 192/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.939, Loss: 1.385 Epoch 5 Batch 193/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.943, Loss: 1.487 Epoch 5 Batch 194/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.943, Loss: 1.469 Epoch 5 Batch 195/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.943, Loss: 1.443 Epoch 5 Batch 196/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.945, Loss: 1.399 Epoch 5 Batch 197/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.945, Loss: 1.446 Epoch 5 Batch 198/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.946, Loss: 1.427 Epoch 5 Batch 199/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.946, Loss: 1.475 Epoch 5 Batch 200/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.946, Loss: 1.455 Epoch 5 Batch 201/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.942, Loss: 1.462 Epoch 5 Batch 202/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.935, Loss: 1.442 Epoch 5 Batch 203/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.931, Loss: 1.438 Epoch 5 Batch 204/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.925, Loss: 1.574 Epoch 5 Batch 205/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.938, Loss: 1.506 Epoch 5 Batch 206/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.933, Loss: 1.443 Epoch 5 Batch 207/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.929, Loss: 1.450 Epoch 5 Batch 208/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.935, Loss: 1.455 Epoch 5 Batch 209/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.935, Loss: 1.406 Epoch 5 Batch 210/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.939, Loss: 1.493 Epoch 5 Batch 211/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.421 Epoch 5 Batch 212/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.930, Loss: 1.464 Epoch 5 Batch 213/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.932, Loss: 1.460 Epoch 5 Batch 214/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.945, Loss: 1.425 Epoch 5 Batch 215/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.931, Loss: 1.474 Epoch 5 Batch 216/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.931, Loss: 1.451 Epoch 5 Batch 217/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.931, Loss: 1.488 Epoch 5 Batch 218/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.920, Loss: 1.470 Epoch 5 Batch 219/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.920, Loss: 1.392 Epoch 5 Batch 220/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.919, Loss: 1.477 Epoch 5 Batch 221/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.919, Loss: 1.494 Epoch 5 Batch 222/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.930, Loss: 1.424 Epoch 5 Batch 223/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.940, Loss: 1.436 Epoch 5 Batch 224/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.945, Loss: 1.414 Epoch 5 Batch 225/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.944, Loss: 1.511 Epoch 5 Batch 226/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.945, Loss: 1.399 Epoch 5 Batch 227/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.936, Loss: 1.475 Epoch 5 Batch 228/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.931, Loss: 1.474 Epoch 5 Batch 229/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.931, Loss: 1.430 Epoch 5 Batch 230/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.931, Loss: 1.446 Epoch 5 Batch 231/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.927, Loss: 1.516 Epoch 5 Batch 232/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.928, Loss: 1.412 Epoch 5 Batch 233/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.931, Loss: 1.395 Epoch 5 Batch 234/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.935, Loss: 1.477 Epoch 5 Batch 235/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.932, Loss: 1.443 Epoch 5 Batch 236/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.933, Loss: 1.521 Epoch 5 Batch 237/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.938, Loss: 1.525 Epoch 5 Batch 238/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.936, Loss: 1.483 Epoch 5 Batch 239/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.931, Loss: 1.472 Epoch 5 Batch 240/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.938, Loss: 1.458 Epoch 5 Batch 241/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.938, Loss: 1.473 Epoch 5 Batch 242/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.942, Loss: 1.480 Epoch 5 Batch 243/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.942, Loss: 1.414 Epoch 5 Batch 244/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.935, Loss: 1.454 Epoch 5 Batch 245/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.936, Loss: 1.389 Epoch 5 Batch 246/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.935, Loss: 1.422 Epoch 5 Batch 247/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.935, Loss: 1.491 Epoch 5 Batch 248/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.931, Loss: 1.448 Epoch 5 Batch 249/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.927, Loss: 1.453 Epoch 5 Batch 250/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.932, Loss: 1.458 Epoch 5 Batch 251/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.936, Loss: 1.475 Epoch 5 Batch 252/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.935, Loss: 1.448 Epoch 5 Batch 253/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.927, Loss: 1.510 Epoch 5 Batch 254/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.928, Loss: 1.498 Epoch 5 Batch 255/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.926, Loss: 1.427 Epoch 5 Batch 256/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.935, Loss: 1.414 Epoch 5 Batch 257/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.932, Loss: 1.421 Epoch 5 Batch 258/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.932, Loss: 1.391 Epoch 5 Batch 259/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.933, Loss: 1.409 Epoch 5 Batch 260/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.944, Loss: 1.497 Epoch 5 Batch 261/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.944, Loss: 1.489 Epoch 5 Batch 262/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.939, Loss: 1.459 Epoch 5 Batch 263/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.937, Loss: 1.485 Epoch 5 Batch 264/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.941, Loss: 1.456 Epoch 5 Batch 265/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.941, Loss: 1.448 Epoch 5 Batch 266/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.941, Loss: 1.387 Epoch 5 Batch 267/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.946, Loss: 1.420 Epoch 5 Batch 268/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.940, Loss: 1.512 Epoch 5 Batch 269/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.945, Loss: 1.576 Epoch 5 Batch 270/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.945, Loss: 1.513 Epoch 5 Batch 271/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.940, Loss: 1.489 Epoch 5 Batch 272/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.937, Loss: 1.453 Epoch 5 Batch 273/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.941, Loss: 1.469 Epoch 5 Batch 274/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.941, Loss: 1.435 Epoch 5 Batch 275/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.930, Loss: 1.421 Epoch 5 Batch 276/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.930, Loss: 1.474 Epoch 5 Batch 277/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.932, Loss: 1.476 Epoch 5 Batch 278/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.929, Loss: 1.402 Epoch 5 Batch 279/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.937, Loss: 1.458 Epoch 5 Batch 280/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.940, Loss: 1.486 Epoch 5 Batch 281/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.940, Loss: 1.461 Epoch 5 Batch 282/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.938, Loss: 1.484 Epoch 5 Batch 283/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.926, Loss: 1.462 Epoch 5 Batch 284/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.913, Loss: 1.453 Epoch 5 Batch 285/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.912, Loss: 1.464 Epoch 5 Batch 286/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.912, Loss: 1.475 Epoch 5 Batch 287/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.925, Loss: 1.386 Epoch 5 Batch 288/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.937, Loss: 1.453 Epoch 5 Batch 289/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.936, Loss: 1.476 Epoch 5 Batch 290/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.932, Loss: 1.406 Epoch 5 Batch 291/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.932, Loss: 1.395 Epoch 5 Batch 292/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.930, Loss: 1.421 Epoch 5 Batch 293/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.924, Loss: 1.440 Epoch 5 Batch 294/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.929, Loss: 1.436 Epoch 5 Batch 295/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.933, Loss: 1.440 Epoch 5 Batch 296/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.933, Loss: 1.411 Epoch 5 Batch 297/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.931, Loss: 1.439 Epoch 5 Batch 298/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.936, Loss: 1.468 Epoch 5 Batch 299/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.936, Loss: 1.488 Epoch 5 Batch 300/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.942, Loss: 1.445 Epoch 5 Batch 301/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.938, Loss: 1.453 Epoch 5 Batch 302/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.941, Loss: 1.447 Epoch 5 Batch 303/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.935, Loss: 1.441 Epoch 5 Batch 304/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.929, Loss: 1.437 Epoch 5 Batch 305/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.929, Loss: 1.448 Epoch 5 Batch 306/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.929, Loss: 1.485 Epoch 5 Batch 307/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.932, Loss: 1.359 Epoch 5 Batch 308/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.932, Loss: 1.458 Epoch 5 Batch 309/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.936, Loss: 1.435 Epoch 5 Batch 310/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.936, Loss: 1.404 Epoch 5 Batch 311/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.937, Loss: 1.410 Epoch 5 Batch 312/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.936, Loss: 1.471 Epoch 5 Batch 313/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.936, Loss: 1.449 Epoch 5 Batch 314/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.942, Loss: 1.451 Epoch 5 Batch 315/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.937, Loss: 1.481 Epoch 5 Batch 316/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.940, Loss: 1.470 Epoch 5 Batch 317/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.940, Loss: 1.442 Epoch 5 Batch 318/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.933, Loss: 1.505 Epoch 5 Batch 319/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.928, Loss: 1.487 Epoch 5 Batch 320/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.925, Loss: 1.537 Epoch 5 Batch 321/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.909, Loss: 1.393 Epoch 5 Batch 322/1077 - Train Accuracy: 0.911, Validation Accuracy: 0.915, Loss: 1.501 Epoch 5 Batch 323/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.918, Loss: 1.465 Epoch 5 Batch 324/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.918, Loss: 1.442 Epoch 5 Batch 325/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.911, Loss: 1.498 Epoch 5 Batch 326/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.934, Loss: 1.497 Epoch 5 Batch 327/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.945, Loss: 1.502 Epoch 5 Batch 328/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.945, Loss: 1.422 Epoch 5 Batch 329/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.944, Loss: 1.461 Epoch 5 Batch 330/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.949, Loss: 1.471 Epoch 5 Batch 331/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.949, Loss: 1.478 Epoch 5 Batch 332/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.950, Loss: 1.437 Epoch 5 Batch 333/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.946, Loss: 1.465 Epoch 5 Batch 334/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.929, Loss: 1.374 Epoch 5 Batch 335/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.935, Loss: 1.471 Epoch 5 Batch 336/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.930, Loss: 1.444 Epoch 5 Batch 337/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.938, Loss: 1.478 Epoch 5 Batch 338/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.925, Loss: 1.430 Epoch 5 Batch 339/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.926, Loss: 1.473 Epoch 5 Batch 340/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.913, Loss: 1.459 Epoch 5 Batch 341/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.914, Loss: 1.556 Epoch 5 Batch 342/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.919, Loss: 1.496 Epoch 5 Batch 343/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.913, Loss: 1.414 Epoch 5 Batch 344/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.923, Loss: 1.509 Epoch 5 Batch 345/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.921, Loss: 1.427 Epoch 5 Batch 346/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.926, Loss: 1.459 Epoch 5 Batch 347/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.917, Loss: 1.446 Epoch 5 Batch 348/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.912, Loss: 1.456 Epoch 5 Batch 349/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.916, Loss: 1.384 Epoch 5 Batch 350/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.924, Loss: 1.455 Epoch 5 Batch 351/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.918, Loss: 1.422 Epoch 5 Batch 352/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.911, Loss: 1.424 Epoch 5 Batch 353/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.921, Loss: 1.429 Epoch 5 Batch 354/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.926, Loss: 1.436 Epoch 5 Batch 355/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.934, Loss: 1.488 Epoch 5 Batch 356/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.939, Loss: 1.460 Epoch 5 Batch 357/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.407 Epoch 5 Batch 358/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.949, Loss: 1.449 Epoch 5 Batch 359/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.949, Loss: 1.502 Epoch 5 Batch 360/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.942, Loss: 1.474 Epoch 5 Batch 361/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.942, Loss: 1.389 Epoch 5 Batch 362/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.935, Loss: 1.474 Epoch 5 Batch 363/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.928, Loss: 1.463 Epoch 5 Batch 364/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.926, Loss: 1.428 Epoch 5 Batch 365/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.942, Loss: 1.452 Epoch 5 Batch 366/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.927, Loss: 1.425 Epoch 5 Batch 367/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.929, Loss: 1.456 Epoch 5 Batch 368/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.940, Loss: 1.454 Epoch 5 Batch 369/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.940, Loss: 1.476 Epoch 5 Batch 370/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.940, Loss: 1.377 Epoch 5 Batch 371/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.936, Loss: 1.497 Epoch 5 Batch 372/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.938, Loss: 1.385 Epoch 5 Batch 373/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.939, Loss: 1.411 Epoch 5 Batch 374/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.941, Loss: 1.450 Epoch 5 Batch 375/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.941, Loss: 1.436 Epoch 5 Batch 376/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.941, Loss: 1.505 Epoch 5 Batch 377/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.946, Loss: 1.453 Epoch 5 Batch 378/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.942, Loss: 1.442 Epoch 5 Batch 379/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.936, Loss: 1.377 Epoch 5 Batch 380/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.936, Loss: 1.436 Epoch 5 Batch 381/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.942, Loss: 1.522 Epoch 5 Batch 382/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.949, Loss: 1.436 Epoch 5 Batch 383/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.942, Loss: 1.481 Epoch 5 Batch 384/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.942, Loss: 1.526 Epoch 5 Batch 385/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.943, Loss: 1.458 Epoch 5 Batch 386/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.948, Loss: 1.450 Epoch 5 Batch 387/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.943, Loss: 1.409 Epoch 5 Batch 388/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.940, Loss: 1.426 Epoch 5 Batch 389/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.953, Loss: 1.440 Epoch 5 Batch 390/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.951, Loss: 1.514 Epoch 5 Batch 391/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.949, Loss: 1.430 Epoch 5 Batch 392/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.942, Loss: 1.449 Epoch 5 Batch 393/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.939, Loss: 1.431 Epoch 5 Batch 394/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.938, Loss: 1.448 Epoch 5 Batch 395/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.939, Loss: 1.487 Epoch 5 Batch 396/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.939, Loss: 1.478 Epoch 5 Batch 397/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.939, Loss: 1.430 Epoch 5 Batch 398/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.939, Loss: 1.364 Epoch 5 Batch 399/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.938, Loss: 1.411 Epoch 5 Batch 400/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.939, Loss: 1.470 Epoch 5 Batch 401/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.941, Loss: 1.391 Epoch 5 Batch 402/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.936, Loss: 1.486 Epoch 5 Batch 403/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.935, Loss: 1.502 Epoch 5 Batch 404/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.936, Loss: 1.497 Epoch 5 Batch 405/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.936, Loss: 1.377 Epoch 5 Batch 406/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.934, Loss: 1.442 Epoch 5 Batch 407/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.936, Loss: 1.476 Epoch 5 Batch 408/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.932, Loss: 1.423 Epoch 5 Batch 409/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.935, Loss: 1.497 Epoch 5 Batch 410/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.938, Loss: 1.463 Epoch 5 Batch 411/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.935, Loss: 1.401 Epoch 5 Batch 412/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.931, Loss: 1.461 Epoch 5 Batch 413/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.941, Loss: 1.511 Epoch 5 Batch 414/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.947, Loss: 1.413 Epoch 5 Batch 415/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.942, Loss: 1.424 Epoch 5 Batch 416/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.940, Loss: 1.445 Epoch 5 Batch 417/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.938, Loss: 1.460 Epoch 5 Batch 418/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.942, Loss: 1.473 Epoch 5 Batch 419/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.946, Loss: 1.475 Epoch 5 Batch 420/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.946, Loss: 1.432 Epoch 5 Batch 421/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.936, Loss: 1.374 Epoch 5 Batch 422/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.936, Loss: 1.463 Epoch 5 Batch 423/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.925, Loss: 1.461 Epoch 5 Batch 424/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.919, Loss: 1.458 Epoch 5 Batch 425/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.925, Loss: 1.430 Epoch 5 Batch 426/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.935, Loss: 1.516 Epoch 5 Batch 427/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.935, Loss: 1.490 Epoch 5 Batch 428/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.943, Loss: 1.483 Epoch 5 Batch 429/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.939, Loss: 1.519 Epoch 5 Batch 430/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.939, Loss: 1.371 Epoch 5 Batch 431/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.939, Loss: 1.455 Epoch 5 Batch 432/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.941, Loss: 1.453 Epoch 5 Batch 433/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.951, Loss: 1.401 Epoch 5 Batch 434/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.942, Loss: 1.425 Epoch 5 Batch 435/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.928, Loss: 1.400 Epoch 5 Batch 436/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.928, Loss: 1.493 Epoch 5 Batch 437/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.926, Loss: 1.489 Epoch 5 Batch 438/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.926, Loss: 1.395 Epoch 5 Batch 439/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.924, Loss: 1.437 Epoch 5 Batch 440/1077 - Train Accuracy: 0.917, Validation Accuracy: 0.926, Loss: 1.446 Epoch 5 Batch 441/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.927, Loss: 1.429 Epoch 5 Batch 442/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.927, Loss: 1.415 Epoch 5 Batch 443/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.937, Loss: 1.431 Epoch 5 Batch 444/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.935, Loss: 1.463 Epoch 5 Batch 445/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.935, Loss: 1.439 Epoch 5 Batch 446/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.935, Loss: 1.382 Epoch 5 Batch 447/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.937, Loss: 1.477 Epoch 5 Batch 448/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.932, Loss: 1.438 Epoch 5 Batch 449/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.932, Loss: 1.445 Epoch 5 Batch 450/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.931, Loss: 1.523 Epoch 5 Batch 451/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.927, Loss: 1.428 Epoch 5 Batch 452/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.927, Loss: 1.358 Epoch 5 Batch 453/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.928, Loss: 1.424 Epoch 5 Batch 454/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.929, Loss: 1.475 Epoch 5 Batch 455/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.929, Loss: 1.487 Epoch 5 Batch 456/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.934, Loss: 1.465 Epoch 5 Batch 457/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.934, Loss: 1.432 Epoch 5 Batch 458/1077 - Train Accuracy: 0.915, Validation Accuracy: 0.934, Loss: 1.419 Epoch 5 Batch 459/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.938, Loss: 1.375 Epoch 5 Batch 460/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.938, Loss: 1.495 Epoch 5 Batch 461/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.943, Loss: 1.488 Epoch 5 Batch 462/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.944, Loss: 1.501 Epoch 5 Batch 463/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.944, Loss: 1.485 Epoch 5 Batch 464/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.939, Loss: 1.470 Epoch 5 Batch 465/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.471 Epoch 5 Batch 466/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.935, Loss: 1.477 Epoch 5 Batch 467/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.933, Loss: 1.451 Epoch 5 Batch 468/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.926, Loss: 1.459 Epoch 5 Batch 469/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.931, Loss: 1.464 Epoch 5 Batch 470/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.934, Loss: 1.382 Epoch 5 Batch 471/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.927, Loss: 1.445 Epoch 5 Batch 472/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.913, Loss: 1.461 Epoch 5 Batch 473/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.924, Loss: 1.457 Epoch 5 Batch 474/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.924, Loss: 1.446 Epoch 5 Batch 475/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.924, Loss: 1.492 Epoch 5 Batch 476/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.923, Loss: 1.370 Epoch 5 Batch 477/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.933, Loss: 1.398 Epoch 5 Batch 478/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.934, Loss: 1.459 Epoch 5 Batch 479/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.939, Loss: 1.436 Epoch 5 Batch 480/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.933, Loss: 1.485 Epoch 5 Batch 481/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.937, Loss: 1.354 Epoch 5 Batch 482/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.933, Loss: 1.472 Epoch 5 Batch 483/1077 - Train Accuracy: 0.914, Validation Accuracy: 0.939, Loss: 1.413 Epoch 5 Batch 484/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.935, Loss: 1.444 Epoch 5 Batch 485/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.935, Loss: 1.437 Epoch 5 Batch 486/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.936, Loss: 1.455 Epoch 5 Batch 487/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.938, Loss: 1.445 Epoch 5 Batch 488/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.945, Loss: 1.429 Epoch 5 Batch 489/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.939, Loss: 1.420 Epoch 5 Batch 490/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.933, Loss: 1.464 Epoch 5 Batch 491/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.931, Loss: 1.458 Epoch 5 Batch 492/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.931, Loss: 1.436 Epoch 5 Batch 493/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.919, Loss: 1.447 Epoch 5 Batch 494/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.918, Loss: 1.409 Epoch 5 Batch 495/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.920, Loss: 1.430 Epoch 5 Batch 496/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.924, Loss: 1.482 Epoch 5 Batch 497/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.928, Loss: 1.435 Epoch 5 Batch 498/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.938, Loss: 1.393 Epoch 5 Batch 499/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.944, Loss: 1.429 Epoch 5 Batch 500/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.944, Loss: 1.412 Epoch 5 Batch 501/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.944, Loss: 1.453 Epoch 5 Batch 502/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.944, Loss: 1.463 Epoch 5 Batch 503/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.939, Loss: 1.401 Epoch 5 Batch 504/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.512 Epoch 5 Batch 505/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.939, Loss: 1.489 Epoch 5 Batch 506/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.944, Loss: 1.506 Epoch 5 Batch 507/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.952, Loss: 1.455 Epoch 5 Batch 508/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.952, Loss: 1.453 Epoch 5 Batch 509/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.953, Loss: 1.403 Epoch 5 Batch 510/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.946, Loss: 1.459 Epoch 5 Batch 511/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.941, Loss: 1.442 Epoch 5 Batch 512/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.944, Loss: 1.375 Epoch 5 Batch 513/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.945, Loss: 1.430 Epoch 5 Batch 514/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.944, Loss: 1.430 Epoch 5 Batch 515/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.944, Loss: 1.437 Epoch 5 Batch 516/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.944, Loss: 1.485 Epoch 5 Batch 517/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.944, Loss: 1.483 Epoch 5 Batch 518/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.933, Loss: 1.420 Epoch 5 Batch 519/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.933, Loss: 1.506 Epoch 5 Batch 520/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.933, Loss: 1.443 Epoch 5 Batch 521/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.933, Loss: 1.362 Epoch 5 Batch 522/1077 - Train Accuracy: 0.910, Validation Accuracy: 0.931, Loss: 1.421 Epoch 5 Batch 523/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.941, Loss: 1.441 Epoch 5 Batch 524/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.948, Loss: 1.575 Epoch 5 Batch 525/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.948, Loss: 1.470 Epoch 5 Batch 526/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.948, Loss: 1.510 Epoch 5 Batch 527/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.948, Loss: 1.460 Epoch 5 Batch 528/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.943, Loss: 1.503 Epoch 5 Batch 529/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.932, Loss: 1.416 Epoch 5 Batch 530/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.932, Loss: 1.473 Epoch 5 Batch 531/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.941, Loss: 1.516 Epoch 5 Batch 532/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.943, Loss: 1.498 Epoch 5 Batch 533/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.939, Loss: 1.460 Epoch 5 Batch 534/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.935, Loss: 1.431 Epoch 5 Batch 535/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.927, Loss: 1.472 Epoch 5 Batch 536/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.918, Loss: 1.449 Epoch 5 Batch 537/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.912, Loss: 1.504 Epoch 5 Batch 538/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.912, Loss: 1.479 Epoch 5 Batch 539/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.907, Loss: 1.460 Epoch 5 Batch 540/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.921, Loss: 1.441 Epoch 5 Batch 541/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.921, Loss: 1.482 Epoch 5 Batch 542/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.934, Loss: 1.425 Epoch 5 Batch 543/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.940, Loss: 1.431 Epoch 5 Batch 544/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.940, Loss: 1.493 Epoch 5 Batch 545/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.930, Loss: 1.469 Epoch 5 Batch 546/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.925, Loss: 1.423 Epoch 5 Batch 547/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.930, Loss: 1.445 Epoch 5 Batch 548/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.930, Loss: 1.451 Epoch 5 Batch 549/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.931, Loss: 1.467 Epoch 5 Batch 550/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.930, Loss: 1.479 Epoch 5 Batch 551/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.926, Loss: 1.413 Epoch 5 Batch 552/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.931, Loss: 1.519 Epoch 5 Batch 553/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.929, Loss: 1.523 Epoch 5 Batch 554/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.925, Loss: 1.389 Epoch 5 Batch 555/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.925, Loss: 1.444 Epoch 5 Batch 556/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.925, Loss: 1.441 Epoch 5 Batch 557/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.918, Loss: 1.484 Epoch 5 Batch 558/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.913, Loss: 1.453 Epoch 5 Batch 559/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.913, Loss: 1.506 Epoch 5 Batch 560/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.924, Loss: 1.421 Epoch 5 Batch 561/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.924, Loss: 1.442 Epoch 5 Batch 562/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.929, Loss: 1.505 Epoch 5 Batch 563/1077 - Train Accuracy: 0.919, Validation Accuracy: 0.922, Loss: 1.399 Epoch 5 Batch 564/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.922, Loss: 1.356 Epoch 5 Batch 565/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.922, Loss: 1.389 Epoch 5 Batch 566/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.918, Loss: 1.446 Epoch 5 Batch 567/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.922, Loss: 1.437 Epoch 5 Batch 568/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.922, Loss: 1.483 Epoch 5 Batch 569/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.927, Loss: 1.411 Epoch 5 Batch 570/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.926, Loss: 1.476 Epoch 5 Batch 571/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.923, Loss: 1.422 Epoch 5 Batch 572/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.928, Loss: 1.463 Epoch 5 Batch 573/1077 - Train Accuracy: 0.923, Validation Accuracy: 0.927, Loss: 1.363 Epoch 5 Batch 574/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.929, Loss: 1.413 Epoch 5 Batch 575/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.934, Loss: 1.451 Epoch 5 Batch 576/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.925, Loss: 1.506 Epoch 5 Batch 577/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.921, Loss: 1.481 Epoch 5 Batch 578/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.920, Loss: 1.504 Epoch 5 Batch 579/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.925, Loss: 1.484 Epoch 5 Batch 580/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.929, Loss: 1.424 Epoch 5 Batch 581/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.930, Loss: 1.434 Epoch 5 Batch 582/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.931, Loss: 1.402 Epoch 5 Batch 583/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.942, Loss: 1.412 Epoch 5 Batch 584/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.938, Loss: 1.418 Epoch 5 Batch 585/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.940, Loss: 1.361 Epoch 5 Batch 586/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.939, Loss: 1.471 Epoch 5 Batch 587/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.939, Loss: 1.504 Epoch 5 Batch 588/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.390 Epoch 5 Batch 589/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.939, Loss: 1.450 Epoch 5 Batch 590/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.939, Loss: 1.417 Epoch 5 Batch 591/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.939, Loss: 1.379 Epoch 5 Batch 592/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.937, Loss: 1.338 Epoch 5 Batch 593/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.937, Loss: 1.393 Epoch 5 Batch 594/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.942, Loss: 1.432 Epoch 5 Batch 595/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.932, Loss: 1.456 Epoch 5 Batch 596/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.927, Loss: 1.516 Epoch 5 Batch 597/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.927, Loss: 1.492 Epoch 5 Batch 598/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.932, Loss: 1.455 Epoch 5 Batch 599/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.932, Loss: 1.460 Epoch 5 Batch 600/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.936, Loss: 1.481 Epoch 5 Batch 601/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.938, Loss: 1.364 Epoch 5 Batch 602/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.944, Loss: 1.386 Epoch 5 Batch 603/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.949, Loss: 1.411 Epoch 5 Batch 604/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.938, Loss: 1.439 Epoch 5 Batch 605/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.932, Loss: 1.397 Epoch 5 Batch 606/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.932, Loss: 1.431 Epoch 5 Batch 607/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.933, Loss: 1.408 Epoch 5 Batch 608/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.935, Loss: 1.418 Epoch 5 Batch 609/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.941, Loss: 1.429 Epoch 5 Batch 610/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.939, Loss: 1.435 Epoch 5 Batch 611/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.939, Loss: 1.325 Epoch 5 Batch 612/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.935, Loss: 1.458 Epoch 5 Batch 613/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.943, Loss: 1.448 Epoch 5 Batch 614/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.943, Loss: 1.443 Epoch 5 Batch 615/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.935, Loss: 1.507 Epoch 5 Batch 616/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.935, Loss: 1.477 Epoch 5 Batch 617/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.935, Loss: 1.347 Epoch 5 Batch 618/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.930, Loss: 1.526 Epoch 5 Batch 619/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.935, Loss: 1.366 Epoch 5 Batch 620/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.931, Loss: 1.384 Epoch 5 Batch 621/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.936, Loss: 1.469 Epoch 5 Batch 622/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.933, Loss: 1.476 Epoch 5 Batch 623/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.935, Loss: 1.532 Epoch 5 Batch 624/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.941, Loss: 1.510 Epoch 5 Batch 625/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.939, Loss: 1.384 Epoch 5 Batch 626/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.941, Loss: 1.412 Epoch 5 Batch 627/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.941, Loss: 1.454 Epoch 5 Batch 628/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.946, Loss: 1.553 Epoch 5 Batch 629/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.941, Loss: 1.388 Epoch 5 Batch 630/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.951, Loss: 1.428 Epoch 5 Batch 631/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.941, Loss: 1.476 Epoch 5 Batch 632/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.941, Loss: 1.398 Epoch 5 Batch 633/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.931, Loss: 1.377 Epoch 5 Batch 634/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.927, Loss: 1.506 Epoch 5 Batch 635/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.931, Loss: 1.389 Epoch 5 Batch 636/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.928, Loss: 1.398 Epoch 5 Batch 637/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.928, Loss: 1.410 Epoch 5 Batch 638/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.919, Loss: 1.457 Epoch 5 Batch 639/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.922, Loss: 1.404 Epoch 5 Batch 640/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.920, Loss: 1.453 Epoch 5 Batch 641/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.920, Loss: 1.407 Epoch 5 Batch 642/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.926, Loss: 1.452 Epoch 5 Batch 643/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.926, Loss: 1.432 Epoch 5 Batch 644/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.928, Loss: 1.502 Epoch 5 Batch 645/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.922, Loss: 1.489 Epoch 5 Batch 646/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.915, Loss: 1.468 Epoch 5 Batch 647/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.919, Loss: 1.438 Epoch 5 Batch 648/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.915, Loss: 1.317 Epoch 5 Batch 649/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.919, Loss: 1.462 Epoch 5 Batch 650/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.915, Loss: 1.398 Epoch 5 Batch 651/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.925, Loss: 1.400 Epoch 5 Batch 652/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.921, Loss: 1.531 Epoch 5 Batch 653/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.920, Loss: 1.448 Epoch 5 Batch 654/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.911, Loss: 1.397 Epoch 5 Batch 655/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.926, Loss: 1.513 Epoch 5 Batch 656/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.933, Loss: 1.432 Epoch 5 Batch 657/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.931, Loss: 1.439 Epoch 5 Batch 658/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.935, Loss: 1.474 Epoch 5 Batch 659/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.931, Loss: 1.469 Epoch 5 Batch 660/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.931, Loss: 1.462 Epoch 5 Batch 661/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.931, Loss: 1.508 Epoch 5 Batch 662/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.931, Loss: 1.446 Epoch 5 Batch 663/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.934, Loss: 1.474 Epoch 5 Batch 664/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.938, Loss: 1.410 Epoch 5 Batch 665/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.947, Loss: 1.483 Epoch 5 Batch 666/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.459 Epoch 5 Batch 667/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.954, Loss: 1.454 Epoch 5 Batch 668/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.945, Loss: 1.525 Epoch 5 Batch 669/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.945, Loss: 1.404 Epoch 5 Batch 670/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.941, Loss: 1.468 Epoch 5 Batch 671/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.935, Loss: 1.464 Epoch 5 Batch 672/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.937, Loss: 1.420 Epoch 5 Batch 673/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.942, Loss: 1.431 Epoch 5 Batch 674/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.936, Loss: 1.374 Epoch 5 Batch 675/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.936, Loss: 1.411 Epoch 5 Batch 676/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.936, Loss: 1.525 Epoch 5 Batch 677/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.936, Loss: 1.430 Epoch 5 Batch 678/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.945, Loss: 1.452 Epoch 5 Batch 679/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.948, Loss: 1.423 Epoch 5 Batch 680/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.938, Loss: 1.413 Epoch 5 Batch 681/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.934, Loss: 1.444 Epoch 5 Batch 682/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.929, Loss: 1.398 Epoch 5 Batch 683/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.938, Loss: 1.458 Epoch 5 Batch 684/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.935, Loss: 1.426 Epoch 5 Batch 685/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.933, Loss: 1.442 Epoch 5 Batch 686/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.935, Loss: 1.449 Epoch 5 Batch 687/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.933, Loss: 1.404 Epoch 5 Batch 688/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.929, Loss: 1.417 Epoch 5 Batch 689/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.920, Loss: 1.515 Epoch 5 Batch 690/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.929, Loss: 1.373 Epoch 5 Batch 691/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.929, Loss: 1.426 Epoch 5 Batch 692/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.939, Loss: 1.424 Epoch 5 Batch 693/1077 - Train Accuracy: 0.913, Validation Accuracy: 0.939, Loss: 1.475 Epoch 5 Batch 694/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.936, Loss: 1.444 Epoch 5 Batch 695/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.930, Loss: 1.447 Epoch 5 Batch 696/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.927, Loss: 1.479 Epoch 5 Batch 697/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.927, Loss: 1.430 Epoch 5 Batch 698/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.919, Loss: 1.475 Epoch 5 Batch 699/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.919, Loss: 1.420 Epoch 5 Batch 700/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.924, Loss: 1.405 Epoch 5 Batch 701/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.924, Loss: 1.400 Epoch 5 Batch 702/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.921, Loss: 1.476 Epoch 5 Batch 703/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.921, Loss: 1.463 Epoch 5 Batch 704/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.926, Loss: 1.403 Epoch 5 Batch 705/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.933, Loss: 1.439 Epoch 5 Batch 706/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.936, Loss: 1.498 Epoch 5 Batch 707/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.932, Loss: 1.461 Epoch 5 Batch 708/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.931, Loss: 1.399 Epoch 5 Batch 709/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.929, Loss: 1.542 Epoch 5 Batch 710/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.919, Loss: 1.418 Epoch 5 Batch 711/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.929, Loss: 1.503 Epoch 5 Batch 712/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.929, Loss: 1.500 Epoch 5 Batch 713/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.925, Loss: 1.502 Epoch 5 Batch 714/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.933, Loss: 1.469 Epoch 5 Batch 715/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.931, Loss: 1.427 Epoch 5 Batch 716/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.943, Loss: 1.394 Epoch 5 Batch 717/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.947, Loss: 1.403 Epoch 5 Batch 718/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.950, Loss: 1.412 Epoch 5 Batch 719/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.952, Loss: 1.399 Epoch 5 Batch 720/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.952, Loss: 1.358 Epoch 5 Batch 721/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.947, Loss: 1.505 Epoch 5 Batch 722/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.950, Loss: 1.487 Epoch 5 Batch 723/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.395 Epoch 5 Batch 724/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.937, Loss: 1.482 Epoch 5 Batch 725/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.930, Loss: 1.446 Epoch 5 Batch 726/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.930, Loss: 1.440 Epoch 5 Batch 727/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.930, Loss: 1.469 Epoch 5 Batch 728/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.941, Loss: 1.406 Epoch 5 Batch 729/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.941, Loss: 1.512 Epoch 5 Batch 730/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.941, Loss: 1.522 Epoch 5 Batch 731/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.941, Loss: 1.452 Epoch 5 Batch 732/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.944, Loss: 1.416 Epoch 5 Batch 733/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.935, Loss: 1.316 Epoch 5 Batch 734/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.940, Loss: 1.472 Epoch 5 Batch 735/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.935, Loss: 1.445 Epoch 5 Batch 736/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.935, Loss: 1.411 Epoch 5 Batch 737/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.940, Loss: 1.372 Epoch 5 Batch 738/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.936, Loss: 1.386 Epoch 5 Batch 739/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.936, Loss: 1.523 Epoch 5 Batch 740/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.940, Loss: 1.496 Epoch 5 Batch 741/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.939, Loss: 1.439 Epoch 5 Batch 742/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.944, Loss: 1.384 Epoch 5 Batch 743/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.937, Loss: 1.483 Epoch 5 Batch 744/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.942, Loss: 1.406 Epoch 5 Batch 745/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.942, Loss: 1.443 Epoch 5 Batch 746/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.941, Loss: 1.450 Epoch 5 Batch 747/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.946, Loss: 1.407 Epoch 5 Batch 748/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.945, Loss: 1.528 Epoch 5 Batch 749/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.942, Loss: 1.438 Epoch 5 Batch 750/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.938, Loss: 1.475 Epoch 5 Batch 751/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.940, Loss: 1.393 Epoch 5 Batch 752/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.945, Loss: 1.479 Epoch 5 Batch 753/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.460 Epoch 5 Batch 754/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.946, Loss: 1.416 Epoch 5 Batch 755/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.946, Loss: 1.460 Epoch 5 Batch 756/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.946, Loss: 1.486 Epoch 5 Batch 757/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.946, Loss: 1.473 Epoch 5 Batch 758/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.940, Loss: 1.480 Epoch 5 Batch 759/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.942, Loss: 1.407 Epoch 5 Batch 760/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.947, Loss: 1.423 Epoch 5 Batch 761/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.948, Loss: 1.472 Epoch 5 Batch 762/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.947, Loss: 1.429 Epoch 5 Batch 763/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.946, Loss: 1.383 Epoch 5 Batch 764/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.952, Loss: 1.439 Epoch 5 Batch 765/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.947, Loss: 1.501 Epoch 5 Batch 766/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.947, Loss: 1.539 Epoch 5 Batch 767/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.943, Loss: 1.429 Epoch 5 Batch 768/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.395 Epoch 5 Batch 769/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.952, Loss: 1.407 Epoch 5 Batch 770/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.958, Loss: 1.399 Epoch 5 Batch 771/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.943, Loss: 1.480 Epoch 5 Batch 772/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.948, Loss: 1.397 Epoch 5 Batch 773/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.943, Loss: 1.501 Epoch 5 Batch 774/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.948, Loss: 1.460 Epoch 5 Batch 775/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.948, Loss: 1.345 Epoch 5 Batch 776/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.948, Loss: 1.452 Epoch 5 Batch 777/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.948, Loss: 1.467 Epoch 5 Batch 778/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.957, Loss: 1.431 Epoch 5 Batch 779/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.957, Loss: 1.431 Epoch 5 Batch 780/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.954, Loss: 1.411 Epoch 5 Batch 781/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.950, Loss: 1.403 Epoch 5 Batch 782/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.943, Loss: 1.432 Epoch 5 Batch 783/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.939, Loss: 1.396 Epoch 5 Batch 784/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.939, Loss: 1.460 Epoch 5 Batch 785/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.934, Loss: 1.414 Epoch 5 Batch 786/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.934, Loss: 1.345 Epoch 5 Batch 787/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.940, Loss: 1.444 Epoch 5 Batch 788/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.943, Loss: 1.468 Epoch 5 Batch 789/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.939, Loss: 1.404 Epoch 5 Batch 790/1077 - Train Accuracy: 0.906, Validation Accuracy: 0.938, Loss: 1.490 Epoch 5 Batch 791/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.931, Loss: 1.334 Epoch 5 Batch 792/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.938, Loss: 1.480 Epoch 5 Batch 793/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.946, Loss: 1.491 Epoch 5 Batch 794/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.935, Loss: 1.501 Epoch 5 Batch 795/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.936, Loss: 1.501 Epoch 5 Batch 796/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.937, Loss: 1.517 Epoch 5 Batch 797/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.937, Loss: 1.476 Epoch 5 Batch 798/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.937, Loss: 1.469 Epoch 5 Batch 799/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.938, Loss: 1.498 Epoch 5 Batch 800/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.938, Loss: 1.411 Epoch 5 Batch 801/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.938, Loss: 1.426 Epoch 5 Batch 802/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.938, Loss: 1.484 Epoch 5 Batch 803/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.935, Loss: 1.475 Epoch 5 Batch 804/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.940, Loss: 1.449 Epoch 5 Batch 805/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.942, Loss: 1.434 Epoch 5 Batch 806/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.936, Loss: 1.400 Epoch 5 Batch 807/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.936, Loss: 1.425 Epoch 5 Batch 808/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.936, Loss: 1.405 Epoch 5 Batch 809/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.936, Loss: 1.473 Epoch 5 Batch 810/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.937, Loss: 1.421 Epoch 5 Batch 811/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.937, Loss: 1.441 Epoch 5 Batch 812/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.934, Loss: 1.427 Epoch 5 Batch 813/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.938, Loss: 1.431 Epoch 5 Batch 814/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.935, Loss: 1.522 Epoch 5 Batch 815/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.933, Loss: 1.453 Epoch 5 Batch 816/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.938, Loss: 1.428 Epoch 5 Batch 817/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.938, Loss: 1.487 Epoch 5 Batch 818/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.949, Loss: 1.478 Epoch 5 Batch 819/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.954, Loss: 1.409 Epoch 5 Batch 820/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.433 Epoch 5 Batch 821/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.947, Loss: 1.471 Epoch 5 Batch 822/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.947, Loss: 1.474 Epoch 5 Batch 823/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.947, Loss: 1.562 Epoch 5 Batch 824/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.944, Loss: 1.464 Epoch 5 Batch 825/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.948, Loss: 1.439 Epoch 5 Batch 826/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.948, Loss: 1.504 Epoch 5 Batch 827/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.948, Loss: 1.405 Epoch 5 Batch 828/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.948, Loss: 1.402 Epoch 5 Batch 829/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.948, Loss: 1.493 Epoch 5 Batch 830/1077 - Train Accuracy: 0.916, Validation Accuracy: 0.945, Loss: 1.425 Epoch 5 Batch 831/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.955, Loss: 1.441 Epoch 5 Batch 832/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.399 Epoch 5 Batch 833/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.950, Loss: 1.415 Epoch 5 Batch 834/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.960, Loss: 1.433 Epoch 5 Batch 835/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.958, Loss: 1.491 Epoch 5 Batch 836/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.958, Loss: 1.430 Epoch 5 Batch 837/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.956, Loss: 1.423 Epoch 5 Batch 838/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.951, Loss: 1.471 Epoch 5 Batch 839/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.955, Loss: 1.471 Epoch 5 Batch 840/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.448 Epoch 5 Batch 841/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.960, Loss: 1.425 Epoch 5 Batch 842/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.950, Loss: 1.443 Epoch 5 Batch 843/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.933, Loss: 1.437 Epoch 5 Batch 844/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.943, Loss: 1.511 Epoch 5 Batch 845/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.944, Loss: 1.471 Epoch 5 Batch 846/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.956, Loss: 1.465 Epoch 5 Batch 847/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.951, Loss: 1.369 Epoch 5 Batch 848/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.951, Loss: 1.448 Epoch 5 Batch 849/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.952, Loss: 1.407 Epoch 5 Batch 850/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.953, Loss: 1.462 Epoch 5 Batch 851/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.960, Loss: 1.458 Epoch 5 Batch 852/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.960, Loss: 1.406 Epoch 5 Batch 853/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.949, Loss: 1.495 Epoch 5 Batch 854/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.949, Loss: 1.451 Epoch 5 Batch 855/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.951, Loss: 1.475 Epoch 5 Batch 856/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.946, Loss: 1.371 Epoch 5 Batch 857/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.956, Loss: 1.386 Epoch 5 Batch 858/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.400 Epoch 5 Batch 859/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.953, Loss: 1.421 Epoch 5 Batch 860/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.949, Loss: 1.491 Epoch 5 Batch 861/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.949, Loss: 1.481 Epoch 5 Batch 862/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.955, Loss: 1.481 Epoch 5 Batch 863/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.943, Loss: 1.414 Epoch 5 Batch 864/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.943, Loss: 1.519 Epoch 5 Batch 865/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.943, Loss: 1.446 Epoch 5 Batch 866/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.944, Loss: 1.364 Epoch 5 Batch 867/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.944, Loss: 1.468 Epoch 5 Batch 868/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.944, Loss: 1.535 Epoch 5 Batch 869/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.944, Loss: 1.415 Epoch 5 Batch 870/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.946, Loss: 1.406 Epoch 5 Batch 871/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.950, Loss: 1.452 Epoch 5 Batch 872/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.946, Loss: 1.493 Epoch 5 Batch 873/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.945, Loss: 1.444 Epoch 5 Batch 874/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.933, Loss: 1.391 Epoch 5 Batch 875/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.935, Loss: 1.426 Epoch 5 Batch 876/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.943, Loss: 1.492 Epoch 5 Batch 877/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.946, Loss: 1.425 Epoch 5 Batch 878/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.946, Loss: 1.464 Epoch 5 Batch 879/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.950, Loss: 1.439 Epoch 5 Batch 880/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.946, Loss: 1.506 Epoch 5 Batch 881/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.952, Loss: 1.417 Epoch 5 Batch 882/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.960, Loss: 1.380 Epoch 5 Batch 883/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.956, Loss: 1.377 Epoch 5 Batch 884/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.956, Loss: 1.356 Epoch 5 Batch 885/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.956, Loss: 1.438 Epoch 5 Batch 886/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.956, Loss: 1.361 Epoch 5 Batch 887/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.957, Loss: 1.424 Epoch 5 Batch 888/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.951, Loss: 1.410 Epoch 5 Batch 889/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.953, Loss: 1.471 Epoch 5 Batch 890/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.947, Loss: 1.489 Epoch 5 Batch 891/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.942, Loss: 1.491 Epoch 5 Batch 892/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.938, Loss: 1.460 Epoch 5 Batch 893/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.936, Loss: 1.438 Epoch 5 Batch 894/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.936, Loss: 1.437 Epoch 5 Batch 895/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.941, Loss: 1.416 Epoch 5 Batch 896/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.941, Loss: 1.479 Epoch 5 Batch 897/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.946, Loss: 1.392 Epoch 5 Batch 898/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.951, Loss: 1.452 Epoch 5 Batch 899/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.956, Loss: 1.451 Epoch 5 Batch 900/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.960, Loss: 1.432 Epoch 5 Batch 901/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.960, Loss: 1.526 Epoch 5 Batch 902/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.960, Loss: 1.415 Epoch 5 Batch 903/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.955, Loss: 1.467 Epoch 5 Batch 904/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.954, Loss: 1.438 Epoch 5 Batch 905/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.954, Loss: 1.482 Epoch 5 Batch 906/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.954, Loss: 1.414 Epoch 5 Batch 907/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.954, Loss: 1.433 Epoch 5 Batch 908/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.956, Loss: 1.459 Epoch 5 Batch 909/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.959, Loss: 1.450 Epoch 5 Batch 910/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.958, Loss: 1.440 Epoch 5 Batch 911/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.961, Loss: 1.446 Epoch 5 Batch 912/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.968, Loss: 1.384 Epoch 5 Batch 913/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.967, Loss: 1.470 Epoch 5 Batch 914/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.961, Loss: 1.473 Epoch 5 Batch 915/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.956, Loss: 1.476 Epoch 5 Batch 916/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.947, Loss: 1.439 Epoch 5 Batch 917/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.948, Loss: 1.517 Epoch 5 Batch 918/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.953, Loss: 1.419 Epoch 5 Batch 919/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.504 Epoch 5 Batch 920/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.962, Loss: 1.438 Epoch 5 Batch 921/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.961, Loss: 1.436 Epoch 5 Batch 922/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.963, Loss: 1.496 Epoch 5 Batch 923/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.967, Loss: 1.441 Epoch 5 Batch 924/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.967, Loss: 1.508 Epoch 5 Batch 925/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.967, Loss: 1.529 Epoch 5 Batch 926/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.960, Loss: 1.394 Epoch 5 Batch 927/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.963, Loss: 1.406 Epoch 5 Batch 928/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.961, Loss: 1.390 Epoch 5 Batch 929/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.962, Loss: 1.509 Epoch 5 Batch 930/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.962, Loss: 1.468 Epoch 5 Batch 931/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.965, Loss: 1.467 Epoch 5 Batch 932/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.963, Loss: 1.481 Epoch 5 Batch 933/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.963, Loss: 1.377 Epoch 5 Batch 934/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.439 Epoch 5 Batch 935/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.968, Loss: 1.423 Epoch 5 Batch 936/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.977, Loss: 1.478 Epoch 5 Batch 937/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.970, Loss: 1.427 Epoch 5 Batch 938/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.970, Loss: 1.434 Epoch 5 Batch 939/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.963, Loss: 1.495 Epoch 5 Batch 940/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.426 Epoch 5 Batch 941/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.966, Loss: 1.479 Epoch 5 Batch 942/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.962, Loss: 1.450 Epoch 5 Batch 943/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.953, Loss: 1.443 Epoch 5 Batch 944/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.946, Loss: 1.459 Epoch 5 Batch 945/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.945, Loss: 1.376 Epoch 5 Batch 946/1077 - Train Accuracy: 0.994, Validation Accuracy: 0.945, Loss: 1.431 Epoch 5 Batch 947/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.941, Loss: 1.416 Epoch 5 Batch 948/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.946, Loss: 1.486 Epoch 5 Batch 949/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.945, Loss: 1.371 Epoch 5 Batch 950/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.951, Loss: 1.440 Epoch 5 Batch 951/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.950, Loss: 1.439 Epoch 5 Batch 952/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.947, Loss: 1.488 Epoch 5 Batch 953/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.948, Loss: 1.504 Epoch 5 Batch 954/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.952, Loss: 1.386 Epoch 5 Batch 955/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.947, Loss: 1.453 Epoch 5 Batch 956/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.947, Loss: 1.417 Epoch 5 Batch 957/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.947, Loss: 1.409 Epoch 5 Batch 958/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.950, Loss: 1.454 Epoch 5 Batch 959/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.945, Loss: 1.465 Epoch 5 Batch 960/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.945, Loss: 1.408 Epoch 5 Batch 961/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.950, Loss: 1.512 Epoch 5 Batch 962/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.950, Loss: 1.423 Epoch 5 Batch 963/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.945, Loss: 1.551 Epoch 5 Batch 964/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.945, Loss: 1.453 Epoch 5 Batch 965/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.951, Loss: 1.507 Epoch 5 Batch 966/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.475 Epoch 5 Batch 967/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.955, Loss: 1.487 Epoch 5 Batch 968/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.959, Loss: 1.380 Epoch 5 Batch 969/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.953, Loss: 1.412 Epoch 5 Batch 970/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.953, Loss: 1.429 Epoch 5 Batch 971/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.953, Loss: 1.524 Epoch 5 Batch 972/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.405 Epoch 5 Batch 973/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.951, Loss: 1.356 Epoch 5 Batch 974/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.951, Loss: 1.485 Epoch 5 Batch 975/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.955, Loss: 1.395 Epoch 5 Batch 976/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.436 Epoch 5 Batch 977/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.950, Loss: 1.366 Epoch 5 Batch 978/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.959, Loss: 1.463 Epoch 5 Batch 979/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.959, Loss: 1.381 Epoch 5 Batch 980/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.959, Loss: 1.447 Epoch 5 Batch 981/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.959, Loss: 1.426 Epoch 5 Batch 982/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.953, Loss: 1.426 Epoch 5 Batch 983/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.953, Loss: 1.359 Epoch 5 Batch 984/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.953, Loss: 1.485 Epoch 5 Batch 985/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.953, Loss: 1.395 Epoch 5 Batch 986/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.952, Loss: 1.412 Epoch 5 Batch 987/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.950, Loss: 1.412 Epoch 5 Batch 988/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.940, Loss: 1.464 Epoch 5 Batch 989/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.940, Loss: 1.409 Epoch 5 Batch 990/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.938, Loss: 1.517 Epoch 5 Batch 991/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.938, Loss: 1.421 Epoch 5 Batch 992/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.938, Loss: 1.416 Epoch 5 Batch 993/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.939, Loss: 1.384 Epoch 5 Batch 994/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.942, Loss: 1.467 Epoch 5 Batch 995/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.442 Epoch 5 Batch 996/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.445 Epoch 5 Batch 997/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.955, Loss: 1.474 Epoch 5 Batch 998/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.955, Loss: 1.389 Epoch 5 Batch 999/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.955, Loss: 1.524 Epoch 5 Batch 1000/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.955, Loss: 1.384 Epoch 5 Batch 1001/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.960, Loss: 1.390 Epoch 5 Batch 1002/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.365 Epoch 5 Batch 1003/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.433 Epoch 5 Batch 1004/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.457 Epoch 5 Batch 1005/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.951, Loss: 1.522 Epoch 5 Batch 1006/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.951, Loss: 1.410 Epoch 5 Batch 1007/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.951, Loss: 1.411 Epoch 5 Batch 1008/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.951, Loss: 1.402 Epoch 5 Batch 1009/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.951, Loss: 1.432 Epoch 5 Batch 1010/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.951, Loss: 1.376 Epoch 5 Batch 1011/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.956, Loss: 1.511 Epoch 5 Batch 1012/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.956, Loss: 1.379 Epoch 5 Batch 1013/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.946, Loss: 1.410 Epoch 5 Batch 1014/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.947, Loss: 1.413 Epoch 5 Batch 1015/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.946, Loss: 1.446 Epoch 5 Batch 1016/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.948, Loss: 1.490 Epoch 5 Batch 1017/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.946, Loss: 1.448 Epoch 5 Batch 1018/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.946, Loss: 1.503 Epoch 5 Batch 1019/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.944, Loss: 1.400 Epoch 5 Batch 1020/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.938, Loss: 1.410 Epoch 5 Batch 1021/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.935, Loss: 1.407 Epoch 5 Batch 1022/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.945, Loss: 1.419 Epoch 5 Batch 1023/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.946, Loss: 1.375 Epoch 5 Batch 1024/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.944, Loss: 1.461 Epoch 5 Batch 1025/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.949, Loss: 1.477 Epoch 5 Batch 1026/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.953, Loss: 1.476 Epoch 5 Batch 1027/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.948, Loss: 1.477 Epoch 5 Batch 1028/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.948, Loss: 1.480 Epoch 5 Batch 1029/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.942, Loss: 1.440 Epoch 5 Batch 1030/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.940, Loss: 1.455 Epoch 5 Batch 1031/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.940, Loss: 1.438 Epoch 5 Batch 1032/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.931, Loss: 1.423 Epoch 5 Batch 1033/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.924, Loss: 1.514 Epoch 5 Batch 1034/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.924, Loss: 1.521 Epoch 5 Batch 1035/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.924, Loss: 1.493 Epoch 5 Batch 1036/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.934, Loss: 1.421 Epoch 5 Batch 1037/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.934, Loss: 1.452 Epoch 5 Batch 1038/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.934, Loss: 1.449 Epoch 5 Batch 1039/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.939, Loss: 1.444 Epoch 5 Batch 1040/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.939, Loss: 1.450 Epoch 5 Batch 1041/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.946, Loss: 1.427 Epoch 5 Batch 1042/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.951, Loss: 1.423 Epoch 5 Batch 1043/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.948, Loss: 1.421 Epoch 5 Batch 1044/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.943, Loss: 1.467 Epoch 5 Batch 1045/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.944, Loss: 1.475 Epoch 5 Batch 1046/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.946, Loss: 1.385 Epoch 5 Batch 1047/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.951, Loss: 1.494 Epoch 5 Batch 1048/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.950, Loss: 1.411 Epoch 5 Batch 1049/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.950, Loss: 1.465 Epoch 5 Batch 1050/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.947, Loss: 1.317 Epoch 5 Batch 1051/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.947, Loss: 1.425 Epoch 5 Batch 1052/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.946, Loss: 1.424 Epoch 5 Batch 1053/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.946, Loss: 1.410 Epoch 5 Batch 1054/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.940, Loss: 1.585 Epoch 5 Batch 1055/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.939, Loss: 1.423 Epoch 5 Batch 1056/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.939, Loss: 1.444 Epoch 5 Batch 1057/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.935, Loss: 1.457 Epoch 5 Batch 1058/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.936, Loss: 1.505 Epoch 5 Batch 1059/1077 - Train Accuracy: 0.922, Validation Accuracy: 0.936, Loss: 1.407 Epoch 5 Batch 1060/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.936, Loss: 1.447 Epoch 5 Batch 1061/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.936, Loss: 1.539 Epoch 5 Batch 1062/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.931, Loss: 1.382 Epoch 5 Batch 1063/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.939, Loss: 1.367 Epoch 5 Batch 1064/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.939, Loss: 1.435 Epoch 5 Batch 1065/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.934, Loss: 1.379 Epoch 5 Batch 1066/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.931, Loss: 1.381 Epoch 5 Batch 1067/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.931, Loss: 1.416 Epoch 5 Batch 1068/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.931, Loss: 1.497 Epoch 5 Batch 1069/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.926, Loss: 1.453 Epoch 5 Batch 1070/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.931, Loss: 1.461 Epoch 5 Batch 1071/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.927, Loss: 1.476 Epoch 5 Batch 1072/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.932, Loss: 1.488 Epoch 5 Batch 1073/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.932, Loss: 1.430 Epoch 5 Batch 1074/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.937, Loss: 1.422 Epoch 5 Batch 1075/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.936, Loss: 1.514 Epoch 6 Batch 0/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.936, Loss: 1.442 Epoch 6 Batch 1/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.941, Loss: 1.417 Epoch 6 Batch 2/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.941, Loss: 1.471 Epoch 6 Batch 3/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.944, Loss: 1.484 Epoch 6 Batch 4/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.944, Loss: 1.522 Epoch 6 Batch 5/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.947, Loss: 1.470 Epoch 6 Batch 6/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.946, Loss: 1.428 Epoch 6 Batch 7/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.951, Loss: 1.420 Epoch 6 Batch 8/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.949, Loss: 1.386 Epoch 6 Batch 9/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.946, Loss: 1.474 Epoch 6 Batch 10/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.950, Loss: 1.460 Epoch 6 Batch 11/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.950, Loss: 1.514 Epoch 6 Batch 12/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.945, Loss: 1.548 Epoch 6 Batch 13/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.945, Loss: 1.427 Epoch 6 Batch 14/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.940, Loss: 1.377 Epoch 6 Batch 15/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.935, Loss: 1.454 Epoch 6 Batch 16/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.935, Loss: 1.416 Epoch 6 Batch 17/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.939, Loss: 1.541 Epoch 6 Batch 18/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.939, Loss: 1.433 Epoch 6 Batch 19/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.932, Loss: 1.425 Epoch 6 Batch 20/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.935, Loss: 1.481 Epoch 6 Batch 21/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.940, Loss: 1.349 Epoch 6 Batch 22/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.946, Loss: 1.393 Epoch 6 Batch 23/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.946, Loss: 1.479 Epoch 6 Batch 24/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.950, Loss: 1.483 Epoch 6 Batch 25/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.950, Loss: 1.361 Epoch 6 Batch 26/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.946, Loss: 1.464 Epoch 6 Batch 27/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.944, Loss: 1.334 Epoch 6 Batch 28/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.955, Loss: 1.428 Epoch 6 Batch 29/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.414 Epoch 6 Batch 30/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.953, Loss: 1.436 Epoch 6 Batch 31/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.953, Loss: 1.474 Epoch 6 Batch 32/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.949, Loss: 1.518 Epoch 6 Batch 33/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.949, Loss: 1.454 Epoch 6 Batch 34/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.949, Loss: 1.477 Epoch 6 Batch 35/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.953, Loss: 1.485 Epoch 6 Batch 36/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.953, Loss: 1.479 Epoch 6 Batch 37/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.953, Loss: 1.466 Epoch 6 Batch 38/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.953, Loss: 1.431 Epoch 6 Batch 39/1077 - Train Accuracy: 0.928, Validation Accuracy: 0.953, Loss: 1.469 Epoch 6 Batch 40/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.950, Loss: 1.395 Epoch 6 Batch 41/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.950, Loss: 1.407 Epoch 6 Batch 42/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.944, Loss: 1.387 Epoch 6 Batch 43/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.944, Loss: 1.484 Epoch 6 Batch 44/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.944, Loss: 1.366 Epoch 6 Batch 45/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.942, Loss: 1.461 Epoch 6 Batch 46/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.951, Loss: 1.384 Epoch 6 Batch 47/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.951, Loss: 1.459 Epoch 6 Batch 48/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.958, Loss: 1.516 Epoch 6 Batch 49/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.958, Loss: 1.424 Epoch 6 Batch 50/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.957, Loss: 1.380 Epoch 6 Batch 51/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.957, Loss: 1.416 Epoch 6 Batch 52/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.952, Loss: 1.477 Epoch 6 Batch 53/1077 - Train Accuracy: 0.920, Validation Accuracy: 0.952, Loss: 1.382 Epoch 6 Batch 54/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.952, Loss: 1.462 Epoch 6 Batch 55/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.952, Loss: 1.452 Epoch 6 Batch 56/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.946, Loss: 1.461 Epoch 6 Batch 57/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.945, Loss: 1.504 Epoch 6 Batch 58/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.943, Loss: 1.374 Epoch 6 Batch 59/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.942, Loss: 1.418 Epoch 6 Batch 60/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.942, Loss: 1.455 Epoch 6 Batch 61/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.933, Loss: 1.485 Epoch 6 Batch 62/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.933, Loss: 1.403 Epoch 6 Batch 63/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.933, Loss: 1.510 Epoch 6 Batch 64/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.934, Loss: 1.448 Epoch 6 Batch 65/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.932, Loss: 1.459 Epoch 6 Batch 66/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.937, Loss: 1.424 Epoch 6 Batch 67/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.937, Loss: 1.477 Epoch 6 Batch 68/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.942, Loss: 1.451 Epoch 6 Batch 69/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.942, Loss: 1.449 Epoch 6 Batch 70/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.947, Loss: 1.460 Epoch 6 Batch 71/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.487 Epoch 6 Batch 72/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.947, Loss: 1.431 Epoch 6 Batch 73/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.949, Loss: 1.446 Epoch 6 Batch 74/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.949, Loss: 1.398 Epoch 6 Batch 75/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.944, Loss: 1.384 Epoch 6 Batch 76/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.949, Loss: 1.470 Epoch 6 Batch 77/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.942, Loss: 1.408 Epoch 6 Batch 78/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.938, Loss: 1.486 Epoch 6 Batch 79/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.938, Loss: 1.426 Epoch 6 Batch 80/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.942, Loss: 1.476 Epoch 6 Batch 81/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.945, Loss: 1.428 Epoch 6 Batch 82/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.940, Loss: 1.437 Epoch 6 Batch 83/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.933, Loss: 1.489 Epoch 6 Batch 84/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.933, Loss: 1.534 Epoch 6 Batch 85/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.942, Loss: 1.391 Epoch 6 Batch 86/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.942, Loss: 1.423 Epoch 6 Batch 87/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.943, Loss: 1.447 Epoch 6 Batch 88/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.946, Loss: 1.486 Epoch 6 Batch 89/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.951, Loss: 1.431 Epoch 6 Batch 90/1077 - Train Accuracy: 0.924, Validation Accuracy: 0.953, Loss: 1.463 Epoch 6 Batch 91/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.952, Loss: 1.389 Epoch 6 Batch 92/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.426 Epoch 6 Batch 93/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.955, Loss: 1.434 Epoch 6 Batch 94/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.954, Loss: 1.411 Epoch 6 Batch 95/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.954, Loss: 1.456 Epoch 6 Batch 96/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.948, Loss: 1.444 Epoch 6 Batch 97/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.942, Loss: 1.439 Epoch 6 Batch 98/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.942, Loss: 1.486 Epoch 6 Batch 99/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.937, Loss: 1.351 Epoch 6 Batch 100/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.932, Loss: 1.372 Epoch 6 Batch 101/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.934, Loss: 1.430 Epoch 6 Batch 102/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.935, Loss: 1.376 Epoch 6 Batch 103/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.930, Loss: 1.443 Epoch 6 Batch 104/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.942, Loss: 1.505 Epoch 6 Batch 105/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.951, Loss: 1.508 Epoch 6 Batch 106/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.946, Loss: 1.393 Epoch 6 Batch 107/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.946, Loss: 1.417 Epoch 6 Batch 108/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.956, Loss: 1.445 Epoch 6 Batch 109/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.939, Loss: 1.414 Epoch 6 Batch 110/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.939, Loss: 1.507 Epoch 6 Batch 111/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.948, Loss: 1.394 Epoch 6 Batch 112/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.948, Loss: 1.470 Epoch 6 Batch 113/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.947, Loss: 1.493 Epoch 6 Batch 114/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.943, Loss: 1.372 Epoch 6 Batch 115/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.947, Loss: 1.469 Epoch 6 Batch 116/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.945, Loss: 1.452 Epoch 6 Batch 117/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.942, Loss: 1.455 Epoch 6 Batch 118/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.933, Loss: 1.364 Epoch 6 Batch 119/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.939, Loss: 1.402 Epoch 6 Batch 120/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.948, Loss: 1.390 Epoch 6 Batch 121/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.949, Loss: 1.390 Epoch 6 Batch 122/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.949, Loss: 1.461 Epoch 6 Batch 123/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.520 Epoch 6 Batch 124/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.951, Loss: 1.473 Epoch 6 Batch 125/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.958, Loss: 1.436 Epoch 6 Batch 126/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.963, Loss: 1.426 Epoch 6 Batch 127/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.958, Loss: 1.471 Epoch 6 Batch 128/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.967, Loss: 1.432 Epoch 6 Batch 129/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.964, Loss: 1.414 Epoch 6 Batch 130/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.959, Loss: 1.424 Epoch 6 Batch 131/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.955, Loss: 1.442 Epoch 6 Batch 132/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.960, Loss: 1.364 Epoch 6 Batch 133/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.960, Loss: 1.437 Epoch 6 Batch 134/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.960, Loss: 1.477 Epoch 6 Batch 135/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.950, Loss: 1.387 Epoch 6 Batch 136/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.955, Loss: 1.415 Epoch 6 Batch 137/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.361 Epoch 6 Batch 138/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.947, Loss: 1.380 Epoch 6 Batch 139/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.482 Epoch 6 Batch 140/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.456 Epoch 6 Batch 141/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.944, Loss: 1.401 Epoch 6 Batch 142/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.946, Loss: 1.441 Epoch 6 Batch 143/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.946, Loss: 1.454 Epoch 6 Batch 144/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.946, Loss: 1.462 Epoch 6 Batch 145/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.941, Loss: 1.411 Epoch 6 Batch 146/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.945, Loss: 1.410 Epoch 6 Batch 147/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.455 Epoch 6 Batch 148/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.944, Loss: 1.420 Epoch 6 Batch 149/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.939, Loss: 1.456 Epoch 6 Batch 150/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.939, Loss: 1.464 Epoch 6 Batch 151/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.939, Loss: 1.464 Epoch 6 Batch 152/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.939, Loss: 1.446 Epoch 6 Batch 153/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.939, Loss: 1.412 Epoch 6 Batch 154/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.944, Loss: 1.452 Epoch 6 Batch 155/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.945, Loss: 1.486 Epoch 6 Batch 156/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.950, Loss: 1.478 Epoch 6 Batch 157/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.950, Loss: 1.469 Epoch 6 Batch 158/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.949, Loss: 1.401 Epoch 6 Batch 159/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.949, Loss: 1.486 Epoch 6 Batch 160/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.950, Loss: 1.312 Epoch 6 Batch 161/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.942, Loss: 1.443 Epoch 6 Batch 162/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.943, Loss: 1.462 Epoch 6 Batch 163/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.948, Loss: 1.413 Epoch 6 Batch 164/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.435 Epoch 6 Batch 165/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.946, Loss: 1.432 Epoch 6 Batch 166/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.949, Loss: 1.405 Epoch 6 Batch 167/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.949, Loss: 1.504 Epoch 6 Batch 168/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.942, Loss: 1.496 Epoch 6 Batch 169/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.944, Loss: 1.431 Epoch 6 Batch 170/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.946, Loss: 1.443 Epoch 6 Batch 171/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.947, Loss: 1.452 Epoch 6 Batch 172/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.942, Loss: 1.414 Epoch 6 Batch 173/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.933, Loss: 1.440 Epoch 6 Batch 174/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.937, Loss: 1.406 Epoch 6 Batch 175/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.937, Loss: 1.511 Epoch 6 Batch 176/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.937, Loss: 1.411 Epoch 6 Batch 177/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.937, Loss: 1.426 Epoch 6 Batch 178/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.931, Loss: 1.354 Epoch 6 Batch 179/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.936, Loss: 1.484 Epoch 6 Batch 180/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.945, Loss: 1.434 Epoch 6 Batch 181/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.945, Loss: 1.478 Epoch 6 Batch 182/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.945, Loss: 1.430 Epoch 6 Batch 183/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.945, Loss: 1.411 Epoch 6 Batch 184/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.945, Loss: 1.397 Epoch 6 Batch 185/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.951, Loss: 1.479 Epoch 6 Batch 186/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.951, Loss: 1.385 Epoch 6 Batch 187/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.951, Loss: 1.437 Epoch 6 Batch 188/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.951, Loss: 1.421 Epoch 6 Batch 189/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.951, Loss: 1.416 Epoch 6 Batch 190/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.957, Loss: 1.428 Epoch 6 Batch 191/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.952, Loss: 1.429 Epoch 6 Batch 192/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.954, Loss: 1.453 Epoch 6 Batch 193/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.955, Loss: 1.462 Epoch 6 Batch 194/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.955, Loss: 1.420 Epoch 6 Batch 195/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.950, Loss: 1.382 Epoch 6 Batch 196/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.950, Loss: 1.420 Epoch 6 Batch 197/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.955, Loss: 1.424 Epoch 6 Batch 198/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.955, Loss: 1.399 Epoch 6 Batch 199/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.950, Loss: 1.518 Epoch 6 Batch 200/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.942, Loss: 1.457 Epoch 6 Batch 201/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.944, Loss: 1.334 Epoch 6 Batch 202/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.945, Loss: 1.397 Epoch 6 Batch 203/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.945, Loss: 1.418 Epoch 6 Batch 204/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.945, Loss: 1.362 Epoch 6 Batch 205/1077 - Train Accuracy: 0.921, Validation Accuracy: 0.952, Loss: 1.481 Epoch 6 Batch 206/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.947, Loss: 1.405 Epoch 6 Batch 207/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.946, Loss: 1.384 Epoch 6 Batch 208/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.940, Loss: 1.444 Epoch 6 Batch 209/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.935, Loss: 1.469 Epoch 6 Batch 210/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.930, Loss: 1.438 Epoch 6 Batch 211/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.925, Loss: 1.481 Epoch 6 Batch 212/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.920, Loss: 1.396 Epoch 6 Batch 213/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.926, Loss: 1.490 Epoch 6 Batch 214/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.936, Loss: 1.520 Epoch 6 Batch 215/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.942, Loss: 1.400 Epoch 6 Batch 216/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.942, Loss: 1.418 Epoch 6 Batch 217/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.941, Loss: 1.423 Epoch 6 Batch 218/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.941, Loss: 1.424 Epoch 6 Batch 219/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.942, Loss: 1.363 Epoch 6 Batch 220/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.944, Loss: 1.452 Epoch 6 Batch 221/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.947, Loss: 1.375 Epoch 6 Batch 222/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.941, Loss: 1.409 Epoch 6 Batch 223/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.951, Loss: 1.476 Epoch 6 Batch 224/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.950, Loss: 1.485 Epoch 6 Batch 225/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.951, Loss: 1.431 Epoch 6 Batch 226/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.464 Epoch 6 Batch 227/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.956, Loss: 1.406 Epoch 6 Batch 228/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.954, Loss: 1.542 Epoch 6 Batch 229/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.954, Loss: 1.352 Epoch 6 Batch 230/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.959, Loss: 1.415 Epoch 6 Batch 231/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.957, Loss: 1.548 Epoch 6 Batch 232/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.438 Epoch 6 Batch 233/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.956, Loss: 1.511 Epoch 6 Batch 234/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.956, Loss: 1.474 Epoch 6 Batch 235/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.956, Loss: 1.458 Epoch 6 Batch 236/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.957, Loss: 1.452 Epoch 6 Batch 237/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.947, Loss: 1.378 Epoch 6 Batch 238/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.947, Loss: 1.515 Epoch 6 Batch 239/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.951, Loss: 1.438 Epoch 6 Batch 240/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.947, Loss: 1.480 Epoch 6 Batch 241/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.947, Loss: 1.449 Epoch 6 Batch 242/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.942, Loss: 1.421 Epoch 6 Batch 243/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.942, Loss: 1.442 Epoch 6 Batch 244/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.942, Loss: 1.454 Epoch 6 Batch 245/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.943, Loss: 1.410 Epoch 6 Batch 246/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.943, Loss: 1.405 Epoch 6 Batch 247/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.942, Loss: 1.456 Epoch 6 Batch 248/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.952, Loss: 1.489 Epoch 6 Batch 249/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.950, Loss: 1.474 Epoch 6 Batch 250/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.947, Loss: 1.425 Epoch 6 Batch 251/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.425 Epoch 6 Batch 252/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.942, Loss: 1.366 Epoch 6 Batch 253/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.942, Loss: 1.397 Epoch 6 Batch 254/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.942, Loss: 1.458 Epoch 6 Batch 255/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.948, Loss: 1.527 Epoch 6 Batch 256/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.946, Loss: 1.439 Epoch 6 Batch 257/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.951, Loss: 1.422 Epoch 6 Batch 258/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.946, Loss: 1.505 Epoch 6 Batch 259/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.951, Loss: 1.399 Epoch 6 Batch 260/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.951, Loss: 1.420 Epoch 6 Batch 261/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.955, Loss: 1.453 Epoch 6 Batch 262/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.961, Loss: 1.399 Epoch 6 Batch 263/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.963, Loss: 1.422 Epoch 6 Batch 264/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.958, Loss: 1.356 Epoch 6 Batch 265/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.952, Loss: 1.429 Epoch 6 Batch 266/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.946, Loss: 1.412 Epoch 6 Batch 267/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.946, Loss: 1.401 Epoch 6 Batch 268/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.941, Loss: 1.487 Epoch 6 Batch 269/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.444 Epoch 6 Batch 270/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.941, Loss: 1.506 Epoch 6 Batch 271/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.950, Loss: 1.387 Epoch 6 Batch 272/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.940, Loss: 1.512 Epoch 6 Batch 273/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.938, Loss: 1.438 Epoch 6 Batch 274/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.938, Loss: 1.486 Epoch 6 Batch 275/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.935, Loss: 1.482 Epoch 6 Batch 276/1077 - Train Accuracy: 0.927, Validation Accuracy: 0.931, Loss: 1.447 Epoch 6 Batch 277/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.935, Loss: 1.401 Epoch 6 Batch 278/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.935, Loss: 1.415 Epoch 6 Batch 279/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.942, Loss: 1.410 Epoch 6 Batch 280/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.420 Epoch 6 Batch 281/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.460 Epoch 6 Batch 282/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.947, Loss: 1.402 Epoch 6 Batch 283/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.947, Loss: 1.432 Epoch 6 Batch 284/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.942, Loss: 1.381 Epoch 6 Batch 285/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.948, Loss: 1.392 Epoch 6 Batch 286/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.944, Loss: 1.509 Epoch 6 Batch 287/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.944, Loss: 1.452 Epoch 6 Batch 288/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.950, Loss: 1.384 Epoch 6 Batch 289/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.438 Epoch 6 Batch 290/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.952, Loss: 1.426 Epoch 6 Batch 291/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.943, Loss: 1.490 Epoch 6 Batch 292/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.943, Loss: 1.448 Epoch 6 Batch 293/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.953, Loss: 1.402 Epoch 6 Batch 294/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.455 Epoch 6 Batch 295/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.947, Loss: 1.460 Epoch 6 Batch 296/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.951, Loss: 1.346 Epoch 6 Batch 297/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.952, Loss: 1.415 Epoch 6 Batch 298/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.950, Loss: 1.500 Epoch 6 Batch 299/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.953, Loss: 1.388 Epoch 6 Batch 300/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.390 Epoch 6 Batch 301/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.957, Loss: 1.394 Epoch 6 Batch 302/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.957, Loss: 1.398 Epoch 6 Batch 303/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.382 Epoch 6 Batch 304/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.950, Loss: 1.388 Epoch 6 Batch 305/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.950, Loss: 1.421 Epoch 6 Batch 306/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.502 Epoch 6 Batch 307/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.392 Epoch 6 Batch 308/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.953, Loss: 1.459 Epoch 6 Batch 309/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.954, Loss: 1.338 Epoch 6 Batch 310/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.942, Loss: 1.426 Epoch 6 Batch 311/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.942, Loss: 1.449 Epoch 6 Batch 312/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.932, Loss: 1.517 Epoch 6 Batch 313/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.932, Loss: 1.412 Epoch 6 Batch 314/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.932, Loss: 1.432 Epoch 6 Batch 315/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.934, Loss: 1.503 Epoch 6 Batch 316/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.940, Loss: 1.370 Epoch 6 Batch 317/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.940, Loss: 1.446 Epoch 6 Batch 318/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.938, Loss: 1.401 Epoch 6 Batch 319/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.935, Loss: 1.432 Epoch 6 Batch 320/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.933, Loss: 1.449 Epoch 6 Batch 321/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.933, Loss: 1.393 Epoch 6 Batch 322/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.945, Loss: 1.423 Epoch 6 Batch 323/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.941, Loss: 1.444 Epoch 6 Batch 324/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.946, Loss: 1.435 Epoch 6 Batch 325/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.487 Epoch 6 Batch 326/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.390 Epoch 6 Batch 327/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.956, Loss: 1.370 Epoch 6 Batch 328/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.960, Loss: 1.411 Epoch 6 Batch 329/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.967, Loss: 1.503 Epoch 6 Batch 330/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.956, Loss: 1.445 Epoch 6 Batch 331/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.953, Loss: 1.472 Epoch 6 Batch 332/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.953, Loss: 1.425 Epoch 6 Batch 333/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.953, Loss: 1.457 Epoch 6 Batch 334/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.481 Epoch 6 Batch 335/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.374 Epoch 6 Batch 336/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.954, Loss: 1.473 Epoch 6 Batch 337/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.394 Epoch 6 Batch 338/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.950, Loss: 1.454 Epoch 6 Batch 339/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.945, Loss: 1.411 Epoch 6 Batch 340/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.944, Loss: 1.417 Epoch 6 Batch 341/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.943, Loss: 1.403 Epoch 6 Batch 342/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.948, Loss: 1.425 Epoch 6 Batch 343/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.948, Loss: 1.441 Epoch 6 Batch 344/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.954, Loss: 1.457 Epoch 6 Batch 345/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.958, Loss: 1.472 Epoch 6 Batch 346/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.957, Loss: 1.471 Epoch 6 Batch 347/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.949, Loss: 1.399 Epoch 6 Batch 348/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.958, Loss: 1.359 Epoch 6 Batch 349/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.958, Loss: 1.414 Epoch 6 Batch 350/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.943, Loss: 1.478 Epoch 6 Batch 351/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.941, Loss: 1.430 Epoch 6 Batch 352/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.941, Loss: 1.485 Epoch 6 Batch 353/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.941, Loss: 1.413 Epoch 6 Batch 354/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.935, Loss: 1.456 Epoch 6 Batch 355/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.935, Loss: 1.401 Epoch 6 Batch 356/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.940, Loss: 1.497 Epoch 6 Batch 357/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.936, Loss: 1.463 Epoch 6 Batch 358/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.952, Loss: 1.479 Epoch 6 Batch 359/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.952, Loss: 1.442 Epoch 6 Batch 360/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.952, Loss: 1.427 Epoch 6 Batch 361/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.950, Loss: 1.533 Epoch 6 Batch 362/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.944, Loss: 1.445 Epoch 6 Batch 363/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.939, Loss: 1.413 Epoch 6 Batch 364/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.942, Loss: 1.462 Epoch 6 Batch 365/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.942, Loss: 1.501 Epoch 6 Batch 366/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.942, Loss: 1.471 Epoch 6 Batch 367/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.937, Loss: 1.434 Epoch 6 Batch 368/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.932, Loss: 1.407 Epoch 6 Batch 369/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.928, Loss: 1.483 Epoch 6 Batch 370/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.932, Loss: 1.360 Epoch 6 Batch 371/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.927, Loss: 1.488 Epoch 6 Batch 372/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.932, Loss: 1.382 Epoch 6 Batch 373/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.932, Loss: 1.356 Epoch 6 Batch 374/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.935, Loss: 1.393 Epoch 6 Batch 375/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.941, Loss: 1.427 Epoch 6 Batch 376/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.941, Loss: 1.437 Epoch 6 Batch 377/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.947, Loss: 1.492 Epoch 6 Batch 378/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.947, Loss: 1.432 Epoch 6 Batch 379/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.952, Loss: 1.479 Epoch 6 Batch 380/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.951, Loss: 1.451 Epoch 6 Batch 381/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.946, Loss: 1.460 Epoch 6 Batch 382/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.951, Loss: 1.401 Epoch 6 Batch 383/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.952, Loss: 1.386 Epoch 6 Batch 384/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.417 Epoch 6 Batch 385/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.949, Loss: 1.467 Epoch 6 Batch 386/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.949, Loss: 1.432 Epoch 6 Batch 387/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.940, Loss: 1.404 Epoch 6 Batch 388/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.940, Loss: 1.403 Epoch 6 Batch 389/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.945, Loss: 1.496 Epoch 6 Batch 390/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.950, Loss: 1.524 Epoch 6 Batch 391/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.959, Loss: 1.417 Epoch 6 Batch 392/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.476 Epoch 6 Batch 393/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.958, Loss: 1.426 Epoch 6 Batch 394/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.960, Loss: 1.463 Epoch 6 Batch 395/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.965, Loss: 1.440 Epoch 6 Batch 396/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.960, Loss: 1.387 Epoch 6 Batch 397/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.952, Loss: 1.447 Epoch 6 Batch 398/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.946, Loss: 1.514 Epoch 6 Batch 399/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.937, Loss: 1.363 Epoch 6 Batch 400/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.946, Loss: 1.493 Epoch 6 Batch 401/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.941, Loss: 1.498 Epoch 6 Batch 402/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.940, Loss: 1.488 Epoch 6 Batch 403/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.936, Loss: 1.548 Epoch 6 Batch 404/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.943, Loss: 1.422 Epoch 6 Batch 405/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.936, Loss: 1.466 Epoch 6 Batch 406/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.934, Loss: 1.485 Epoch 6 Batch 407/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.934, Loss: 1.430 Epoch 6 Batch 408/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.944, Loss: 1.402 Epoch 6 Batch 409/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.947, Loss: 1.441 Epoch 6 Batch 410/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.947, Loss: 1.508 Epoch 6 Batch 411/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.944, Loss: 1.418 Epoch 6 Batch 412/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.949, Loss: 1.500 Epoch 6 Batch 413/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.949, Loss: 1.529 Epoch 6 Batch 414/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.949, Loss: 1.437 Epoch 6 Batch 415/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.949, Loss: 1.395 Epoch 6 Batch 416/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.949, Loss: 1.426 Epoch 6 Batch 417/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.475 Epoch 6 Batch 418/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.949, Loss: 1.432 Epoch 6 Batch 419/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.949, Loss: 1.452 Epoch 6 Batch 420/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.950, Loss: 1.329 Epoch 6 Batch 421/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.944, Loss: 1.425 Epoch 6 Batch 422/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.949, Loss: 1.485 Epoch 6 Batch 423/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.944, Loss: 1.442 Epoch 6 Batch 424/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.950, Loss: 1.384 Epoch 6 Batch 425/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.950, Loss: 1.430 Epoch 6 Batch 426/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.450 Epoch 6 Batch 427/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.949, Loss: 1.447 Epoch 6 Batch 428/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.949, Loss: 1.410 Epoch 6 Batch 429/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.949, Loss: 1.426 Epoch 6 Batch 430/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.949, Loss: 1.434 Epoch 6 Batch 431/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.949, Loss: 1.432 Epoch 6 Batch 432/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.951, Loss: 1.400 Epoch 6 Batch 433/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.943, Loss: 1.395 Epoch 6 Batch 434/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.939, Loss: 1.384 Epoch 6 Batch 435/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.941, Loss: 1.458 Epoch 6 Batch 436/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.941, Loss: 1.445 Epoch 6 Batch 437/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.941, Loss: 1.452 Epoch 6 Batch 438/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.942, Loss: 1.452 Epoch 6 Batch 439/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.943, Loss: 1.397 Epoch 6 Batch 440/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.942, Loss: 1.453 Epoch 6 Batch 441/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.942, Loss: 1.502 Epoch 6 Batch 442/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.946, Loss: 1.488 Epoch 6 Batch 443/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.442 Epoch 6 Batch 444/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.947, Loss: 1.419 Epoch 6 Batch 445/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.948, Loss: 1.460 Epoch 6 Batch 446/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.942, Loss: 1.429 Epoch 6 Batch 447/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.942, Loss: 1.527 Epoch 6 Batch 448/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.941, Loss: 1.437 Epoch 6 Batch 449/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.931, Loss: 1.452 Epoch 6 Batch 450/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.931, Loss: 1.427 Epoch 6 Batch 451/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.933, Loss: 1.319 Epoch 6 Batch 452/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.933, Loss: 1.472 Epoch 6 Batch 453/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.938, Loss: 1.519 Epoch 6 Batch 454/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.940, Loss: 1.453 Epoch 6 Batch 455/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.940, Loss: 1.412 Epoch 6 Batch 456/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.934, Loss: 1.485 Epoch 6 Batch 457/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.934, Loss: 1.412 Epoch 6 Batch 458/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.939, Loss: 1.365 Epoch 6 Batch 459/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.945, Loss: 1.434 Epoch 6 Batch 460/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.425 Epoch 6 Batch 461/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.949, Loss: 1.434 Epoch 6 Batch 462/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.944, Loss: 1.445 Epoch 6 Batch 463/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.943, Loss: 1.416 Epoch 6 Batch 464/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.944, Loss: 1.433 Epoch 6 Batch 465/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.939, Loss: 1.461 Epoch 6 Batch 466/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.939, Loss: 1.466 Epoch 6 Batch 467/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.939, Loss: 1.445 Epoch 6 Batch 468/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.944, Loss: 1.425 Epoch 6 Batch 469/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.946, Loss: 1.530 Epoch 6 Batch 470/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.945, Loss: 1.462 Epoch 6 Batch 471/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.945, Loss: 1.395 Epoch 6 Batch 472/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.945, Loss: 1.377 Epoch 6 Batch 473/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.940, Loss: 1.473 Epoch 6 Batch 474/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.945, Loss: 1.449 Epoch 6 Batch 475/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.936, Loss: 1.437 Epoch 6 Batch 476/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.945, Loss: 1.297 Epoch 6 Batch 477/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.944, Loss: 1.444 Epoch 6 Batch 478/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.415 Epoch 6 Batch 479/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.435 Epoch 6 Batch 480/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.958, Loss: 1.430 Epoch 6 Batch 481/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.958, Loss: 1.446 Epoch 6 Batch 482/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.962, Loss: 1.357 Epoch 6 Batch 483/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.958, Loss: 1.444 Epoch 6 Batch 484/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.962, Loss: 1.453 Epoch 6 Batch 485/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.962, Loss: 1.381 Epoch 6 Batch 486/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.446 Epoch 6 Batch 487/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.963, Loss: 1.311 Epoch 6 Batch 488/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.953, Loss: 1.449 Epoch 6 Batch 489/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.958, Loss: 1.394 Epoch 6 Batch 490/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.958, Loss: 1.516 Epoch 6 Batch 491/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.951, Loss: 1.454 Epoch 6 Batch 492/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.945, Loss: 1.413 Epoch 6 Batch 493/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.433 Epoch 6 Batch 494/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.945, Loss: 1.391 Epoch 6 Batch 495/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.945, Loss: 1.394 Epoch 6 Batch 496/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.945, Loss: 1.458 Epoch 6 Batch 497/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.945, Loss: 1.521 Epoch 6 Batch 498/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.448 Epoch 6 Batch 499/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.952, Loss: 1.411 Epoch 6 Batch 500/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.952, Loss: 1.454 Epoch 6 Batch 501/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.952, Loss: 1.489 Epoch 6 Batch 502/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.962, Loss: 1.443 Epoch 6 Batch 503/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.962, Loss: 1.416 Epoch 6 Batch 504/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.962, Loss: 1.512 Epoch 6 Batch 505/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.424 Epoch 6 Batch 506/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.958, Loss: 1.432 Epoch 6 Batch 507/1077 - Train Accuracy: 0.907, Validation Accuracy: 0.953, Loss: 1.404 Epoch 6 Batch 508/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.953, Loss: 1.416 Epoch 6 Batch 509/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.953, Loss: 1.422 Epoch 6 Batch 510/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.953, Loss: 1.434 Epoch 6 Batch 511/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.949, Loss: 1.455 Epoch 6 Batch 512/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.949, Loss: 1.357 Epoch 6 Batch 513/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.949, Loss: 1.436 Epoch 6 Batch 514/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.942, Loss: 1.409 Epoch 6 Batch 515/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.942, Loss: 1.470 Epoch 6 Batch 516/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.944, Loss: 1.466 Epoch 6 Batch 517/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.937, Loss: 1.393 Epoch 6 Batch 518/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.937, Loss: 1.452 Epoch 6 Batch 519/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.937, Loss: 1.495 Epoch 6 Batch 520/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.950, Loss: 1.409 Epoch 6 Batch 521/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.955, Loss: 1.450 Epoch 6 Batch 522/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.955, Loss: 1.393 Epoch 6 Batch 523/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.454 Epoch 6 Batch 524/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.949, Loss: 1.368 Epoch 6 Batch 525/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.953, Loss: 1.531 Epoch 6 Batch 526/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.423 Epoch 6 Batch 527/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.951, Loss: 1.457 Epoch 6 Batch 528/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.951, Loss: 1.389 Epoch 6 Batch 529/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.954, Loss: 1.443 Epoch 6 Batch 530/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.954, Loss: 1.385 Epoch 6 Batch 531/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.959, Loss: 1.449 Epoch 6 Batch 532/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.950, Loss: 1.441 Epoch 6 Batch 533/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.946, Loss: 1.521 Epoch 6 Batch 534/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.937, Loss: 1.381 Epoch 6 Batch 535/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.946, Loss: 1.363 Epoch 6 Batch 536/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.941, Loss: 1.464 Epoch 6 Batch 537/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.950, Loss: 1.490 Epoch 6 Batch 538/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.950, Loss: 1.404 Epoch 6 Batch 539/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.949, Loss: 1.507 Epoch 6 Batch 540/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.954, Loss: 1.403 Epoch 6 Batch 541/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.476 Epoch 6 Batch 542/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.958, Loss: 1.395 Epoch 6 Batch 543/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.963, Loss: 1.407 Epoch 6 Batch 544/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.958, Loss: 1.461 Epoch 6 Batch 545/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.960, Loss: 1.503 Epoch 6 Batch 546/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.957, Loss: 1.463 Epoch 6 Batch 547/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.957, Loss: 1.395 Epoch 6 Batch 548/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.962, Loss: 1.502 Epoch 6 Batch 549/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.962, Loss: 1.434 Epoch 6 Batch 550/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.956, Loss: 1.441 Epoch 6 Batch 551/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.958, Loss: 1.433 Epoch 6 Batch 552/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.949, Loss: 1.465 Epoch 6 Batch 553/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.949, Loss: 1.449 Epoch 6 Batch 554/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.947, Loss: 1.420 Epoch 6 Batch 555/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.952, Loss: 1.393 Epoch 6 Batch 556/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.952, Loss: 1.439 Epoch 6 Batch 557/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.947, Loss: 1.401 Epoch 6 Batch 558/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.439 Epoch 6 Batch 559/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.959, Loss: 1.366 Epoch 6 Batch 560/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.960, Loss: 1.393 Epoch 6 Batch 561/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.402 Epoch 6 Batch 562/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.448 Epoch 6 Batch 563/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.949, Loss: 1.486 Epoch 6 Batch 564/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.949, Loss: 1.403 Epoch 6 Batch 565/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.945, Loss: 1.453 Epoch 6 Batch 566/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.945, Loss: 1.447 Epoch 6 Batch 567/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.452 Epoch 6 Batch 568/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.953, Loss: 1.434 Epoch 6 Batch 569/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.953, Loss: 1.402 Epoch 6 Batch 570/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.953, Loss: 1.392 Epoch 6 Batch 571/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.949, Loss: 1.423 Epoch 6 Batch 572/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.953, Loss: 1.375 Epoch 6 Batch 573/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.958, Loss: 1.384 Epoch 6 Batch 574/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.958, Loss: 1.494 Epoch 6 Batch 575/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.953, Loss: 1.427 Epoch 6 Batch 576/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.953, Loss: 1.454 Epoch 6 Batch 577/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.951, Loss: 1.414 Epoch 6 Batch 578/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.951, Loss: 1.419 Epoch 6 Batch 579/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.413 Epoch 6 Batch 580/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.950, Loss: 1.434 Epoch 6 Batch 581/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.950, Loss: 1.445 Epoch 6 Batch 582/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.950, Loss: 1.400 Epoch 6 Batch 583/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.945, Loss: 1.366 Epoch 6 Batch 584/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.945, Loss: 1.415 Epoch 6 Batch 585/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.943, Loss: 1.429 Epoch 6 Batch 586/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.417 Epoch 6 Batch 587/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.952, Loss: 1.460 Epoch 6 Batch 588/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.954, Loss: 1.493 Epoch 6 Batch 589/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.950, Loss: 1.370 Epoch 6 Batch 590/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.947, Loss: 1.474 Epoch 6 Batch 591/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.953, Loss: 1.503 Epoch 6 Batch 592/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.953, Loss: 1.461 Epoch 6 Batch 593/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.953, Loss: 1.516 Epoch 6 Batch 594/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.944, Loss: 1.403 Epoch 6 Batch 595/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.947, Loss: 1.410 Epoch 6 Batch 596/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.949, Loss: 1.420 Epoch 6 Batch 597/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.954, Loss: 1.498 Epoch 6 Batch 598/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.955, Loss: 1.457 Epoch 6 Batch 599/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.955, Loss: 1.403 Epoch 6 Batch 600/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.949, Loss: 1.420 Epoch 6 Batch 601/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.949, Loss: 1.381 Epoch 6 Batch 602/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.956, Loss: 1.397 Epoch 6 Batch 603/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.428 Epoch 6 Batch 604/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.955, Loss: 1.449 Epoch 6 Batch 605/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.469 Epoch 6 Batch 606/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.425 Epoch 6 Batch 607/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.960, Loss: 1.418 Epoch 6 Batch 608/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.411 Epoch 6 Batch 609/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.957, Loss: 1.544 Epoch 6 Batch 610/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.957, Loss: 1.456 Epoch 6 Batch 611/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.960, Loss: 1.496 Epoch 6 Batch 612/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.476 Epoch 6 Batch 613/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.955, Loss: 1.494 Epoch 6 Batch 614/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.955, Loss: 1.426 Epoch 6 Batch 615/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.949, Loss: 1.483 Epoch 6 Batch 616/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.949, Loss: 1.536 Epoch 6 Batch 617/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.945, Loss: 1.414 Epoch 6 Batch 618/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.945, Loss: 1.444 Epoch 6 Batch 619/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.947, Loss: 1.420 Epoch 6 Batch 620/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.945, Loss: 1.325 Epoch 6 Batch 621/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.945, Loss: 1.443 Epoch 6 Batch 622/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.945, Loss: 1.424 Epoch 6 Batch 623/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.947, Loss: 1.439 Epoch 6 Batch 624/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.952, Loss: 1.481 Epoch 6 Batch 625/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.957, Loss: 1.412 Epoch 6 Batch 626/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.948, Loss: 1.452 Epoch 6 Batch 627/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.948, Loss: 1.355 Epoch 6 Batch 628/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.957, Loss: 1.408 Epoch 6 Batch 629/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.957, Loss: 1.355 Epoch 6 Batch 630/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.957, Loss: 1.397 Epoch 6 Batch 631/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.957, Loss: 1.430 Epoch 6 Batch 632/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.951, Loss: 1.454 Epoch 6 Batch 633/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.951, Loss: 1.397 Epoch 6 Batch 634/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.432 Epoch 6 Batch 635/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.949, Loss: 1.469 Epoch 6 Batch 636/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.949, Loss: 1.424 Epoch 6 Batch 637/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.951, Loss: 1.503 Epoch 6 Batch 638/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.951, Loss: 1.438 Epoch 6 Batch 639/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.951, Loss: 1.463 Epoch 6 Batch 640/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.356 Epoch 6 Batch 641/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.956, Loss: 1.525 Epoch 6 Batch 642/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.956, Loss: 1.479 Epoch 6 Batch 643/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.385 Epoch 6 Batch 644/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.962, Loss: 1.354 Epoch 6 Batch 645/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.951, Loss: 1.465 Epoch 6 Batch 646/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.945, Loss: 1.442 Epoch 6 Batch 647/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.945, Loss: 1.419 Epoch 6 Batch 648/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.460 Epoch 6 Batch 649/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.940, Loss: 1.377 Epoch 6 Batch 650/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.477 Epoch 6 Batch 651/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.947, Loss: 1.425 Epoch 6 Batch 652/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.942, Loss: 1.406 Epoch 6 Batch 653/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.949, Loss: 1.482 Epoch 6 Batch 654/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.940, Loss: 1.422 Epoch 6 Batch 655/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.940, Loss: 1.376 Epoch 6 Batch 656/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.940, Loss: 1.417 Epoch 6 Batch 657/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.944, Loss: 1.390 Epoch 6 Batch 658/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.407 Epoch 6 Batch 659/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.951, Loss: 1.458 Epoch 6 Batch 660/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.950, Loss: 1.485 Epoch 6 Batch 661/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.952, Loss: 1.484 Epoch 6 Batch 662/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.947, Loss: 1.399 Epoch 6 Batch 663/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.954, Loss: 1.392 Epoch 6 Batch 664/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.949, Loss: 1.456 Epoch 6 Batch 665/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.951, Loss: 1.350 Epoch 6 Batch 666/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.941, Loss: 1.328 Epoch 6 Batch 667/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.932, Loss: 1.392 Epoch 6 Batch 668/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.934, Loss: 1.407 Epoch 6 Batch 669/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.939, Loss: 1.435 Epoch 6 Batch 670/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.938, Loss: 1.386 Epoch 6 Batch 671/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.947, Loss: 1.424 Epoch 6 Batch 672/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.953, Loss: 1.450 Epoch 6 Batch 673/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.953, Loss: 1.478 Epoch 6 Batch 674/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.952, Loss: 1.453 Epoch 6 Batch 675/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.952, Loss: 1.490 Epoch 6 Batch 676/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.953, Loss: 1.382 Epoch 6 Batch 677/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.953, Loss: 1.429 Epoch 6 Batch 678/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.396 Epoch 6 Batch 679/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.963, Loss: 1.415 Epoch 6 Batch 680/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.955, Loss: 1.467 Epoch 6 Batch 681/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.952, Loss: 1.436 Epoch 6 Batch 682/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.952, Loss: 1.370 Epoch 6 Batch 683/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.957, Loss: 1.368 Epoch 6 Batch 684/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.957, Loss: 1.374 Epoch 6 Batch 685/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.957, Loss: 1.453 Epoch 6 Batch 686/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.472 Epoch 6 Batch 687/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.451 Epoch 6 Batch 688/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.951, Loss: 1.420 Epoch 6 Batch 689/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.457 Epoch 6 Batch 690/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.952, Loss: 1.407 Epoch 6 Batch 691/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.961, Loss: 1.321 Epoch 6 Batch 692/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.463 Epoch 6 Batch 693/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.963, Loss: 1.464 Epoch 6 Batch 694/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.963, Loss: 1.449 Epoch 6 Batch 695/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.963, Loss: 1.415 Epoch 6 Batch 696/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.959, Loss: 1.383 Epoch 6 Batch 697/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.964, Loss: 1.420 Epoch 6 Batch 698/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.959, Loss: 1.469 Epoch 6 Batch 699/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.428 Epoch 6 Batch 700/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.959, Loss: 1.443 Epoch 6 Batch 701/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.954, Loss: 1.458 Epoch 6 Batch 702/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.954, Loss: 1.458 Epoch 6 Batch 703/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.407 Epoch 6 Batch 704/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.951, Loss: 1.440 Epoch 6 Batch 705/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.961, Loss: 1.499 Epoch 6 Batch 706/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.961, Loss: 1.483 Epoch 6 Batch 707/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.443 Epoch 6 Batch 708/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.954, Loss: 1.482 Epoch 6 Batch 709/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.954, Loss: 1.454 Epoch 6 Batch 710/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.949, Loss: 1.365 Epoch 6 Batch 711/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.950, Loss: 1.439 Epoch 6 Batch 712/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.953, Loss: 1.447 Epoch 6 Batch 713/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.956, Loss: 1.386 Epoch 6 Batch 714/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.479 Epoch 6 Batch 715/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.957, Loss: 1.433 Epoch 6 Batch 716/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.957, Loss: 1.422 Epoch 6 Batch 717/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.952, Loss: 1.416 Epoch 6 Batch 718/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.953, Loss: 1.393 Epoch 6 Batch 719/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.950, Loss: 1.526 Epoch 6 Batch 720/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.953, Loss: 1.484 Epoch 6 Batch 721/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.953, Loss: 1.340 Epoch 6 Batch 722/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.957, Loss: 1.469 Epoch 6 Batch 723/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.957, Loss: 1.401 Epoch 6 Batch 724/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.952, Loss: 1.426 Epoch 6 Batch 725/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.947, Loss: 1.467 Epoch 6 Batch 726/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.954, Loss: 1.486 Epoch 6 Batch 727/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.954, Loss: 1.482 Epoch 6 Batch 728/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.954, Loss: 1.435 Epoch 6 Batch 729/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.954, Loss: 1.343 Epoch 6 Batch 730/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.953, Loss: 1.382 Epoch 6 Batch 731/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.949, Loss: 1.382 Epoch 6 Batch 732/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.949, Loss: 1.437 Epoch 6 Batch 733/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.945, Loss: 1.438 Epoch 6 Batch 734/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.945, Loss: 1.429 Epoch 6 Batch 735/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.945, Loss: 1.465 Epoch 6 Batch 736/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.945, Loss: 1.462 Epoch 6 Batch 737/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.940, Loss: 1.461 Epoch 6 Batch 738/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.942, Loss: 1.370 Epoch 6 Batch 739/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.942, Loss: 1.382 Epoch 6 Batch 740/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.946, Loss: 1.392 Epoch 6 Batch 741/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.960, Loss: 1.438 Epoch 6 Batch 742/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.958, Loss: 1.436 Epoch 6 Batch 743/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.955, Loss: 1.427 Epoch 6 Batch 744/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.426 Epoch 6 Batch 745/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.954, Loss: 1.431 Epoch 6 Batch 746/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.954, Loss: 1.392 Epoch 6 Batch 747/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.953, Loss: 1.398 Epoch 6 Batch 748/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.953, Loss: 1.427 Epoch 6 Batch 749/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.438 Epoch 6 Batch 750/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.955, Loss: 1.422 Epoch 6 Batch 751/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.946, Loss: 1.395 Epoch 6 Batch 752/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.952, Loss: 1.398 Epoch 6 Batch 753/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.444 Epoch 6 Batch 754/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.947, Loss: 1.467 Epoch 6 Batch 755/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.947, Loss: 1.505 Epoch 6 Batch 756/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.950, Loss: 1.439 Epoch 6 Batch 757/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.950, Loss: 1.431 Epoch 6 Batch 758/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.950, Loss: 1.380 Epoch 6 Batch 759/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.950, Loss: 1.379 Epoch 6 Batch 760/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.952, Loss: 1.385 Epoch 6 Batch 761/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.950, Loss: 1.438 Epoch 6 Batch 762/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.954, Loss: 1.403 Epoch 6 Batch 763/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.949, Loss: 1.409 Epoch 6 Batch 764/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.954, Loss: 1.443 Epoch 6 Batch 765/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.958, Loss: 1.431 Epoch 6 Batch 766/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.953, Loss: 1.500 Epoch 6 Batch 767/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.958, Loss: 1.415 Epoch 6 Batch 768/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.448 Epoch 6 Batch 769/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.958, Loss: 1.403 Epoch 6 Batch 770/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.954, Loss: 1.465 Epoch 6 Batch 771/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.394 Epoch 6 Batch 772/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.426 Epoch 6 Batch 773/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.953, Loss: 1.483 Epoch 6 Batch 774/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.953, Loss: 1.454 Epoch 6 Batch 775/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.954, Loss: 1.509 Epoch 6 Batch 776/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.957, Loss: 1.382 Epoch 6 Batch 777/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.358 Epoch 6 Batch 778/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.961, Loss: 1.408 Epoch 6 Batch 779/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.964, Loss: 1.404 Epoch 6 Batch 780/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.964, Loss: 1.492 Epoch 6 Batch 781/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.955, Loss: 1.391 Epoch 6 Batch 782/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.959, Loss: 1.485 Epoch 6 Batch 783/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.955, Loss: 1.404 Epoch 6 Batch 784/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.950, Loss: 1.392 Epoch 6 Batch 785/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.950, Loss: 1.385 Epoch 6 Batch 786/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.382 Epoch 6 Batch 787/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.946, Loss: 1.403 Epoch 6 Batch 788/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.951, Loss: 1.386 Epoch 6 Batch 789/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.950, Loss: 1.441 Epoch 6 Batch 790/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.948, Loss: 1.397 Epoch 6 Batch 791/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.948, Loss: 1.512 Epoch 6 Batch 792/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.943, Loss: 1.476 Epoch 6 Batch 793/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.938, Loss: 1.410 Epoch 6 Batch 794/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.932, Loss: 1.405 Epoch 6 Batch 795/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.934, Loss: 1.431 Epoch 6 Batch 796/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.934, Loss: 1.476 Epoch 6 Batch 797/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.940, Loss: 1.418 Epoch 6 Batch 798/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.943, Loss: 1.394 Epoch 6 Batch 799/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.943, Loss: 1.530 Epoch 6 Batch 800/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.950, Loss: 1.469 Epoch 6 Batch 801/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.950, Loss: 1.467 Epoch 6 Batch 802/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.410 Epoch 6 Batch 803/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.965, Loss: 1.362 Epoch 6 Batch 804/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.965, Loss: 1.368 Epoch 6 Batch 805/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.960, Loss: 1.399 Epoch 6 Batch 806/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.956, Loss: 1.391 Epoch 6 Batch 807/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.956, Loss: 1.415 Epoch 6 Batch 808/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.954, Loss: 1.475 Epoch 6 Batch 809/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.952, Loss: 1.404 Epoch 6 Batch 810/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.952, Loss: 1.424 Epoch 6 Batch 811/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.947, Loss: 1.454 Epoch 6 Batch 812/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.436 Epoch 6 Batch 813/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.942, Loss: 1.418 Epoch 6 Batch 814/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.942, Loss: 1.433 Epoch 6 Batch 815/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.941, Loss: 1.514 Epoch 6 Batch 816/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.934, Loss: 1.475 Epoch 6 Batch 817/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.934, Loss: 1.430 Epoch 6 Batch 818/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.934, Loss: 1.462 Epoch 6 Batch 819/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.943, Loss: 1.497 Epoch 6 Batch 820/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.945, Loss: 1.453 Epoch 6 Batch 821/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.407 Epoch 6 Batch 822/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.954, Loss: 1.387 Epoch 6 Batch 823/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.384 Epoch 6 Batch 824/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.387 Epoch 6 Batch 825/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.956, Loss: 1.505 Epoch 6 Batch 826/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.958, Loss: 1.495 Epoch 6 Batch 827/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.949, Loss: 1.463 Epoch 6 Batch 828/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.948, Loss: 1.463 Epoch 6 Batch 829/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.949, Loss: 1.427 Epoch 6 Batch 830/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.942, Loss: 1.459 Epoch 6 Batch 831/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.950, Loss: 1.500 Epoch 6 Batch 832/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.357 Epoch 6 Batch 833/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.949, Loss: 1.447 Epoch 6 Batch 834/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.948, Loss: 1.378 Epoch 6 Batch 835/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.949, Loss: 1.423 Epoch 6 Batch 836/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.949, Loss: 1.497 Epoch 6 Batch 837/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.945, Loss: 1.434 Epoch 6 Batch 838/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.945, Loss: 1.379 Epoch 6 Batch 839/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.944, Loss: 1.426 Epoch 6 Batch 840/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.951, Loss: 1.416 Epoch 6 Batch 841/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.951, Loss: 1.367 Epoch 6 Batch 842/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.951, Loss: 1.462 Epoch 6 Batch 843/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.951, Loss: 1.479 Epoch 6 Batch 844/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.946, Loss: 1.489 Epoch 6 Batch 845/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.943, Loss: 1.452 Epoch 6 Batch 846/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.943, Loss: 1.442 Epoch 6 Batch 847/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.947, Loss: 1.489 Epoch 6 Batch 848/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.954, Loss: 1.538 Epoch 6 Batch 849/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.952, Loss: 1.536 Epoch 6 Batch 850/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.954, Loss: 1.451 Epoch 6 Batch 851/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.951, Loss: 1.393 Epoch 6 Batch 852/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.951, Loss: 1.412 Epoch 6 Batch 853/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.949, Loss: 1.474 Epoch 6 Batch 854/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.949, Loss: 1.539 Epoch 6 Batch 855/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.944, Loss: 1.516 Epoch 6 Batch 856/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.946, Loss: 1.432 Epoch 6 Batch 857/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.946, Loss: 1.393 Epoch 6 Batch 858/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.946, Loss: 1.463 Epoch 6 Batch 859/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.951, Loss: 1.422 Epoch 6 Batch 860/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.955, Loss: 1.372 Epoch 6 Batch 861/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.951, Loss: 1.444 Epoch 6 Batch 862/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.951, Loss: 1.525 Epoch 6 Batch 863/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.955, Loss: 1.419 Epoch 6 Batch 864/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.952, Loss: 1.434 Epoch 6 Batch 865/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.957, Loss: 1.401 Epoch 6 Batch 866/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.957, Loss: 1.435 Epoch 6 Batch 867/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.945, Loss: 1.465 Epoch 6 Batch 868/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.949, Loss: 1.459 Epoch 6 Batch 869/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.949, Loss: 1.404 Epoch 6 Batch 870/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.958, Loss: 1.438 Epoch 6 Batch 871/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.958, Loss: 1.492 Epoch 6 Batch 872/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.958, Loss: 1.466 Epoch 6 Batch 873/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.958, Loss: 1.436 Epoch 6 Batch 874/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.956, Loss: 1.377 Epoch 6 Batch 875/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.960, Loss: 1.407 Epoch 6 Batch 876/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.956, Loss: 1.458 Epoch 6 Batch 877/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.956, Loss: 1.375 Epoch 6 Batch 878/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.947, Loss: 1.392 Epoch 6 Batch 879/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.943, Loss: 1.425 Epoch 6 Batch 880/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.446 Epoch 6 Batch 881/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.947, Loss: 1.447 Epoch 6 Batch 882/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.959, Loss: 1.381 Epoch 6 Batch 883/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.954, Loss: 1.556 Epoch 6 Batch 884/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.953, Loss: 1.446 Epoch 6 Batch 885/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.948, Loss: 1.383 Epoch 6 Batch 886/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.952, Loss: 1.458 Epoch 6 Batch 887/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.953, Loss: 1.385 Epoch 6 Batch 888/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.958, Loss: 1.456 Epoch 6 Batch 889/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.956, Loss: 1.450 Epoch 6 Batch 890/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.955, Loss: 1.439 Epoch 6 Batch 891/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.419 Epoch 6 Batch 892/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.952, Loss: 1.461 Epoch 6 Batch 893/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.952, Loss: 1.399 Epoch 6 Batch 894/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.947, Loss: 1.394 Epoch 6 Batch 895/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.947, Loss: 1.428 Epoch 6 Batch 896/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.947, Loss: 1.399 Epoch 6 Batch 897/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.947, Loss: 1.467 Epoch 6 Batch 898/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.952, Loss: 1.448 Epoch 6 Batch 899/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.952, Loss: 1.394 Epoch 6 Batch 900/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.952, Loss: 1.511 Epoch 6 Batch 901/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.952, Loss: 1.459 Epoch 6 Batch 902/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.953, Loss: 1.461 Epoch 6 Batch 903/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.953, Loss: 1.492 Epoch 6 Batch 904/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.959, Loss: 1.476 Epoch 6 Batch 905/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.960, Loss: 1.450 Epoch 6 Batch 906/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.960, Loss: 1.429 Epoch 6 Batch 907/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.425 Epoch 6 Batch 908/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.960, Loss: 1.509 Epoch 6 Batch 909/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.960, Loss: 1.454 Epoch 6 Batch 910/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.960, Loss: 1.417 Epoch 6 Batch 911/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.957, Loss: 1.438 Epoch 6 Batch 912/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.956, Loss: 1.437 Epoch 6 Batch 913/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.956, Loss: 1.489 Epoch 6 Batch 914/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.469 Epoch 6 Batch 915/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.954, Loss: 1.453 Epoch 6 Batch 916/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.965, Loss: 1.445 Epoch 6 Batch 917/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.965, Loss: 1.328 Epoch 6 Batch 918/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.965, Loss: 1.478 Epoch 6 Batch 919/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.961, Loss: 1.489 Epoch 6 Batch 920/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.956, Loss: 1.406 Epoch 6 Batch 921/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.956, Loss: 1.472 Epoch 6 Batch 922/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.961, Loss: 1.393 Epoch 6 Batch 923/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.456 Epoch 6 Batch 924/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.972, Loss: 1.368 Epoch 6 Batch 925/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.963, Loss: 1.450 Epoch 6 Batch 926/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.963, Loss: 1.490 Epoch 6 Batch 927/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.963, Loss: 1.427 Epoch 6 Batch 928/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.956, Loss: 1.511 Epoch 6 Batch 929/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.963, Loss: 1.467 Epoch 6 Batch 930/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.960, Loss: 1.495 Epoch 6 Batch 931/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.960, Loss: 1.407 Epoch 6 Batch 932/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.960, Loss: 1.407 Epoch 6 Batch 933/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.953, Loss: 1.439 Epoch 6 Batch 934/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.434 Epoch 6 Batch 935/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.960, Loss: 1.458 Epoch 6 Batch 936/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.960, Loss: 1.507 Epoch 6 Batch 937/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.962, Loss: 1.449 Epoch 6 Batch 938/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.419 Epoch 6 Batch 939/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.951, Loss: 1.429 Epoch 6 Batch 940/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.365 Epoch 6 Batch 941/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.447 Epoch 6 Batch 942/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.955, Loss: 1.440 Epoch 6 Batch 943/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.960, Loss: 1.469 Epoch 6 Batch 944/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.960, Loss: 1.432 Epoch 6 Batch 945/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.960, Loss: 1.413 Epoch 6 Batch 946/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.957, Loss: 1.484 Epoch 6 Batch 947/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.957, Loss: 1.487 Epoch 6 Batch 948/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.954, Loss: 1.519 Epoch 6 Batch 949/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.954, Loss: 1.459 Epoch 6 Batch 950/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.465 Epoch 6 Batch 951/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.961, Loss: 1.419 Epoch 6 Batch 952/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.960, Loss: 1.431 Epoch 6 Batch 953/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.960, Loss: 1.488 Epoch 6 Batch 954/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.960, Loss: 1.397 Epoch 6 Batch 955/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.391 Epoch 6 Batch 956/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.947, Loss: 1.474 Epoch 6 Batch 957/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.948, Loss: 1.359 Epoch 6 Batch 958/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.948, Loss: 1.413 Epoch 6 Batch 959/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.943, Loss: 1.335 Epoch 6 Batch 960/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.943, Loss: 1.416 Epoch 6 Batch 961/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.943, Loss: 1.410 Epoch 6 Batch 962/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.938, Loss: 1.360 Epoch 6 Batch 963/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.944, Loss: 1.370 Epoch 6 Batch 964/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.945, Loss: 1.454 Epoch 6 Batch 965/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.955, Loss: 1.465 Epoch 6 Batch 966/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.957, Loss: 1.403 Epoch 6 Batch 967/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.955, Loss: 1.457 Epoch 6 Batch 968/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.955, Loss: 1.421 Epoch 6 Batch 969/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.950, Loss: 1.500 Epoch 6 Batch 970/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.946, Loss: 1.468 Epoch 6 Batch 971/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.946, Loss: 1.475 Epoch 6 Batch 972/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.946, Loss: 1.379 Epoch 6 Batch 973/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.951, Loss: 1.432 Epoch 6 Batch 974/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.957, Loss: 1.427 Epoch 6 Batch 975/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.963, Loss: 1.419 Epoch 6 Batch 976/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.953, Loss: 1.504 Epoch 6 Batch 977/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.953, Loss: 1.468 Epoch 6 Batch 978/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.949, Loss: 1.549 Epoch 6 Batch 979/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.955, Loss: 1.402 Epoch 6 Batch 980/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.953, Loss: 1.506 Epoch 6 Batch 981/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.958, Loss: 1.453 Epoch 6 Batch 982/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.958, Loss: 1.401 Epoch 6 Batch 983/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.958, Loss: 1.411 Epoch 6 Batch 984/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.958, Loss: 1.360 Epoch 6 Batch 985/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.957, Loss: 1.399 Epoch 6 Batch 986/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.957, Loss: 1.411 Epoch 6 Batch 987/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.947, Loss: 1.517 Epoch 6 Batch 988/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.954, Loss: 1.351 Epoch 6 Batch 989/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.383 Epoch 6 Batch 990/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.948, Loss: 1.485 Epoch 6 Batch 991/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.943, Loss: 1.432 Epoch 6 Batch 992/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.943, Loss: 1.438 Epoch 6 Batch 993/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.943, Loss: 1.493 Epoch 6 Batch 994/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.948, Loss: 1.471 Epoch 6 Batch 995/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.949, Loss: 1.481 Epoch 6 Batch 996/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.949, Loss: 1.373 Epoch 6 Batch 997/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.949, Loss: 1.388 Epoch 6 Batch 998/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.939, Loss: 1.393 Epoch 6 Batch 999/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.939, Loss: 1.376 Epoch 6 Batch 1000/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.939, Loss: 1.507 Epoch 6 Batch 1001/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.934, Loss: 1.407 Epoch 6 Batch 1002/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.939, Loss: 1.489 Epoch 6 Batch 1003/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.936, Loss: 1.430 Epoch 6 Batch 1004/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.942, Loss: 1.438 Epoch 6 Batch 1005/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.943, Loss: 1.462 Epoch 6 Batch 1006/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.943, Loss: 1.414 Epoch 6 Batch 1007/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.943, Loss: 1.485 Epoch 6 Batch 1008/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.943, Loss: 1.371 Epoch 6 Batch 1009/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.943, Loss: 1.441 Epoch 6 Batch 1010/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.943, Loss: 1.398 Epoch 6 Batch 1011/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.947, Loss: 1.467 Epoch 6 Batch 1012/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.943, Loss: 1.448 Epoch 6 Batch 1013/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.938, Loss: 1.403 Epoch 6 Batch 1014/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.937, Loss: 1.475 Epoch 6 Batch 1015/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.935, Loss: 1.448 Epoch 6 Batch 1016/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.942, Loss: 1.406 Epoch 6 Batch 1017/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.942, Loss: 1.419 Epoch 6 Batch 1018/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.941, Loss: 1.466 Epoch 6 Batch 1019/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.940, Loss: 1.403 Epoch 6 Batch 1020/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.940, Loss: 1.353 Epoch 6 Batch 1021/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.942, Loss: 1.445 Epoch 6 Batch 1022/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.947, Loss: 1.401 Epoch 6 Batch 1023/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.951, Loss: 1.400 Epoch 6 Batch 1024/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.949, Loss: 1.429 Epoch 6 Batch 1025/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.944, Loss: 1.417 Epoch 6 Batch 1026/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.944, Loss: 1.402 Epoch 6 Batch 1027/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.954, Loss: 1.442 Epoch 6 Batch 1028/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.965, Loss: 1.460 Epoch 6 Batch 1029/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.969, Loss: 1.398 Epoch 6 Batch 1030/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.964, Loss: 1.488 Epoch 6 Batch 1031/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.964, Loss: 1.420 Epoch 6 Batch 1032/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.964, Loss: 1.449 Epoch 6 Batch 1033/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.964, Loss: 1.417 Epoch 6 Batch 1034/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.964, Loss: 1.428 Epoch 6 Batch 1035/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.383 Epoch 6 Batch 1036/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.395 Epoch 6 Batch 1037/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.946, Loss: 1.469 Epoch 6 Batch 1038/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.947, Loss: 1.419 Epoch 6 Batch 1039/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.947, Loss: 1.394 Epoch 6 Batch 1040/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.947, Loss: 1.471 Epoch 6 Batch 1041/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.947, Loss: 1.517 Epoch 6 Batch 1042/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.483 Epoch 6 Batch 1043/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.952, Loss: 1.491 Epoch 6 Batch 1044/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.444 Epoch 6 Batch 1045/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.347 Epoch 6 Batch 1046/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.961, Loss: 1.462 Epoch 6 Batch 1047/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.959, Loss: 1.413 Epoch 6 Batch 1048/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.963, Loss: 1.458 Epoch 6 Batch 1049/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.961, Loss: 1.428 Epoch 6 Batch 1050/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.964, Loss: 1.445 Epoch 6 Batch 1051/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.964, Loss: 1.509 Epoch 6 Batch 1052/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.966, Loss: 1.402 Epoch 6 Batch 1053/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.966, Loss: 1.361 Epoch 6 Batch 1054/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.963, Loss: 1.344 Epoch 6 Batch 1055/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.352 Epoch 6 Batch 1056/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.958, Loss: 1.367 Epoch 6 Batch 1057/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.952, Loss: 1.399 Epoch 6 Batch 1058/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.475 Epoch 6 Batch 1059/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.961, Loss: 1.455 Epoch 6 Batch 1060/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.958, Loss: 1.381 Epoch 6 Batch 1061/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.953, Loss: 1.413 Epoch 6 Batch 1062/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.949, Loss: 1.415 Epoch 6 Batch 1063/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.949, Loss: 1.417 Epoch 6 Batch 1064/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.953, Loss: 1.491 Epoch 6 Batch 1065/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.949, Loss: 1.399 Epoch 6 Batch 1066/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.948, Loss: 1.413 Epoch 6 Batch 1067/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.938, Loss: 1.500 Epoch 6 Batch 1068/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.938, Loss: 1.433 Epoch 6 Batch 1069/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.928, Loss: 1.422 Epoch 6 Batch 1070/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.928, Loss: 1.535 Epoch 6 Batch 1071/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.932, Loss: 1.509 Epoch 6 Batch 1072/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.932, Loss: 1.399 Epoch 6 Batch 1073/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.932, Loss: 1.411 Epoch 6 Batch 1074/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.934, Loss: 1.413 Epoch 6 Batch 1075/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.931, Loss: 1.457 Epoch 7 Batch 0/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.932, Loss: 1.532 Epoch 7 Batch 1/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.936, Loss: 1.480 Epoch 7 Batch 2/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.936, Loss: 1.396 Epoch 7 Batch 3/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.936, Loss: 1.452 Epoch 7 Batch 4/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.936, Loss: 1.371 Epoch 7 Batch 5/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.937, Loss: 1.467 Epoch 7 Batch 6/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.937, Loss: 1.523 Epoch 7 Batch 7/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.953, Loss: 1.448 Epoch 7 Batch 8/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.958, Loss: 1.391 Epoch 7 Batch 9/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.956, Loss: 1.322 Epoch 7 Batch 10/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.956, Loss: 1.409 Epoch 7 Batch 11/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.958, Loss: 1.435 Epoch 7 Batch 12/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.956, Loss: 1.402 Epoch 7 Batch 13/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.958, Loss: 1.518 Epoch 7 Batch 14/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.960, Loss: 1.370 Epoch 7 Batch 15/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.960, Loss: 1.472 Epoch 7 Batch 16/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.955, Loss: 1.378 Epoch 7 Batch 17/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.393 Epoch 7 Batch 18/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.437 Epoch 7 Batch 19/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.430 Epoch 7 Batch 20/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.960, Loss: 1.371 Epoch 7 Batch 21/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.962, Loss: 1.437 Epoch 7 Batch 22/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.958, Loss: 1.454 Epoch 7 Batch 23/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.370 Epoch 7 Batch 24/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.396 Epoch 7 Batch 25/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.950, Loss: 1.415 Epoch 7 Batch 26/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.509 Epoch 7 Batch 27/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.369 Epoch 7 Batch 28/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.949, Loss: 1.387 Epoch 7 Batch 29/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.947, Loss: 1.434 Epoch 7 Batch 30/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.947, Loss: 1.417 Epoch 7 Batch 31/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.953, Loss: 1.468 Epoch 7 Batch 32/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.949, Loss: 1.460 Epoch 7 Batch 33/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.951, Loss: 1.483 Epoch 7 Batch 34/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.388 Epoch 7 Batch 35/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.420 Epoch 7 Batch 36/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.950, Loss: 1.408 Epoch 7 Batch 37/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.939, Loss: 1.465 Epoch 7 Batch 38/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.937, Loss: 1.401 Epoch 7 Batch 39/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.937, Loss: 1.448 Epoch 7 Batch 40/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.928, Loss: 1.394 Epoch 7 Batch 41/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.932, Loss: 1.417 Epoch 7 Batch 42/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.927, Loss: 1.476 Epoch 7 Batch 43/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.927, Loss: 1.450 Epoch 7 Batch 44/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.929, Loss: 1.393 Epoch 7 Batch 45/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.929, Loss: 1.415 Epoch 7 Batch 46/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.938, Loss: 1.485 Epoch 7 Batch 47/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.947, Loss: 1.366 Epoch 7 Batch 48/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.951, Loss: 1.450 Epoch 7 Batch 49/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.953, Loss: 1.417 Epoch 7 Batch 50/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.949, Loss: 1.470 Epoch 7 Batch 51/1077 - Train Accuracy: 0.929, Validation Accuracy: 0.950, Loss: 1.364 Epoch 7 Batch 52/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.950, Loss: 1.484 Epoch 7 Batch 53/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.950, Loss: 1.451 Epoch 7 Batch 54/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.955, Loss: 1.445 Epoch 7 Batch 55/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.420 Epoch 7 Batch 56/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.954, Loss: 1.417 Epoch 7 Batch 57/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.954, Loss: 1.471 Epoch 7 Batch 58/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.949, Loss: 1.407 Epoch 7 Batch 59/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.941, Loss: 1.466 Epoch 7 Batch 60/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.941, Loss: 1.494 Epoch 7 Batch 61/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.941, Loss: 1.395 Epoch 7 Batch 62/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.937, Loss: 1.451 Epoch 7 Batch 63/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.940, Loss: 1.476 Epoch 7 Batch 64/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.940, Loss: 1.445 Epoch 7 Batch 65/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.940, Loss: 1.448 Epoch 7 Batch 66/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.940, Loss: 1.498 Epoch 7 Batch 67/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.940, Loss: 1.403 Epoch 7 Batch 68/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.935, Loss: 1.510 Epoch 7 Batch 69/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.947, Loss: 1.481 Epoch 7 Batch 70/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.942, Loss: 1.437 Epoch 7 Batch 71/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.946, Loss: 1.486 Epoch 7 Batch 72/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.951, Loss: 1.492 Epoch 7 Batch 73/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.946, Loss: 1.468 Epoch 7 Batch 74/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.946, Loss: 1.507 Epoch 7 Batch 75/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.951, Loss: 1.436 Epoch 7 Batch 76/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.951, Loss: 1.405 Epoch 7 Batch 77/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.951, Loss: 1.394 Epoch 7 Batch 78/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.460 Epoch 7 Batch 79/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.943, Loss: 1.506 Epoch 7 Batch 80/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.939, Loss: 1.404 Epoch 7 Batch 81/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.946, Loss: 1.378 Epoch 7 Batch 82/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.941, Loss: 1.434 Epoch 7 Batch 83/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.946, Loss: 1.494 Epoch 7 Batch 84/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.946, Loss: 1.374 Epoch 7 Batch 85/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.941, Loss: 1.397 Epoch 7 Batch 86/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.940, Loss: 1.503 Epoch 7 Batch 87/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.940, Loss: 1.462 Epoch 7 Batch 88/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.950, Loss: 1.463 Epoch 7 Batch 89/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.451 Epoch 7 Batch 90/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.952, Loss: 1.393 Epoch 7 Batch 91/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.952, Loss: 1.438 Epoch 7 Batch 92/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.952, Loss: 1.414 Epoch 7 Batch 93/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.341 Epoch 7 Batch 94/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.952, Loss: 1.378 Epoch 7 Batch 95/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.964, Loss: 1.441 Epoch 7 Batch 96/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.959, Loss: 1.408 Epoch 7 Batch 97/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.954, Loss: 1.390 Epoch 7 Batch 98/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.959, Loss: 1.432 Epoch 7 Batch 99/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.959, Loss: 1.430 Epoch 7 Batch 100/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.954, Loss: 1.435 Epoch 7 Batch 101/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.948, Loss: 1.457 Epoch 7 Batch 102/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.944, Loss: 1.394 Epoch 7 Batch 103/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.941, Loss: 1.432 Epoch 7 Batch 104/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.941, Loss: 1.385 Epoch 7 Batch 105/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.941, Loss: 1.441 Epoch 7 Batch 106/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.941, Loss: 1.477 Epoch 7 Batch 107/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.934, Loss: 1.471 Epoch 7 Batch 108/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.935, Loss: 1.466 Epoch 7 Batch 109/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.940, Loss: 1.436 Epoch 7 Batch 110/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.944, Loss: 1.433 Epoch 7 Batch 111/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.945, Loss: 1.513 Epoch 7 Batch 112/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.953, Loss: 1.455 Epoch 7 Batch 113/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.949, Loss: 1.458 Epoch 7 Batch 114/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.452 Epoch 7 Batch 115/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.950, Loss: 1.424 Epoch 7 Batch 116/1077 - Train Accuracy: 0.895, Validation Accuracy: 0.954, Loss: 1.537 Epoch 7 Batch 117/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.954, Loss: 1.424 Epoch 7 Batch 118/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.954, Loss: 1.423 Epoch 7 Batch 119/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.954, Loss: 1.436 Epoch 7 Batch 120/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.954, Loss: 1.351 Epoch 7 Batch 121/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.954, Loss: 1.473 Epoch 7 Batch 122/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.954, Loss: 1.455 Epoch 7 Batch 123/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.954, Loss: 1.408 Epoch 7 Batch 124/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.956, Loss: 1.441 Epoch 7 Batch 125/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.951, Loss: 1.435 Epoch 7 Batch 126/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.949, Loss: 1.363 Epoch 7 Batch 127/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.949, Loss: 1.436 Epoch 7 Batch 128/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.949, Loss: 1.327 Epoch 7 Batch 129/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.944, Loss: 1.432 Epoch 7 Batch 130/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.944, Loss: 1.431 Epoch 7 Batch 131/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.942, Loss: 1.356 Epoch 7 Batch 132/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.944, Loss: 1.405 Epoch 7 Batch 133/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.939, Loss: 1.373 Epoch 7 Batch 134/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.935, Loss: 1.443 Epoch 7 Batch 135/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.934, Loss: 1.456 Epoch 7 Batch 136/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.930, Loss: 1.424 Epoch 7 Batch 137/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.930, Loss: 1.385 Epoch 7 Batch 138/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.930, Loss: 1.462 Epoch 7 Batch 139/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.930, Loss: 1.419 Epoch 7 Batch 140/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.934, Loss: 1.418 Epoch 7 Batch 141/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.929, Loss: 1.384 Epoch 7 Batch 142/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.937, Loss: 1.436 Epoch 7 Batch 143/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.932, Loss: 1.476 Epoch 7 Batch 144/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.936, Loss: 1.493 Epoch 7 Batch 145/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.936, Loss: 1.378 Epoch 7 Batch 146/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.936, Loss: 1.426 Epoch 7 Batch 147/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.936, Loss: 1.437 Epoch 7 Batch 148/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.938, Loss: 1.463 Epoch 7 Batch 149/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.939, Loss: 1.380 Epoch 7 Batch 150/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.938, Loss: 1.376 Epoch 7 Batch 151/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.934, Loss: 1.442 Epoch 7 Batch 152/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.934, Loss: 1.435 Epoch 7 Batch 153/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.929, Loss: 1.457 Epoch 7 Batch 154/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.933, Loss: 1.443 Epoch 7 Batch 155/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.938, Loss: 1.411 Epoch 7 Batch 156/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.938, Loss: 1.430 Epoch 7 Batch 157/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.938, Loss: 1.437 Epoch 7 Batch 158/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.942, Loss: 1.418 Epoch 7 Batch 159/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.945, Loss: 1.516 Epoch 7 Batch 160/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.939, Loss: 1.418 Epoch 7 Batch 161/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.937, Loss: 1.449 Epoch 7 Batch 162/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.948, Loss: 1.542 Epoch 7 Batch 163/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.948, Loss: 1.501 Epoch 7 Batch 164/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.948, Loss: 1.438 Epoch 7 Batch 165/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.948, Loss: 1.439 Epoch 7 Batch 166/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.950, Loss: 1.428 Epoch 7 Batch 167/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.402 Epoch 7 Batch 168/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.950, Loss: 1.398 Epoch 7 Batch 169/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.949, Loss: 1.440 Epoch 7 Batch 170/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.953, Loss: 1.443 Epoch 7 Batch 171/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.953, Loss: 1.430 Epoch 7 Batch 172/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.942, Loss: 1.467 Epoch 7 Batch 173/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.937, Loss: 1.444 Epoch 7 Batch 174/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.939, Loss: 1.408 Epoch 7 Batch 175/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.941, Loss: 1.402 Epoch 7 Batch 176/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.941, Loss: 1.438 Epoch 7 Batch 177/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.941, Loss: 1.472 Epoch 7 Batch 178/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.941, Loss: 1.462 Epoch 7 Batch 179/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.941, Loss: 1.394 Epoch 7 Batch 180/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.941, Loss: 1.436 Epoch 7 Batch 181/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.941, Loss: 1.391 Epoch 7 Batch 182/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.951, Loss: 1.445 Epoch 7 Batch 183/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.424 Epoch 7 Batch 184/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.485 Epoch 7 Batch 185/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.945, Loss: 1.467 Epoch 7 Batch 186/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.945, Loss: 1.396 Epoch 7 Batch 187/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.944, Loss: 1.444 Epoch 7 Batch 188/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.944, Loss: 1.380 Epoch 7 Batch 189/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.944, Loss: 1.495 Epoch 7 Batch 190/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.944, Loss: 1.426 Epoch 7 Batch 191/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.950, Loss: 1.380 Epoch 7 Batch 192/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.950, Loss: 1.520 Epoch 7 Batch 193/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.945, Loss: 1.420 Epoch 7 Batch 194/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.945, Loss: 1.437 Epoch 7 Batch 195/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.948, Loss: 1.407 Epoch 7 Batch 196/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.948, Loss: 1.391 Epoch 7 Batch 197/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.950, Loss: 1.407 Epoch 7 Batch 198/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.949, Loss: 1.484 Epoch 7 Batch 199/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.383 Epoch 7 Batch 200/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.947, Loss: 1.470 Epoch 7 Batch 201/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.942, Loss: 1.467 Epoch 7 Batch 202/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.942, Loss: 1.384 Epoch 7 Batch 203/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.938, Loss: 1.420 Epoch 7 Batch 204/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.938, Loss: 1.362 Epoch 7 Batch 205/1077 - Train Accuracy: 0.909, Validation Accuracy: 0.938, Loss: 1.407 Epoch 7 Batch 206/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.934, Loss: 1.379 Epoch 7 Batch 207/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.934, Loss: 1.403 Epoch 7 Batch 208/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.942, Loss: 1.485 Epoch 7 Batch 209/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.947, Loss: 1.457 Epoch 7 Batch 210/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.949, Loss: 1.466 Epoch 7 Batch 211/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.950, Loss: 1.379 Epoch 7 Batch 212/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.400 Epoch 7 Batch 213/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.955, Loss: 1.413 Epoch 7 Batch 214/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.941, Loss: 1.417 Epoch 7 Batch 215/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.938, Loss: 1.502 Epoch 7 Batch 216/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.942, Loss: 1.382 Epoch 7 Batch 217/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.940, Loss: 1.351 Epoch 7 Batch 218/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.940, Loss: 1.417 Epoch 7 Batch 219/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.945, Loss: 1.497 Epoch 7 Batch 220/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.945, Loss: 1.484 Epoch 7 Batch 221/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.945, Loss: 1.503 Epoch 7 Batch 222/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.950, Loss: 1.308 Epoch 7 Batch 223/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.950, Loss: 1.475 Epoch 7 Batch 224/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.952, Loss: 1.425 Epoch 7 Batch 225/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.949, Loss: 1.544 Epoch 7 Batch 226/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.953, Loss: 1.397 Epoch 7 Batch 227/1077 - Train Accuracy: 0.930, Validation Accuracy: 0.953, Loss: 1.494 Epoch 7 Batch 228/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.459 Epoch 7 Batch 229/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.465 Epoch 7 Batch 230/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.957, Loss: 1.345 Epoch 7 Batch 231/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.957, Loss: 1.406 Epoch 7 Batch 232/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.957, Loss: 1.408 Epoch 7 Batch 233/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.952, Loss: 1.528 Epoch 7 Batch 234/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.951, Loss: 1.503 Epoch 7 Batch 235/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.952, Loss: 1.472 Epoch 7 Batch 236/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.951, Loss: 1.393 Epoch 7 Batch 237/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.951, Loss: 1.462 Epoch 7 Batch 238/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.951, Loss: 1.440 Epoch 7 Batch 239/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.947, Loss: 1.403 Epoch 7 Batch 240/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.943, Loss: 1.398 Epoch 7 Batch 241/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.948, Loss: 1.427 Epoch 7 Batch 242/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.958, Loss: 1.466 Epoch 7 Batch 243/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.958, Loss: 1.400 Epoch 7 Batch 244/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.956, Loss: 1.463 Epoch 7 Batch 245/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.963, Loss: 1.518 Epoch 7 Batch 246/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.960, Loss: 1.489 Epoch 7 Batch 247/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.955, Loss: 1.399 Epoch 7 Batch 248/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.950, Loss: 1.362 Epoch 7 Batch 249/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.945, Loss: 1.437 Epoch 7 Batch 250/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.371 Epoch 7 Batch 251/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.945, Loss: 1.426 Epoch 7 Batch 252/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.945, Loss: 1.413 Epoch 7 Batch 253/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.946, Loss: 1.416 Epoch 7 Batch 254/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.439 Epoch 7 Batch 255/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.952, Loss: 1.452 Epoch 7 Batch 256/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.533 Epoch 7 Batch 257/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.950, Loss: 1.418 Epoch 7 Batch 258/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.369 Epoch 7 Batch 259/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.422 Epoch 7 Batch 260/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.960, Loss: 1.348 Epoch 7 Batch 261/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.961, Loss: 1.396 Epoch 7 Batch 262/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.391 Epoch 7 Batch 263/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.956, Loss: 1.398 Epoch 7 Batch 264/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.956, Loss: 1.450 Epoch 7 Batch 265/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.516 Epoch 7 Batch 266/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.402 Epoch 7 Batch 267/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.955, Loss: 1.425 Epoch 7 Batch 268/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.955, Loss: 1.415 Epoch 7 Batch 269/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.529 Epoch 7 Batch 270/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.963, Loss: 1.425 Epoch 7 Batch 271/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.957, Loss: 1.386 Epoch 7 Batch 272/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.578 Epoch 7 Batch 273/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.957, Loss: 1.494 Epoch 7 Batch 274/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.955, Loss: 1.353 Epoch 7 Batch 275/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.413 Epoch 7 Batch 276/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.944, Loss: 1.496 Epoch 7 Batch 277/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.940, Loss: 1.428 Epoch 7 Batch 278/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.940, Loss: 1.447 Epoch 7 Batch 279/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.943, Loss: 1.390 Epoch 7 Batch 280/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.943, Loss: 1.407 Epoch 7 Batch 281/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.943, Loss: 1.519 Epoch 7 Batch 282/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.948, Loss: 1.450 Epoch 7 Batch 283/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.948, Loss: 1.540 Epoch 7 Batch 284/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.948, Loss: 1.452 Epoch 7 Batch 285/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.948, Loss: 1.432 Epoch 7 Batch 286/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.948, Loss: 1.399 Epoch 7 Batch 287/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.943, Loss: 1.378 Epoch 7 Batch 288/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.944, Loss: 1.480 Epoch 7 Batch 289/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.950, Loss: 1.425 Epoch 7 Batch 290/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.952, Loss: 1.457 Epoch 7 Batch 291/1077 - Train Accuracy: 0.935, Validation Accuracy: 0.947, Loss: 1.376 Epoch 7 Batch 292/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.944, Loss: 1.395 Epoch 7 Batch 293/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.950, Loss: 1.407 Epoch 7 Batch 294/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.955, Loss: 1.467 Epoch 7 Batch 295/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.350 Epoch 7 Batch 296/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.955, Loss: 1.457 Epoch 7 Batch 297/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.950, Loss: 1.379 Epoch 7 Batch 298/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.949, Loss: 1.459 Epoch 7 Batch 299/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.949, Loss: 1.407 Epoch 7 Batch 300/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.949, Loss: 1.410 Epoch 7 Batch 301/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.949, Loss: 1.396 Epoch 7 Batch 302/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.949, Loss: 1.472 Epoch 7 Batch 303/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.944, Loss: 1.383 Epoch 7 Batch 304/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.939, Loss: 1.407 Epoch 7 Batch 305/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.950, Loss: 1.428 Epoch 7 Batch 306/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.946, Loss: 1.450 Epoch 7 Batch 307/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.945, Loss: 1.348 Epoch 7 Batch 308/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.940, Loss: 1.490 Epoch 7 Batch 309/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.940, Loss: 1.400 Epoch 7 Batch 310/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.949, Loss: 1.476 Epoch 7 Batch 311/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.409 Epoch 7 Batch 312/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.950, Loss: 1.513 Epoch 7 Batch 313/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.955, Loss: 1.451 Epoch 7 Batch 314/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.955, Loss: 1.411 Epoch 7 Batch 315/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.404 Epoch 7 Batch 316/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.955, Loss: 1.433 Epoch 7 Batch 317/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.482 Epoch 7 Batch 318/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.958, Loss: 1.496 Epoch 7 Batch 319/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.958, Loss: 1.441 Epoch 7 Batch 320/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.953, Loss: 1.392 Epoch 7 Batch 321/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.953, Loss: 1.399 Epoch 7 Batch 322/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.958, Loss: 1.483 Epoch 7 Batch 323/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.388 Epoch 7 Batch 324/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.961, Loss: 1.402 Epoch 7 Batch 325/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.956, Loss: 1.447 Epoch 7 Batch 326/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.957, Loss: 1.431 Epoch 7 Batch 327/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.966, Loss: 1.467 Epoch 7 Batch 328/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.437 Epoch 7 Batch 329/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.960, Loss: 1.418 Epoch 7 Batch 330/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.956, Loss: 1.461 Epoch 7 Batch 331/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.946, Loss: 1.394 Epoch 7 Batch 332/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.946, Loss: 1.399 Epoch 7 Batch 333/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.946, Loss: 1.419 Epoch 7 Batch 334/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.951, Loss: 1.429 Epoch 7 Batch 335/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.951, Loss: 1.434 Epoch 7 Batch 336/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.951, Loss: 1.460 Epoch 7 Batch 337/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.952, Loss: 1.387 Epoch 7 Batch 338/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.957, Loss: 1.449 Epoch 7 Batch 339/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.950, Loss: 1.391 Epoch 7 Batch 340/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.481 Epoch 7 Batch 341/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.955, Loss: 1.445 Epoch 7 Batch 342/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.955, Loss: 1.408 Epoch 7 Batch 343/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.955, Loss: 1.399 Epoch 7 Batch 344/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.950, Loss: 1.371 Epoch 7 Batch 345/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.952, Loss: 1.501 Epoch 7 Batch 346/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.952, Loss: 1.365 Epoch 7 Batch 347/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.952, Loss: 1.388 Epoch 7 Batch 348/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.946, Loss: 1.437 Epoch 7 Batch 349/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.946, Loss: 1.398 Epoch 7 Batch 350/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.946, Loss: 1.425 Epoch 7 Batch 351/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.951, Loss: 1.460 Epoch 7 Batch 352/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.940, Loss: 1.360 Epoch 7 Batch 353/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.935, Loss: 1.414 Epoch 7 Batch 354/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.935, Loss: 1.494 Epoch 7 Batch 355/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.941, Loss: 1.444 Epoch 7 Batch 356/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.943, Loss: 1.423 Epoch 7 Batch 357/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.943, Loss: 1.436 Epoch 7 Batch 358/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.948, Loss: 1.490 Epoch 7 Batch 359/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.948, Loss: 1.315 Epoch 7 Batch 360/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.947, Loss: 1.409 Epoch 7 Batch 361/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.442 Epoch 7 Batch 362/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.952, Loss: 1.480 Epoch 7 Batch 363/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.955, Loss: 1.482 Epoch 7 Batch 364/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.954, Loss: 1.444 Epoch 7 Batch 365/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.958, Loss: 1.476 Epoch 7 Batch 366/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.301 Epoch 7 Batch 367/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.410 Epoch 7 Batch 368/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.959, Loss: 1.435 Epoch 7 Batch 369/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.543 Epoch 7 Batch 370/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.435 Epoch 7 Batch 371/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.953, Loss: 1.359 Epoch 7 Batch 372/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.953, Loss: 1.404 Epoch 7 Batch 373/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.953, Loss: 1.395 Epoch 7 Batch 374/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.953, Loss: 1.482 Epoch 7 Batch 375/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.464 Epoch 7 Batch 376/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.952, Loss: 1.440 Epoch 7 Batch 377/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.947, Loss: 1.325 Epoch 7 Batch 378/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.947, Loss: 1.386 Epoch 7 Batch 379/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.952, Loss: 1.452 Epoch 7 Batch 380/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.425 Epoch 7 Batch 381/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.495 Epoch 7 Batch 382/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.952, Loss: 1.381 Epoch 7 Batch 383/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.954, Loss: 1.424 Epoch 7 Batch 384/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.945, Loss: 1.443 Epoch 7 Batch 385/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.939, Loss: 1.399 Epoch 7 Batch 386/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.940, Loss: 1.399 Epoch 7 Batch 387/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.939, Loss: 1.384 Epoch 7 Batch 388/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.945, Loss: 1.414 Epoch 7 Batch 389/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.946, Loss: 1.416 Epoch 7 Batch 390/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.951, Loss: 1.463 Epoch 7 Batch 391/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.944, Loss: 1.484 Epoch 7 Batch 392/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.944, Loss: 1.433 Epoch 7 Batch 393/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.944, Loss: 1.459 Epoch 7 Batch 394/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.941, Loss: 1.430 Epoch 7 Batch 395/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.941, Loss: 1.390 Epoch 7 Batch 396/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.937, Loss: 1.438 Epoch 7 Batch 397/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.936, Loss: 1.455 Epoch 7 Batch 398/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.946, Loss: 1.468 Epoch 7 Batch 399/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.951, Loss: 1.443 Epoch 7 Batch 400/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.951, Loss: 1.417 Epoch 7 Batch 401/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.960, Loss: 1.383 Epoch 7 Batch 402/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.960, Loss: 1.391 Epoch 7 Batch 403/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.960, Loss: 1.489 Epoch 7 Batch 404/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.951, Loss: 1.387 Epoch 7 Batch 405/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.950, Loss: 1.492 Epoch 7 Batch 406/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.416 Epoch 7 Batch 407/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.955, Loss: 1.478 Epoch 7 Batch 408/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.949, Loss: 1.487 Epoch 7 Batch 409/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.949, Loss: 1.452 Epoch 7 Batch 410/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.956, Loss: 1.420 Epoch 7 Batch 411/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.958, Loss: 1.524 Epoch 7 Batch 412/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.958, Loss: 1.381 Epoch 7 Batch 413/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.958, Loss: 1.387 Epoch 7 Batch 414/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.958, Loss: 1.491 Epoch 7 Batch 415/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.518 Epoch 7 Batch 416/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.950, Loss: 1.438 Epoch 7 Batch 417/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.460 Epoch 7 Batch 418/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.950, Loss: 1.418 Epoch 7 Batch 419/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.950, Loss: 1.457 Epoch 7 Batch 420/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.944, Loss: 1.487 Epoch 7 Batch 421/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.944, Loss: 1.496 Epoch 7 Batch 422/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.945, Loss: 1.413 Epoch 7 Batch 423/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.935, Loss: 1.460 Epoch 7 Batch 424/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.935, Loss: 1.425 Epoch 7 Batch 425/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.940, Loss: 1.416 Epoch 7 Batch 426/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.939, Loss: 1.462 Epoch 7 Batch 427/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.941, Loss: 1.421 Epoch 7 Batch 428/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.951, Loss: 1.467 Epoch 7 Batch 429/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.951, Loss: 1.452 Epoch 7 Batch 430/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.946, Loss: 1.411 Epoch 7 Batch 431/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.950, Loss: 1.440 Epoch 7 Batch 432/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.950, Loss: 1.435 Epoch 7 Batch 433/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.423 Epoch 7 Batch 434/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.956, Loss: 1.392 Epoch 7 Batch 435/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.405 Epoch 7 Batch 436/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.955, Loss: 1.392 Epoch 7 Batch 437/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.954, Loss: 1.414 Epoch 7 Batch 438/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.955, Loss: 1.492 Epoch 7 Batch 439/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.490 Epoch 7 Batch 440/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.950, Loss: 1.396 Epoch 7 Batch 441/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.950, Loss: 1.465 Epoch 7 Batch 442/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.950, Loss: 1.463 Epoch 7 Batch 443/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.945, Loss: 1.465 Epoch 7 Batch 444/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.950, Loss: 1.400 Epoch 7 Batch 445/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.950, Loss: 1.440 Epoch 7 Batch 446/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.950, Loss: 1.405 Epoch 7 Batch 447/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.946, Loss: 1.423 Epoch 7 Batch 448/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.946, Loss: 1.545 Epoch 7 Batch 449/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.946, Loss: 1.433 Epoch 7 Batch 450/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.947, Loss: 1.395 Epoch 7 Batch 451/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.951, Loss: 1.382 Epoch 7 Batch 452/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.487 Epoch 7 Batch 453/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.962, Loss: 1.502 Epoch 7 Batch 454/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.962, Loss: 1.356 Epoch 7 Batch 455/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.966, Loss: 1.514 Epoch 7 Batch 456/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.966, Loss: 1.353 Epoch 7 Batch 457/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.966, Loss: 1.475 Epoch 7 Batch 458/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.966, Loss: 1.413 Epoch 7 Batch 459/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.971, Loss: 1.421 Epoch 7 Batch 460/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.967, Loss: 1.392 Epoch 7 Batch 461/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.965, Loss: 1.501 Epoch 7 Batch 462/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.965, Loss: 1.440 Epoch 7 Batch 463/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.965, Loss: 1.382 Epoch 7 Batch 464/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.965, Loss: 1.512 Epoch 7 Batch 465/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.960, Loss: 1.441 Epoch 7 Batch 466/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.960, Loss: 1.391 Epoch 7 Batch 467/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.955, Loss: 1.418 Epoch 7 Batch 468/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.457 Epoch 7 Batch 469/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.396 Epoch 7 Batch 470/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.956, Loss: 1.449 Epoch 7 Batch 471/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.956, Loss: 1.427 Epoch 7 Batch 472/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.956, Loss: 1.510 Epoch 7 Batch 473/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.950, Loss: 1.406 Epoch 7 Batch 474/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.955, Loss: 1.409 Epoch 7 Batch 475/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.955, Loss: 1.507 Epoch 7 Batch 476/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.956, Loss: 1.446 Epoch 7 Batch 477/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.956, Loss: 1.433 Epoch 7 Batch 478/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.449 Epoch 7 Batch 479/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.413 Epoch 7 Batch 480/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.952, Loss: 1.405 Epoch 7 Batch 481/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.428 Epoch 7 Batch 482/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.951, Loss: 1.444 Epoch 7 Batch 483/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.956, Loss: 1.421 Epoch 7 Batch 484/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.425 Epoch 7 Batch 485/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.392 Epoch 7 Batch 486/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.390 Epoch 7 Batch 487/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.951, Loss: 1.446 Epoch 7 Batch 488/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.951, Loss: 1.438 Epoch 7 Batch 489/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.951, Loss: 1.521 Epoch 7 Batch 490/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.951, Loss: 1.466 Epoch 7 Batch 491/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.947, Loss: 1.459 Epoch 7 Batch 492/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.945, Loss: 1.426 Epoch 7 Batch 493/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.945, Loss: 1.447 Epoch 7 Batch 494/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.945, Loss: 1.388 Epoch 7 Batch 495/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.944, Loss: 1.491 Epoch 7 Batch 496/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.950, Loss: 1.417 Epoch 7 Batch 497/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.950, Loss: 1.531 Epoch 7 Batch 498/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.945, Loss: 1.472 Epoch 7 Batch 499/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.945, Loss: 1.429 Epoch 7 Batch 500/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.945, Loss: 1.402 Epoch 7 Batch 501/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.952, Loss: 1.440 Epoch 7 Batch 502/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.952, Loss: 1.485 Epoch 7 Batch 503/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.946, Loss: 1.418 Epoch 7 Batch 504/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.947, Loss: 1.437 Epoch 7 Batch 505/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.952, Loss: 1.446 Epoch 7 Batch 506/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.961, Loss: 1.402 Epoch 7 Batch 507/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.955, Loss: 1.471 Epoch 7 Batch 508/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.449 Epoch 7 Batch 509/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.955, Loss: 1.428 Epoch 7 Batch 510/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.431 Epoch 7 Batch 511/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.405 Epoch 7 Batch 512/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.955, Loss: 1.344 Epoch 7 Batch 513/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.438 Epoch 7 Batch 514/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.953, Loss: 1.462 Epoch 7 Batch 515/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.953, Loss: 1.407 Epoch 7 Batch 516/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.426 Epoch 7 Batch 517/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.953, Loss: 1.458 Epoch 7 Batch 518/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.952, Loss: 1.447 Epoch 7 Batch 519/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.958, Loss: 1.434 Epoch 7 Batch 520/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.958, Loss: 1.447 Epoch 7 Batch 521/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.958, Loss: 1.429 Epoch 7 Batch 522/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.958, Loss: 1.437 Epoch 7 Batch 523/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.960, Loss: 1.461 Epoch 7 Batch 524/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.960, Loss: 1.463 Epoch 7 Batch 525/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.961, Loss: 1.441 Epoch 7 Batch 526/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.461 Epoch 7 Batch 527/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.965, Loss: 1.475 Epoch 7 Batch 528/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.966, Loss: 1.443 Epoch 7 Batch 529/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.971, Loss: 1.433 Epoch 7 Batch 530/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.971, Loss: 1.423 Epoch 7 Batch 531/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.964, Loss: 1.449 Epoch 7 Batch 532/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.968, Loss: 1.360 Epoch 7 Batch 533/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.967, Loss: 1.352 Epoch 7 Batch 534/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.967, Loss: 1.399 Epoch 7 Batch 535/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.967, Loss: 1.373 Epoch 7 Batch 536/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.967, Loss: 1.380 Epoch 7 Batch 537/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.971, Loss: 1.369 Epoch 7 Batch 538/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.972, Loss: 1.405 Epoch 7 Batch 539/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.972, Loss: 1.466 Epoch 7 Batch 540/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.977, Loss: 1.428 Epoch 7 Batch 541/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.977, Loss: 1.464 Epoch 7 Batch 542/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.966, Loss: 1.412 Epoch 7 Batch 543/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.971, Loss: 1.403 Epoch 7 Batch 544/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.965, Loss: 1.382 Epoch 7 Batch 545/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.965, Loss: 1.419 Epoch 7 Batch 546/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.965, Loss: 1.399 Epoch 7 Batch 547/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.960, Loss: 1.442 Epoch 7 Batch 548/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.966, Loss: 1.544 Epoch 7 Batch 549/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.966, Loss: 1.501 Epoch 7 Batch 550/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.966, Loss: 1.381 Epoch 7 Batch 551/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.967, Loss: 1.457 Epoch 7 Batch 552/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.965, Loss: 1.413 Epoch 7 Batch 553/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.966, Loss: 1.364 Epoch 7 Batch 554/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.966, Loss: 1.430 Epoch 7 Batch 555/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.966, Loss: 1.430 Epoch 7 Batch 556/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.971, Loss: 1.399 Epoch 7 Batch 557/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.966, Loss: 1.435 Epoch 7 Batch 558/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.966, Loss: 1.459 Epoch 7 Batch 559/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.966, Loss: 1.517 Epoch 7 Batch 560/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.966, Loss: 1.460 Epoch 7 Batch 561/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.966, Loss: 1.418 Epoch 7 Batch 562/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.966, Loss: 1.494 Epoch 7 Batch 563/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.961, Loss: 1.487 Epoch 7 Batch 564/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.961, Loss: 1.380 Epoch 7 Batch 565/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.971, Loss: 1.498 Epoch 7 Batch 566/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.971, Loss: 1.443 Epoch 7 Batch 567/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.967, Loss: 1.425 Epoch 7 Batch 568/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.967, Loss: 1.421 Epoch 7 Batch 569/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.960, Loss: 1.435 Epoch 7 Batch 570/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.960, Loss: 1.490 Epoch 7 Batch 571/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.440 Epoch 7 Batch 572/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.960, Loss: 1.419 Epoch 7 Batch 573/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.961, Loss: 1.448 Epoch 7 Batch 574/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.432 Epoch 7 Batch 575/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.951, Loss: 1.429 Epoch 7 Batch 576/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.415 Epoch 7 Batch 577/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.458 Epoch 7 Batch 578/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.472 Epoch 7 Batch 579/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.954, Loss: 1.398 Epoch 7 Batch 580/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.954, Loss: 1.376 Epoch 7 Batch 581/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.371 Epoch 7 Batch 582/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.448 Epoch 7 Batch 583/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.957, Loss: 1.421 Epoch 7 Batch 584/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.957, Loss: 1.431 Epoch 7 Batch 585/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.955, Loss: 1.372 Epoch 7 Batch 586/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.960, Loss: 1.449 Epoch 7 Batch 587/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.960, Loss: 1.401 Epoch 7 Batch 588/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.960, Loss: 1.443 Epoch 7 Batch 589/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.966, Loss: 1.514 Epoch 7 Batch 590/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.966, Loss: 1.388 Epoch 7 Batch 591/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.966, Loss: 1.451 Epoch 7 Batch 592/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.960, Loss: 1.454 Epoch 7 Batch 593/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.967, Loss: 1.444 Epoch 7 Batch 594/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.962, Loss: 1.449 Epoch 7 Batch 595/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.960, Loss: 1.372 Epoch 7 Batch 596/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.964, Loss: 1.403 Epoch 7 Batch 597/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.962, Loss: 1.357 Epoch 7 Batch 598/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.964, Loss: 1.427 Epoch 7 Batch 599/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.964, Loss: 1.408 Epoch 7 Batch 600/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.969, Loss: 1.403 Epoch 7 Batch 601/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.969, Loss: 1.425 Epoch 7 Batch 602/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.968, Loss: 1.464 Epoch 7 Batch 603/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.973, Loss: 1.357 Epoch 7 Batch 604/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.968, Loss: 1.499 Epoch 7 Batch 605/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.423 Epoch 7 Batch 606/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.963, Loss: 1.464 Epoch 7 Batch 607/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.965, Loss: 1.449 Epoch 7 Batch 608/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.963, Loss: 1.366 Epoch 7 Batch 609/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.963, Loss: 1.447 Epoch 7 Batch 610/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.962, Loss: 1.432 Epoch 7 Batch 611/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.966, Loss: 1.398 Epoch 7 Batch 612/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.957, Loss: 1.456 Epoch 7 Batch 613/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.952, Loss: 1.444 Epoch 7 Batch 614/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.947, Loss: 1.385 Epoch 7 Batch 615/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.952, Loss: 1.455 Epoch 7 Batch 616/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.957, Loss: 1.444 Epoch 7 Batch 617/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.957, Loss: 1.371 Epoch 7 Batch 618/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.959, Loss: 1.472 Epoch 7 Batch 619/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.959, Loss: 1.481 Epoch 7 Batch 620/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.452 Epoch 7 Batch 621/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.960, Loss: 1.436 Epoch 7 Batch 622/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.964, Loss: 1.450 Epoch 7 Batch 623/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.964, Loss: 1.433 Epoch 7 Batch 624/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.964, Loss: 1.494 Epoch 7 Batch 625/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.963, Loss: 1.389 Epoch 7 Batch 626/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.962, Loss: 1.481 Epoch 7 Batch 627/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.958, Loss: 1.439 Epoch 7 Batch 628/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.958, Loss: 1.400 Epoch 7 Batch 629/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.958, Loss: 1.399 Epoch 7 Batch 630/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.370 Epoch 7 Batch 631/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.958, Loss: 1.403 Epoch 7 Batch 632/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.962, Loss: 1.403 Epoch 7 Batch 633/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.962, Loss: 1.486 Epoch 7 Batch 634/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.962, Loss: 1.360 Epoch 7 Batch 635/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.962, Loss: 1.423 Epoch 7 Batch 636/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.962, Loss: 1.303 Epoch 7 Batch 637/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.459 Epoch 7 Batch 638/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.957, Loss: 1.429 Epoch 7 Batch 639/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.477 Epoch 7 Batch 640/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.962, Loss: 1.416 Epoch 7 Batch 641/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.962, Loss: 1.471 Epoch 7 Batch 642/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.962, Loss: 1.451 Epoch 7 Batch 643/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.963, Loss: 1.409 Epoch 7 Batch 644/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.963, Loss: 1.507 Epoch 7 Batch 645/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.969, Loss: 1.404 Epoch 7 Batch 646/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.969, Loss: 1.367 Epoch 7 Batch 647/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.963, Loss: 1.397 Epoch 7 Batch 648/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.963, Loss: 1.492 Epoch 7 Batch 649/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.961, Loss: 1.459 Epoch 7 Batch 650/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.955, Loss: 1.459 Epoch 7 Batch 651/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.359 Epoch 7 Batch 652/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.953, Loss: 1.372 Epoch 7 Batch 653/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.953, Loss: 1.401 Epoch 7 Batch 654/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.949, Loss: 1.431 Epoch 7 Batch 655/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.953, Loss: 1.385 Epoch 7 Batch 656/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.944, Loss: 1.497 Epoch 7 Batch 657/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.944, Loss: 1.400 Epoch 7 Batch 658/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.463 Epoch 7 Batch 659/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.957, Loss: 1.391 Epoch 7 Batch 660/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.947, Loss: 1.460 Epoch 7 Batch 661/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.942, Loss: 1.397 Epoch 7 Batch 662/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.944, Loss: 1.439 Epoch 7 Batch 663/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.949, Loss: 1.393 Epoch 7 Batch 664/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.944, Loss: 1.440 Epoch 7 Batch 665/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.944, Loss: 1.465 Epoch 7 Batch 666/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.948, Loss: 1.468 Epoch 7 Batch 667/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.949, Loss: 1.453 Epoch 7 Batch 668/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.492 Epoch 7 Batch 669/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.966, Loss: 1.374 Epoch 7 Batch 670/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.961, Loss: 1.426 Epoch 7 Batch 671/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.966, Loss: 1.509 Epoch 7 Batch 672/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.407 Epoch 7 Batch 673/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.964, Loss: 1.484 Epoch 7 Batch 674/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.964, Loss: 1.470 Epoch 7 Batch 675/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.969, Loss: 1.429 Epoch 7 Batch 676/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.973, Loss: 1.442 Epoch 7 Batch 677/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.973, Loss: 1.465 Epoch 7 Batch 678/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.973, Loss: 1.493 Epoch 7 Batch 679/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.974, Loss: 1.391 Epoch 7 Batch 680/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.979, Loss: 1.456 Epoch 7 Batch 681/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.974, Loss: 1.403 Epoch 7 Batch 682/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.969, Loss: 1.376 Epoch 7 Batch 683/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.974, Loss: 1.393 Epoch 7 Batch 684/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.972, Loss: 1.428 Epoch 7 Batch 685/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.492 Epoch 7 Batch 686/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.947, Loss: 1.462 Epoch 7 Batch 687/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.396 Epoch 7 Batch 688/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.420 Epoch 7 Batch 689/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.423 Epoch 7 Batch 690/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.952, Loss: 1.525 Epoch 7 Batch 691/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.420 Epoch 7 Batch 692/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.952, Loss: 1.411 Epoch 7 Batch 693/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.469 Epoch 7 Batch 694/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.466 Epoch 7 Batch 695/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.963, Loss: 1.422 Epoch 7 Batch 696/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.968, Loss: 1.448 Epoch 7 Batch 697/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.968, Loss: 1.468 Epoch 7 Batch 698/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.417 Epoch 7 Batch 699/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.958, Loss: 1.378 Epoch 7 Batch 700/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.957, Loss: 1.439 Epoch 7 Batch 701/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.417 Epoch 7 Batch 702/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.952, Loss: 1.437 Epoch 7 Batch 703/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.954, Loss: 1.446 Epoch 7 Batch 704/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.949, Loss: 1.415 Epoch 7 Batch 705/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.957, Loss: 1.405 Epoch 7 Batch 706/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.955, Loss: 1.421 Epoch 7 Batch 707/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.961, Loss: 1.381 Epoch 7 Batch 708/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.961, Loss: 1.542 Epoch 7 Batch 709/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.961, Loss: 1.357 Epoch 7 Batch 710/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.961, Loss: 1.481 Epoch 7 Batch 711/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.961, Loss: 1.456 Epoch 7 Batch 712/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.961, Loss: 1.395 Epoch 7 Batch 713/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.971, Loss: 1.421 Epoch 7 Batch 714/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.966, Loss: 1.408 Epoch 7 Batch 715/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.966, Loss: 1.410 Epoch 7 Batch 716/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.468 Epoch 7 Batch 717/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.961, Loss: 1.437 Epoch 7 Batch 718/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.967, Loss: 1.363 Epoch 7 Batch 719/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.972, Loss: 1.394 Epoch 7 Batch 720/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.972, Loss: 1.397 Epoch 7 Batch 721/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.972, Loss: 1.430 Epoch 7 Batch 722/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.971, Loss: 1.410 Epoch 7 Batch 723/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.970, Loss: 1.455 Epoch 7 Batch 724/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.972, Loss: 1.412 Epoch 7 Batch 725/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.972, Loss: 1.499 Epoch 7 Batch 726/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.977, Loss: 1.382 Epoch 7 Batch 727/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.977, Loss: 1.391 Epoch 7 Batch 728/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.972, Loss: 1.385 Epoch 7 Batch 729/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.965, Loss: 1.378 Epoch 7 Batch 730/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.965, Loss: 1.436 Epoch 7 Batch 731/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.961, Loss: 1.474 Epoch 7 Batch 732/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.961, Loss: 1.502 Epoch 7 Batch 733/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.961, Loss: 1.442 Epoch 7 Batch 734/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.955, Loss: 1.426 Epoch 7 Batch 735/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.955, Loss: 1.410 Epoch 7 Batch 736/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.953, Loss: 1.397 Epoch 7 Batch 737/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.953, Loss: 1.399 Epoch 7 Batch 738/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.953, Loss: 1.403 Epoch 7 Batch 739/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.953, Loss: 1.441 Epoch 7 Batch 740/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.953, Loss: 1.413 Epoch 7 Batch 741/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.949, Loss: 1.397 Epoch 7 Batch 742/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.462 Epoch 7 Batch 743/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.949, Loss: 1.344 Epoch 7 Batch 744/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.949, Loss: 1.487 Epoch 7 Batch 745/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.947, Loss: 1.453 Epoch 7 Batch 746/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.947, Loss: 1.380 Epoch 7 Batch 747/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.951, Loss: 1.460 Epoch 7 Batch 748/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.951, Loss: 1.432 Epoch 7 Batch 749/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.956, Loss: 1.451 Epoch 7 Batch 750/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.956, Loss: 1.420 Epoch 7 Batch 751/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.523 Epoch 7 Batch 752/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.958, Loss: 1.402 Epoch 7 Batch 753/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.969, Loss: 1.374 Epoch 7 Batch 754/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.970, Loss: 1.443 Epoch 7 Batch 755/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.970, Loss: 1.396 Epoch 7 Batch 756/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.970, Loss: 1.485 Epoch 7 Batch 757/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.974, Loss: 1.465 Epoch 7 Batch 758/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.974, Loss: 1.440 Epoch 7 Batch 759/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.975, Loss: 1.499 Epoch 7 Batch 760/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.971, Loss: 1.444 Epoch 7 Batch 761/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.971, Loss: 1.456 Epoch 7 Batch 762/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.975, Loss: 1.432 Epoch 7 Batch 763/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.975, Loss: 1.374 Epoch 7 Batch 764/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.975, Loss: 1.436 Epoch 7 Batch 765/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.975, Loss: 1.444 Epoch 7 Batch 766/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.973, Loss: 1.433 Epoch 7 Batch 767/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.975, Loss: 1.429 Epoch 7 Batch 768/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.971, Loss: 1.375 Epoch 7 Batch 769/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.966, Loss: 1.438 Epoch 7 Batch 770/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.968, Loss: 1.378 Epoch 7 Batch 771/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.968, Loss: 1.446 Epoch 7 Batch 772/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.452 Epoch 7 Batch 773/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.960, Loss: 1.506 Epoch 7 Batch 774/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.360 Epoch 7 Batch 775/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.451 Epoch 7 Batch 776/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.955, Loss: 1.356 Epoch 7 Batch 777/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.456 Epoch 7 Batch 778/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.956, Loss: 1.396 Epoch 7 Batch 779/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.953, Loss: 1.448 Epoch 7 Batch 780/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.948, Loss: 1.429 Epoch 7 Batch 781/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.948, Loss: 1.424 Epoch 7 Batch 782/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.956, Loss: 1.461 Epoch 7 Batch 783/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.948, Loss: 1.469 Epoch 7 Batch 784/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.948, Loss: 1.385 Epoch 7 Batch 785/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.948, Loss: 1.407 Epoch 7 Batch 786/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.948, Loss: 1.379 Epoch 7 Batch 787/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.395 Epoch 7 Batch 788/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.952, Loss: 1.402 Epoch 7 Batch 789/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.952, Loss: 1.371 Epoch 7 Batch 790/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.958, Loss: 1.495 Epoch 7 Batch 791/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.959, Loss: 1.492 Epoch 7 Batch 792/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.955, Loss: 1.476 Epoch 7 Batch 793/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.962, Loss: 1.484 Epoch 7 Batch 794/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.956, Loss: 1.489 Epoch 7 Batch 795/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.372 Epoch 7 Batch 796/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.955, Loss: 1.446 Epoch 7 Batch 797/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.485 Epoch 7 Batch 798/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.960, Loss: 1.462 Epoch 7 Batch 799/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.950, Loss: 1.467 Epoch 7 Batch 800/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.404 Epoch 7 Batch 801/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.458 Epoch 7 Batch 802/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.955, Loss: 1.374 Epoch 7 Batch 803/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.951, Loss: 1.437 Epoch 7 Batch 804/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.397 Epoch 7 Batch 805/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.952, Loss: 1.422 Epoch 7 Batch 806/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.456 Epoch 7 Batch 807/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.962, Loss: 1.422 Epoch 7 Batch 808/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.962, Loss: 1.405 Epoch 7 Batch 809/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.436 Epoch 7 Batch 810/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.959, Loss: 1.387 Epoch 7 Batch 811/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.959, Loss: 1.519 Epoch 7 Batch 812/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.959, Loss: 1.352 Epoch 7 Batch 813/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.959, Loss: 1.456 Epoch 7 Batch 814/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.434 Epoch 7 Batch 815/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.355 Epoch 7 Batch 816/1077 - Train Accuracy: 0.992, Validation Accuracy: 0.967, Loss: 1.483 Epoch 7 Batch 817/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.967, Loss: 1.409 Epoch 7 Batch 818/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.967, Loss: 1.466 Epoch 7 Batch 819/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.962, Loss: 1.360 Epoch 7 Batch 820/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.962, Loss: 1.409 Epoch 7 Batch 821/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.963, Loss: 1.461 Epoch 7 Batch 822/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.962, Loss: 1.469 Epoch 7 Batch 823/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.962, Loss: 1.415 Epoch 7 Batch 824/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.967, Loss: 1.422 Epoch 7 Batch 825/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.955, Loss: 1.371 Epoch 7 Batch 826/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.957, Loss: 1.454 Epoch 7 Batch 827/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.957, Loss: 1.435 Epoch 7 Batch 828/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.957, Loss: 1.445 Epoch 7 Batch 829/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.957, Loss: 1.438 Epoch 7 Batch 830/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.961, Loss: 1.367 Epoch 7 Batch 831/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.957, Loss: 1.438 Epoch 7 Batch 832/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.954, Loss: 1.432 Epoch 7 Batch 833/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.959, Loss: 1.440 Epoch 7 Batch 834/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.959, Loss: 1.550 Epoch 7 Batch 835/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.959, Loss: 1.386 Epoch 7 Batch 836/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.386 Epoch 7 Batch 837/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.968, Loss: 1.402 Epoch 7 Batch 838/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.968, Loss: 1.441 Epoch 7 Batch 839/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.973, Loss: 1.392 Epoch 7 Batch 840/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.972, Loss: 1.479 Epoch 7 Batch 841/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.960, Loss: 1.402 Epoch 7 Batch 842/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.958, Loss: 1.400 Epoch 7 Batch 843/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.478 Epoch 7 Batch 844/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.965, Loss: 1.439 Epoch 7 Batch 845/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.967, Loss: 1.417 Epoch 7 Batch 846/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.967, Loss: 1.441 Epoch 7 Batch 847/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.963, Loss: 1.434 Epoch 7 Batch 848/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.967, Loss: 1.294 Epoch 7 Batch 849/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.968, Loss: 1.425 Epoch 7 Batch 850/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.972, Loss: 1.463 Epoch 7 Batch 851/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.972, Loss: 1.433 Epoch 7 Batch 852/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.972, Loss: 1.452 Epoch 7 Batch 853/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.964, Loss: 1.460 Epoch 7 Batch 854/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.965, Loss: 1.450 Epoch 7 Batch 855/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.965, Loss: 1.397 Epoch 7 Batch 856/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.958, Loss: 1.513 Epoch 7 Batch 857/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.968, Loss: 1.382 Epoch 7 Batch 858/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.968, Loss: 1.450 Epoch 7 Batch 859/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.499 Epoch 7 Batch 860/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.970, Loss: 1.485 Epoch 7 Batch 861/1077 - Train Accuracy: 0.932, Validation Accuracy: 0.967, Loss: 1.511 Epoch 7 Batch 862/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.967, Loss: 1.385 Epoch 7 Batch 863/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.972, Loss: 1.467 Epoch 7 Batch 864/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.972, Loss: 1.510 Epoch 7 Batch 865/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.975, Loss: 1.437 Epoch 7 Batch 866/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.975, Loss: 1.423 Epoch 7 Batch 867/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.970, Loss: 1.460 Epoch 7 Batch 868/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.974, Loss: 1.417 Epoch 7 Batch 869/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.975, Loss: 1.341 Epoch 7 Batch 870/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.980, Loss: 1.389 Epoch 7 Batch 871/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.980, Loss: 1.531 Epoch 7 Batch 872/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.980, Loss: 1.472 Epoch 7 Batch 873/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.971, Loss: 1.347 Epoch 7 Batch 874/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.971, Loss: 1.377 Epoch 7 Batch 875/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.964, Loss: 1.397 Epoch 7 Batch 876/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.965, Loss: 1.450 Epoch 7 Batch 877/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.963, Loss: 1.389 Epoch 7 Batch 878/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.962, Loss: 1.359 Epoch 7 Batch 879/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.962, Loss: 1.396 Epoch 7 Batch 880/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.964, Loss: 1.402 Epoch 7 Batch 881/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.962, Loss: 1.466 Epoch 7 Batch 882/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.960, Loss: 1.450 Epoch 7 Batch 883/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.422 Epoch 7 Batch 884/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.960, Loss: 1.458 Epoch 7 Batch 885/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.966, Loss: 1.485 Epoch 7 Batch 886/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.966, Loss: 1.408 Epoch 7 Batch 887/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.400 Epoch 7 Batch 888/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.403 Epoch 7 Batch 889/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.968, Loss: 1.376 Epoch 7 Batch 890/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.964, Loss: 1.382 Epoch 7 Batch 891/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.962, Loss: 1.420 Epoch 7 Batch 892/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.951, Loss: 1.411 Epoch 7 Batch 893/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.956, Loss: 1.493 Epoch 7 Batch 894/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.956, Loss: 1.455 Epoch 7 Batch 895/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.955, Loss: 1.449 Epoch 7 Batch 896/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.950, Loss: 1.409 Epoch 7 Batch 897/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.950, Loss: 1.471 Epoch 7 Batch 898/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.404 Epoch 7 Batch 899/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.477 Epoch 7 Batch 900/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.960, Loss: 1.488 Epoch 7 Batch 901/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.957, Loss: 1.445 Epoch 7 Batch 902/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.957, Loss: 1.462 Epoch 7 Batch 903/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.953, Loss: 1.424 Epoch 7 Batch 904/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.422 Epoch 7 Batch 905/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.433 Epoch 7 Batch 906/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.950, Loss: 1.447 Epoch 7 Batch 907/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.463 Epoch 7 Batch 908/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.950, Loss: 1.436 Epoch 7 Batch 909/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.506 Epoch 7 Batch 910/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.955, Loss: 1.482 Epoch 7 Batch 911/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.955, Loss: 1.421 Epoch 7 Batch 912/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.955, Loss: 1.415 Epoch 7 Batch 913/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.967, Loss: 1.435 Epoch 7 Batch 914/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.972, Loss: 1.407 Epoch 7 Batch 915/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.972, Loss: 1.410 Epoch 7 Batch 916/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.972, Loss: 1.412 Epoch 7 Batch 917/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.972, Loss: 1.488 Epoch 7 Batch 918/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.972, Loss: 1.416 Epoch 7 Batch 919/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.972, Loss: 1.417 Epoch 7 Batch 920/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.972, Loss: 1.423 Epoch 7 Batch 921/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.972, Loss: 1.436 Epoch 7 Batch 922/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.972, Loss: 1.376 Epoch 7 Batch 923/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.972, Loss: 1.501 Epoch 7 Batch 924/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.973, Loss: 1.468 Epoch 7 Batch 925/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.972, Loss: 1.410 Epoch 7 Batch 926/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.972, Loss: 1.397 Epoch 7 Batch 927/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.977, Loss: 1.474 Epoch 7 Batch 928/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.977, Loss: 1.521 Epoch 7 Batch 929/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.977, Loss: 1.450 Epoch 7 Batch 930/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.977, Loss: 1.376 Epoch 7 Batch 931/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.977, Loss: 1.391 Epoch 7 Batch 932/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.977, Loss: 1.507 Epoch 7 Batch 933/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.979, Loss: 1.497 Epoch 7 Batch 934/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.979, Loss: 1.454 Epoch 7 Batch 935/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.984, Loss: 1.404 Epoch 7 Batch 936/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.984, Loss: 1.415 Epoch 7 Batch 937/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.979, Loss: 1.421 Epoch 7 Batch 938/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.979, Loss: 1.394 Epoch 7 Batch 939/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.974, Loss: 1.451 Epoch 7 Batch 940/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.965, Loss: 1.469 Epoch 7 Batch 941/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.965, Loss: 1.490 Epoch 7 Batch 942/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.965, Loss: 1.493 Epoch 7 Batch 943/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.970, Loss: 1.362 Epoch 7 Batch 944/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.970, Loss: 1.450 Epoch 7 Batch 945/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.974, Loss: 1.421 Epoch 7 Batch 946/1077 - Train Accuracy: 0.994, Validation Accuracy: 0.974, Loss: 1.418 Epoch 7 Batch 947/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.974, Loss: 1.370 Epoch 7 Batch 948/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.969, Loss: 1.441 Epoch 7 Batch 949/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.970, Loss: 1.405 Epoch 7 Batch 950/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.980, Loss: 1.423 Epoch 7 Batch 951/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.980, Loss: 1.337 Epoch 7 Batch 952/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.980, Loss: 1.422 Epoch 7 Batch 953/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.977, Loss: 1.487 Epoch 7 Batch 954/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.977, Loss: 1.483 Epoch 7 Batch 955/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.977, Loss: 1.485 Epoch 7 Batch 956/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.977, Loss: 1.437 Epoch 7 Batch 957/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.977, Loss: 1.394 Epoch 7 Batch 958/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.972, Loss: 1.455 Epoch 7 Batch 959/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.410 Epoch 7 Batch 960/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.969, Loss: 1.375 Epoch 7 Batch 961/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.969, Loss: 1.428 Epoch 7 Batch 962/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.429 Epoch 7 Batch 963/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.430 Epoch 7 Batch 964/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.955, Loss: 1.395 Epoch 7 Batch 965/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.955, Loss: 1.445 Epoch 7 Batch 966/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.365 Epoch 7 Batch 967/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.961, Loss: 1.435 Epoch 7 Batch 968/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.961, Loss: 1.415 Epoch 7 Batch 969/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.961, Loss: 1.338 Epoch 7 Batch 970/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.434 Epoch 7 Batch 971/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.966, Loss: 1.367 Epoch 7 Batch 972/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.965, Loss: 1.406 Epoch 7 Batch 973/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.965, Loss: 1.426 Epoch 7 Batch 974/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.970, Loss: 1.413 Epoch 7 Batch 975/1077 - Train Accuracy: 0.992, Validation Accuracy: 0.965, Loss: 1.439 Epoch 7 Batch 976/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.966, Loss: 1.420 Epoch 7 Batch 977/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.966, Loss: 1.428 Epoch 7 Batch 978/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.966, Loss: 1.422 Epoch 7 Batch 979/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.966, Loss: 1.433 Epoch 7 Batch 980/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.968, Loss: 1.457 Epoch 7 Batch 981/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.968, Loss: 1.417 Epoch 7 Batch 982/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.968, Loss: 1.483 Epoch 7 Batch 983/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.968, Loss: 1.455 Epoch 7 Batch 984/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.971, Loss: 1.377 Epoch 7 Batch 985/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.967, Loss: 1.441 Epoch 7 Batch 986/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.967, Loss: 1.452 Epoch 7 Batch 987/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.967, Loss: 1.505 Epoch 7 Batch 988/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.962, Loss: 1.443 Epoch 7 Batch 989/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.962, Loss: 1.430 Epoch 7 Batch 990/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.962, Loss: 1.377 Epoch 7 Batch 991/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.962, Loss: 1.380 Epoch 7 Batch 992/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.957, Loss: 1.425 Epoch 7 Batch 993/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.455 Epoch 7 Batch 994/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.966, Loss: 1.445 Epoch 7 Batch 995/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.969, Loss: 1.425 Epoch 7 Batch 996/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.969, Loss: 1.402 Epoch 7 Batch 997/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.969, Loss: 1.459 Epoch 7 Batch 998/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.969, Loss: 1.406 Epoch 7 Batch 999/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.969, Loss: 1.403 Epoch 7 Batch 1000/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.973, Loss: 1.418 Epoch 7 Batch 1001/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.973, Loss: 1.419 Epoch 7 Batch 1002/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.973, Loss: 1.371 Epoch 7 Batch 1003/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.973, Loss: 1.490 Epoch 7 Batch 1004/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.973, Loss: 1.409 Epoch 7 Batch 1005/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.973, Loss: 1.413 Epoch 7 Batch 1006/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.978, Loss: 1.356 Epoch 7 Batch 1007/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.974, Loss: 1.444 Epoch 7 Batch 1008/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.974, Loss: 1.488 Epoch 7 Batch 1009/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.974, Loss: 1.408 Epoch 7 Batch 1010/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.374 Epoch 7 Batch 1011/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.976, Loss: 1.398 Epoch 7 Batch 1012/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.976, Loss: 1.440 Epoch 7 Batch 1013/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.978, Loss: 1.441 Epoch 7 Batch 1014/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.973, Loss: 1.369 Epoch 7 Batch 1015/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.973, Loss: 1.435 Epoch 7 Batch 1016/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.973, Loss: 1.483 Epoch 7 Batch 1017/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.973, Loss: 1.444 Epoch 7 Batch 1018/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.441 Epoch 7 Batch 1019/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.484 Epoch 7 Batch 1020/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.968, Loss: 1.425 Epoch 7 Batch 1021/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.443 Epoch 7 Batch 1022/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.964, Loss: 1.432 Epoch 7 Batch 1023/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.964, Loss: 1.485 Epoch 7 Batch 1024/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.968, Loss: 1.465 Epoch 7 Batch 1025/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.968, Loss: 1.344 Epoch 7 Batch 1026/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.969, Loss: 1.554 Epoch 7 Batch 1027/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.969, Loss: 1.418 Epoch 7 Batch 1028/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.971, Loss: 1.411 Epoch 7 Batch 1029/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.971, Loss: 1.474 Epoch 7 Batch 1030/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.971, Loss: 1.357 Epoch 7 Batch 1031/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.971, Loss: 1.369 Epoch 7 Batch 1032/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.971, Loss: 1.454 Epoch 7 Batch 1033/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.971, Loss: 1.441 Epoch 7 Batch 1034/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.966, Loss: 1.435 Epoch 7 Batch 1035/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.967, Loss: 1.413 Epoch 7 Batch 1036/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.965, Loss: 1.445 Epoch 7 Batch 1037/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.961, Loss: 1.413 Epoch 7 Batch 1038/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.961, Loss: 1.461 Epoch 7 Batch 1039/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.956, Loss: 1.454 Epoch 7 Batch 1040/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.433 Epoch 7 Batch 1041/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.963, Loss: 1.478 Epoch 7 Batch 1042/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.426 Epoch 7 Batch 1043/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.972, Loss: 1.414 Epoch 7 Batch 1044/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.972, Loss: 1.388 Epoch 7 Batch 1045/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.972, Loss: 1.462 Epoch 7 Batch 1046/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.967, Loss: 1.411 Epoch 7 Batch 1047/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.967, Loss: 1.394 Epoch 7 Batch 1048/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.967, Loss: 1.381 Epoch 7 Batch 1049/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.967, Loss: 1.388 Epoch 7 Batch 1050/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.962, Loss: 1.397 Epoch 7 Batch 1051/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.962, Loss: 1.432 Epoch 7 Batch 1052/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.965, Loss: 1.335 Epoch 7 Batch 1053/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.965, Loss: 1.530 Epoch 7 Batch 1054/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.965, Loss: 1.464 Epoch 7 Batch 1055/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.964, Loss: 1.340 Epoch 7 Batch 1056/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.964, Loss: 1.507 Epoch 7 Batch 1057/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.964, Loss: 1.447 Epoch 7 Batch 1058/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.964, Loss: 1.421 Epoch 7 Batch 1059/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.969, Loss: 1.463 Epoch 7 Batch 1060/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.969, Loss: 1.417 Epoch 7 Batch 1061/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.965, Loss: 1.471 Epoch 7 Batch 1062/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.964, Loss: 1.437 Epoch 7 Batch 1063/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.962, Loss: 1.401 Epoch 7 Batch 1064/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.962, Loss: 1.372 Epoch 7 Batch 1065/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.957, Loss: 1.294 Epoch 7 Batch 1066/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.957, Loss: 1.341 Epoch 7 Batch 1067/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.957, Loss: 1.432 Epoch 7 Batch 1068/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.399 Epoch 7 Batch 1069/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.950, Loss: 1.393 Epoch 7 Batch 1070/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.460 Epoch 7 Batch 1071/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.960, Loss: 1.411 Epoch 7 Batch 1072/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.960, Loss: 1.388 Epoch 7 Batch 1073/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.960, Loss: 1.428 Epoch 7 Batch 1074/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.960, Loss: 1.428 Epoch 7 Batch 1075/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.965, Loss: 1.361 Epoch 8 Batch 0/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.970, Loss: 1.486 Epoch 8 Batch 1/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.968, Loss: 1.411 Epoch 8 Batch 2/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.977, Loss: 1.392 Epoch 8 Batch 3/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.972, Loss: 1.435 Epoch 8 Batch 4/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.409 Epoch 8 Batch 5/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.967, Loss: 1.413 Epoch 8 Batch 6/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.967, Loss: 1.363 Epoch 8 Batch 7/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.967, Loss: 1.424 Epoch 8 Batch 8/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.967, Loss: 1.435 Epoch 8 Batch 9/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.972, Loss: 1.421 Epoch 8 Batch 10/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.969, Loss: 1.392 Epoch 8 Batch 11/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.964, Loss: 1.450 Epoch 8 Batch 12/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.443 Epoch 8 Batch 13/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.967, Loss: 1.458 Epoch 8 Batch 14/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.967, Loss: 1.478 Epoch 8 Batch 15/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.967, Loss: 1.363 Epoch 8 Batch 16/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.967, Loss: 1.412 Epoch 8 Batch 17/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.967, Loss: 1.454 Epoch 8 Batch 18/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.972, Loss: 1.427 Epoch 8 Batch 19/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.977, Loss: 1.443 Epoch 8 Batch 20/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.972, Loss: 1.433 Epoch 8 Batch 21/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.972, Loss: 1.432 Epoch 8 Batch 22/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.972, Loss: 1.397 Epoch 8 Batch 23/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.461 Epoch 8 Batch 24/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.374 Epoch 8 Batch 25/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.963, Loss: 1.348 Epoch 8 Batch 26/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.963, Loss: 1.385 Epoch 8 Batch 27/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.419 Epoch 8 Batch 28/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.968, Loss: 1.474 Epoch 8 Batch 29/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.966, Loss: 1.444 Epoch 8 Batch 30/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.966, Loss: 1.403 Epoch 8 Batch 31/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.966, Loss: 1.432 Epoch 8 Batch 32/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.959, Loss: 1.438 Epoch 8 Batch 33/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.367 Epoch 8 Batch 34/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.954, Loss: 1.416 Epoch 8 Batch 35/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.949, Loss: 1.447 Epoch 8 Batch 36/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.949, Loss: 1.403 Epoch 8 Batch 37/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.954, Loss: 1.349 Epoch 8 Batch 38/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.954, Loss: 1.472 Epoch 8 Batch 39/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.954, Loss: 1.406 Epoch 8 Batch 40/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.960, Loss: 1.450 Epoch 8 Batch 41/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.373 Epoch 8 Batch 42/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.461 Epoch 8 Batch 43/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.959, Loss: 1.411 Epoch 8 Batch 44/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.959, Loss: 1.416 Epoch 8 Batch 45/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.446 Epoch 8 Batch 46/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.963, Loss: 1.440 Epoch 8 Batch 47/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.958, Loss: 1.408 Epoch 8 Batch 48/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.954, Loss: 1.499 Epoch 8 Batch 49/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.952, Loss: 1.466 Epoch 8 Batch 50/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.956, Loss: 1.396 Epoch 8 Batch 51/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.412 Epoch 8 Batch 52/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.954, Loss: 1.433 Epoch 8 Batch 53/1077 - Train Accuracy: 0.936, Validation Accuracy: 0.954, Loss: 1.431 Epoch 8 Batch 54/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.954, Loss: 1.530 Epoch 8 Batch 55/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.383 Epoch 8 Batch 56/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.947, Loss: 1.449 Epoch 8 Batch 57/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.947, Loss: 1.459 Epoch 8 Batch 58/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.952, Loss: 1.461 Epoch 8 Batch 59/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.952, Loss: 1.407 Epoch 8 Batch 60/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.952, Loss: 1.395 Epoch 8 Batch 61/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.952, Loss: 1.422 Epoch 8 Batch 62/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.954, Loss: 1.390 Epoch 8 Batch 63/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.966, Loss: 1.411 Epoch 8 Batch 64/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.960, Loss: 1.428 Epoch 8 Batch 65/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.960, Loss: 1.432 Epoch 8 Batch 66/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.965, Loss: 1.433 Epoch 8 Batch 67/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.964, Loss: 1.469 Epoch 8 Batch 68/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.964, Loss: 1.417 Epoch 8 Batch 69/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.964, Loss: 1.445 Epoch 8 Batch 70/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.968, Loss: 1.397 Epoch 8 Batch 71/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.968, Loss: 1.406 Epoch 8 Batch 72/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.968, Loss: 1.451 Epoch 8 Batch 73/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.968, Loss: 1.377 Epoch 8 Batch 74/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.962, Loss: 1.419 Epoch 8 Batch 75/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.962, Loss: 1.445 Epoch 8 Batch 76/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.963, Loss: 1.415 Epoch 8 Batch 77/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.959, Loss: 1.445 Epoch 8 Batch 78/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.959, Loss: 1.365 Epoch 8 Batch 79/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.959, Loss: 1.407 Epoch 8 Batch 80/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.959, Loss: 1.437 Epoch 8 Batch 81/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.959, Loss: 1.296 Epoch 8 Batch 82/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.957, Loss: 1.481 Epoch 8 Batch 83/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.957, Loss: 1.446 Epoch 8 Batch 84/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.372 Epoch 8 Batch 85/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.393 Epoch 8 Batch 86/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.955, Loss: 1.490 Epoch 8 Batch 87/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.950, Loss: 1.390 Epoch 8 Batch 88/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.956, Loss: 1.490 Epoch 8 Batch 89/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.440 Epoch 8 Batch 90/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.394 Epoch 8 Batch 91/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.956, Loss: 1.492 Epoch 8 Batch 92/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.489 Epoch 8 Batch 93/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.956, Loss: 1.493 Epoch 8 Batch 94/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.961, Loss: 1.395 Epoch 8 Batch 95/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.963, Loss: 1.401 Epoch 8 Batch 96/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.958, Loss: 1.354 Epoch 8 Batch 97/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.962, Loss: 1.423 Epoch 8 Batch 98/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.963, Loss: 1.493 Epoch 8 Batch 99/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.963, Loss: 1.410 Epoch 8 Batch 100/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.968, Loss: 1.367 Epoch 8 Batch 101/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.964, Loss: 1.406 Epoch 8 Batch 102/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.964, Loss: 1.343 Epoch 8 Batch 103/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.419 Epoch 8 Batch 104/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.958, Loss: 1.501 Epoch 8 Batch 105/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.961, Loss: 1.430 Epoch 8 Batch 106/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.478 Epoch 8 Batch 107/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.956, Loss: 1.420 Epoch 8 Batch 108/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.950, Loss: 1.436 Epoch 8 Batch 109/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.440 Epoch 8 Batch 110/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.954, Loss: 1.454 Epoch 8 Batch 111/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.473 Epoch 8 Batch 112/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.954, Loss: 1.422 Epoch 8 Batch 113/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.391 Epoch 8 Batch 114/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.947, Loss: 1.380 Epoch 8 Batch 115/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.953, Loss: 1.406 Epoch 8 Batch 116/1077 - Train Accuracy: 0.926, Validation Accuracy: 0.958, Loss: 1.401 Epoch 8 Batch 117/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.427 Epoch 8 Batch 118/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.965, Loss: 1.408 Epoch 8 Batch 119/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.964, Loss: 1.473 Epoch 8 Batch 120/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.960, Loss: 1.546 Epoch 8 Batch 121/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.447 Epoch 8 Batch 122/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.950, Loss: 1.450 Epoch 8 Batch 123/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.953, Loss: 1.418 Epoch 8 Batch 124/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.953, Loss: 1.432 Epoch 8 Batch 125/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.952, Loss: 1.435 Epoch 8 Batch 126/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.954, Loss: 1.491 Epoch 8 Batch 127/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.959, Loss: 1.434 Epoch 8 Batch 128/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.959, Loss: 1.419 Epoch 8 Batch 129/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.959, Loss: 1.467 Epoch 8 Batch 130/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.958, Loss: 1.392 Epoch 8 Batch 131/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.958, Loss: 1.367 Epoch 8 Batch 132/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.961, Loss: 1.407 Epoch 8 Batch 133/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.961, Loss: 1.441 Epoch 8 Batch 134/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.961, Loss: 1.493 Epoch 8 Batch 135/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.959, Loss: 1.477 Epoch 8 Batch 136/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.959, Loss: 1.379 Epoch 8 Batch 137/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.961, Loss: 1.466 Epoch 8 Batch 138/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.505 Epoch 8 Batch 139/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.966, Loss: 1.421 Epoch 8 Batch 140/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.966, Loss: 1.358 Epoch 8 Batch 141/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.966, Loss: 1.473 Epoch 8 Batch 142/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.959, Loss: 1.399 Epoch 8 Batch 143/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.953, Loss: 1.476 Epoch 8 Batch 144/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.952, Loss: 1.452 Epoch 8 Batch 145/1077 - Train Accuracy: 0.992, Validation Accuracy: 0.951, Loss: 1.401 Epoch 8 Batch 146/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.946, Loss: 1.466 Epoch 8 Batch 147/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.946, Loss: 1.372 Epoch 8 Batch 148/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.956, Loss: 1.400 Epoch 8 Batch 149/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.468 Epoch 8 Batch 150/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.492 Epoch 8 Batch 151/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.957, Loss: 1.455 Epoch 8 Batch 152/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.455 Epoch 8 Batch 153/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.955, Loss: 1.473 Epoch 8 Batch 154/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.955, Loss: 1.518 Epoch 8 Batch 155/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.445 Epoch 8 Batch 156/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.376 Epoch 8 Batch 157/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.955, Loss: 1.416 Epoch 8 Batch 158/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.954, Loss: 1.415 Epoch 8 Batch 159/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.497 Epoch 8 Batch 160/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.952, Loss: 1.438 Epoch 8 Batch 161/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.953, Loss: 1.373 Epoch 8 Batch 162/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.953, Loss: 1.455 Epoch 8 Batch 163/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.954, Loss: 1.437 Epoch 8 Batch 164/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.954, Loss: 1.476 Epoch 8 Batch 165/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.405 Epoch 8 Batch 166/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.949, Loss: 1.323 Epoch 8 Batch 167/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.956, Loss: 1.419 Epoch 8 Batch 168/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.951, Loss: 1.426 Epoch 8 Batch 169/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.951, Loss: 1.434 Epoch 8 Batch 170/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.951, Loss: 1.390 Epoch 8 Batch 171/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.959, Loss: 1.496 Epoch 8 Batch 172/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.959, Loss: 1.420 Epoch 8 Batch 173/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.467 Epoch 8 Batch 174/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.418 Epoch 8 Batch 175/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.960, Loss: 1.466 Epoch 8 Batch 176/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.960, Loss: 1.329 Epoch 8 Batch 177/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.961, Loss: 1.503 Epoch 8 Batch 178/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.961, Loss: 1.450 Epoch 8 Batch 179/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.962, Loss: 1.343 Epoch 8 Batch 180/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.959, Loss: 1.377 Epoch 8 Batch 181/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.959, Loss: 1.395 Epoch 8 Batch 182/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.959, Loss: 1.403 Epoch 8 Batch 183/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.428 Epoch 8 Batch 184/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.954, Loss: 1.422 Epoch 8 Batch 185/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.462 Epoch 8 Batch 186/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.954, Loss: 1.449 Epoch 8 Batch 187/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.951, Loss: 1.362 Epoch 8 Batch 188/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.951, Loss: 1.497 Epoch 8 Batch 189/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.951, Loss: 1.392 Epoch 8 Batch 190/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.956, Loss: 1.471 Epoch 8 Batch 191/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.422 Epoch 8 Batch 192/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.957, Loss: 1.450 Epoch 8 Batch 193/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.483 Epoch 8 Batch 194/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.961, Loss: 1.421 Epoch 8 Batch 195/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.962, Loss: 1.429 Epoch 8 Batch 196/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.963, Loss: 1.439 Epoch 8 Batch 197/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.449 Epoch 8 Batch 198/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.401 Epoch 8 Batch 199/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.956, Loss: 1.470 Epoch 8 Batch 200/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.959, Loss: 1.434 Epoch 8 Batch 201/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.960, Loss: 1.411 Epoch 8 Batch 202/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.962, Loss: 1.401 Epoch 8 Batch 203/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.963, Loss: 1.387 Epoch 8 Batch 204/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.961, Loss: 1.444 Epoch 8 Batch 205/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.961, Loss: 1.382 Epoch 8 Batch 206/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.957, Loss: 1.488 Epoch 8 Batch 207/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.398 Epoch 8 Batch 208/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.957, Loss: 1.378 Epoch 8 Batch 209/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.960, Loss: 1.388 Epoch 8 Batch 210/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.442 Epoch 8 Batch 211/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.965, Loss: 1.445 Epoch 8 Batch 212/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.970, Loss: 1.421 Epoch 8 Batch 213/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.963, Loss: 1.552 Epoch 8 Batch 214/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.374 Epoch 8 Batch 215/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.963, Loss: 1.422 Epoch 8 Batch 216/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.960, Loss: 1.462 Epoch 8 Batch 217/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.958, Loss: 1.411 Epoch 8 Batch 218/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.958, Loss: 1.441 Epoch 8 Batch 219/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.955, Loss: 1.412 Epoch 8 Batch 220/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.959, Loss: 1.412 Epoch 8 Batch 221/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.492 Epoch 8 Batch 222/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.950, Loss: 1.487 Epoch 8 Batch 223/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.949, Loss: 1.494 Epoch 8 Batch 224/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.950, Loss: 1.428 Epoch 8 Batch 225/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.950, Loss: 1.411 Epoch 8 Batch 226/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.955, Loss: 1.417 Epoch 8 Batch 227/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.959, Loss: 1.427 Epoch 8 Batch 228/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.959, Loss: 1.498 Epoch 8 Batch 229/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.959, Loss: 1.486 Epoch 8 Batch 230/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.412 Epoch 8 Batch 231/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.955, Loss: 1.379 Epoch 8 Batch 232/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.955, Loss: 1.446 Epoch 8 Batch 233/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.955, Loss: 1.430 Epoch 8 Batch 234/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.955, Loss: 1.417 Epoch 8 Batch 235/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.953, Loss: 1.405 Epoch 8 Batch 236/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.953, Loss: 1.382 Epoch 8 Batch 237/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.435 Epoch 8 Batch 238/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.956, Loss: 1.311 Epoch 8 Batch 239/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.952, Loss: 1.381 Epoch 8 Batch 240/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.947, Loss: 1.461 Epoch 8 Batch 241/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.947, Loss: 1.475 Epoch 8 Batch 242/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.954, Loss: 1.393 Epoch 8 Batch 243/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.958, Loss: 1.387 Epoch 8 Batch 244/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.958, Loss: 1.451 Epoch 8 Batch 245/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.960, Loss: 1.336 Epoch 8 Batch 246/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.965, Loss: 1.390 Epoch 8 Batch 247/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.965, Loss: 1.426 Epoch 8 Batch 248/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.966, Loss: 1.516 Epoch 8 Batch 249/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.966, Loss: 1.421 Epoch 8 Batch 250/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.966, Loss: 1.383 Epoch 8 Batch 251/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.966, Loss: 1.446 Epoch 8 Batch 252/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.966, Loss: 1.422 Epoch 8 Batch 253/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.967, Loss: 1.486 Epoch 8 Batch 254/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.967, Loss: 1.479 Epoch 8 Batch 255/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.967, Loss: 1.405 Epoch 8 Batch 256/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.968, Loss: 1.411 Epoch 8 Batch 257/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.968, Loss: 1.493 Epoch 8 Batch 258/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.968, Loss: 1.473 Epoch 8 Batch 259/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.966, Loss: 1.445 Epoch 8 Batch 260/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.966, Loss: 1.406 Epoch 8 Batch 261/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.966, Loss: 1.474 Epoch 8 Batch 262/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.966, Loss: 1.383 Epoch 8 Batch 263/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.966, Loss: 1.425 Epoch 8 Batch 264/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.961, Loss: 1.423 Epoch 8 Batch 265/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.961, Loss: 1.415 Epoch 8 Batch 266/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.954, Loss: 1.411 Epoch 8 Batch 267/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.954, Loss: 1.439 Epoch 8 Batch 268/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.948, Loss: 1.472 Epoch 8 Batch 269/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.948, Loss: 1.442 Epoch 8 Batch 270/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.948, Loss: 1.414 Epoch 8 Batch 271/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.948, Loss: 1.353 Epoch 8 Batch 272/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.948, Loss: 1.519 Epoch 8 Batch 273/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.948, Loss: 1.430 Epoch 8 Batch 274/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.948, Loss: 1.439 Epoch 8 Batch 275/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.948, Loss: 1.488 Epoch 8 Batch 276/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.949, Loss: 1.463 Epoch 8 Batch 277/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.949, Loss: 1.460 Epoch 8 Batch 278/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.949, Loss: 1.420 Epoch 8 Batch 279/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.949, Loss: 1.481 Epoch 8 Batch 280/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.949, Loss: 1.420 Epoch 8 Batch 281/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.949, Loss: 1.446 Epoch 8 Batch 282/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.949, Loss: 1.504 Epoch 8 Batch 283/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.949, Loss: 1.399 Epoch 8 Batch 284/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.953, Loss: 1.402 Epoch 8 Batch 285/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.946, Loss: 1.385 Epoch 8 Batch 286/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.951, Loss: 1.397 Epoch 8 Batch 287/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.951, Loss: 1.382 Epoch 8 Batch 288/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.952, Loss: 1.427 Epoch 8 Batch 289/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.952, Loss: 1.492 Epoch 8 Batch 290/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.952, Loss: 1.401 Epoch 8 Batch 291/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.952, Loss: 1.461 Epoch 8 Batch 292/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.435 Epoch 8 Batch 293/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.384 Epoch 8 Batch 294/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.960, Loss: 1.395 Epoch 8 Batch 295/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.960, Loss: 1.550 Epoch 8 Batch 296/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.960, Loss: 1.364 Epoch 8 Batch 297/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.960, Loss: 1.432 Epoch 8 Batch 298/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.530 Epoch 8 Batch 299/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.401 Epoch 8 Batch 300/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.967, Loss: 1.511 Epoch 8 Batch 301/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.967, Loss: 1.487 Epoch 8 Batch 302/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.962, Loss: 1.368 Epoch 8 Batch 303/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.962, Loss: 1.491 Epoch 8 Batch 304/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.957, Loss: 1.452 Epoch 8 Batch 305/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.364 Epoch 8 Batch 306/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.961, Loss: 1.351 Epoch 8 Batch 307/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.957, Loss: 1.445 Epoch 8 Batch 308/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.957, Loss: 1.538 Epoch 8 Batch 309/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.952, Loss: 1.408 Epoch 8 Batch 310/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.957, Loss: 1.420 Epoch 8 Batch 311/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.957, Loss: 1.401 Epoch 8 Batch 312/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.952, Loss: 1.490 Epoch 8 Batch 313/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.952, Loss: 1.504 Epoch 8 Batch 314/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.348 Epoch 8 Batch 315/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.952, Loss: 1.374 Epoch 8 Batch 316/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.951, Loss: 1.378 Epoch 8 Batch 317/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.951, Loss: 1.492 Epoch 8 Batch 318/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.956, Loss: 1.406 Epoch 8 Batch 319/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.961, Loss: 1.439 Epoch 8 Batch 320/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.415 Epoch 8 Batch 321/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.947, Loss: 1.452 Epoch 8 Batch 322/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.941, Loss: 1.405 Epoch 8 Batch 323/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.946, Loss: 1.424 Epoch 8 Batch 324/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.948, Loss: 1.422 Epoch 8 Batch 325/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.944, Loss: 1.381 Epoch 8 Batch 326/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.944, Loss: 1.432 Epoch 8 Batch 327/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.944, Loss: 1.486 Epoch 8 Batch 328/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.944, Loss: 1.434 Epoch 8 Batch 329/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.944, Loss: 1.495 Epoch 8 Batch 330/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.944, Loss: 1.464 Epoch 8 Batch 331/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.944, Loss: 1.494 Epoch 8 Batch 332/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.944, Loss: 1.345 Epoch 8 Batch 333/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.949, Loss: 1.393 Epoch 8 Batch 334/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.953, Loss: 1.373 Epoch 8 Batch 335/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.446 Epoch 8 Batch 336/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.958, Loss: 1.468 Epoch 8 Batch 337/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.457 Epoch 8 Batch 338/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.968, Loss: 1.459 Epoch 8 Batch 339/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.968, Loss: 1.460 Epoch 8 Batch 340/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.966, Loss: 1.402 Epoch 8 Batch 341/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.963, Loss: 1.422 Epoch 8 Batch 342/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.451 Epoch 8 Batch 343/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.953, Loss: 1.404 Epoch 8 Batch 344/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.953, Loss: 1.516 Epoch 8 Batch 345/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.951, Loss: 1.422 Epoch 8 Batch 346/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.947, Loss: 1.446 Epoch 8 Batch 347/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.947, Loss: 1.417 Epoch 8 Batch 348/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.946, Loss: 1.394 Epoch 8 Batch 349/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.946, Loss: 1.402 Epoch 8 Batch 350/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.949, Loss: 1.485 Epoch 8 Batch 351/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.949, Loss: 1.420 Epoch 8 Batch 352/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.944, Loss: 1.432 Epoch 8 Batch 353/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.942, Loss: 1.458 Epoch 8 Batch 354/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.938, Loss: 1.473 Epoch 8 Batch 355/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.933, Loss: 1.432 Epoch 8 Batch 356/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.939, Loss: 1.392 Epoch 8 Batch 357/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.935, Loss: 1.436 Epoch 8 Batch 358/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.940, Loss: 1.504 Epoch 8 Batch 359/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.940, Loss: 1.413 Epoch 8 Batch 360/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.940, Loss: 1.430 Epoch 8 Batch 361/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.950, Loss: 1.376 Epoch 8 Batch 362/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.416 Epoch 8 Batch 363/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.954, Loss: 1.489 Epoch 8 Batch 364/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.954, Loss: 1.435 Epoch 8 Batch 365/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.958, Loss: 1.431 Epoch 8 Batch 366/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.456 Epoch 8 Batch 367/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.958, Loss: 1.422 Epoch 8 Batch 368/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.956, Loss: 1.382 Epoch 8 Batch 369/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.956, Loss: 1.406 Epoch 8 Batch 370/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.961, Loss: 1.460 Epoch 8 Batch 371/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.956, Loss: 1.429 Epoch 8 Batch 372/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.956, Loss: 1.471 Epoch 8 Batch 373/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.951, Loss: 1.556 Epoch 8 Batch 374/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.956, Loss: 1.431 Epoch 8 Batch 375/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.956, Loss: 1.404 Epoch 8 Batch 376/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.465 Epoch 8 Batch 377/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.440 Epoch 8 Batch 378/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.390 Epoch 8 Batch 379/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.427 Epoch 8 Batch 380/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.956, Loss: 1.369 Epoch 8 Batch 381/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.956, Loss: 1.423 Epoch 8 Batch 382/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.956, Loss: 1.444 Epoch 8 Batch 383/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.435 Epoch 8 Batch 384/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.956, Loss: 1.405 Epoch 8 Batch 385/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.956, Loss: 1.415 Epoch 8 Batch 386/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.954, Loss: 1.420 Epoch 8 Batch 387/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.954, Loss: 1.485 Epoch 8 Batch 388/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.955, Loss: 1.474 Epoch 8 Batch 389/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.518 Epoch 8 Batch 390/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.958, Loss: 1.439 Epoch 8 Batch 391/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.487 Epoch 8 Batch 392/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.436 Epoch 8 Batch 393/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.958, Loss: 1.456 Epoch 8 Batch 394/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.958, Loss: 1.475 Epoch 8 Batch 395/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.959, Loss: 1.506 Epoch 8 Batch 396/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.418 Epoch 8 Batch 397/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.953, Loss: 1.442 Epoch 8 Batch 398/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.948, Loss: 1.417 Epoch 8 Batch 399/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.948, Loss: 1.441 Epoch 8 Batch 400/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.431 Epoch 8 Batch 401/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.952, Loss: 1.475 Epoch 8 Batch 402/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.398 Epoch 8 Batch 403/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.955, Loss: 1.420 Epoch 8 Batch 404/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.950, Loss: 1.431 Epoch 8 Batch 405/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.950, Loss: 1.400 Epoch 8 Batch 406/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.432 Epoch 8 Batch 407/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.949, Loss: 1.337 Epoch 8 Batch 408/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.410 Epoch 8 Batch 409/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.949, Loss: 1.474 Epoch 8 Batch 410/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.953, Loss: 1.508 Epoch 8 Batch 411/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.953, Loss: 1.479 Epoch 8 Batch 412/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.954, Loss: 1.414 Epoch 8 Batch 413/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.954, Loss: 1.417 Epoch 8 Batch 414/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.954, Loss: 1.374 Epoch 8 Batch 415/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.959, Loss: 1.441 Epoch 8 Batch 416/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.958, Loss: 1.316 Epoch 8 Batch 417/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.435 Epoch 8 Batch 418/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.383 Epoch 8 Batch 419/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.956, Loss: 1.434 Epoch 8 Batch 420/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.956, Loss: 1.349 Epoch 8 Batch 421/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.395 Epoch 8 Batch 422/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.369 Epoch 8 Batch 423/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.956, Loss: 1.419 Epoch 8 Batch 424/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.956, Loss: 1.328 Epoch 8 Batch 425/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.958, Loss: 1.413 Epoch 8 Batch 426/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.505 Epoch 8 Batch 427/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.965, Loss: 1.368 Epoch 8 Batch 428/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.966, Loss: 1.452 Epoch 8 Batch 429/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.966, Loss: 1.365 Epoch 8 Batch 430/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.966, Loss: 1.439 Epoch 8 Batch 431/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.966, Loss: 1.470 Epoch 8 Batch 432/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.966, Loss: 1.428 Epoch 8 Batch 433/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.966, Loss: 1.471 Epoch 8 Batch 434/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.960, Loss: 1.438 Epoch 8 Batch 435/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.955, Loss: 1.442 Epoch 8 Batch 436/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.514 Epoch 8 Batch 437/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.955, Loss: 1.372 Epoch 8 Batch 438/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.955, Loss: 1.407 Epoch 8 Batch 439/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.405 Epoch 8 Batch 440/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.958, Loss: 1.432 Epoch 8 Batch 441/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.953, Loss: 1.476 Epoch 8 Batch 442/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.955, Loss: 1.417 Epoch 8 Batch 443/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.954, Loss: 1.456 Epoch 8 Batch 444/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.954, Loss: 1.385 Epoch 8 Batch 445/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.953, Loss: 1.412 Epoch 8 Batch 446/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.953, Loss: 1.409 Epoch 8 Batch 447/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.956, Loss: 1.409 Epoch 8 Batch 448/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.429 Epoch 8 Batch 449/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.431 Epoch 8 Batch 450/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.950, Loss: 1.441 Epoch 8 Batch 451/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.945, Loss: 1.446 Epoch 8 Batch 452/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.945, Loss: 1.372 Epoch 8 Batch 453/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.945, Loss: 1.408 Epoch 8 Batch 454/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.945, Loss: 1.364 Epoch 8 Batch 455/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.945, Loss: 1.355 Epoch 8 Batch 456/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.945, Loss: 1.453 Epoch 8 Batch 457/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.945, Loss: 1.385 Epoch 8 Batch 458/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.945, Loss: 1.403 Epoch 8 Batch 459/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.430 Epoch 8 Batch 460/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.362 Epoch 8 Batch 461/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.435 Epoch 8 Batch 462/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.954, Loss: 1.366 Epoch 8 Batch 463/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.447 Epoch 8 Batch 464/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.405 Epoch 8 Batch 465/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.954, Loss: 1.505 Epoch 8 Batch 466/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.954, Loss: 1.428 Epoch 8 Batch 467/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.954, Loss: 1.455 Epoch 8 Batch 468/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.392 Epoch 8 Batch 469/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.950, Loss: 1.437 Epoch 8 Batch 470/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.945, Loss: 1.455 Epoch 8 Batch 471/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.946, Loss: 1.419 Epoch 8 Batch 472/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.946, Loss: 1.413 Epoch 8 Batch 473/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.950, Loss: 1.415 Epoch 8 Batch 474/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.389 Epoch 8 Batch 475/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.953, Loss: 1.468 Epoch 8 Batch 476/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.957, Loss: 1.369 Epoch 8 Batch 477/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.953, Loss: 1.423 Epoch 8 Batch 478/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.953, Loss: 1.416 Epoch 8 Batch 479/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.455 Epoch 8 Batch 480/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.957, Loss: 1.408 Epoch 8 Batch 481/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.952, Loss: 1.440 Epoch 8 Batch 482/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.435 Epoch 8 Batch 483/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.954, Loss: 1.508 Epoch 8 Batch 484/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.959, Loss: 1.425 Epoch 8 Batch 485/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.412 Epoch 8 Batch 486/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.961, Loss: 1.433 Epoch 8 Batch 487/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.961, Loss: 1.429 Epoch 8 Batch 488/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.400 Epoch 8 Batch 489/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.952, Loss: 1.423 Epoch 8 Batch 490/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.941, Loss: 1.376 Epoch 8 Batch 491/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.945, Loss: 1.435 Epoch 8 Batch 492/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.945, Loss: 1.478 Epoch 8 Batch 493/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.945, Loss: 1.427 Epoch 8 Batch 494/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.945, Loss: 1.424 Epoch 8 Batch 495/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.945, Loss: 1.390 Epoch 8 Batch 496/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.945, Loss: 1.412 Epoch 8 Batch 497/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.945, Loss: 1.407 Epoch 8 Batch 498/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.959, Loss: 1.446 Epoch 8 Batch 499/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.964, Loss: 1.411 Epoch 8 Batch 500/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.964, Loss: 1.425 Epoch 8 Batch 501/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.968, Loss: 1.383 Epoch 8 Batch 502/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.968, Loss: 1.499 Epoch 8 Batch 503/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.975, Loss: 1.481 Epoch 8 Batch 504/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.970, Loss: 1.465 Epoch 8 Batch 505/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.399 Epoch 8 Batch 506/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.960, Loss: 1.464 Epoch 8 Batch 507/1077 - Train Accuracy: 0.937, Validation Accuracy: 0.960, Loss: 1.483 Epoch 8 Batch 508/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.959, Loss: 1.388 Epoch 8 Batch 509/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.959, Loss: 1.411 Epoch 8 Batch 510/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.959, Loss: 1.394 Epoch 8 Batch 511/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.959, Loss: 1.382 Epoch 8 Batch 512/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.965, Loss: 1.411 Epoch 8 Batch 513/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.960, Loss: 1.487 Epoch 8 Batch 514/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.967, Loss: 1.453 Epoch 8 Batch 515/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.967, Loss: 1.442 Epoch 8 Batch 516/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.967, Loss: 1.426 Epoch 8 Batch 517/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.967, Loss: 1.416 Epoch 8 Batch 518/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.967, Loss: 1.418 Epoch 8 Batch 519/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.959, Loss: 1.372 Epoch 8 Batch 520/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.959, Loss: 1.414 Epoch 8 Batch 521/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.959, Loss: 1.427 Epoch 8 Batch 522/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.954, Loss: 1.416 Epoch 8 Batch 523/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.367 Epoch 8 Batch 524/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.424 Epoch 8 Batch 525/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.962, Loss: 1.452 Epoch 8 Batch 526/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.962, Loss: 1.423 Epoch 8 Batch 527/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.963, Loss: 1.467 Epoch 8 Batch 528/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.963, Loss: 1.460 Epoch 8 Batch 529/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.967, Loss: 1.464 Epoch 8 Batch 530/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.960, Loss: 1.455 Epoch 8 Batch 531/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.960, Loss: 1.458 Epoch 8 Batch 532/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.955, Loss: 1.431 Epoch 8 Batch 533/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.955, Loss: 1.436 Epoch 8 Batch 534/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.959, Loss: 1.439 Epoch 8 Batch 535/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.428 Epoch 8 Batch 536/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.963, Loss: 1.384 Epoch 8 Batch 537/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.963, Loss: 1.425 Epoch 8 Batch 538/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.434 Epoch 8 Batch 539/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.963, Loss: 1.463 Epoch 8 Batch 540/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.958, Loss: 1.384 Epoch 8 Batch 541/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.963, Loss: 1.369 Epoch 8 Batch 542/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.384 Epoch 8 Batch 543/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.964, Loss: 1.407 Epoch 8 Batch 544/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.963, Loss: 1.482 Epoch 8 Batch 545/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.968, Loss: 1.340 Epoch 8 Batch 546/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.968, Loss: 1.422 Epoch 8 Batch 547/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.968, Loss: 1.438 Epoch 8 Batch 548/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.968, Loss: 1.482 Epoch 8 Batch 549/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.973, Loss: 1.467 Epoch 8 Batch 550/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.968, Loss: 1.434 Epoch 8 Batch 551/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.963, Loss: 1.437 Epoch 8 Batch 552/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.509 Epoch 8 Batch 553/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.963, Loss: 1.433 Epoch 8 Batch 554/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.963, Loss: 1.408 Epoch 8 Batch 555/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.959, Loss: 1.384 Epoch 8 Batch 556/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.499 Epoch 8 Batch 557/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.964, Loss: 1.413 Epoch 8 Batch 558/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.331 Epoch 8 Batch 559/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.964, Loss: 1.430 Epoch 8 Batch 560/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.964, Loss: 1.417 Epoch 8 Batch 561/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.964, Loss: 1.371 Epoch 8 Batch 562/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.962, Loss: 1.380 Epoch 8 Batch 563/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.962, Loss: 1.441 Epoch 8 Batch 564/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.962, Loss: 1.460 Epoch 8 Batch 565/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.962, Loss: 1.408 Epoch 8 Batch 566/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.425 Epoch 8 Batch 567/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.963, Loss: 1.433 Epoch 8 Batch 568/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.963, Loss: 1.374 Epoch 8 Batch 569/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.968, Loss: 1.404 Epoch 8 Batch 570/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.968, Loss: 1.466 Epoch 8 Batch 571/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.973, Loss: 1.441 Epoch 8 Batch 572/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.970, Loss: 1.489 Epoch 8 Batch 573/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.966, Loss: 1.375 Epoch 8 Batch 574/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.447 Epoch 8 Batch 575/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.484 Epoch 8 Batch 576/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.959, Loss: 1.460 Epoch 8 Batch 577/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.396 Epoch 8 Batch 578/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.399 Epoch 8 Batch 579/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.967, Loss: 1.534 Epoch 8 Batch 580/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.968, Loss: 1.435 Epoch 8 Batch 581/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.967, Loss: 1.425 Epoch 8 Batch 582/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.968, Loss: 1.475 Epoch 8 Batch 583/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.968, Loss: 1.386 Epoch 8 Batch 584/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.474 Epoch 8 Batch 585/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.973, Loss: 1.386 Epoch 8 Batch 586/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.973, Loss: 1.469 Epoch 8 Batch 587/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.973, Loss: 1.459 Epoch 8 Batch 588/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.972, Loss: 1.398 Epoch 8 Batch 589/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.966, Loss: 1.458 Epoch 8 Batch 590/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.966, Loss: 1.450 Epoch 8 Batch 591/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.966, Loss: 1.411 Epoch 8 Batch 592/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.971, Loss: 1.356 Epoch 8 Batch 593/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.971, Loss: 1.379 Epoch 8 Batch 594/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.971, Loss: 1.406 Epoch 8 Batch 595/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.971, Loss: 1.356 Epoch 8 Batch 596/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.970, Loss: 1.463 Epoch 8 Batch 597/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.970, Loss: 1.422 Epoch 8 Batch 598/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.970, Loss: 1.368 Epoch 8 Batch 599/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.972, Loss: 1.463 Epoch 8 Batch 600/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.972, Loss: 1.391 Epoch 8 Batch 601/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.972, Loss: 1.451 Epoch 8 Batch 602/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.974, Loss: 1.408 Epoch 8 Batch 603/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.972, Loss: 1.440 Epoch 8 Batch 604/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.965, Loss: 1.482 Epoch 8 Batch 605/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.965, Loss: 1.467 Epoch 8 Batch 606/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.965, Loss: 1.316 Epoch 8 Batch 607/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.965, Loss: 1.423 Epoch 8 Batch 608/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.475 Epoch 8 Batch 609/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.957, Loss: 1.394 Epoch 8 Batch 610/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.433 Epoch 8 Batch 611/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.961, Loss: 1.387 Epoch 8 Batch 612/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.961, Loss: 1.386 Epoch 8 Batch 613/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.957, Loss: 1.419 Epoch 8 Batch 614/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.957, Loss: 1.427 Epoch 8 Batch 615/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.957, Loss: 1.462 Epoch 8 Batch 616/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.961, Loss: 1.434 Epoch 8 Batch 617/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.974, Loss: 1.427 Epoch 8 Batch 618/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.969, Loss: 1.414 Epoch 8 Batch 619/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.971, Loss: 1.479 Epoch 8 Batch 620/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.969, Loss: 1.507 Epoch 8 Batch 621/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.970, Loss: 1.434 Epoch 8 Batch 622/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.970, Loss: 1.368 Epoch 8 Batch 623/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.969, Loss: 1.496 Epoch 8 Batch 624/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.969, Loss: 1.365 Epoch 8 Batch 625/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.963, Loss: 1.480 Epoch 8 Batch 626/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.963, Loss: 1.399 Epoch 8 Batch 627/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.963, Loss: 1.381 Epoch 8 Batch 628/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.381 Epoch 8 Batch 629/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.958, Loss: 1.401 Epoch 8 Batch 630/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.958, Loss: 1.381 Epoch 8 Batch 631/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.352 Epoch 8 Batch 632/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.460 Epoch 8 Batch 633/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.958, Loss: 1.442 Epoch 8 Batch 634/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.958, Loss: 1.432 Epoch 8 Batch 635/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.958, Loss: 1.375 Epoch 8 Batch 636/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.958, Loss: 1.406 Epoch 8 Batch 637/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.958, Loss: 1.460 Epoch 8 Batch 638/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.963, Loss: 1.456 Epoch 8 Batch 639/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.487 Epoch 8 Batch 640/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.382 Epoch 8 Batch 641/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.963, Loss: 1.447 Epoch 8 Batch 642/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.963, Loss: 1.444 Epoch 8 Batch 643/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.968, Loss: 1.459 Epoch 8 Batch 644/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.966, Loss: 1.423 Epoch 8 Batch 645/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.966, Loss: 1.460 Epoch 8 Batch 646/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.961, Loss: 1.498 Epoch 8 Batch 647/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.472 Epoch 8 Batch 648/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.955, Loss: 1.416 Epoch 8 Batch 649/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.351 Epoch 8 Batch 650/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.961, Loss: 1.443 Epoch 8 Batch 651/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.402 Epoch 8 Batch 652/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.966, Loss: 1.426 Epoch 8 Batch 653/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.966, Loss: 1.431 Epoch 8 Batch 654/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.963, Loss: 1.369 Epoch 8 Batch 655/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.961, Loss: 1.331 Epoch 8 Batch 656/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.435 Epoch 8 Batch 657/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.958, Loss: 1.464 Epoch 8 Batch 658/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.965, Loss: 1.427 Epoch 8 Batch 659/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.970, Loss: 1.445 Epoch 8 Batch 660/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.970, Loss: 1.480 Epoch 8 Batch 661/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.970, Loss: 1.450 Epoch 8 Batch 662/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.970, Loss: 1.437 Epoch 8 Batch 663/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.974, Loss: 1.443 Epoch 8 Batch 664/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.974, Loss: 1.404 Epoch 8 Batch 665/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.974, Loss: 1.496 Epoch 8 Batch 666/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.972, Loss: 1.493 Epoch 8 Batch 667/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.457 Epoch 8 Batch 668/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.972, Loss: 1.415 Epoch 8 Batch 669/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.972, Loss: 1.457 Epoch 8 Batch 670/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.962, Loss: 1.380 Epoch 8 Batch 671/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.963, Loss: 1.342 Epoch 8 Batch 672/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.425 Epoch 8 Batch 673/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.971, Loss: 1.395 Epoch 8 Batch 674/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.975, Loss: 1.422 Epoch 8 Batch 675/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.975, Loss: 1.454 Epoch 8 Batch 676/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.973, Loss: 1.390 Epoch 8 Batch 677/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.978, Loss: 1.482 Epoch 8 Batch 678/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.973, Loss: 1.363 Epoch 8 Batch 679/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.973, Loss: 1.456 Epoch 8 Batch 680/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.973, Loss: 1.474 Epoch 8 Batch 681/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.973, Loss: 1.392 Epoch 8 Batch 682/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.959, Loss: 1.425 Epoch 8 Batch 683/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.353 Epoch 8 Batch 684/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.959, Loss: 1.436 Epoch 8 Batch 685/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.959, Loss: 1.366 Epoch 8 Batch 686/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.949, Loss: 1.481 Epoch 8 Batch 687/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.949, Loss: 1.457 Epoch 8 Batch 688/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.941, Loss: 1.402 Epoch 8 Batch 689/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.941, Loss: 1.381 Epoch 8 Batch 690/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.941, Loss: 1.366 Epoch 8 Batch 691/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.941, Loss: 1.378 Epoch 8 Batch 692/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.952, Loss: 1.373 Epoch 8 Batch 693/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.952, Loss: 1.424 Epoch 8 Batch 694/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.952, Loss: 1.456 Epoch 8 Batch 695/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.954, Loss: 1.365 Epoch 8 Batch 696/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.955, Loss: 1.448 Epoch 8 Batch 697/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.423 Epoch 8 Batch 698/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.950, Loss: 1.435 Epoch 8 Batch 699/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.950, Loss: 1.447 Epoch 8 Batch 700/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.954, Loss: 1.373 Epoch 8 Batch 701/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.959, Loss: 1.384 Epoch 8 Batch 702/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.959, Loss: 1.467 Epoch 8 Batch 703/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.959, Loss: 1.479 Epoch 8 Batch 704/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.959, Loss: 1.515 Epoch 8 Batch 705/1077 - Train Accuracy: 0.931, Validation Accuracy: 0.964, Loss: 1.498 Epoch 8 Batch 706/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.964, Loss: 1.459 Epoch 8 Batch 707/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.968, Loss: 1.411 Epoch 8 Batch 708/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.404 Epoch 8 Batch 709/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.968, Loss: 1.544 Epoch 8 Batch 710/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.968, Loss: 1.412 Epoch 8 Batch 711/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.968, Loss: 1.465 Epoch 8 Batch 712/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.963, Loss: 1.414 Epoch 8 Batch 713/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.958, Loss: 1.395 Epoch 8 Batch 714/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.958, Loss: 1.474 Epoch 8 Batch 715/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.425 Epoch 8 Batch 716/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.463 Epoch 8 Batch 717/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.955, Loss: 1.424 Epoch 8 Batch 718/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.430 Epoch 8 Batch 719/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.957, Loss: 1.374 Epoch 8 Batch 720/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.959, Loss: 1.414 Epoch 8 Batch 721/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.959, Loss: 1.385 Epoch 8 Batch 722/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.964, Loss: 1.392 Epoch 8 Batch 723/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.964, Loss: 1.436 Epoch 8 Batch 724/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.511 Epoch 8 Batch 725/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.963, Loss: 1.475 Epoch 8 Batch 726/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.961, Loss: 1.358 Epoch 8 Batch 727/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.960, Loss: 1.442 Epoch 8 Batch 728/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.955, Loss: 1.524 Epoch 8 Batch 729/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.955, Loss: 1.408 Epoch 8 Batch 730/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.949, Loss: 1.448 Epoch 8 Batch 731/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.951, Loss: 1.546 Epoch 8 Batch 732/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.951, Loss: 1.369 Epoch 8 Batch 733/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.949, Loss: 1.503 Epoch 8 Batch 734/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.425 Epoch 8 Batch 735/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.942, Loss: 1.424 Epoch 8 Batch 736/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.939, Loss: 1.384 Epoch 8 Batch 737/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.940, Loss: 1.446 Epoch 8 Batch 738/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.941, Loss: 1.483 Epoch 8 Batch 739/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.941, Loss: 1.443 Epoch 8 Batch 740/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.941, Loss: 1.401 Epoch 8 Batch 741/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.453 Epoch 8 Batch 742/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.960, Loss: 1.516 Epoch 8 Batch 743/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.435 Epoch 8 Batch 744/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.477 Epoch 8 Batch 745/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.957, Loss: 1.456 Epoch 8 Batch 746/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.957, Loss: 1.401 Epoch 8 Batch 747/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.498 Epoch 8 Batch 748/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.964, Loss: 1.411 Epoch 8 Batch 749/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.969, Loss: 1.485 Epoch 8 Batch 750/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.969, Loss: 1.447 Epoch 8 Batch 751/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.969, Loss: 1.392 Epoch 8 Batch 752/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.968, Loss: 1.357 Epoch 8 Batch 753/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.966, Loss: 1.329 Epoch 8 Batch 754/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.962, Loss: 1.491 Epoch 8 Batch 755/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.962, Loss: 1.435 Epoch 8 Batch 756/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.967, Loss: 1.404 Epoch 8 Batch 757/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.967, Loss: 1.363 Epoch 8 Batch 758/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.965, Loss: 1.463 Epoch 8 Batch 759/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.965, Loss: 1.423 Epoch 8 Batch 760/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.965, Loss: 1.524 Epoch 8 Batch 761/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.971, Loss: 1.411 Epoch 8 Batch 762/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.971, Loss: 1.501 Epoch 8 Batch 763/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.971, Loss: 1.443 Epoch 8 Batch 764/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.971, Loss: 1.414 Epoch 8 Batch 765/1077 - Train Accuracy: 0.941, Validation Accuracy: 0.982, Loss: 1.462 Epoch 8 Batch 766/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.982, Loss: 1.531 Epoch 8 Batch 767/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.968, Loss: 1.534 Epoch 8 Batch 768/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.968, Loss: 1.422 Epoch 8 Batch 769/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.416 Epoch 8 Batch 770/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.412 Epoch 8 Batch 771/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.958, Loss: 1.400 Epoch 8 Batch 772/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.958, Loss: 1.524 Epoch 8 Batch 773/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.489 Epoch 8 Batch 774/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.958, Loss: 1.441 Epoch 8 Batch 775/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.969, Loss: 1.383 Epoch 8 Batch 776/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.970, Loss: 1.396 Epoch 8 Batch 777/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.963, Loss: 1.442 Epoch 8 Batch 778/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.488 Epoch 8 Batch 779/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.958, Loss: 1.343 Epoch 8 Batch 780/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.953, Loss: 1.441 Epoch 8 Batch 781/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.963, Loss: 1.467 Epoch 8 Batch 782/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.958, Loss: 1.433 Epoch 8 Batch 783/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.361 Epoch 8 Batch 784/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.963, Loss: 1.534 Epoch 8 Batch 785/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.957, Loss: 1.426 Epoch 8 Batch 786/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.952, Loss: 1.416 Epoch 8 Batch 787/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.952, Loss: 1.420 Epoch 8 Batch 788/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.950, Loss: 1.424 Epoch 8 Batch 789/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.945, Loss: 1.501 Epoch 8 Batch 790/1077 - Train Accuracy: 0.933, Validation Accuracy: 0.945, Loss: 1.493 Epoch 8 Batch 791/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.950, Loss: 1.393 Epoch 8 Batch 792/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.949, Loss: 1.558 Epoch 8 Batch 793/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.950, Loss: 1.392 Epoch 8 Batch 794/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.953, Loss: 1.450 Epoch 8 Batch 795/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.953, Loss: 1.433 Epoch 8 Batch 796/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.953, Loss: 1.411 Epoch 8 Batch 797/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.953, Loss: 1.410 Epoch 8 Batch 798/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.953, Loss: 1.436 Epoch 8 Batch 799/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.953, Loss: 1.433 Epoch 8 Batch 800/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.954, Loss: 1.445 Epoch 8 Batch 801/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.945, Loss: 1.370 Epoch 8 Batch 802/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.941, Loss: 1.473 Epoch 8 Batch 803/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.497 Epoch 8 Batch 804/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.952, Loss: 1.424 Epoch 8 Batch 805/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.407 Epoch 8 Batch 806/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.953, Loss: 1.439 Epoch 8 Batch 807/1077 - Train Accuracy: 0.994, Validation Accuracy: 0.955, Loss: 1.475 Epoch 8 Batch 808/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.955, Loss: 1.598 Epoch 8 Batch 809/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.955, Loss: 1.455 Epoch 8 Batch 810/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.955, Loss: 1.475 Epoch 8 Batch 811/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.955, Loss: 1.523 Epoch 8 Batch 812/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.426 Epoch 8 Batch 813/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.953, Loss: 1.493 Epoch 8 Batch 814/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.953, Loss: 1.478 Epoch 8 Batch 815/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.956, Loss: 1.480 Epoch 8 Batch 816/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.956, Loss: 1.360 Epoch 8 Batch 817/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.953, Loss: 1.450 Epoch 8 Batch 818/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.953, Loss: 1.410 Epoch 8 Batch 819/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.958, Loss: 1.410 Epoch 8 Batch 820/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.430 Epoch 8 Batch 821/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.965, Loss: 1.455 Epoch 8 Batch 822/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.962, Loss: 1.419 Epoch 8 Batch 823/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.967, Loss: 1.456 Epoch 8 Batch 824/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.968, Loss: 1.417 Epoch 8 Batch 825/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.974, Loss: 1.404 Epoch 8 Batch 826/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.972, Loss: 1.401 Epoch 8 Batch 827/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.974, Loss: 1.420 Epoch 8 Batch 828/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.972, Loss: 1.467 Epoch 8 Batch 829/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.972, Loss: 1.443 Epoch 8 Batch 830/1077 - Train Accuracy: 0.940, Validation Accuracy: 0.972, Loss: 1.405 Epoch 8 Batch 831/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.972, Loss: 1.490 Epoch 8 Batch 832/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.972, Loss: 1.342 Epoch 8 Batch 833/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.971, Loss: 1.425 Epoch 8 Batch 834/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.972, Loss: 1.395 Epoch 8 Batch 835/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.972, Loss: 1.465 Epoch 8 Batch 836/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.972, Loss: 1.449 Epoch 8 Batch 837/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.972, Loss: 1.495 Epoch 8 Batch 838/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.977, Loss: 1.354 Epoch 8 Batch 839/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.976, Loss: 1.403 Epoch 8 Batch 840/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.981, Loss: 1.433 Epoch 8 Batch 841/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.976, Loss: 1.444 Epoch 8 Batch 842/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.976, Loss: 1.397 Epoch 8 Batch 843/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.971, Loss: 1.386 Epoch 8 Batch 844/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.969, Loss: 1.453 Epoch 8 Batch 845/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.969, Loss: 1.412 Epoch 8 Batch 846/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.964, Loss: 1.431 Epoch 8 Batch 847/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.960, Loss: 1.466 Epoch 8 Batch 848/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.960, Loss: 1.484 Epoch 8 Batch 849/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.964, Loss: 1.370 Epoch 8 Batch 850/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.964, Loss: 1.405 Epoch 8 Batch 851/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.969, Loss: 1.386 Epoch 8 Batch 852/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.969, Loss: 1.437 Epoch 8 Batch 853/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.967, Loss: 1.434 Epoch 8 Batch 854/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.962, Loss: 1.449 Epoch 8 Batch 855/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.969, Loss: 1.454 Epoch 8 Batch 856/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.404 Epoch 8 Batch 857/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.402 Epoch 8 Batch 858/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.373 Epoch 8 Batch 859/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.963, Loss: 1.501 Epoch 8 Batch 860/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.958, Loss: 1.388 Epoch 8 Batch 861/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.963, Loss: 1.396 Epoch 8 Batch 862/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.963, Loss: 1.374 Epoch 8 Batch 863/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.968, Loss: 1.433 Epoch 8 Batch 864/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.962, Loss: 1.348 Epoch 8 Batch 865/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.961, Loss: 1.418 Epoch 8 Batch 866/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.437 Epoch 8 Batch 867/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.963, Loss: 1.489 Epoch 8 Batch 868/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.969, Loss: 1.435 Epoch 8 Batch 869/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.974, Loss: 1.448 Epoch 8 Batch 870/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.976, Loss: 1.496 Epoch 8 Batch 871/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.976, Loss: 1.369 Epoch 8 Batch 872/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.976, Loss: 1.416 Epoch 8 Batch 873/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.976, Loss: 1.463 Epoch 8 Batch 874/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.974, Loss: 1.428 Epoch 8 Batch 875/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.974, Loss: 1.451 Epoch 8 Batch 876/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.974, Loss: 1.457 Epoch 8 Batch 877/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.973, Loss: 1.342 Epoch 8 Batch 878/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.973, Loss: 1.452 Epoch 8 Batch 879/1077 - Train Accuracy: 0.996, Validation Accuracy: 0.973, Loss: 1.412 Epoch 8 Batch 880/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.967, Loss: 1.373 Epoch 8 Batch 881/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.967, Loss: 1.385 Epoch 8 Batch 882/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.968, Loss: 1.386 Epoch 8 Batch 883/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.968, Loss: 1.397 Epoch 8 Batch 884/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.967, Loss: 1.406 Epoch 8 Batch 885/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.967, Loss: 1.394 Epoch 8 Batch 886/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.383 Epoch 8 Batch 887/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.961, Loss: 1.398 Epoch 8 Batch 888/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.966, Loss: 1.400 Epoch 8 Batch 889/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.968, Loss: 1.413 Epoch 8 Batch 890/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.968, Loss: 1.433 Epoch 8 Batch 891/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.968, Loss: 1.463 Epoch 8 Batch 892/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.968, Loss: 1.449 Epoch 8 Batch 893/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.968, Loss: 1.451 Epoch 8 Batch 894/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.969, Loss: 1.362 Epoch 8 Batch 895/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.969, Loss: 1.464 Epoch 8 Batch 896/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.969, Loss: 1.477 Epoch 8 Batch 897/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.969, Loss: 1.461 Epoch 8 Batch 898/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.969, Loss: 1.437 Epoch 8 Batch 899/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.969, Loss: 1.436 Epoch 8 Batch 900/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.970, Loss: 1.455 Epoch 8 Batch 901/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.970, Loss: 1.447 Epoch 8 Batch 902/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.968, Loss: 1.393 Epoch 8 Batch 903/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.964, Loss: 1.437 Epoch 8 Batch 904/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.964, Loss: 1.416 Epoch 8 Batch 905/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.968, Loss: 1.482 Epoch 8 Batch 906/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.968, Loss: 1.407 Epoch 8 Batch 907/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.974, Loss: 1.395 Epoch 8 Batch 908/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.969, Loss: 1.449 Epoch 8 Batch 909/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.969, Loss: 1.458 Epoch 8 Batch 910/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.969, Loss: 1.419 Epoch 8 Batch 911/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.969, Loss: 1.410 Epoch 8 Batch 912/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.969, Loss: 1.363 Epoch 8 Batch 913/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.968, Loss: 1.492 Epoch 8 Batch 914/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.968, Loss: 1.431 Epoch 8 Batch 915/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.968, Loss: 1.395 Epoch 8 Batch 916/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.968, Loss: 1.448 Epoch 8 Batch 917/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.970, Loss: 1.354 Epoch 8 Batch 918/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.969, Loss: 1.424 Epoch 8 Batch 919/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.974, Loss: 1.392 Epoch 8 Batch 920/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.970, Loss: 1.474 Epoch 8 Batch 921/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.967, Loss: 1.449 Epoch 8 Batch 922/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.967, Loss: 1.337 Epoch 8 Batch 923/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.963, Loss: 1.425 Epoch 8 Batch 924/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.963, Loss: 1.391 Epoch 8 Batch 925/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.968, Loss: 1.420 Epoch 8 Batch 926/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.969, Loss: 1.404 Epoch 8 Batch 927/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.974, Loss: 1.432 Epoch 8 Batch 928/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.973, Loss: 1.371 Epoch 8 Batch 929/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.973, Loss: 1.439 Epoch 8 Batch 930/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.973, Loss: 1.452 Epoch 8 Batch 931/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.972, Loss: 1.410 Epoch 8 Batch 932/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.977, Loss: 1.348 Epoch 8 Batch 933/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.966, Loss: 1.386 Epoch 8 Batch 934/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.968, Loss: 1.433 Epoch 8 Batch 935/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.968, Loss: 1.452 Epoch 8 Batch 936/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.968, Loss: 1.433 Epoch 8 Batch 937/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.968, Loss: 1.482 Epoch 8 Batch 938/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.963, Loss: 1.415 Epoch 8 Batch 939/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.966, Loss: 1.481 Epoch 8 Batch 940/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.435 Epoch 8 Batch 941/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.968, Loss: 1.424 Epoch 8 Batch 942/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.965, Loss: 1.398 Epoch 8 Batch 943/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.970, Loss: 1.464 Epoch 8 Batch 944/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.970, Loss: 1.424 Epoch 8 Batch 945/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.975, Loss: 1.405 Epoch 8 Batch 946/1077 - Train Accuracy: 0.995, Validation Accuracy: 0.981, Loss: 1.420 Epoch 8 Batch 947/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.979, Loss: 1.442 Epoch 8 Batch 948/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.975, Loss: 1.507 Epoch 8 Batch 949/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.975, Loss: 1.450 Epoch 8 Batch 950/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.975, Loss: 1.391 Epoch 8 Batch 951/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.974, Loss: 1.461 Epoch 8 Batch 952/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.973, Loss: 1.490 Epoch 8 Batch 953/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.973, Loss: 1.421 Epoch 8 Batch 954/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.972, Loss: 1.397 Epoch 8 Batch 955/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.967, Loss: 1.422 Epoch 8 Batch 956/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.967, Loss: 1.350 Epoch 8 Batch 957/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.967, Loss: 1.421 Epoch 8 Batch 958/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.962, Loss: 1.449 Epoch 8 Batch 959/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.962, Loss: 1.453 Epoch 8 Batch 960/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.358 Epoch 8 Batch 961/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.436 Epoch 8 Batch 962/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.964, Loss: 1.423 Epoch 8 Batch 963/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.967, Loss: 1.409 Epoch 8 Batch 964/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.407 Epoch 8 Batch 965/1077 - Train Accuracy: 0.938, Validation Accuracy: 0.958, Loss: 1.485 Epoch 8 Batch 966/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.958, Loss: 1.427 Epoch 8 Batch 967/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.394 Epoch 8 Batch 968/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.963, Loss: 1.390 Epoch 8 Batch 969/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.963, Loss: 1.375 Epoch 8 Batch 970/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.963, Loss: 1.315 Epoch 8 Batch 971/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.963, Loss: 1.394 Epoch 8 Batch 972/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.962, Loss: 1.436 Epoch 8 Batch 973/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.962, Loss: 1.480 Epoch 8 Batch 974/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.962, Loss: 1.424 Epoch 8 Batch 975/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.972, Loss: 1.457 Epoch 8 Batch 976/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.971, Loss: 1.437 Epoch 8 Batch 977/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.971, Loss: 1.423 Epoch 8 Batch 978/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.974, Loss: 1.364 Epoch 8 Batch 979/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.974, Loss: 1.426 Epoch 8 Batch 980/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.974, Loss: 1.369 Epoch 8 Batch 981/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.969, Loss: 1.387 Epoch 8 Batch 982/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.515 Epoch 8 Batch 983/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.964, Loss: 1.430 Epoch 8 Batch 984/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.964, Loss: 1.430 Epoch 8 Batch 985/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.964, Loss: 1.433 Epoch 8 Batch 986/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.964, Loss: 1.409 Epoch 8 Batch 987/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.965, Loss: 1.435 Epoch 8 Batch 988/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.965, Loss: 1.340 Epoch 8 Batch 989/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.974, Loss: 1.380 Epoch 8 Batch 990/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.974, Loss: 1.405 Epoch 8 Batch 991/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.969, Loss: 1.382 Epoch 8 Batch 992/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.543 Epoch 8 Batch 993/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.967, Loss: 1.482 Epoch 8 Batch 994/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.972, Loss: 1.453 Epoch 8 Batch 995/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.451 Epoch 8 Batch 996/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.969, Loss: 1.421 Epoch 8 Batch 997/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.970, Loss: 1.434 Epoch 8 Batch 998/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.970, Loss: 1.409 Epoch 8 Batch 999/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.970, Loss: 1.438 Epoch 8 Batch 1000/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.975, Loss: 1.363 Epoch 8 Batch 1001/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.975, Loss: 1.397 Epoch 8 Batch 1002/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.975, Loss: 1.374 Epoch 8 Batch 1003/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.975, Loss: 1.494 Epoch 8 Batch 1004/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.970, Loss: 1.431 Epoch 8 Batch 1005/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.965, Loss: 1.417 Epoch 8 Batch 1006/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.970, Loss: 1.514 Epoch 8 Batch 1007/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.970, Loss: 1.398 Epoch 8 Batch 1008/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.970, Loss: 1.390 Epoch 8 Batch 1009/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.968, Loss: 1.393 Epoch 8 Batch 1010/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.963, Loss: 1.450 Epoch 8 Batch 1011/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.963, Loss: 1.412 Epoch 8 Batch 1012/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.388 Epoch 8 Batch 1013/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.963, Loss: 1.361 Epoch 8 Batch 1014/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.963, Loss: 1.465 Epoch 8 Batch 1015/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.963, Loss: 1.428 Epoch 8 Batch 1016/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.963, Loss: 1.404 Epoch 8 Batch 1017/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.411 Epoch 8 Batch 1018/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.963, Loss: 1.391 Epoch 8 Batch 1019/1077 - Train Accuracy: 0.944, Validation Accuracy: 0.963, Loss: 1.466 Epoch 8 Batch 1020/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.959, Loss: 1.366 Epoch 8 Batch 1021/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.959, Loss: 1.468 Epoch 8 Batch 1022/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.963, Loss: 1.379 Epoch 8 Batch 1023/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.968, Loss: 1.467 Epoch 8 Batch 1024/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.968, Loss: 1.377 Epoch 8 Batch 1025/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.974, Loss: 1.426 Epoch 8 Batch 1026/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.970, Loss: 1.486 Epoch 8 Batch 1027/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.970, Loss: 1.430 Epoch 8 Batch 1028/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.970, Loss: 1.428 Epoch 8 Batch 1029/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.970, Loss: 1.454 Epoch 8 Batch 1030/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.970, Loss: 1.403 Epoch 8 Batch 1031/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.969, Loss: 1.426 Epoch 8 Batch 1032/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.967, Loss: 1.412 Epoch 8 Batch 1033/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.962, Loss: 1.483 Epoch 8 Batch 1034/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.962, Loss: 1.490 Epoch 8 Batch 1035/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.962, Loss: 1.449 Epoch 8 Batch 1036/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.964, Loss: 1.480 Epoch 8 Batch 1037/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.964, Loss: 1.446 Epoch 8 Batch 1038/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.960, Loss: 1.433 Epoch 8 Batch 1039/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.395 Epoch 8 Batch 1040/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.954, Loss: 1.402 Epoch 8 Batch 1041/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.957, Loss: 1.466 Epoch 8 Batch 1042/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.953, Loss: 1.472 Epoch 8 Batch 1043/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.958, Loss: 1.426 Epoch 8 Batch 1044/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.958, Loss: 1.436 Epoch 8 Batch 1045/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.408 Epoch 8 Batch 1046/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.957, Loss: 1.407 Epoch 8 Batch 1047/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.957, Loss: 1.416 Epoch 8 Batch 1048/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.334 Epoch 8 Batch 1049/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.416 Epoch 8 Batch 1050/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.971, Loss: 1.491 Epoch 8 Batch 1051/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.434 Epoch 8 Batch 1052/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.966, Loss: 1.381 Epoch 8 Batch 1053/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.966, Loss: 1.497 Epoch 8 Batch 1054/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.963, Loss: 1.433 Epoch 8 Batch 1055/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.963, Loss: 1.440 Epoch 8 Batch 1056/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.958, Loss: 1.421 Epoch 8 Batch 1057/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.949, Loss: 1.433 Epoch 8 Batch 1058/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.953, Loss: 1.363 Epoch 8 Batch 1059/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.958, Loss: 1.399 Epoch 8 Batch 1060/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.953, Loss: 1.444 Epoch 8 Batch 1061/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.954, Loss: 1.406 Epoch 8 Batch 1062/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.388 Epoch 8 Batch 1063/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.955, Loss: 1.422 Epoch 8 Batch 1064/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.954, Loss: 1.411 Epoch 8 Batch 1065/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.413 Epoch 8 Batch 1066/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.956, Loss: 1.376 Epoch 8 Batch 1067/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.956, Loss: 1.453 Epoch 8 Batch 1068/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.956, Loss: 1.394 Epoch 8 Batch 1069/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.961, Loss: 1.436 Epoch 8 Batch 1070/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.961, Loss: 1.452 Epoch 8 Batch 1071/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.417 Epoch 8 Batch 1072/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.956, Loss: 1.494 Epoch 8 Batch 1073/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.577 Epoch 8 Batch 1074/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.956, Loss: 1.440 Epoch 8 Batch 1075/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.483 Epoch 9 Batch 0/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.474 Epoch 9 Batch 1/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.956, Loss: 1.392 Epoch 9 Batch 2/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.950, Loss: 1.433 Epoch 9 Batch 3/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.950, Loss: 1.398 Epoch 9 Batch 4/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.956, Loss: 1.461 Epoch 9 Batch 5/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.950, Loss: 1.385 Epoch 9 Batch 6/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.950, Loss: 1.482 Epoch 9 Batch 7/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.475 Epoch 9 Batch 8/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.954, Loss: 1.498 Epoch 9 Batch 9/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.954, Loss: 1.377 Epoch 9 Batch 10/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.961, Loss: 1.448 Epoch 9 Batch 11/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.962, Loss: 1.413 Epoch 9 Batch 12/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.960, Loss: 1.390 Epoch 9 Batch 13/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.412 Epoch 9 Batch 14/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.407 Epoch 9 Batch 15/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.961, Loss: 1.419 Epoch 9 Batch 16/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.961, Loss: 1.494 Epoch 9 Batch 17/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.469 Epoch 9 Batch 18/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.962, Loss: 1.413 Epoch 9 Batch 19/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.962, Loss: 1.456 Epoch 9 Batch 20/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.385 Epoch 9 Batch 21/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.968, Loss: 1.431 Epoch 9 Batch 22/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.968, Loss: 1.487 Epoch 9 Batch 23/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.968, Loss: 1.451 Epoch 9 Batch 24/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.963, Loss: 1.417 Epoch 9 Batch 25/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.347 Epoch 9 Batch 26/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.968, Loss: 1.342 Epoch 9 Batch 27/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.973, Loss: 1.428 Epoch 9 Batch 28/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.967, Loss: 1.472 Epoch 9 Batch 29/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.967, Loss: 1.393 Epoch 9 Batch 30/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.967, Loss: 1.429 Epoch 9 Batch 31/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.429 Epoch 9 Batch 32/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.959, Loss: 1.490 Epoch 9 Batch 33/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.464 Epoch 9 Batch 34/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.955, Loss: 1.402 Epoch 9 Batch 35/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.956, Loss: 1.443 Epoch 9 Batch 36/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.954, Loss: 1.430 Epoch 9 Batch 37/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.954, Loss: 1.423 Epoch 9 Batch 38/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.954, Loss: 1.411 Epoch 9 Batch 39/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.953, Loss: 1.388 Epoch 9 Batch 40/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.960, Loss: 1.418 Epoch 9 Batch 41/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.959, Loss: 1.471 Epoch 9 Batch 42/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.954, Loss: 1.386 Epoch 9 Batch 43/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.953, Loss: 1.362 Epoch 9 Batch 44/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.958, Loss: 1.356 Epoch 9 Batch 45/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.963, Loss: 1.486 Epoch 9 Batch 46/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.963, Loss: 1.388 Epoch 9 Batch 47/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.960, Loss: 1.340 Epoch 9 Batch 48/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.376 Epoch 9 Batch 49/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.952, Loss: 1.466 Epoch 9 Batch 50/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.419 Epoch 9 Batch 51/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.952, Loss: 1.452 Epoch 9 Batch 52/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.947, Loss: 1.458 Epoch 9 Batch 53/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.950, Loss: 1.428 Epoch 9 Batch 54/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.947, Loss: 1.440 Epoch 9 Batch 55/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.946, Loss: 1.436 Epoch 9 Batch 56/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.946, Loss: 1.430 Epoch 9 Batch 57/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.947, Loss: 1.441 Epoch 9 Batch 58/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.949, Loss: 1.469 Epoch 9 Batch 59/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.949, Loss: 1.468 Epoch 9 Batch 60/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.953, Loss: 1.407 Epoch 9 Batch 61/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.953, Loss: 1.410 Epoch 9 Batch 62/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.951, Loss: 1.412 Epoch 9 Batch 63/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.954, Loss: 1.485 Epoch 9 Batch 64/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.958, Loss: 1.402 Epoch 9 Batch 65/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.956, Loss: 1.506 Epoch 9 Batch 66/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.956, Loss: 1.404 Epoch 9 Batch 67/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.956, Loss: 1.406 Epoch 9 Batch 68/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.956, Loss: 1.418 Epoch 9 Batch 69/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.378 Epoch 9 Batch 70/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.428 Epoch 9 Batch 71/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.961, Loss: 1.463 Epoch 9 Batch 72/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.961, Loss: 1.409 Epoch 9 Batch 73/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.961, Loss: 1.384 Epoch 9 Batch 74/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.961, Loss: 1.454 Epoch 9 Batch 75/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.334 Epoch 9 Batch 76/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.961, Loss: 1.496 Epoch 9 Batch 77/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.411 Epoch 9 Batch 78/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.961, Loss: 1.444 Epoch 9 Batch 79/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.961, Loss: 1.447 Epoch 9 Batch 80/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.961, Loss: 1.358 Epoch 9 Batch 81/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.959, Loss: 1.382 Epoch 9 Batch 82/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.959, Loss: 1.419 Epoch 9 Batch 83/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.949, Loss: 1.441 Epoch 9 Batch 84/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.949, Loss: 1.480 Epoch 9 Batch 85/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.949, Loss: 1.472 Epoch 9 Batch 86/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.954, Loss: 1.428 Epoch 9 Batch 87/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.959, Loss: 1.353 Epoch 9 Batch 88/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.959, Loss: 1.438 Epoch 9 Batch 89/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.959, Loss: 1.450 Epoch 9 Batch 90/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.958, Loss: 1.438 Epoch 9 Batch 91/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.427 Epoch 9 Batch 92/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.466 Epoch 9 Batch 93/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.955, Loss: 1.400 Epoch 9 Batch 94/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.955, Loss: 1.433 Epoch 9 Batch 95/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.958, Loss: 1.474 Epoch 9 Batch 96/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.963, Loss: 1.349 Epoch 9 Batch 97/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.962, Loss: 1.437 Epoch 9 Batch 98/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.963, Loss: 1.414 Epoch 9 Batch 99/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.958, Loss: 1.367 Epoch 9 Batch 100/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.958, Loss: 1.379 Epoch 9 Batch 101/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.958, Loss: 1.438 Epoch 9 Batch 102/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.958, Loss: 1.509 Epoch 9 Batch 103/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.953, Loss: 1.419 Epoch 9 Batch 104/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.950, Loss: 1.409 Epoch 9 Batch 105/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.429 Epoch 9 Batch 106/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.950, Loss: 1.421 Epoch 9 Batch 107/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.950, Loss: 1.450 Epoch 9 Batch 108/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.950, Loss: 1.439 Epoch 9 Batch 109/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.950, Loss: 1.438 Epoch 9 Batch 110/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.954, Loss: 1.354 Epoch 9 Batch 111/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.952, Loss: 1.357 Epoch 9 Batch 112/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.947, Loss: 1.432 Epoch 9 Batch 113/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.935, Loss: 1.430 Epoch 9 Batch 114/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.935, Loss: 1.339 Epoch 9 Batch 115/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.936, Loss: 1.454 Epoch 9 Batch 116/1077 - Train Accuracy: 0.925, Validation Accuracy: 0.938, Loss: 1.431 Epoch 9 Batch 117/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.933, Loss: 1.360 Epoch 9 Batch 118/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.943, Loss: 1.408 Epoch 9 Batch 119/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.952, Loss: 1.445 Epoch 9 Batch 120/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.446 Epoch 9 Batch 121/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.952, Loss: 1.430 Epoch 9 Batch 122/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.952, Loss: 1.381 Epoch 9 Batch 123/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.952, Loss: 1.406 Epoch 9 Batch 124/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.958, Loss: 1.461 Epoch 9 Batch 125/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.956, Loss: 1.404 Epoch 9 Batch 126/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.956, Loss: 1.403 Epoch 9 Batch 127/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.407 Epoch 9 Batch 128/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.956, Loss: 1.472 Epoch 9 Batch 129/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.442 Epoch 9 Batch 130/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.958, Loss: 1.450 Epoch 9 Batch 131/1077 - Train Accuracy: 0.949, Validation Accuracy: 0.958, Loss: 1.445 Epoch 9 Batch 132/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.962, Loss: 1.460 Epoch 9 Batch 133/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.959, Loss: 1.392 Epoch 9 Batch 134/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.959, Loss: 1.461 Epoch 9 Batch 135/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.961, Loss: 1.420 Epoch 9 Batch 136/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.956, Loss: 1.403 Epoch 9 Batch 137/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.951, Loss: 1.463 Epoch 9 Batch 138/1077 - Train Accuracy: 0.943, Validation Accuracy: 0.957, Loss: 1.501 Epoch 9 Batch 139/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.957, Loss: 1.398 Epoch 9 Batch 140/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.957, Loss: 1.478 Epoch 9 Batch 141/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.957, Loss: 1.480 Epoch 9 Batch 142/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.957, Loss: 1.403 Epoch 9 Batch 143/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.955, Loss: 1.424 Epoch 9 Batch 144/1077 - Train Accuracy: 0.942, Validation Accuracy: 0.955, Loss: 1.488 Epoch 9 Batch 145/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.960, Loss: 1.410 Epoch 9 Batch 146/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.965, Loss: 1.464 Epoch 9 Batch 147/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.961, Loss: 1.469 Epoch 9 Batch 148/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.432 Epoch 9 Batch 149/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.956, Loss: 1.419 Epoch 9 Batch 150/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.412 Epoch 9 Batch 151/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.434 Epoch 9 Batch 152/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.964, Loss: 1.384 Epoch 9 Batch 153/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.964, Loss: 1.389 Epoch 9 Batch 154/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.964, Loss: 1.348 Epoch 9 Batch 155/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.959, Loss: 1.477 Epoch 9 Batch 156/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.417 Epoch 9 Batch 157/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.955, Loss: 1.361 Epoch 9 Batch 158/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.417 Epoch 9 Batch 159/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.950, Loss: 1.415 Epoch 9 Batch 160/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.954, Loss: 1.425 Epoch 9 Batch 161/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.954, Loss: 1.330 Epoch 9 Batch 162/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.959, Loss: 1.483 Epoch 9 Batch 163/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.965, Loss: 1.437 Epoch 9 Batch 164/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.965, Loss: 1.364 Epoch 9 Batch 165/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.465 Epoch 9 Batch 166/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.950, Loss: 1.461 Epoch 9 Batch 167/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.950, Loss: 1.492 Epoch 9 Batch 168/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.950, Loss: 1.435 Epoch 9 Batch 169/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.950, Loss: 1.433 Epoch 9 Batch 170/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.950, Loss: 1.465 Epoch 9 Batch 171/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.955, Loss: 1.381 Epoch 9 Batch 172/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.441 Epoch 9 Batch 173/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.950, Loss: 1.438 Epoch 9 Batch 174/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.950, Loss: 1.370 Epoch 9 Batch 175/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.947, Loss: 1.310 Epoch 9 Batch 176/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.947, Loss: 1.454 Epoch 9 Batch 177/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.952, Loss: 1.448 Epoch 9 Batch 178/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.952, Loss: 1.438 Epoch 9 Batch 179/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.950, Loss: 1.421 Epoch 9 Batch 180/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.953, Loss: 1.390 Epoch 9 Batch 181/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.953, Loss: 1.460 Epoch 9 Batch 182/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.953, Loss: 1.408 Epoch 9 Batch 183/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.392 Epoch 9 Batch 184/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.960, Loss: 1.453 Epoch 9 Batch 185/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.954, Loss: 1.451 Epoch 9 Batch 186/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.393 Epoch 9 Batch 187/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.964, Loss: 1.322 Epoch 9 Batch 188/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.964, Loss: 1.455 Epoch 9 Batch 189/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.955, Loss: 1.482 Epoch 9 Batch 190/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.960, Loss: 1.432 Epoch 9 Batch 191/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.435 Epoch 9 Batch 192/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.960, Loss: 1.397 Epoch 9 Batch 193/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.960, Loss: 1.425 Epoch 9 Batch 194/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.469 Epoch 9 Batch 195/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.380 Epoch 9 Batch 196/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.959, Loss: 1.439 Epoch 9 Batch 197/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.964, Loss: 1.433 Epoch 9 Batch 198/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.417 Epoch 9 Batch 199/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.963, Loss: 1.396 Epoch 9 Batch 200/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.365 Epoch 9 Batch 201/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.965, Loss: 1.367 Epoch 9 Batch 202/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.970, Loss: 1.436 Epoch 9 Batch 203/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.965, Loss: 1.434 Epoch 9 Batch 204/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.958, Loss: 1.461 Epoch 9 Batch 205/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.953, Loss: 1.446 Epoch 9 Batch 206/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.953, Loss: 1.466 Epoch 9 Batch 207/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.958, Loss: 1.402 Epoch 9 Batch 208/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.968, Loss: 1.397 Epoch 9 Batch 209/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.968, Loss: 1.459 Epoch 9 Batch 210/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.968, Loss: 1.317 Epoch 9 Batch 211/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.968, Loss: 1.444 Epoch 9 Batch 212/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.965, Loss: 1.425 Epoch 9 Batch 213/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.965, Loss: 1.381 Epoch 9 Batch 214/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.964, Loss: 1.454 Epoch 9 Batch 215/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.964, Loss: 1.429 Epoch 9 Batch 216/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.966, Loss: 1.348 Epoch 9 Batch 217/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.969, Loss: 1.432 Epoch 9 Batch 218/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.964, Loss: 1.569 Epoch 9 Batch 219/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.955, Loss: 1.492 Epoch 9 Batch 220/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.954, Loss: 1.502 Epoch 9 Batch 221/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.379 Epoch 9 Batch 222/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.952, Loss: 1.471 Epoch 9 Batch 223/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.947, Loss: 1.440 Epoch 9 Batch 224/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.379 Epoch 9 Batch 225/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.956, Loss: 1.427 Epoch 9 Batch 226/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.956, Loss: 1.387 Epoch 9 Batch 227/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.956, Loss: 1.416 Epoch 9 Batch 228/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.955, Loss: 1.386 Epoch 9 Batch 229/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.955, Loss: 1.436 Epoch 9 Batch 230/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.955, Loss: 1.424 Epoch 9 Batch 231/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.955, Loss: 1.431 Epoch 9 Batch 232/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.955, Loss: 1.380 Epoch 9 Batch 233/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.957, Loss: 1.415 Epoch 9 Batch 234/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.959, Loss: 1.412 Epoch 9 Batch 235/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.957, Loss: 1.391 Epoch 9 Batch 236/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.952, Loss: 1.427 Epoch 9 Batch 237/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.954, Loss: 1.449 Epoch 9 Batch 238/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.954, Loss: 1.445 Epoch 9 Batch 239/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.954, Loss: 1.411 Epoch 9 Batch 240/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.959, Loss: 1.379 Epoch 9 Batch 241/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.959, Loss: 1.501 Epoch 9 Batch 242/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.959, Loss: 1.367 Epoch 9 Batch 243/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.964, Loss: 1.455 Epoch 9 Batch 244/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.965, Loss: 1.477 Epoch 9 Batch 245/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.971, Loss: 1.386 Epoch 9 Batch 246/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.967, Loss: 1.417 Epoch 9 Batch 247/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.967, Loss: 1.444 Epoch 9 Batch 248/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.967, Loss: 1.383 Epoch 9 Batch 249/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.965, Loss: 1.402 Epoch 9 Batch 250/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.963, Loss: 1.404 Epoch 9 Batch 251/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.967, Loss: 1.422 Epoch 9 Batch 252/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.967, Loss: 1.359 Epoch 9 Batch 253/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.967, Loss: 1.421 Epoch 9 Batch 254/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.967, Loss: 1.413 Epoch 9 Batch 255/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.962, Loss: 1.451 Epoch 9 Batch 256/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.962, Loss: 1.494 Epoch 9 Batch 257/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.962, Loss: 1.428 Epoch 9 Batch 258/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.956, Loss: 1.423 Epoch 9 Batch 259/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.958, Loss: 1.352 Epoch 9 Batch 260/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.444 Epoch 9 Batch 261/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.524 Epoch 9 Batch 262/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.969, Loss: 1.461 Epoch 9 Batch 263/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.969, Loss: 1.480 Epoch 9 Batch 264/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.969, Loss: 1.413 Epoch 9 Batch 265/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.974, Loss: 1.501 Epoch 9 Batch 266/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.972, Loss: 1.442 Epoch 9 Batch 267/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.961, Loss: 1.455 Epoch 9 Batch 268/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.447 Epoch 9 Batch 269/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.426 Epoch 9 Batch 270/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.961, Loss: 1.426 Epoch 9 Batch 271/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.963, Loss: 1.401 Epoch 9 Batch 272/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.962, Loss: 1.441 Epoch 9 Batch 273/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.962, Loss: 1.519 Epoch 9 Batch 274/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.962, Loss: 1.395 Epoch 9 Batch 275/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.962, Loss: 1.481 Epoch 9 Batch 276/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.962, Loss: 1.425 Epoch 9 Batch 277/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.962, Loss: 1.366 Epoch 9 Batch 278/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.962, Loss: 1.442 Epoch 9 Batch 279/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.962, Loss: 1.477 Epoch 9 Batch 280/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.961, Loss: 1.397 Epoch 9 Batch 281/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.961, Loss: 1.438 Epoch 9 Batch 282/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.961, Loss: 1.423 Epoch 9 Batch 283/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.961, Loss: 1.445 Epoch 9 Batch 284/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.955, Loss: 1.494 Epoch 9 Batch 285/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.960, Loss: 1.415 Epoch 9 Batch 286/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.960, Loss: 1.464 Epoch 9 Batch 287/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.953, Loss: 1.442 Epoch 9 Batch 288/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.953, Loss: 1.443 Epoch 9 Batch 289/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.953, Loss: 1.447 Epoch 9 Batch 290/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.953, Loss: 1.411 Epoch 9 Batch 291/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.953, Loss: 1.450 Epoch 9 Batch 292/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.949, Loss: 1.438 Epoch 9 Batch 293/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.944, Loss: 1.414 Epoch 9 Batch 294/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.948, Loss: 1.412 Epoch 9 Batch 295/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.452 Epoch 9 Batch 296/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.951, Loss: 1.403 Epoch 9 Batch 297/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.951, Loss: 1.441 Epoch 9 Batch 298/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.951, Loss: 1.412 Epoch 9 Batch 299/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.951, Loss: 1.350 Epoch 9 Batch 300/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.952, Loss: 1.395 Epoch 9 Batch 301/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.960, Loss: 1.353 Epoch 9 Batch 302/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.960, Loss: 1.444 Epoch 9 Batch 303/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.960, Loss: 1.411 Epoch 9 Batch 304/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.965, Loss: 1.409 Epoch 9 Batch 305/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.960, Loss: 1.365 Epoch 9 Batch 306/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.961, Loss: 1.423 Epoch 9 Batch 307/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.959, Loss: 1.384 Epoch 9 Batch 308/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.959, Loss: 1.435 Epoch 9 Batch 309/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.961, Loss: 1.410 Epoch 9 Batch 310/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.961, Loss: 1.391 Epoch 9 Batch 311/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.356 Epoch 9 Batch 312/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.960, Loss: 1.515 Epoch 9 Batch 313/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.957, Loss: 1.423 Epoch 9 Batch 314/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.951, Loss: 1.520 Epoch 9 Batch 315/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.351 Epoch 9 Batch 316/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.958, Loss: 1.379 Epoch 9 Batch 317/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.464 Epoch 9 Batch 318/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.960, Loss: 1.327 Epoch 9 Batch 319/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.398 Epoch 9 Batch 320/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.960, Loss: 1.434 Epoch 9 Batch 321/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.960, Loss: 1.407 Epoch 9 Batch 322/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.960, Loss: 1.417 Epoch 9 Batch 323/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.960, Loss: 1.451 Epoch 9 Batch 324/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.960, Loss: 1.407 Epoch 9 Batch 325/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.966, Loss: 1.419 Epoch 9 Batch 326/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.966, Loss: 1.339 Epoch 9 Batch 327/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.961, Loss: 1.456 Epoch 9 Batch 328/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.488 Epoch 9 Batch 329/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.961, Loss: 1.431 Epoch 9 Batch 330/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.430 Epoch 9 Batch 331/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.955, Loss: 1.463 Epoch 9 Batch 332/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.486 Epoch 9 Batch 333/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.955, Loss: 1.415 Epoch 9 Batch 334/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.959, Loss: 1.366 Epoch 9 Batch 335/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.957, Loss: 1.392 Epoch 9 Batch 336/1077 - Train Accuracy: 0.948, Validation Accuracy: 0.956, Loss: 1.394 Epoch 9 Batch 337/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.956, Loss: 1.412 Epoch 9 Batch 338/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.950, Loss: 1.400 Epoch 9 Batch 339/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.405 Epoch 9 Batch 340/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.490 Epoch 9 Batch 341/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.952, Loss: 1.454 Epoch 9 Batch 342/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.433 Epoch 9 Batch 343/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.952, Loss: 1.436 Epoch 9 Batch 344/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.952, Loss: 1.417 Epoch 9 Batch 345/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.951, Loss: 1.443 Epoch 9 Batch 346/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.957, Loss: 1.452 Epoch 9 Batch 347/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.957, Loss: 1.493 Epoch 9 Batch 348/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.956, Loss: 1.366 Epoch 9 Batch 349/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.963, Loss: 1.361 Epoch 9 Batch 350/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.961, Loss: 1.468 Epoch 9 Batch 351/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.958, Loss: 1.434 Epoch 9 Batch 352/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.958, Loss: 1.420 Epoch 9 Batch 353/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.954, Loss: 1.482 Epoch 9 Batch 354/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.952, Loss: 1.452 Epoch 9 Batch 355/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.952, Loss: 1.404 Epoch 9 Batch 356/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.952, Loss: 1.455 Epoch 9 Batch 357/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.947, Loss: 1.462 Epoch 9 Batch 358/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.952, Loss: 1.475 Epoch 9 Batch 359/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.952, Loss: 1.352 Epoch 9 Batch 360/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.952, Loss: 1.454 Epoch 9 Batch 361/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.957, Loss: 1.401 Epoch 9 Batch 362/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.452 Epoch 9 Batch 363/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.954, Loss: 1.388 Epoch 9 Batch 364/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.953, Loss: 1.398 Epoch 9 Batch 365/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.955, Loss: 1.351 Epoch 9 Batch 366/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.952, Loss: 1.385 Epoch 9 Batch 367/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.952, Loss: 1.453 Epoch 9 Batch 368/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.952, Loss: 1.445 Epoch 9 Batch 369/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.952, Loss: 1.405 Epoch 9 Batch 370/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.952, Loss: 1.466 Epoch 9 Batch 371/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.952, Loss: 1.505 Epoch 9 Batch 372/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.952, Loss: 1.389 Epoch 9 Batch 373/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.952, Loss: 1.461 Epoch 9 Batch 374/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.955, Loss: 1.361 Epoch 9 Batch 375/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.955, Loss: 1.491 Epoch 9 Batch 376/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.955, Loss: 1.417 Epoch 9 Batch 377/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.955, Loss: 1.418 Epoch 9 Batch 378/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.954, Loss: 1.442 Epoch 9 Batch 379/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.955, Loss: 1.481 Epoch 9 Batch 380/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.955, Loss: 1.443 Epoch 9 Batch 381/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.955, Loss: 1.453 Epoch 9 Batch 382/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.949, Loss: 1.432 Epoch 9 Batch 383/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.949, Loss: 1.384 Epoch 9 Batch 384/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.947, Loss: 1.419 Epoch 9 Batch 385/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.947, Loss: 1.400 Epoch 9 Batch 386/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.947, Loss: 1.409 Epoch 9 Batch 387/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.947, Loss: 1.374 Epoch 9 Batch 388/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.948, Loss: 1.373 Epoch 9 Batch 389/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.952, Loss: 1.432 Epoch 9 Batch 390/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.948, Loss: 1.452 Epoch 9 Batch 391/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.948, Loss: 1.396 Epoch 9 Batch 392/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.948, Loss: 1.509 Epoch 9 Batch 393/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.948, Loss: 1.419 Epoch 9 Batch 394/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.953, Loss: 1.386 Epoch 9 Batch 395/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.950, Loss: 1.387 Epoch 9 Batch 396/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.941, Loss: 1.377 Epoch 9 Batch 397/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.940, Loss: 1.445 Epoch 9 Batch 398/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.940, Loss: 1.476 Epoch 9 Batch 399/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.940, Loss: 1.443 Epoch 9 Batch 400/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.945, Loss: 1.443 Epoch 9 Batch 401/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.950, Loss: 1.469 Epoch 9 Batch 402/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.355 Epoch 9 Batch 403/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.440 Epoch 9 Batch 404/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.950, Loss: 1.401 Epoch 9 Batch 405/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.946, Loss: 1.475 Epoch 9 Batch 406/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.945, Loss: 1.430 Epoch 9 Batch 407/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.945, Loss: 1.415 Epoch 9 Batch 408/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.944, Loss: 1.468 Epoch 9 Batch 409/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.949, Loss: 1.422 Epoch 9 Batch 410/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.949, Loss: 1.352 Epoch 9 Batch 411/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.949, Loss: 1.379 Epoch 9 Batch 412/1077 - Train Accuracy: 0.939, Validation Accuracy: 0.953, Loss: 1.402 Epoch 9 Batch 413/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.953, Loss: 1.357 Epoch 9 Batch 414/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.953, Loss: 1.389 Epoch 9 Batch 415/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.953, Loss: 1.465 Epoch 9 Batch 416/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.953, Loss: 1.348 Epoch 9 Batch 417/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.959, Loss: 1.428 Epoch 9 Batch 418/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.959, Loss: 1.410 Epoch 9 Batch 419/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.440 Epoch 9 Batch 420/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.952, Loss: 1.415 Epoch 9 Batch 421/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.954, Loss: 1.439 Epoch 9 Batch 422/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.954, Loss: 1.442 Epoch 9 Batch 423/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.408 Epoch 9 Batch 424/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.966, Loss: 1.394 Epoch 9 Batch 425/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.962, Loss: 1.500 Epoch 9 Batch 426/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.962, Loss: 1.438 Epoch 9 Batch 427/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.952, Loss: 1.427 Epoch 9 Batch 428/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.955, Loss: 1.478 Epoch 9 Batch 429/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.957, Loss: 1.436 Epoch 9 Batch 430/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.957, Loss: 1.444 Epoch 9 Batch 431/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.957, Loss: 1.454 Epoch 9 Batch 432/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.447 Epoch 9 Batch 433/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.962, Loss: 1.508 Epoch 9 Batch 434/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.962, Loss: 1.447 Epoch 9 Batch 435/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.962, Loss: 1.371 Epoch 9 Batch 436/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.957, Loss: 1.433 Epoch 9 Batch 437/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.960, Loss: 1.469 Epoch 9 Batch 438/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.960, Loss: 1.423 Epoch 9 Batch 439/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.429 Epoch 9 Batch 440/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.407 Epoch 9 Batch 441/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.955, Loss: 1.437 Epoch 9 Batch 442/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.955, Loss: 1.424 Epoch 9 Batch 443/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.955, Loss: 1.463 Epoch 9 Batch 444/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.962, Loss: 1.431 Epoch 9 Batch 445/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.962, Loss: 1.435 Epoch 9 Batch 446/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.962, Loss: 1.367 Epoch 9 Batch 447/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.961, Loss: 1.374 Epoch 9 Batch 448/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.422 Epoch 9 Batch 449/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.446 Epoch 9 Batch 450/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.961, Loss: 1.427 Epoch 9 Batch 451/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.966, Loss: 1.411 Epoch 9 Batch 452/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.361 Epoch 9 Batch 453/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.960, Loss: 1.468 Epoch 9 Batch 454/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.960, Loss: 1.418 Epoch 9 Batch 455/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.957, Loss: 1.508 Epoch 9 Batch 456/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.963, Loss: 1.370 Epoch 9 Batch 457/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.961, Loss: 1.441 Epoch 9 Batch 458/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.483 Epoch 9 Batch 459/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.956, Loss: 1.402 Epoch 9 Batch 460/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.956, Loss: 1.413 Epoch 9 Batch 461/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.445 Epoch 9 Batch 462/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.494 Epoch 9 Batch 463/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.956, Loss: 1.453 Epoch 9 Batch 464/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.956, Loss: 1.382 Epoch 9 Batch 465/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.429 Epoch 9 Batch 466/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.956, Loss: 1.560 Epoch 9 Batch 467/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.956, Loss: 1.342 Epoch 9 Batch 468/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.953, Loss: 1.456 Epoch 9 Batch 469/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.949, Loss: 1.408 Epoch 9 Batch 470/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.949, Loss: 1.337 Epoch 9 Batch 471/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.949, Loss: 1.459 Epoch 9 Batch 472/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.950, Loss: 1.457 Epoch 9 Batch 473/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.950, Loss: 1.458 Epoch 9 Batch 474/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.955, Loss: 1.462 Epoch 9 Batch 475/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.415 Epoch 9 Batch 476/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.955, Loss: 1.435 Epoch 9 Batch 477/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.389 Epoch 9 Batch 478/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.952, Loss: 1.370 Epoch 9 Batch 479/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.957, Loss: 1.406 Epoch 9 Batch 480/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.960, Loss: 1.462 Epoch 9 Batch 481/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.965, Loss: 1.417 Epoch 9 Batch 482/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.440 Epoch 9 Batch 483/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.960, Loss: 1.413 Epoch 9 Batch 484/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.381 Epoch 9 Batch 485/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.962, Loss: 1.427 Epoch 9 Batch 486/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.962, Loss: 1.369 Epoch 9 Batch 487/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.961, Loss: 1.502 Epoch 9 Batch 488/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.961, Loss: 1.359 Epoch 9 Batch 489/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.956, Loss: 1.357 Epoch 9 Batch 490/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.406 Epoch 9 Batch 491/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.960, Loss: 1.467 Epoch 9 Batch 492/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.960, Loss: 1.444 Epoch 9 Batch 493/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.960, Loss: 1.375 Epoch 9 Batch 494/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.965, Loss: 1.415 Epoch 9 Batch 495/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.965, Loss: 1.452 Epoch 9 Batch 496/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.965, Loss: 1.462 Epoch 9 Batch 497/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.962, Loss: 1.449 Epoch 9 Batch 498/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.962, Loss: 1.445 Epoch 9 Batch 499/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.962, Loss: 1.358 Epoch 9 Batch 500/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.962, Loss: 1.406 Epoch 9 Batch 501/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.962, Loss: 1.373 Epoch 9 Batch 502/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.477 Epoch 9 Batch 503/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.957, Loss: 1.426 Epoch 9 Batch 504/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.956, Loss: 1.522 Epoch 9 Batch 505/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.956, Loss: 1.401 Epoch 9 Batch 506/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.950, Loss: 1.331 Epoch 9 Batch 507/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.955, Loss: 1.324 Epoch 9 Batch 508/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.955, Loss: 1.426 Epoch 9 Batch 509/1077 - Train Accuracy: 0.956, Validation Accuracy: 0.955, Loss: 1.422 Epoch 9 Batch 510/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.950, Loss: 1.422 Epoch 9 Batch 511/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.950, Loss: 1.412 Epoch 9 Batch 512/1077 - Train Accuracy: 0.996, Validation Accuracy: 0.955, Loss: 1.413 Epoch 9 Batch 513/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.951, Loss: 1.367 Epoch 9 Batch 514/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.951, Loss: 1.481 Epoch 9 Batch 515/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.951, Loss: 1.329 Epoch 9 Batch 516/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.955, Loss: 1.413 Epoch 9 Batch 517/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.956, Loss: 1.434 Epoch 9 Batch 518/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.960, Loss: 1.439 Epoch 9 Batch 519/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.965, Loss: 1.412 Epoch 9 Batch 520/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.971, Loss: 1.390 Epoch 9 Batch 521/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.971, Loss: 1.381 Epoch 9 Batch 522/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.966, Loss: 1.433 Epoch 9 Batch 523/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.966, Loss: 1.444 Epoch 9 Batch 524/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.966, Loss: 1.386 Epoch 9 Batch 525/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.961, Loss: 1.387 Epoch 9 Batch 526/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.967, Loss: 1.405 Epoch 9 Batch 527/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.966, Loss: 1.461 Epoch 9 Batch 528/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.970, Loss: 1.399 Epoch 9 Batch 529/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.971, Loss: 1.456 Epoch 9 Batch 530/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.966, Loss: 1.426 Epoch 9 Batch 531/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.966, Loss: 1.432 Epoch 9 Batch 532/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.966, Loss: 1.489 Epoch 9 Batch 533/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.968, Loss: 1.422 Epoch 9 Batch 534/1077 - Train Accuracy: 0.934, Validation Accuracy: 0.966, Loss: 1.374 Epoch 9 Batch 535/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.966, Loss: 1.445 Epoch 9 Batch 536/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.971, Loss: 1.443 Epoch 9 Batch 537/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.966, Loss: 1.410 Epoch 9 Batch 538/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.961, Loss: 1.384 Epoch 9 Batch 539/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.960, Loss: 1.407 Epoch 9 Batch 540/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.958, Loss: 1.419 Epoch 9 Batch 541/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.958, Loss: 1.471 Epoch 9 Batch 542/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.354 Epoch 9 Batch 543/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.958, Loss: 1.395 Epoch 9 Batch 544/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.428 Epoch 9 Batch 545/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.964, Loss: 1.391 Epoch 9 Batch 546/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.964, Loss: 1.471 Epoch 9 Batch 547/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.974, Loss: 1.355 Epoch 9 Batch 548/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.974, Loss: 1.409 Epoch 9 Batch 549/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.969, Loss: 1.387 Epoch 9 Batch 550/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.964, Loss: 1.398 Epoch 9 Batch 551/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.959, Loss: 1.433 Epoch 9 Batch 552/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.964, Loss: 1.386 Epoch 9 Batch 553/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.961, Loss: 1.397 Epoch 9 Batch 554/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.962, Loss: 1.433 Epoch 9 Batch 555/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.971, Loss: 1.438 Epoch 9 Batch 556/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.972, Loss: 1.342 Epoch 9 Batch 557/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.971, Loss: 1.416 Epoch 9 Batch 558/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.971, Loss: 1.448 Epoch 9 Batch 559/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.972, Loss: 1.395 Epoch 9 Batch 560/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.967, Loss: 1.374 Epoch 9 Batch 561/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.971, Loss: 1.419 Epoch 9 Batch 562/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.971, Loss: 1.410 Epoch 9 Batch 563/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.971, Loss: 1.343 Epoch 9 Batch 564/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.971, Loss: 1.481 Epoch 9 Batch 565/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.969, Loss: 1.481 Epoch 9 Batch 566/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.973, Loss: 1.411 Epoch 9 Batch 567/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.973, Loss: 1.437 Epoch 9 Batch 568/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.968, Loss: 1.389 Epoch 9 Batch 569/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.968, Loss: 1.377 Epoch 9 Batch 570/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.966, Loss: 1.413 Epoch 9 Batch 571/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.966, Loss: 1.461 Epoch 9 Batch 572/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.961, Loss: 1.421 Epoch 9 Batch 573/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.966, Loss: 1.486 Epoch 9 Batch 574/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.966, Loss: 1.434 Epoch 9 Batch 575/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.966, Loss: 1.462 Epoch 9 Batch 576/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.964, Loss: 1.457 Epoch 9 Batch 577/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.969, Loss: 1.434 Epoch 9 Batch 578/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.968, Loss: 1.339 Epoch 9 Batch 579/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.968, Loss: 1.390 Epoch 9 Batch 580/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.351 Epoch 9 Batch 581/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.959, Loss: 1.464 Epoch 9 Batch 582/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.959, Loss: 1.454 Epoch 9 Batch 583/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.397 Epoch 9 Batch 584/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.961, Loss: 1.398 Epoch 9 Batch 585/1077 - Train Accuracy: 0.995, Validation Accuracy: 0.965, Loss: 1.354 Epoch 9 Batch 586/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.965, Loss: 1.462 Epoch 9 Batch 587/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.965, Loss: 1.454 Epoch 9 Batch 588/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.965, Loss: 1.406 Epoch 9 Batch 589/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.965, Loss: 1.474 Epoch 9 Batch 590/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.963, Loss: 1.422 Epoch 9 Batch 591/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.963, Loss: 1.382 Epoch 9 Batch 592/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.965, Loss: 1.409 Epoch 9 Batch 593/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.960, Loss: 1.422 Epoch 9 Batch 594/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.452 Epoch 9 Batch 595/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.960, Loss: 1.404 Epoch 9 Batch 596/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.960, Loss: 1.468 Epoch 9 Batch 597/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.953, Loss: 1.369 Epoch 9 Batch 598/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.955, Loss: 1.377 Epoch 9 Batch 599/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.389 Epoch 9 Batch 600/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.472 Epoch 9 Batch 601/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.963, Loss: 1.379 Epoch 9 Batch 602/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.969, Loss: 1.386 Epoch 9 Batch 603/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.965, Loss: 1.423 Epoch 9 Batch 604/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.965, Loss: 1.499 Epoch 9 Batch 605/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.960, Loss: 1.472 Epoch 9 Batch 606/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.964, Loss: 1.368 Epoch 9 Batch 607/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.964, Loss: 1.394 Epoch 9 Batch 608/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.964, Loss: 1.374 Epoch 9 Batch 609/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.440 Epoch 9 Batch 610/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.432 Epoch 9 Batch 611/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.964, Loss: 1.432 Epoch 9 Batch 612/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.964, Loss: 1.421 Epoch 9 Batch 613/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.964, Loss: 1.449 Epoch 9 Batch 614/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.964, Loss: 1.404 Epoch 9 Batch 615/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.958, Loss: 1.421 Epoch 9 Batch 616/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.958, Loss: 1.423 Epoch 9 Batch 617/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.963, Loss: 1.397 Epoch 9 Batch 618/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.963, Loss: 1.349 Epoch 9 Batch 619/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.968, Loss: 1.554 Epoch 9 Batch 620/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.968, Loss: 1.354 Epoch 9 Batch 621/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.459 Epoch 9 Batch 622/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.958, Loss: 1.413 Epoch 9 Batch 623/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.960, Loss: 1.330 Epoch 9 Batch 624/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.439 Epoch 9 Batch 625/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.964, Loss: 1.382 Epoch 9 Batch 626/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.402 Epoch 9 Batch 627/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.964, Loss: 1.472 Epoch 9 Batch 628/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.964, Loss: 1.438 Epoch 9 Batch 629/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.959, Loss: 1.409 Epoch 9 Batch 630/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.959, Loss: 1.393 Epoch 9 Batch 631/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.959, Loss: 1.384 Epoch 9 Batch 632/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.957, Loss: 1.412 Epoch 9 Batch 633/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.957, Loss: 1.393 Epoch 9 Batch 634/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.957, Loss: 1.399 Epoch 9 Batch 635/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.957, Loss: 1.450 Epoch 9 Batch 636/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.962, Loss: 1.412 Epoch 9 Batch 637/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.959, Loss: 1.446 Epoch 9 Batch 638/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.959, Loss: 1.402 Epoch 9 Batch 639/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.959, Loss: 1.396 Epoch 9 Batch 640/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.959, Loss: 1.391 Epoch 9 Batch 641/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.958, Loss: 1.405 Epoch 9 Batch 642/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.962, Loss: 1.431 Epoch 9 Batch 643/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.962, Loss: 1.513 Epoch 9 Batch 644/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.962, Loss: 1.384 Epoch 9 Batch 645/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.964, Loss: 1.356 Epoch 9 Batch 646/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.400 Epoch 9 Batch 647/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.953, Loss: 1.371 Epoch 9 Batch 648/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.953, Loss: 1.456 Epoch 9 Batch 649/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.953, Loss: 1.458 Epoch 9 Batch 650/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.960, Loss: 1.422 Epoch 9 Batch 651/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.955, Loss: 1.390 Epoch 9 Batch 652/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.959, Loss: 1.416 Epoch 9 Batch 653/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.955, Loss: 1.441 Epoch 9 Batch 654/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.955, Loss: 1.387 Epoch 9 Batch 655/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.955, Loss: 1.481 Epoch 9 Batch 656/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.955, Loss: 1.441 Epoch 9 Batch 657/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.955, Loss: 1.507 Epoch 9 Batch 658/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.950, Loss: 1.381 Epoch 9 Batch 659/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.952, Loss: 1.415 Epoch 9 Batch 660/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.952, Loss: 1.426 Epoch 9 Batch 661/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.958, Loss: 1.464 Epoch 9 Batch 662/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.962, Loss: 1.400 Epoch 9 Batch 663/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.962, Loss: 1.397 Epoch 9 Batch 664/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.962, Loss: 1.409 Epoch 9 Batch 665/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.964, Loss: 1.334 Epoch 9 Batch 666/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.964, Loss: 1.431 Epoch 9 Batch 667/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.964, Loss: 1.449 Epoch 9 Batch 668/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.964, Loss: 1.492 Epoch 9 Batch 669/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.962, Loss: 1.402 Epoch 9 Batch 670/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.966, Loss: 1.537 Epoch 9 Batch 671/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.962, Loss: 1.464 Epoch 9 Batch 672/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.962, Loss: 1.480 Epoch 9 Batch 673/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.958, Loss: 1.400 Epoch 9 Batch 674/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.958, Loss: 1.468 Epoch 9 Batch 675/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.956, Loss: 1.536 Epoch 9 Batch 676/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.956, Loss: 1.428 Epoch 9 Batch 677/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.956, Loss: 1.417 Epoch 9 Batch 678/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.956, Loss: 1.341 Epoch 9 Batch 679/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.956, Loss: 1.428 Epoch 9 Batch 680/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.956, Loss: 1.503 Epoch 9 Batch 681/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.953, Loss: 1.404 Epoch 9 Batch 682/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.953, Loss: 1.464 Epoch 9 Batch 683/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.953, Loss: 1.410 Epoch 9 Batch 684/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.951, Loss: 1.484 Epoch 9 Batch 685/1077 - Train Accuracy: 0.947, Validation Accuracy: 0.956, Loss: 1.404 Epoch 9 Batch 686/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.956, Loss: 1.501 Epoch 9 Batch 687/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.961, Loss: 1.462 Epoch 9 Batch 688/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.961, Loss: 1.432 Epoch 9 Batch 689/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.966, Loss: 1.428 Epoch 9 Batch 690/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.966, Loss: 1.371 Epoch 9 Batch 691/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.964, Loss: 1.392 Epoch 9 Batch 692/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.964, Loss: 1.455 Epoch 9 Batch 693/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.964, Loss: 1.424 Epoch 9 Batch 694/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.440 Epoch 9 Batch 695/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.452 Epoch 9 Batch 696/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.964, Loss: 1.411 Epoch 9 Batch 697/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.964, Loss: 1.492 Epoch 9 Batch 698/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.969, Loss: 1.406 Epoch 9 Batch 699/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.969, Loss: 1.463 Epoch 9 Batch 700/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.969, Loss: 1.462 Epoch 9 Batch 701/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.969, Loss: 1.375 Epoch 9 Batch 702/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.969, Loss: 1.442 Epoch 9 Batch 703/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.960, Loss: 1.462 Epoch 9 Batch 704/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.461 Epoch 9 Batch 705/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.960, Loss: 1.384 Epoch 9 Batch 706/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.966, Loss: 1.493 Epoch 9 Batch 707/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.965, Loss: 1.456 Epoch 9 Batch 708/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.965, Loss: 1.397 Epoch 9 Batch 709/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.960, Loss: 1.417 Epoch 9 Batch 710/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.954, Loss: 1.423 Epoch 9 Batch 711/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.954, Loss: 1.468 Epoch 9 Batch 712/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.955, Loss: 1.417 Epoch 9 Batch 713/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.955, Loss: 1.460 Epoch 9 Batch 714/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.955, Loss: 1.462 Epoch 9 Batch 715/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.959, Loss: 1.426 Epoch 9 Batch 716/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.964, Loss: 1.371 Epoch 9 Batch 717/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.964, Loss: 1.428 Epoch 9 Batch 718/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.960, Loss: 1.475 Epoch 9 Batch 719/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.960, Loss: 1.440 Epoch 9 Batch 720/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.403 Epoch 9 Batch 721/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.955, Loss: 1.410 Epoch 9 Batch 722/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.955, Loss: 1.430 Epoch 9 Batch 723/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.955, Loss: 1.451 Epoch 9 Batch 724/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.956, Loss: 1.396 Epoch 9 Batch 725/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.951, Loss: 1.420 Epoch 9 Batch 726/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.958, Loss: 1.431 Epoch 9 Batch 727/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.958, Loss: 1.389 Epoch 9 Batch 728/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.961, Loss: 1.428 Epoch 9 Batch 729/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.961, Loss: 1.445 Epoch 9 Batch 730/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.965, Loss: 1.476 Epoch 9 Batch 731/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.965, Loss: 1.553 Epoch 9 Batch 732/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.970, Loss: 1.373 Epoch 9 Batch 733/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.970, Loss: 1.542 Epoch 9 Batch 734/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.965, Loss: 1.458 Epoch 9 Batch 735/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.965, Loss: 1.412 Epoch 9 Batch 736/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.965, Loss: 1.420 Epoch 9 Batch 737/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.965, Loss: 1.425 Epoch 9 Batch 738/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.967, Loss: 1.363 Epoch 9 Batch 739/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.439 Epoch 9 Batch 740/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.963, Loss: 1.408 Epoch 9 Batch 741/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.343 Epoch 9 Batch 742/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.366 Epoch 9 Batch 743/1077 - Train Accuracy: 0.997, Validation Accuracy: 0.963, Loss: 1.342 Epoch 9 Batch 744/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.972, Loss: 1.399 Epoch 9 Batch 745/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.972, Loss: 1.435 Epoch 9 Batch 746/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.977, Loss: 1.454 Epoch 9 Batch 747/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.977, Loss: 1.441 Epoch 9 Batch 748/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.977, Loss: 1.412 Epoch 9 Batch 749/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.977, Loss: 1.431 Epoch 9 Batch 750/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.977, Loss: 1.348 Epoch 9 Batch 751/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.982, Loss: 1.439 Epoch 9 Batch 752/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.982, Loss: 1.443 Epoch 9 Batch 753/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.982, Loss: 1.391 Epoch 9 Batch 754/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.982, Loss: 1.364 Epoch 9 Batch 755/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.982, Loss: 1.472 Epoch 9 Batch 756/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.982, Loss: 1.442 Epoch 9 Batch 757/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.982, Loss: 1.412 Epoch 9 Batch 758/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.982, Loss: 1.364 Epoch 9 Batch 759/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.983, Loss: 1.448 Epoch 9 Batch 760/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.983, Loss: 1.423 Epoch 9 Batch 761/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.983, Loss: 1.375 Epoch 9 Batch 762/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.983, Loss: 1.486 Epoch 9 Batch 763/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.982, Loss: 1.432 Epoch 9 Batch 764/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.977, Loss: 1.491 Epoch 9 Batch 765/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.977, Loss: 1.437 Epoch 9 Batch 766/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.978, Loss: 1.420 Epoch 9 Batch 767/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.978, Loss: 1.440 Epoch 9 Batch 768/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.978, Loss: 1.420 Epoch 9 Batch 769/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.978, Loss: 1.293 Epoch 9 Batch 770/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.973, Loss: 1.430 Epoch 9 Batch 771/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.970, Loss: 1.418 Epoch 9 Batch 772/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.961, Loss: 1.387 Epoch 9 Batch 773/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.961, Loss: 1.428 Epoch 9 Batch 774/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.961, Loss: 1.434 Epoch 9 Batch 775/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.957, Loss: 1.461 Epoch 9 Batch 776/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.957, Loss: 1.451 Epoch 9 Batch 777/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.957, Loss: 1.418 Epoch 9 Batch 778/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.956, Loss: 1.408 Epoch 9 Batch 779/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.959, Loss: 1.396 Epoch 9 Batch 780/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.959, Loss: 1.443 Epoch 9 Batch 781/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.956, Loss: 1.439 Epoch 9 Batch 782/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.952, Loss: 1.385 Epoch 9 Batch 783/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.961, Loss: 1.457 Epoch 9 Batch 784/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.965, Loss: 1.425 Epoch 9 Batch 785/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.965, Loss: 1.363 Epoch 9 Batch 786/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.963, Loss: 1.496 Epoch 9 Batch 787/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.960, Loss: 1.440 Epoch 9 Batch 788/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.429 Epoch 9 Batch 789/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.964, Loss: 1.371 Epoch 9 Batch 790/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.964, Loss: 1.502 Epoch 9 Batch 791/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.388 Epoch 9 Batch 792/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.963, Loss: 1.403 Epoch 9 Batch 793/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.458 Epoch 9 Batch 794/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.960, Loss: 1.429 Epoch 9 Batch 795/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.960, Loss: 1.417 Epoch 9 Batch 796/1077 - Train Accuracy: 0.994, Validation Accuracy: 0.956, Loss: 1.368 Epoch 9 Batch 797/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.411 Epoch 9 Batch 798/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.951, Loss: 1.497 Epoch 9 Batch 799/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.951, Loss: 1.443 Epoch 9 Batch 800/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.953, Loss: 1.438 Epoch 9 Batch 801/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.953, Loss: 1.391 Epoch 9 Batch 802/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.953, Loss: 1.443 Epoch 9 Batch 803/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.953, Loss: 1.396 Epoch 9 Batch 804/1077 - Train Accuracy: 0.992, Validation Accuracy: 0.962, Loss: 1.448 Epoch 9 Batch 805/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.967, Loss: 1.393 Epoch 9 Batch 806/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.967, Loss: 1.451 Epoch 9 Batch 807/1077 - Train Accuracy: 0.994, Validation Accuracy: 0.967, Loss: 1.408 Epoch 9 Batch 808/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.967, Loss: 1.469 Epoch 9 Batch 809/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.968, Loss: 1.466 Epoch 9 Batch 810/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.966, Loss: 1.410 Epoch 9 Batch 811/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.966, Loss: 1.416 Epoch 9 Batch 812/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.966, Loss: 1.411 Epoch 9 Batch 813/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.966, Loss: 1.450 Epoch 9 Batch 814/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.965, Loss: 1.423 Epoch 9 Batch 815/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.966, Loss: 1.416 Epoch 9 Batch 816/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.966, Loss: 1.473 Epoch 9 Batch 817/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.481 Epoch 9 Batch 818/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.447 Epoch 9 Batch 819/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.959, Loss: 1.444 Epoch 9 Batch 820/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.958, Loss: 1.363 Epoch 9 Batch 821/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.958, Loss: 1.419 Epoch 9 Batch 822/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.958, Loss: 1.419 Epoch 9 Batch 823/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.966, Loss: 1.419 Epoch 9 Batch 824/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.970, Loss: 1.445 Epoch 9 Batch 825/1077 - Train Accuracy: 0.990, Validation Accuracy: 0.970, Loss: 1.442 Epoch 9 Batch 826/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.969, Loss: 1.440 Epoch 9 Batch 827/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.969, Loss: 1.340 Epoch 9 Batch 828/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.969, Loss: 1.369 Epoch 9 Batch 829/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.969, Loss: 1.439 Epoch 9 Batch 830/1077 - Train Accuracy: 0.946, Validation Accuracy: 0.970, Loss: 1.432 Epoch 9 Batch 831/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.970, Loss: 1.441 Epoch 9 Batch 832/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.972, Loss: 1.469 Epoch 9 Batch 833/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.972, Loss: 1.447 Epoch 9 Batch 834/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.972, Loss: 1.478 Epoch 9 Batch 835/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.967, Loss: 1.413 Epoch 9 Batch 836/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.967, Loss: 1.491 Epoch 9 Batch 837/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.967, Loss: 1.487 Epoch 9 Batch 838/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.967, Loss: 1.416 Epoch 9 Batch 839/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.967, Loss: 1.454 Epoch 9 Batch 840/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.957, Loss: 1.419 Epoch 9 Batch 841/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.955, Loss: 1.439 Epoch 9 Batch 842/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.955, Loss: 1.476 Epoch 9 Batch 843/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.958, Loss: 1.351 Epoch 9 Batch 844/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.958, Loss: 1.357 Epoch 9 Batch 845/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.958, Loss: 1.422 Epoch 9 Batch 846/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.958, Loss: 1.378 Epoch 9 Batch 847/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.963, Loss: 1.541 Epoch 9 Batch 848/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.961, Loss: 1.487 Epoch 9 Batch 849/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.961, Loss: 1.444 Epoch 9 Batch 850/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.961, Loss: 1.484 Epoch 9 Batch 851/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.953, Loss: 1.430 Epoch 9 Batch 852/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.953, Loss: 1.498 Epoch 9 Batch 853/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.953, Loss: 1.478 Epoch 9 Batch 854/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.953, Loss: 1.451 Epoch 9 Batch 855/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.958, Loss: 1.493 Epoch 9 Batch 856/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.958, Loss: 1.454 Epoch 9 Batch 857/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.963, Loss: 1.428 Epoch 9 Batch 858/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.964, Loss: 1.438 Epoch 9 Batch 859/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.959, Loss: 1.403 Epoch 9 Batch 860/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.958, Loss: 1.407 Epoch 9 Batch 861/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.963, Loss: 1.486 Epoch 9 Batch 862/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.968, Loss: 1.454 Epoch 9 Batch 863/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.968, Loss: 1.405 Epoch 9 Batch 864/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.972, Loss: 1.343 Epoch 9 Batch 865/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.972, Loss: 1.522 Epoch 9 Batch 866/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.972, Loss: 1.430 Epoch 9 Batch 867/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.972, Loss: 1.479 Epoch 9 Batch 868/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.972, Loss: 1.445 Epoch 9 Batch 869/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.973, Loss: 1.408 Epoch 9 Batch 870/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.974, Loss: 1.429 Epoch 9 Batch 871/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.974, Loss: 1.375 Epoch 9 Batch 872/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.974, Loss: 1.443 Epoch 9 Batch 873/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.969, Loss: 1.433 Epoch 9 Batch 874/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.969, Loss: 1.345 Epoch 9 Batch 875/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.974, Loss: 1.433 Epoch 9 Batch 876/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.974, Loss: 1.387 Epoch 9 Batch 877/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.974, Loss: 1.431 Epoch 9 Batch 878/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.974, Loss: 1.409 Epoch 9 Batch 879/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.974, Loss: 1.419 Epoch 9 Batch 880/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.974, Loss: 1.451 Epoch 9 Batch 881/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.974, Loss: 1.388 Epoch 9 Batch 882/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.974, Loss: 1.461 Epoch 9 Batch 883/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.974, Loss: 1.405 Epoch 9 Batch 884/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.974, Loss: 1.452 Epoch 9 Batch 885/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.974, Loss: 1.403 Epoch 9 Batch 886/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.976, Loss: 1.377 Epoch 9 Batch 887/1077 - Train Accuracy: 0.945, Validation Accuracy: 0.976, Loss: 1.418 Epoch 9 Batch 888/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.976, Loss: 1.454 Epoch 9 Batch 889/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.974, Loss: 1.411 Epoch 9 Batch 890/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.970, Loss: 1.427 Epoch 9 Batch 891/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.975, Loss: 1.431 Epoch 9 Batch 892/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.975, Loss: 1.372 Epoch 9 Batch 893/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.975, Loss: 1.502 Epoch 9 Batch 894/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.975, Loss: 1.420 Epoch 9 Batch 895/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.978, Loss: 1.403 Epoch 9 Batch 896/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.973, Loss: 1.409 Epoch 9 Batch 897/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.978, Loss: 1.433 Epoch 9 Batch 898/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.982, Loss: 1.464 Epoch 9 Batch 899/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.982, Loss: 1.322 Epoch 9 Batch 900/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.978, Loss: 1.432 Epoch 9 Batch 901/1077 - Train Accuracy: 0.954, Validation Accuracy: 0.978, Loss: 1.413 Epoch 9 Batch 902/1077 - Train Accuracy: 0.950, Validation Accuracy: 0.978, Loss: 1.436 Epoch 9 Batch 903/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.978, Loss: 1.476 Epoch 9 Batch 904/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.977, Loss: 1.410 Epoch 9 Batch 905/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.977, Loss: 1.356 Epoch 9 Batch 906/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.973, Loss: 1.410 Epoch 9 Batch 907/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.973, Loss: 1.405 Epoch 9 Batch 908/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.973, Loss: 1.390 Epoch 9 Batch 909/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.973, Loss: 1.403 Epoch 9 Batch 910/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.977, Loss: 1.434 Epoch 9 Batch 911/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.977, Loss: 1.409 Epoch 9 Batch 912/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.977, Loss: 1.424 Epoch 9 Batch 913/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.977, Loss: 1.469 Epoch 9 Batch 914/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.977, Loss: 1.520 Epoch 9 Batch 915/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.977, Loss: 1.446 Epoch 9 Batch 916/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.977, Loss: 1.437 Epoch 9 Batch 917/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.977, Loss: 1.424 Epoch 9 Batch 918/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.972, Loss: 1.358 Epoch 9 Batch 919/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.972, Loss: 1.404 Epoch 9 Batch 920/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.972, Loss: 1.481 Epoch 9 Batch 921/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.972, Loss: 1.397 Epoch 9 Batch 922/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.972, Loss: 1.380 Epoch 9 Batch 923/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.972, Loss: 1.419 Epoch 9 Batch 924/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.973, Loss: 1.376 Epoch 9 Batch 925/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.973, Loss: 1.324 Epoch 9 Batch 926/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.973, Loss: 1.400 Epoch 9 Batch 927/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.973, Loss: 1.469 Epoch 9 Batch 928/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.978, Loss: 1.509 Epoch 9 Batch 929/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.975, Loss: 1.412 Epoch 9 Batch 930/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.975, Loss: 1.409 Epoch 9 Batch 931/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.975, Loss: 1.400 Epoch 9 Batch 932/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.975, Loss: 1.437 Epoch 9 Batch 933/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.975, Loss: 1.429 Epoch 9 Batch 934/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.975, Loss: 1.376 Epoch 9 Batch 935/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.974, Loss: 1.417 Epoch 9 Batch 936/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.974, Loss: 1.451 Epoch 9 Batch 937/1077 - Train Accuracy: 0.961, Validation Accuracy: 0.974, Loss: 1.462 Epoch 9 Batch 938/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.974, Loss: 1.437 Epoch 9 Batch 939/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.974, Loss: 1.409 Epoch 9 Batch 940/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.974, Loss: 1.376 Epoch 9 Batch 941/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.974, Loss: 1.408 Epoch 9 Batch 942/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.972, Loss: 1.376 Epoch 9 Batch 943/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.972, Loss: 1.457 Epoch 9 Batch 944/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.972, Loss: 1.409 Epoch 9 Batch 945/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.972, Loss: 1.469 Epoch 9 Batch 946/1077 - Train Accuracy: 0.995, Validation Accuracy: 0.970, Loss: 1.391 Epoch 9 Batch 947/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.970, Loss: 1.503 Epoch 9 Batch 948/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.970, Loss: 1.380 Epoch 9 Batch 949/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.970, Loss: 1.429 Epoch 9 Batch 950/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.966, Loss: 1.368 Epoch 9 Batch 951/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.966, Loss: 1.445 Epoch 9 Batch 952/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.966, Loss: 1.401 Epoch 9 Batch 953/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.966, Loss: 1.472 Epoch 9 Batch 954/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.966, Loss: 1.374 Epoch 9 Batch 955/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.966, Loss: 1.437 Epoch 9 Batch 956/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.966, Loss: 1.500 Epoch 9 Batch 957/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.956, Loss: 1.415 Epoch 9 Batch 958/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.956, Loss: 1.396 Epoch 9 Batch 959/1077 - Train Accuracy: 0.988, Validation Accuracy: 0.956, Loss: 1.389 Epoch 9 Batch 960/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.956, Loss: 1.382 Epoch 9 Batch 961/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.956, Loss: 1.434 Epoch 9 Batch 962/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.966, Loss: 1.366 Epoch 9 Batch 963/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.966, Loss: 1.369 Epoch 9 Batch 964/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.966, Loss: 1.409 Epoch 9 Batch 965/1077 - Train Accuracy: 0.958, Validation Accuracy: 0.968, Loss: 1.367 Epoch 9 Batch 966/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.968, Loss: 1.426 Epoch 9 Batch 967/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.969, Loss: 1.401 Epoch 9 Batch 968/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.969, Loss: 1.434 Epoch 9 Batch 969/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.968, Loss: 1.390 Epoch 9 Batch 970/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.968, Loss: 1.493 Epoch 9 Batch 971/1077 - Train Accuracy: 0.972, Validation Accuracy: 0.956, Loss: 1.405 Epoch 9 Batch 972/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.955, Loss: 1.437 Epoch 9 Batch 973/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.955, Loss: 1.497 Epoch 9 Batch 974/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.960, Loss: 1.435 Epoch 9 Batch 975/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.960, Loss: 1.418 Epoch 9 Batch 976/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.960, Loss: 1.445 Epoch 9 Batch 977/1077 - Train Accuracy: 0.991, Validation Accuracy: 0.960, Loss: 1.427 Epoch 9 Batch 978/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.958, Loss: 1.381 Epoch 9 Batch 979/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.960, Loss: 1.478 Epoch 9 Batch 980/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.960, Loss: 1.413 Epoch 9 Batch 981/1077 - Train Accuracy: 0.951, Validation Accuracy: 0.961, Loss: 1.408 Epoch 9 Batch 982/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.958, Loss: 1.387 Epoch 9 Batch 983/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.963, Loss: 1.431 Epoch 9 Batch 984/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.972, Loss: 1.392 Epoch 9 Batch 985/1077 - Train Accuracy: 0.989, Validation Accuracy: 0.967, Loss: 1.427 Epoch 9 Batch 986/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.966, Loss: 1.391 Epoch 9 Batch 987/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.965, Loss: 1.476 Epoch 9 Batch 988/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.965, Loss: 1.416 Epoch 9 Batch 989/1077 - Train Accuracy: 0.957, Validation Accuracy: 0.965, Loss: 1.386 Epoch 9 Batch 990/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.960, Loss: 1.435 Epoch 9 Batch 991/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.964, Loss: 1.430 Epoch 9 Batch 992/1077 - Train Accuracy: 0.962, Validation Accuracy: 0.967, Loss: 1.464 Epoch 9 Batch 993/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.962, Loss: 1.380 Epoch 9 Batch 994/1077 - Train Accuracy: 0.983, Validation Accuracy: 0.967, Loss: 1.409 Epoch 9 Batch 995/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.967, Loss: 1.446 Epoch 9 Batch 996/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.967, Loss: 1.534 Epoch 9 Batch 997/1077 - Train Accuracy: 0.987, Validation Accuracy: 0.966, Loss: 1.383 Epoch 9 Batch 998/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.961, Loss: 1.402 Epoch 9 Batch 999/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.966, Loss: 1.460 Epoch 9 Batch 1000/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.962, Loss: 1.445 Epoch 9 Batch 1001/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.966, Loss: 1.547 Epoch 9 Batch 1002/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.967, Loss: 1.361 Epoch 9 Batch 1003/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.957, Loss: 1.377 Epoch 9 Batch 1004/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.957, Loss: 1.467 Epoch 9 Batch 1005/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.961, Loss: 1.399 Epoch 9 Batch 1006/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.432 Epoch 9 Batch 1007/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.961, Loss: 1.399 Epoch 9 Batch 1008/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.962, Loss: 1.486 Epoch 9 Batch 1009/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.959, Loss: 1.353 Epoch 9 Batch 1010/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.959, Loss: 1.355 Epoch 9 Batch 1011/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.959, Loss: 1.443 Epoch 9 Batch 1012/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.966, Loss: 1.355 Epoch 9 Batch 1013/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.971, Loss: 1.453 Epoch 9 Batch 1014/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.969, Loss: 1.391 Epoch 9 Batch 1015/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.973, Loss: 1.452 Epoch 9 Batch 1016/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.968, Loss: 1.469 Epoch 9 Batch 1017/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.973, Loss: 1.473 Epoch 9 Batch 1018/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.973, Loss: 1.457 Epoch 9 Batch 1019/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.973, Loss: 1.521 Epoch 9 Batch 1020/1077 - Train Accuracy: 0.993, Validation Accuracy: 0.969, Loss: 1.422 Epoch 9 Batch 1021/1077 - Train Accuracy: 0.952, Validation Accuracy: 0.969, Loss: 1.478 Epoch 9 Batch 1022/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.967, Loss: 1.302 Epoch 9 Batch 1023/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.967, Loss: 1.448 Epoch 9 Batch 1024/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.967, Loss: 1.426 Epoch 9 Batch 1025/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.971, Loss: 1.374 Epoch 9 Batch 1026/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.976, Loss: 1.400 Epoch 9 Batch 1027/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.976, Loss: 1.456 Epoch 9 Batch 1028/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.980, Loss: 1.410 Epoch 9 Batch 1029/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.985, Loss: 1.458 Epoch 9 Batch 1030/1077 - Train Accuracy: 0.979, Validation Accuracy: 0.983, Loss: 1.451 Epoch 9 Batch 1031/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.974, Loss: 1.480 Epoch 9 Batch 1032/1077 - Train Accuracy: 0.966, Validation Accuracy: 0.974, Loss: 1.436 Epoch 9 Batch 1033/1077 - Train Accuracy: 0.953, Validation Accuracy: 0.971, Loss: 1.512 Epoch 9 Batch 1034/1077 - Train Accuracy: 0.959, Validation Accuracy: 0.972, Loss: 1.462 Epoch 9 Batch 1035/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.972, Loss: 1.445 Epoch 9 Batch 1036/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.972, Loss: 1.450 Epoch 9 Batch 1037/1077 - Train Accuracy: 0.985, Validation Accuracy: 0.972, Loss: 1.380 Epoch 9 Batch 1038/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.972, Loss: 1.477 Epoch 9 Batch 1039/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.965, Loss: 1.499 Epoch 9 Batch 1040/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.479 Epoch 9 Batch 1041/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.432 Epoch 9 Batch 1042/1077 - Train Accuracy: 0.981, Validation Accuracy: 0.968, Loss: 1.462 Epoch 9 Batch 1043/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.968, Loss: 1.433 Epoch 9 Batch 1044/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.967, Loss: 1.462 Epoch 9 Batch 1045/1077 - Train Accuracy: 0.964, Validation Accuracy: 0.967, Loss: 1.423 Epoch 9 Batch 1046/1077 - Train Accuracy: 0.976, Validation Accuracy: 0.967, Loss: 1.428 Epoch 9 Batch 1047/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.967, Loss: 1.420 Epoch 9 Batch 1048/1077 - Train Accuracy: 0.968, Validation Accuracy: 0.967, Loss: 1.450 Epoch 9 Batch 1049/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.967, Loss: 1.429 Epoch 9 Batch 1050/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.967, Loss: 1.419 Epoch 9 Batch 1051/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.968, Loss: 1.445 Epoch 9 Batch 1052/1077 - Train Accuracy: 0.969, Validation Accuracy: 0.968, Loss: 1.447 Epoch 9 Batch 1053/1077 - Train Accuracy: 0.960, Validation Accuracy: 0.968, Loss: 1.422 Epoch 9 Batch 1054/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.966, Loss: 1.388 Epoch 9 Batch 1055/1077 - Train Accuracy: 0.982, Validation Accuracy: 0.969, Loss: 1.463 Epoch 9 Batch 1056/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.969, Loss: 1.433 Epoch 9 Batch 1057/1077 - Train Accuracy: 0.986, Validation Accuracy: 0.969, Loss: 1.331 Epoch 9 Batch 1058/1077 - Train Accuracy: 0.970, Validation Accuracy: 0.969, Loss: 1.450 Epoch 9 Batch 1059/1077 - Train Accuracy: 0.963, Validation Accuracy: 0.969, Loss: 1.443 Epoch 9 Batch 1060/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.964, Loss: 1.413 Epoch 9 Batch 1061/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.969, Loss: 1.438 Epoch 9 Batch 1062/1077 - Train Accuracy: 0.977, Validation Accuracy: 0.969, Loss: 1.377 Epoch 9 Batch 1063/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.969, Loss: 1.360 Epoch 9 Batch 1064/1077 - Train Accuracy: 0.971, Validation Accuracy: 0.969, Loss: 1.385 Epoch 9 Batch 1065/1077 - Train Accuracy: 0.974, Validation Accuracy: 0.969, Loss: 1.464 Epoch 9 Batch 1066/1077 - Train Accuracy: 0.980, Validation Accuracy: 0.964, Loss: 1.321 Epoch 9 Batch 1067/1077 - Train Accuracy: 0.955, Validation Accuracy: 0.964, Loss: 1.411 Epoch 9 Batch 1068/1077 - Train Accuracy: 0.978, Validation Accuracy: 0.964, Loss: 1.452 Epoch 9 Batch 1069/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.964, Loss: 1.494 Epoch 9 Batch 1070/1077 - Train Accuracy: 0.965, Validation Accuracy: 0.964, Loss: 1.486 Epoch 9 Batch 1071/1077 - Train Accuracy: 0.967, Validation Accuracy: 0.964, Loss: 1.440 Epoch 9 Batch 1072/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.966, Loss: 1.427 Epoch 9 Batch 1073/1077 - Train Accuracy: 0.984, Validation Accuracy: 0.966, Loss: 1.565 Epoch 9 Batch 1074/1077 - Train Accuracy: 0.973, Validation Accuracy: 0.961, Loss: 1.443 Epoch 9 Batch 1075/1077 - Train Accuracy: 0.975, Validation Accuracy: 0.968, Loss: 1.380 Model Trained and Saved","title":"Train"},{"location":"dl/translator/dlnd_language_translation/#save-parameters","text":"Save the batch_size and save_path parameters for inference. \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" # Save parameters for checkpoint helper . save_params(save_path)","title":"Save Parameters"},{"location":"dl/translator/dlnd_language_translation/#checkpoint","text":"\"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" import tensorflow as tf import numpy as np import helper import problem_unittests as tests _, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = helper . load_preprocess() load_path = helper . load_params()","title":"Checkpoint"},{"location":"dl/translator/dlnd_language_translation/#sentence-to-sequence","text":"To feed a sentence into the model for translation, you first need to preprocess it. Implement the function sentence_to_seq() to preprocess new sentences. Convert the sentence to lowercase Convert words into ids using vocab_to_int Convert words not in the vocabulary, to the <UNK> word id. def sentence_to_seq (sentence, vocab_to_int): \"\"\" Convert a sentence to a sequence of ids :param sentence: String :param vocab_to_int: Dictionary to go from the words to an id :return: List of word ids \"\"\" # TODO: Implement Function sequence = [vocab_to_int . get(word, vocab_to_int[ '<UNK>' ]) for word in sentence . lower() . split()] return sequence \"\"\" DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE \"\"\" tests . test_sentence_to_seq(sentence_to_seq) Tests Passed","title":"Sentence to Sequence"},{"location":"dl/translator/dlnd_language_translation/#translate","text":"This will translate translate_sentence from English to French. translate_sentence = 'he saw a old yellow truck .' \"\"\" DON'T MODIFY ANYTHING IN THIS CELL \"\"\" translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int) loaded_graph = tf . Graph() with tf . Session(graph = loaded_graph) as sess: # Load saved model loader = tf . train . import_meta_graph(load_path + '.meta' ) loader . restore(sess, load_path) input_data = loaded_graph . get_tensor_by_name( 'input:0' ) logits = loaded_graph . get_tensor_by_name( 'logits:0' ) keep_prob = loaded_graph . get_tensor_by_name( 'keep_prob:0' ) translate_logits = sess . run(logits, {input_data: [translate_sentence], keep_prob: 1.0 })[ 0 ] print ( 'Input' ) print ( ' Word Ids: {}' . format([i for i in translate_sentence])) print ( ' English Words: {}' . format([source_int_to_vocab[i] for i in translate_sentence])) print ( ' \\n Prediction' ) print ( ' Word Ids: {}' . format([i for i in np . argmax(translate_logits, 1 )])) print ( ' French Words: {}' . format([target_int_to_vocab[i] for i in np . argmax(translate_logits, 1 )])) Input Word Ids: [86, 172, 194, 216, 38, 155, 7] English Words: ['he', 'saw', 'a', 'old', 'yellow', 'truck', '.'] Prediction Word Ids: [202, 157, 46, 271, 49, 141, 352, 4, 1] French Words: ['il', 'a', 'vu', 'un', 'vieux', 'camion', 'jaune', '.', '&lt;EOS&gt;']","title":"Translate"},{"location":"dl/translator/dlnd_language_translation/#imperfect-translation","text":"You might notice that some sentences translate better than others. Since the dataset you're using only has a vocabulary of 227 English words of the thousands that you use, you're only going to see good results using these words. For this project, you don't need a perfect translation. However, if you want to create a better translation model, you'll need better data. You can train on the WMT10 French-English corpus . This dataset has more vocabulary and richer in topics discussed. However, this will take you days to train, so make sure you've a GPU and the neural network is performing well on dataset we provided. Just make sure you play with the WMT10 corpus after you've submitted this project.","title":"Imperfect Translation"},{"location":"dl/translator/dlnd_language_translation/#submitting-this-project","text":"When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_language_translation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission.","title":"Submitting This Project"},{"location":"ml/Customer_segments/customer_segments/","text":"Machine Learning Engineer Nanodegree Unsupervised Learning Project: Creating Customer Segments Welcome to the third project of the Machine Learning Engineer Nanodegree! In this notebook, some template code has already been provided for you, and it will be your job to implement the additional functionality necessary to successfully complete this project. Sections that begin with 'Implementation' in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a 'Question X' header. Carefully read each question and provide thorough answers in the following text boxes that begin with 'Answer:' . Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide. Note: Code and Markdown cells can be executed using the Shift + Enter keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode. Getting Started In this project, you will analyze a dataset containing data on various customers' annual spending amounts (reported in monetary units ) of diverse product categories for internal structure. One goal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with. Doing so would equip the distributor with insight into how to best structure their delivery service to meet the needs of each customer. The dataset for this project can be found on the UCI Machine Learning Repository . For the purposes of this project, the features 'Channel' and 'Region' will be excluded in the analysis \u2014 with focus instead on the six product categories recorded for customers. Run the code block below to load the wholesale customers dataset, along with a few of the necessary Python libraries required for this project. You will know the dataset loaded successfully if the size of the dataset is reported. # Import libraries necessary for this project import numpy as np import pandas as pd from IPython.display import display # Allows the use of display() for DataFrames import matplotlib.pyplot as plt # Import supplementary visualizations code visuals.py import visuals as vs import seaborn as sns # Pretty display for notebooks % matplotlib inline # Load the wholesale customers dataset try : data = pd . read_csv( \"customers.csv\" ) data . drop([ 'Region' , 'Channel' ], axis = 1 , inplace = True ) print ( \"Wholesale customers dataset has {} samples with {} features each.\" . format( * data . shape)) except : print ( \"Dataset could not be loaded. Is the dataset missing?\" ) Wholesale customers dataset has 440 samples with 6 features each. Data Exploration In this section, you will begin exploring the data through visualizations and code to understand how each feature is related to the others. You will observe a statistical description of the dataset, consider the relevance of each feature, and select a few sample data points from the dataset which you will track through the course of this project. Run the code block below to observe a statistical description of the dataset. Note that the dataset is composed of six important product categories: 'Fresh' , 'Milk' , 'Grocery' , 'Frozen' , 'Detergents_Paper' , and 'Delicatessen' . Consider what each category represents in terms of products you could purchase. Lets see what are the column names and their types data . info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 440 entries, 0 to 439 Data columns (total 6 columns): Fresh 440 non-null int64 Milk 440 non-null int64 Grocery 440 non-null int64 Frozen 440 non-null int64 Detergents_Paper 440 non-null int64 Delicatessen 440 non-null int64 dtypes: int64(6) memory usage: 20.7 KB Head part of the data data . head( 5 ) Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 0 12669 9656 7561 214 2674 1338 1 7057 9810 9568 1762 3293 1776 2 6353 8808 7684 2405 3516 7844 3 13265 1196 4221 6404 507 1788 4 22615 5410 7198 3915 1777 5185 Lets look how each feature propagates from customer to customers. from matplotlib import cm test_data = data . loc[ 0 : 200 ] test_data . plot(colormap = cm . cubehelix,figsize = ( 16 , 12 )) &lt;matplotlib.axes._subplots.AxesSubplot at 0x10de94950&gt; Fresh and grocery are really high demanding items! But this is not the global nature of the data. Detail will be discussed later. Lets look their propertion on small section of the data test_data = data . loc[ 100 : 150 ] test_data . plot . barh(stacked = True ,figsize = ( 16 , 15 ) ); Customer at index 125 is really heavy consumer of fresh! Little bit of statistics of each columns: # Display a description of the dataset display(data . describe()) Fresh Milk Grocery Frozen Detergents_Paper Delicatessen count 440.000000 440.000000 440.000000 440.000000 440.000000 440.000000 mean 12000.297727 5796.265909 7951.277273 3071.931818 2881.493182 1524.870455 std 12647.328865 7380.377175 9503.162829 4854.673333 4767.854448 2820.105937 min 3.000000 55.000000 3.000000 25.000000 3.000000 3.000000 25% 3127.750000 1533.000000 2153.000000 742.250000 256.750000 408.250000 50% 8504.000000 3627.000000 4755.500000 1526.000000 816.500000 965.500000 75% 16933.750000 7190.250000 10655.750000 3554.250000 3922.000000 1820.250000 max 112151.000000 73498.000000 92780.000000 60869.000000 40827.000000 47943.000000 In general the mean for each feature seems much higher than the median(50%-Quartile), so this data seems somewhat skewed . We can visualize them as well in the histogram comming in next blocks. Histogram for individual data plt . figure(figsize = ( 14 , 14 )) bins = [ 100 , 100 , 100 , 100 , 100 , 100 ] colors = sns . color_palette( \"bright\" , 6 ) items = [ 'Fresh' , 'Milk' , 'Grocery' , 'Frozen' , 'Detergents_Paper' , 'Delicatessen' ] ks = [ 1 , 2 , 3 , 4 , 5 , 6 ] # plot individual histograms for color,item,k in zip (colors,items,ks): plt . subplot( 3 , 2 ,k) plt . title( \"plot of \" + item) sns . distplot(data[item],bins = 50 , kde = True , color = color) Most of these features have peak near zero to 1000. Grocery is pretty much flat among them. We can cut a section from each of these plots and compare between them as shown below: Putting all together # Plot all together plt . figure(figsize = ( 14 , 6 )) plt . title( \"distribution of all items\" ) for ibin,color,item in zip (bins,colors,items): sns . distplot(data[item],bins = ibin, kde = True , color = color) plt . legend([item for item in items]) &lt;matplotlib.legend.Legend at 0x1051439d0&gt; Peak of the distribution shifts to the right while moving from feature Lets make it much lear as shown below: Lets visualize most dynamic part of the data # view most dynamic part of the data plt . figure(figsize = ( 14 , 10 )) plt . title( \"Viewing most dynamic part of the data\" ) for ibin,color,item in zip (bins,colors,items): sns . distplot(data[data[item] <= 5000 ][item],bins = ibin, kde = True , color = color) plt . legend([item for item in items]) &lt;matplotlib.legend.Legend at 0x111c56050&gt; Wow! Look at this trend : Detergent -> Delicatessen-> Fresh-I -> Frozen -> Milk -> Grocery -> Fresh-II . Fresh has two peak representing two types of purcheser : household(low price) and might be resturents(high price) etc. Data skewed to the right as pridected before. Implementation: Selecting Samples To get a better understanding of the customers and how their data will transform through the analysis, it would be best to select a few sample data points and explore them in more detail. In the code block below, add three indices of your choice to the indices list which will represent the customers to track. It is suggested to try different sets of samples until you obtain customers that vary significantly from one another. # TODO: Select three indices of your choice you wish to sample from the dataset indices = [ 12 , 125 , 412 ] # Create a DataFrame of the chosen samples samples = pd . DataFrame(data . loc[indices], columns = data . keys()) . reset_index(drop = True ) print ( \"Chosen samples of wholesale customers dataset:\" ) display(samples) samples . plot . barh(stacked = True ,figsize = ( 10 , 2 ) ); # show devations of each feature from its mean measured by std mean_data = np . mean(data) std_data = np . std(data) deviation_samples = (samples - mean_data) / std_data print ( \" \\n Deviation of chosen samples of wholesale customers dataset in mean+deviation*std:\" ) display(deviation_samples) Chosen samples of wholesale customers dataset: Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 0 31714 12319 11757 287 3881 2931 1 76237 3473 7102 16538 778 918 2 97 3605 12400 98 2970 62 Deviation of chosen samples of wholesale customers dataset in mean+deviation*std: Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 0 1.560499 0.884800 0.400925 -0.574313 0.209873 0.499176 1 5.084854 -0.315148 -0.089470 2.776994 -0.441685 -0.215439 2 -0.942242 -0.297242 0.468664 -0.613289 0.018584 -0.519319 Quartile Visualization of sample plt . figure(figsize = ( 12 , 5 )) percentiles = data . rank(pct = True ) percentiles = 100 * percentiles . round(decimals = 3 ) percentiles = percentiles . iloc[indices] display(percentiles) print \"Quartile Visualization\" sns . heatmap(percentiles, vmin = 1 , vmax = 99 , annot = True ) Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 12 93.6 90.2 79.1 10.7 74.5 89.8 125 99.8 47.7 63.4 98.2 48.6 48.6 412 1.8 49.3 80.9 3.2 69.3 4.8 Quartile Visualization &lt;matplotlib.axes._subplots.AxesSubplot at 0x113465150&gt; Question 1 Consider the total purchase cost of each product category and the statistical description of the dataset above for your sample customers. What kind of establishment (customer) could each of the three samples you've chosen represent? Hint: Examples of establishments include places like markets, cafes, and retailers, among many others. Avoid using names for establishments, such as saying \"McDonalds\" when describing a sample customer as a restaurant. Answer: There are three information hiden in samples which are (1) Total values: It helps to know typr of wholesale distributer (2) Mean value of each category and deviations: It helps to know types of distributer Lets go sample by sample: Sample-1: This customer a heavy consumer of Fresh items, milk and Delicatessen. Spending on Grocery is also above 75th quartile. Frozen is pretty much less and below 25th quartile and mean. It should be Supermarket , because all kinds of produces are sold much more than average value except the frozen. Sample-2: This customer is crazy at Fresh. It touched 100th quartile.Second most purchased items are frozens. Grocery is little below the mean value but pretty above the 50th quartile value. Rest of them are below 5oth quartile.This should be Greengrocery , be cause the main produce being sold is fresh produce and frozen while the total amount of produces is below average Sample-3: For this customer, first priority is Grocery which is above 75th quartile range and pretty above mean. Next priority os Detergent paper which is above 50th quartile range and also above the mean. Milk is below mean and median. Rest of the other are blow mean and 25th quartile range. It is Convenience store , because the main produce being sold is grocery, while the total amount of produces is below average Implementation: Feature Relevance One interesting thought to consider is if one (or more) of the six product categories is actually relevant for understanding customer purchasing. That is to say, is it possible to determine whether customers purchasing some amount of one category of products will necessarily purchase some proportional amount of another category of products? We can make this determination quite easily by training a supervised regression learner on a subset of the data with one feature removed, and then score how well that model can predict the removed feature. In the code block below, you will need to implement the following: - Assign new_data a copy of the data by removing a feature of your choice using the DataFrame.drop function. - Use sklearn.cross_validation.train_test_split to split the dataset into training and testing sets. - Use the removed feature as your target label. Set a test_size of 0.25 and set a random_state . - Import a decision tree regressor, set a random_state , and fit the learner to the training data. - Report the prediction score of the testing set using the regressor's score function. from sklearn.cross_validation import train_test_split from sklearn.tree import DecisionTreeRegressor # TODO: Make a copy of the DataFrame, using the 'drop' function to drop the given feature new_data = data . copy() targets = [ 'Fresh' , 'Milk' , 'Grocery' , 'Frozen' , 'Detergents_Paper' , 'Delicatessen' ] # train and evaluate for each feature scores = {} for target_feature in targets: label = new_data[target_feature] left_features = new_data . drop([target_feature], axis = 1 ) # TODO: Split the data into training and testing sets using the given feature as the target X_train, X_test, y_train, y_test = train_test_split(left_features, label, test_size = 0.25 , random_state = 1 ) # TODO: Create a decision tree regressor and fit it to the training set regressor = DecisionTreeRegressor(random_state = 1 ) regressor . fit(X_train, y_train) # TODO: Report the score of the prediction using the testing set score = regressor . score(X_test, y_test) scores[target_feature] = score # display features and scores result = pd . DataFrame(scores, index = [ 'Score' ]) display(result) Delicatessen Detergents_Paper Fresh Frozen Grocery Milk Score -0.429125 0.815241 -0.923374 -0.649574 0.795768 0.51585 plt . figure(figsize = ( 12 , 12 )) result . plot(kind = \"bar\" ,figsize = ( 8 , 8 )) plt . axhline( 0 , color = 'k' ) &lt;matplotlib.lines.Line2D at 0x113bc6350&gt; &lt;matplotlib.figure.Figure at 0x113bc6550&gt; Question 2 Which feature did you attempt to predict? What was the reported prediction score? Is this feature is necessary for identifying customers' spending habits? Hint: The coefficient of determination, R^2 , is scored between 0 and 1, with 1 being a perfect fit. A negative R^2 implies the model fails to fit the data. Answer: I am going to predict Detergents_Paper or Grocery . Reported prediction score for them are 0.815241 and 0.795768 respectively. No! this feature is less important for identifying customer's spending habits. The reason is following: A predication with higher score (close to 1.0) implies that the training features are likely to predicate the label feature. Alternalely, label feature is more dependent on the other features, which makes it unnecessary for identifying customer's spending habits. Predication with lower score (negative value) indicates the labeling feature is independent to others and necessary for learning algorithm. Visualize Feature Distributions To get a better understanding of the dataset, we can construct a scatter matrix of each of the six product features present in the data. If you found that the feature you attempted to predict above is relevant for identifying a specific customer, then the scatter matrix below may not show any correlation between that feature and the others. Conversely, if you believe that feature is not relevant for identifying a specific customer, the scatter matrix might show a correlation between that feature and another feature in the data. Run the code block below to produce a scatter matrix. plt . figure(figsize = ( 16 , 21 )) sns . pairplot(data) &lt;seaborn.axisgrid.PairGrid at 0x113bc6c90&gt; &lt;matplotlib.figure.Figure at 0x113bc6b90&gt; Study of variance covariance of the data data . cov() Fresh Milk Grocery Frozen Detergents_Paper Delicatessen Fresh 1.599549e+08 9.381789e+06 -1.424713e+06 2.123665e+07 -6.147826e+06 8.727310e+06 Milk 9.381789e+06 5.446997e+07 5.108319e+07 4.442612e+06 2.328834e+07 8.457925e+06 Grocery -1.424713e+06 5.108319e+07 9.031010e+07 -1.854282e+06 4.189519e+07 5.507291e+06 Frozen 2.123665e+07 4.442612e+06 -1.854282e+06 2.356785e+07 -3.044325e+06 5.352342e+06 Detergents_Paper -6.147826e+06 2.328834e+07 4.189519e+07 -3.044325e+06 2.273244e+07 9.316807e+05 Delicatessen 8.727310e+06 8.457925e+06 5.507291e+06 5.352342e+06 9.316807e+05 7.952997e+06 plt . figure(figsize = ( 10 , 10 )) sns . heatmap(data . cov(),annot = True ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x115490890&gt; This plot of covariance gives us little bit more idea about possible correlation between variables. This is symmetric plot. Diagonal values represents the variance where as non diagonal values gives some information about correlation. For example we can look Detergents_Paper and Grocery . Lets look them in separate plot below: plot of Detergents_Paper with Grocery plt . figure(figsize = ( 8 , 8 )) sns . jointplot(x = \"Detergents_Paper\" , y = \"Grocery\" , data = data,size = 15 ,kind = 'reg' ); &lt;matplotlib.figure.Figure at 0x115ec98d0&gt; Question 3 Are there any pairs of features which exhibit some degree of correlation? Does this confirm or deny your suspicions about the relevance of the feature you attempted to predict? How is the data for those features distributed? Hint: Is the data normally distributed? Where do most of the data points lie? Answer: Are there any pairs of features which exhibit some degree of correlation? Yes! Detergents_Paper and Grocery are highly correlated. There may be little correlation between Detergents_Paper and Milk , Grocery and Milk as well. Does this confirm or deny your suspicions about the relevance of the feature you attempted to predict? Yes! It confirms my suspicions about the relevance of the feature in Question 2. How is the data for those features distributed? The data distribution plot looks more like a F distribution. Data Preprocessing In this section, you will preprocess the data to create a better representation of customers by performing a scaling on the data and detecting (and optionally removing) outliers. Preprocessing data is often times a critical step in assuring that results you obtain from your analysis are significant and meaningful. Implementation: Feature Scaling If data is not normally distributed, especially if the mean and median vary significantly (indicating a large skew), it is most often appropriate to apply a non-linear scaling \u2014 particularly for financial data. One way to achieve this scaling is by using a Box-Cox test , which calculates the best power transformation of the data that reduces skewness. A simpler approach which can work in most cases would be applying the natural logarithm. In the code block below, you will need to implement the following: - Assign a copy of the data to log_data after applying logarithmic scaling. Use the np.log function for this. - Assign a copy of the sample data to log_samples after applying logarithmic scaling. Again, use np.log . # TODO: Scale the data using the natural logarithm log_data = np . log(data) # TODO: Scale the sample data using the natural logarithm log_samples = np . log(samples) # Produce a scatter matrix for each pair of newly-transformed features sns . pairplot(log_data) &lt;seaborn.axisgrid.PairGrid at 0x110d1cbd0&gt; Observation After applying a natural logarithm scaling to the data, the distribution of each feature should appear much more normal. For any pairs of features you may have identified earlier as being correlated, observe here whether that correlation is still present (and whether it is now stronger or weaker than before). Run the code below to see how the sample data has changed after having the natural logarithm applied to it. # Display the log-transformed sample data display(log_samples) Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 0 10.364514 9.418898 9.372204 5.659482 8.263848 7.983099 1 11.241602 8.152774 8.868132 9.713416 6.656727 6.822197 2 4.574711 8.190077 9.425452 4.584967 7.996317 4.127134 Implementation: Outlier Detection Detecting outliers in the data is extremely important in the data preprocessing step of any analysis. The presence of outliers can often skew results which take into consideration these data points. There are many \"rules of thumb\" for what constitutes an outlier in a dataset. Here, we will use Tukey's Method for identfying outliers : An outlier step is calculated as 1.5 times the interquartile range (IQR). A data point with a feature that is beyond an outlier step outside of the IQR for that feature is considered abnormal. In the code block below, you will need to implement the following: - Assign the value of the 25th percentile for the given feature to Q1 . Use np.percentile for this. - Assign the value of the 75th percentile for the given feature to Q3 . Again, use np.percentile . - Assign the calculation of an outlier step for the given feature to step . - Optionally remove data points from the dataset by adding indices to the outliers list. NOTE: If you choose to remove any outliers, ensure that the sample data does not contain any of these points! Once you have performed this implementation, the dataset will be stored in the variable good_data . log_data[ 'sn' ] = log_data . index bad_indexes = {} # For each feature find the data points with extreme high or low values for feature in log_data . keys(): feature_data = log_data[feature] # TODO: Calculate Q1 (25th percentile of the data) for the given feature Q1 = np . percentile(feature_data, 25 ) # TODO: Calculate Q3 (75th percentile of the data) for the given feature Q3 = np . percentile(feature_data, 75 ) # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range) step = 1.5 * (Q3 - Q1) feature_outliers = log_data[ ~ ((feature_data >= Q1 - step) & (feature_data <= Q3 + step))] # Display the outliers print ( \"Data points considered outliers for the feature '{}':\" . format(feature)) display(feature_outliers) for i, r in feature_data . iteritems(): if not (Q1 - step <= r <= Q3 + step): if i not in bad_indexes: bad_indexes[i] = 1 else : bad_indexes[i] += 1 # OPTIONAL: Select the indices for data points you wish to remove outliers = [i for i, n in bad_indexes . items() if n > 1 ] # Remove the outliers, if any were specified good_data = log_data . drop(log_data . index[outliers]) . reset_index(drop = True ) outliars_data = log_data . iloc[log_data . index[outliers],:] Data points considered outliers for the feature 'Fresh': Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn 65 4.442651 9.950323 10.732651 3.583519 10.095388 7.260523 65 66 2.197225 7.335634 8.911530 5.164786 8.151333 3.295837 66 81 5.389072 9.163249 9.575192 5.645447 8.964184 5.049856 81 95 1.098612 7.979339 8.740657 6.086775 5.407172 6.563856 95 96 3.135494 7.869402 9.001839 4.976734 8.262043 5.379897 96 128 4.941642 9.087834 8.248791 4.955827 6.967909 1.098612 128 171 5.298317 10.160530 9.894245 6.478510 9.079434 8.740337 171 193 5.192957 8.156223 9.917982 6.865891 8.633731 6.501290 193 218 2.890372 8.923191 9.629380 7.158514 8.475746 8.759669 218 304 5.081404 8.917311 10.117510 6.424869 9.374413 7.787382 304 305 5.493061 9.468001 9.088399 6.683361 8.271037 5.351858 305 338 1.098612 5.808142 8.856661 9.655090 2.708050 6.309918 338 353 4.762174 8.742574 9.961898 5.429346 9.069007 7.013016 353 355 5.247024 6.588926 7.606885 5.501258 5.214936 4.844187 355 357 3.610918 7.150701 10.011086 4.919981 8.816853 4.700480 357 412 4.574711 8.190077 9.425452 4.584967 7.996317 4.127134 412 Data points considered outliers for the feature 'Milk': Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn 86 10.039983 11.205013 10.377047 6.894670 9.906981 6.805723 86 98 6.220590 4.718499 6.656727 6.796824 4.025352 4.882802 98 154 6.432940 4.007333 4.919981 4.317488 1.945910 2.079442 154 356 10.029503 4.897840 5.384495 8.057377 2.197225 6.306275 356 Data points considered outliers for the feature 'Grocery': Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn 75 9.923192 7.036148 1.098612 8.390949 1.098612 6.882437 75 154 6.432940 4.007333 4.919981 4.317488 1.945910 2.079442 154 Data points considered outliers for the feature 'Frozen': Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn 38 8.431853 9.663261 9.723703 3.496508 8.847360 6.070738 38 57 8.597297 9.203618 9.257892 3.637586 8.932213 7.156177 57 65 4.442651 9.950323 10.732651 3.583519 10.095388 7.260523 65 145 10.000569 9.034080 10.457143 3.737670 9.440738 8.396155 145 175 7.759187 8.967632 9.382106 3.951244 8.341887 7.436617 175 264 6.978214 9.177714 9.645041 4.110874 8.696176 7.142827 264 325 10.395650 9.728181 9.519735 11.016479 7.148346 8.632128 325 420 8.402007 8.569026 9.490015 3.218876 8.827321 7.239215 420 429 9.060331 7.467371 8.183118 3.850148 4.430817 7.824446 429 439 7.932721 7.437206 7.828038 4.174387 6.167516 3.951244 439 Data points considered outliers for the feature 'Detergents_Paper': Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn 75 9.923192 7.036148 1.098612 8.390949 1.098612 6.882437 75 161 9.428190 6.291569 5.645447 6.995766 1.098612 7.711101 161 Data points considered outliers for the feature 'Delicatessen': Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn 66 2.197225 7.335634 8.911530 5.164786 8.151333 3.295837 66 109 7.248504 9.724899 10.274568 6.511745 6.728629 1.098612 109 128 4.941642 9.087834 8.248791 4.955827 6.967909 1.098612 128 137 8.034955 8.997147 9.021840 6.493754 6.580639 3.583519 137 142 10.519646 8.875147 9.018332 8.004700 2.995732 1.098612 142 154 6.432940 4.007333 4.919981 4.317488 1.945910 2.079442 154 183 10.514529 10.690808 9.911952 10.505999 5.476464 10.777768 183 184 5.789960 6.822197 8.457443 4.304065 5.811141 2.397895 184 187 7.798933 8.987447 9.192075 8.743372 8.148735 1.098612 187 203 6.368187 6.529419 7.703459 6.150603 6.860664 2.890372 203 233 6.871091 8.513988 8.106515 6.842683 6.013715 1.945910 233 285 10.602965 6.461468 8.188689 6.948897 6.077642 2.890372 285 289 10.663966 5.655992 6.154858 7.235619 3.465736 3.091042 289 343 7.431892 8.848509 10.177932 7.283448 9.646593 3.610918 343 Data points considered outliers for the feature 'sn': Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn Outliars data Outliars indices [i for i, n in bad_indexes . items() if n > 1 ] [128, 154, 65, 66, 75] outliars_data Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn 128 4.941642 9.087834 8.248791 4.955827 6.967909 1.098612 128 154 6.432940 4.007333 4.919981 4.317488 1.945910 2.079442 154 65 4.442651 9.950323 10.732651 3.583519 10.095388 7.260523 65 66 2.197225 7.335634 8.911530 5.164786 8.151333 3.295837 66 75 9.923192 7.036148 1.098612 8.390949 1.098612 6.882437 75 Visualization of outliers def outlier_plotter (Feature): ax = good_data . plot(kind = 'scatter' ,\\ x = 'sn' ,\\ y = Feature,\\ color = 'LightGreen' ,\\ label = 'good_data' ,\\ s = 30 ) outliars_data . plot(kind = 'scatter' , \\ x = 'sn' ,\\ y = Feature,\\ color = 'Darkred' ,\\ label = 'outliars_data' ,\\ s = 60 ,ax = ax) outlier_plotter(Feature = 'Milk' ) outlier_plotter(Feature = 'Grocery' ) outlier_plotter(Feature = 'Fresh' ) outlier_plotter(Feature = 'Frozen' ) good_data = good_data . drop( 'sn' ,axis = 1 ) Question 4 Are there any data points considered outliers for more than one feature based on the definition above? Should these data points be removed from the dataset? If any data points were added to the outliers list to be removed, explain why. Answer: Yes, There are data points considered outliers for more than one feature based on the definition above. Yes, they should be removed and the reason is that the data points at row [128, 154, 65, 66, 75] has more than one outlier features because they are more likely to be true outliers than others. Removing all of them could cause underfitting. So, for ones with one feature outlier, I decided to keep them as single outlier feature. It may be the desirable pattern in the datasets. Feature Transformation In this section you will use principal component analysis (PCA) to draw conclusions about the underlying structure of the wholesale customer data. Since using PCA on a dataset calculates the dimensions which best maximize variance, we will find which compound combinations of features best describe customers. Implementation: PCA Now that the data has been scaled to a more normal distribution and has had any necessary outliers removed, we can now apply PCA to the good_data to discover which dimensions about the data best maximize the variance of features involved. In addition to finding these dimensions, PCA will also report the explained variance ratio of each dimension \u2014 how much variance within the data is explained by that dimension alone. Note that a component (dimension) from PCA can be considered a new \"feature\" of the space, however it is a composition of the original features present in the data. In the code block below, you will need to implement the following: - Import sklearn.decomposition.PCA and assign the results of fitting PCA in six dimensions with good_data to pca . - Apply a PCA transformation of log_samples using pca.transform , and assign the results to pca_samples . from sklearn.decomposition import PCA # TODO: Apply PCA by fitting the good data with the same number of dimensions as features pca = PCA() pca . fit(good_data) # TODO: Transform log_samples using the PCA fit above pca_samples = pca . transform(log_samples) # Generate PCA results plot pca_results = vs . pca_results(good_data, pca) display(pca_results) Explained Variance Fresh Milk Grocery Frozen Detergents_Paper Delicatessen Dimension 1 0.4430 0.1675 -0.4014 -0.4381 0.1782 -0.7514 -0.1499 Dimension 2 0.2638 -0.6859 -0.1672 -0.0707 -0.5005 -0.0424 -0.4941 Dimension 3 0.1231 -0.6774 0.0402 -0.0195 0.3150 -0.2117 0.6286 Dimension 4 0.1012 -0.2043 0.0128 0.0557 0.7854 0.2096 -0.5423 Dimension 5 0.0485 -0.0026 0.7192 0.3554 -0.0331 -0.5582 -0.2092 Dimension 6 0.0204 0.0292 -0.5402 0.8205 0.0205 -0.1824 0.0197 print pca_results[ 'Explained Variance' ] . cumsum() Dimension 1 0.4430 Dimension 2 0.7068 Dimension 3 0.8299 Dimension 4 0.9311 Dimension 5 0.9796 Dimension 6 1.0000 Name: Explained Variance, dtype: float64 Question 5 How much variance in the data is explained in total by the first and second principal component? What about the first four principal components? Using the visualization provided above, discuss what the first four dimensions best represent in terms of customer spending. Hint: A positive increase in a specific dimension corresponds with an increase of the positive-weighted features and a decrease of the negative-weighted features. The rate of increase or decrease is based on the indivdual feature weights. Answer: How much variance in the data is explained in total by the first and second principal component? Total variance by the first and second principal component: 0.7068 What about the first four principal components? Total variance by first four princile components: 0.9311 Using the visualization provided above, discuss what the first four dimensions best represent in terms of customer spending. Dimension 1: It shows large increases for features Milk, Grocery and Detergents_Paper, a small increase for Delicatessen, and small decreases for features Fresh and Frozen. Dimension 2: It shows large increases for Fresh, Frozen and Delicatessen, and small increase for Milk, Grocery and Detergents_Paper. Dimension 3: It shows large increases for Frozen and Delicatessen, and large decreases for Fresh and Detergents_Paper. Dimension 4: It shows large increases for Frozen and Detergents_Paper, and large a decrease for Fish and Delicatessen. Observation Run the code below to see how the log-transformed sample data has changed after having a PCA transformation applied to it in six dimensions. Observe the numerical value for the first four dimensions of the sample points. Consider if this is consistent with your initial interpretation of the sample points. # Display sample log-data after having a PCA transformation applied display(pd . DataFrame(np . round(pca_samples, 4 ), columns = pca_results . index . values)) Dimension 1 Dimension 2 Dimension 3 Dimension 4 Dimension 5 Dimension 6 0 -2.2406 -1.2419 -1.0729 -1.9589 0.2160 -0.1782 1 0.7394 -2.9834 -0.8204 1.2945 0.1297 0.4712 2 -2.1528 5.3859 0.0930 0.4023 0.3577 0.3111 Implementation: Dimensionality Reduction When using principal component analysis, one of the main goals is to reduce the dimensionality of the data \u2014 in effect, reducing the complexity of the problem. Dimensionality reduction comes at a cost: Fewer dimensions used implies less of the total variance in the data is being explained. Because of this, the cumulative explained variance ratio is extremely important for knowing how many dimensions are necessary for the problem. Additionally, if a signifiant amount of variance is explained by only two or three dimensions, the reduced data can be visualized afterwards. In the code block below, you will need to implement the following: - Assign the results of fitting PCA in two dimensions with good_data to pca . - Apply a PCA transformation of good_data using pca.transform , and assign the results to reduced_data . - Apply a PCA transformation of log_samples using pca.transform , and assign the results to pca_samples . # TODO: Apply PCA by fitting the good data with only two dimensions pca = PCA(n_components = 2 ) pca . fit(good_data) # TODO: Transform the good data using the PCA fit above reduced_data = pca . transform(good_data) # TODO: Transform the sample log-data using the PCA fit above pca_samples = pca . transform(log_samples) # Create a DataFrame for the reduced data reduced_data = pd . DataFrame(reduced_data, columns = [ 'Dimension 1' , 'Dimension 2' ]) Observation Run the code below to see how the log-transformed sample data has changed after having a PCA transformation applied to it using only two dimensions. Observe how the values for the first two dimensions remains unchanged when compared to a PCA transformation in six dimensions. # Display sample log-data after applying PCA transformation in two dimensions display(pd . DataFrame(np . round(pca_samples, 4 ), columns = [ 'Dimension 1' , 'Dimension 2' ])) # scatter plot of reduced features #pd.scatter_matrix(reduced_data, alpha = 0.3, figsize = (10,10), diagonal = 'kde'); sns . pairplot(reduced_data,size = 5 ) Dimension 1 Dimension 2 0 -2.2406 -1.2419 1 0.7394 -2.9834 2 -2.1528 5.3859 &lt;seaborn.axisgrid.PairGrid at 0x119715710&gt; Visualizing a Biplot A biplot is a scatterplot where each data point is represented by its scores along the principal components. The axes are the principal components (in this case Dimension 1 and Dimension 2 ). In addition, the biplot shows the projection of the original features along the components. A biplot can help us interpret the reduced dimensions of the data, and discover relationships between the principal components and original features. Run the code cell below to produce a biplot of the reduced-dimension data. # Create a biplot vs . biplot(good_data, reduced_data, pca) &lt;matplotlib.axes._subplots.AxesSubplot at 0x119c34b10&gt; Observation Once we have the original feature projections (in red), it is easier to interpret the relative position of each data point in the scatterplot. For instance, a point the lower right corner of the figure will likely correspond to a customer that spends a lot on 'Milk' , 'Grocery' and 'Detergents_Paper' , but not so much on the other product categories. From the biplot, which of the original features are most strongly correlated with the first component? What about those that are associated with the second component? Do these observations agree with the pca_results plot you obtained earlier? Clustering In this section, you will choose to use either a K-Means clustering algorithm or a Gaussian Mixture Model clustering algorithm to identify the various customer segments hidden in the data. You will then recover specific data points from the clusters to understand their significance by transforming them back into their original dimension and scale. Question 6 What are the advantages to using a K-Means clustering algorithm? What are the advantages to using a Gaussian Mixture Model clustering algorithm? Given your observations about the wholesale customer data so far, which of the two algorithms will you use and why? Answer: What are the advantages to using a K-Means clustering algorithm? It is simple to understand and easy to implement.In computational point of view, it is fast to run and always converge. It is suitable for searching convex clusters. What are the advantages to using a Gaussian Mixture Model clustering algorithm? It is the fastest algorithm for learning mixture models among other available models. In case of overlapping clusters, it has \"soft\" classification technique available. In GMM, there are well-studied statistical inference techniques available. Given your observations about the wholesale customer data so far, which of the two algorithms will you use and why? Gaussian Mixture Model clustering algorithm would be the most appropriate because from preious scatter graph of data sets with reduced features, there are two clusters that overlap with each other. Implementation: Creating Clusters Depending on the problem, the number of clusters that you expect to be in the data may already be known. When the number of clusters is not known a priori , there is no guarantee that a given number of clusters best segments the data, since it is unclear what structure exists in the data \u2014 if any. However, we can quantify the \"goodness\" of a clustering by calculating each data point's silhouette coefficient . The silhouette coefficient for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). Calculating the mean silhouette coefficient provides for a simple scoring method of a given clustering. In the code block below, you will need to implement the following: - Fit a clustering algorithm to the reduced_data and assign it to clusterer . - Predict the cluster for each data point in reduced_data using clusterer.predict and assign them to preds . - Find the cluster centers using the algorithm's respective attribute and assign them to centers . - Predict the cluster for each sample data point in pca_samples and assign them sample_preds . - Import sklearn.metrics.silhouette_score and calculate the silhouette score of reduced_data against preds . - Assign the silhouette score to score and print the result. from sklearn.mixture import GaussianMixture as GMM from sklearn.metrics import silhouette_score # method to find the best n_components def score_cluster (data, num_components): clusterer = GMM(n_components = num_components) clusterer . fit(data) preds = clusterer . predict(data) score = silhouette_score(data, preds) return score print ( \"Silhouette Score for different sizes\" ) silhouette_scores_matrix = pd . DataFrame(index = [ 'Score' ]) for size in range ( 2 , 11 ): silhouette_scores_matrix[size] = pd . Series(score_cluster(reduced_data, size),\\ index = silhouette_scores_matrix . index) display(silhouette_scores_matrix) best_n_components = 2 # Apply the selected clustering algorithm to the reduced data clusterer = GMM(n_components = best_n_components) clusterer . fit(reduced_data) # Predict the cluster for each data point preds = clusterer . predict(reduced_data) # Find the cluster centers centers = clusterer . means_ # Predict the cluster for each transformed sample data point sample_preds = clusterer . predict(pca_samples) # Calculate the mean silhouette coefficient for the number of clusters chosen score = silhouette_score(reduced_data, preds) print ( \"Best Score: {}, n_components={}\" . format(score, best_n_components)) Silhouette Score for different sizes 2 3 4 5 6 7 8 9 10 Score 0.421917 0.395557 0.279418 0.321519 0.25418 0.323427 0.309646 0.328185 0.319549 Best Score: 0.422324682646, n_components=2 silhouette_scores_matrix . plot(kind = \"barh\" ,figsize = ( 14 , 4 )) &lt;matplotlib.axes._subplots.AxesSubplot at 0x11ca59050&gt; Question 7 Report the silhouette score for several cluster numbers you tried. Of these, which number of clusters has the best silhouette score? Answer: It is shown in previous output of \" Implementation: Creating Clusters \". the Best Score: 0.42, for number of components=2. Cluster Visualization Once you've chosen the optimal number of clusters for your clustering algorithm using the scoring metric above, you can now visualize the results by executing the code block below. Note that, for experimentation purposes, you are welcome to adjust the number of clusters for your clustering algorithm to see various visualizations. The final visualization provided should, however, correspond with the optimal number of clusters. # Display the results of the clustering from implementation vs . cluster_results(reduced_data, preds, centers, pca_samples) Implementation: Data Recovery Each cluster present in the visualization above has a central point. These centers (or means) are not specifically data points from the data, but rather the averages of all the data points predicted in the respective clusters. For the problem of creating customer segments, a cluster's center point corresponds to the average customer of that segment . Since the data is currently reduced in dimension and scaled by a logarithm, we can recover the representative customer spending from these data points by applying the inverse transformations. In the code block below, you will need to implement the following: - Apply the inverse transform to centers using pca.inverse_transform and assign the new centers to log_centers . - Apply the inverse function of np.log to log_centers using np.exp and assign the true centers to true_centers . # TODO: Inverse transform the centers log_centers = pca . inverse_transform(centers) # TODO: Exponentiate the centers true_centers = np . exp(log_centers) # Display the true centers segments = [ 'Segment {}' . format(i) for i in range ( 0 , len (centers))] true_centers = pd . DataFrame(np . round(true_centers), columns = data . keys()) true_centers . index = segments display(true_centers) #show segments in percentile newdata = data . append(true_centers) print ( \"Percentiles of the centers\" ) percent_centers = 100.0 * newdata . rank(axis = 0 , pct = True )\\ . loc[[ 'Segment 0' , 'Segment 1' ]]\\ . round(decimals = 3 ) display(percent_centers) Fresh Milk Grocery Frozen Detergents_Paper Delicatessen Segment 0 8939.0 2108.0 2758.0 2073.0 352.0 730.0 Segment 1 3567.0 7860.0 12249.0 873.0 4713.0 966.0 Percentiles of the centers Fresh Milk Grocery Frozen Detergents_Paper Delicatessen Segment 0 52.5 34.4 34.4 58.4 32.0 41.2 Segment 1 28.1 79.0 80.5 31.0 80.3 50.2 Plot of true center true_centers . plot(kind = \"barh\" ,figsize = ( 10 , 6 )) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1196ac210&gt; Plot of Percentile centers percent_centers . plot(kind = \"barh\" ,figsize = ( 10 , 6 )) &lt;matplotlib.axes._subplots.AxesSubplot at 0x11b6d8950&gt; Question 8 Consider the total purchase cost of each product category for the representative data points above, and reference the statistical description of the dataset at the beginning of this project. What set of establishments could each of the customer segments represent? Hint: A customer who is assigned to 'Cluster X' should best identify with the establishments represented by the feature set of 'Segment X' . Answer: Segment 0 customers buy most Fresh, Frozen (> %50) and much Delicatessen (40%), it's mostly likely to be a convinence store or greegrocery . Segment 1 customers buy lots of Grocery, Milk and Detergents_Paper (> 70%), following by Delicatessen (~%50), it indicates it's a supermarket . Question 9 For each sample point, which customer segment from Question 8 best represents it? Are the predictions for each sample point consistent with this? Run the code block below to find which cluster each sample point is predicted to be. # Display the predictions for i, pred in enumerate (sample_preds): print ( \"Sample point\" , i, \"predicted to be in Cluster\" , pred) Sample point 0 predicted to be in Cluster 0 Sample point 1 predicted to be in Cluster 1 Sample point 2 predicted to be in Cluster 0 samples . plot . barh(stacked = True ,figsize = ( 10 , 2 ) ); Answer: For each sample point, which customer segment from Question 8 best represents it? It is shown in output of Question 9 . Are the predictions for each sample point consistent with this? Yes, more or less. The prediction agrees with previous guess that samples are Supermarket, Greengrocer, except for sample 3 as Convenience store. Sample 1 customers buy lots of produces in all category, which is close to Segment 0. Sample 2 customers buy mostly Fresh and frozen, which makes it close to Segment 1. Sample 3 customers buy mostly Grocery and Detergents_Paper, which makes it close to Segment 0. Samples Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 0 1.560499 0.884800 0.400925 -0.574313 0.209873 1 5.084854 -0.315148 -0.089470 2.776994 -0.441685 2 -0.942242 -0.297242 0.468664 -0.613289 0.018584 Conclusion In this final section, you will investigate ways that you can make use of the clustered data. First, you will consider how the different groups of customers, the customer segments , may be affected differently by a specific delivery scheme. Next, you will consider how giving a label to each customer (which segment that customer belongs to) can provide for additional features about the customer data. Finally, you will compare the customer segments to a hidden variable present in the data, to see whether the clustering identified certain relationships. Question 10 Companies will often run A/B tests when making small changes to their products or services to determine whether making that change will affect its customers positively or negatively. The wholesale distributor is considering changing its delivery service from currently 5 days a week to 3 days a week. However, the distributor will only make this change in delivery service for customers that react positively. How can the wholesale distributor use the customer segments to determine which customers, if any, would react positively to the change in delivery service? Hint: Can we assume the change affects all customers equally? How can we determine which group of customers it affects the most? Answer: The items that customer purchase like: Fresh, Milk and Frozen are time sensitive and customer may prefer faster and frequent dilivery. Clustering designed with focusing on these three features may give distributor more intution. One can perform the A/B test by selecting a random sample from each cluster and reducing the delivery frequency for each of them. Then a suvey of customer satisfaction could be made for further analysis. An equally sized group of customers with same shopping conditions except the delivery time selected from the remaining customers could be used for comparison. The A/B test data is prepared, interpreated and cross-validated by changing the delivery service to certain amount of test customers. The process can be iterated until certain model is verified or market goals are met. Question 11 Additional structure is derived from originally unlabeled data when using clustering techniques. Since each customer has a customer segment it best identifies with (depending on the clustering algorithm applied), we can consider 'customer segment' as an engineered feature for the data. Assume the wholesale distributor recently acquired ten new customers and each provided estimates for anticipated annual spending of each product category. Knowing these estimates, the wholesale distributor wants to classify each new customer to a customer segment to determine the most appropriate delivery service. How can the wholesale distributor label the new customers using only their estimated product spending and the customer segment data? *Hint: A supervised learner could be used to train on the original customers. What would be the target variable? Answer: The customer segment (cluster prediction) can be used as target and one can design the supervised learning model based on them. The model developed could be used to predict the new customer data. Visualizing Underlying Distributions At the beginning of this project, it was discussed that the 'Channel' and 'Region' features would be excluded from the dataset so that the customer product categories were emphasized in the analysis. By reintroducing the 'Channel' feature to the dataset, an interesting structure emerges when considering the same PCA dimensionality reduction applied earlier to the original dataset. Run the code block below to see how each data point is labeled either 'HoReCa' (Hotel/Restaurant/Cafe) or 'Retail' the reduced space. In addition, you will find the sample points are circled in the plot, which will identify their labeling. # Display the clustering results based on 'Channel' data vs . channel_results(reduced_data, outliers, pca_samples) Question 12 How well does the clustering algorithm and number of clusters you've chosen compare to this underlying distribution of Hotel/Restaurant/Cafe customers to Retailer customers? Are there customer segments that would be classified as purely 'Retailers' or 'Hotels/Restaurants/Cafes' by this distribution? Would you consider these classifications as consistent with your previous definition of the customer segments? Answer: How well does the clustering algorithm and number of clusters you've chosen compare to this underlying distribution of Hotel/Restaurant/Cafe customers to Retailer customers? The algorithm and the number of clusters chosen matches with the underlying distribution. Are there customer segments that would be classified as purely 'Retailers' or 'Hotels/Restaurants/Cafes' by this distribution? No, there will be overlap areas for two clusters. Although there is some overlap between the two groups in the middle of the distribution, segment 0 in the analysis is clearly \"Retail\", and segment 1 is clearly \"Hotel/Restaurants/Cafes\". Would you consider these classifications as consistent with your previous definition of the customer segments? It shows the previous definition of customer segments is consistent with the classification using the Channel feature. I believe the algorithm did a reasonable job of clustering these customers according to broad categories of business type. Note : Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to File -> Download as -> HTML (.html) . Include the finished document along with this notebook as your submission.","title":"Customer Clustering"},{"location":"ml/Customer_segments/customer_segments/#machine-learning-engineer-nanodegree","text":"","title":"Machine Learning Engineer Nanodegree"},{"location":"ml/Customer_segments/customer_segments/#unsupervised-learning","text":"","title":"Unsupervised Learning"},{"location":"ml/Customer_segments/customer_segments/#project-creating-customer-segments","text":"Welcome to the third project of the Machine Learning Engineer Nanodegree! In this notebook, some template code has already been provided for you, and it will be your job to implement the additional functionality necessary to successfully complete this project. Sections that begin with 'Implementation' in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a 'Question X' header. Carefully read each question and provide thorough answers in the following text boxes that begin with 'Answer:' . Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide. Note: Code and Markdown cells can be executed using the Shift + Enter keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode.","title":"Project: Creating Customer Segments"},{"location":"ml/Customer_segments/customer_segments/#getting-started","text":"In this project, you will analyze a dataset containing data on various customers' annual spending amounts (reported in monetary units ) of diverse product categories for internal structure. One goal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with. Doing so would equip the distributor with insight into how to best structure their delivery service to meet the needs of each customer. The dataset for this project can be found on the UCI Machine Learning Repository . For the purposes of this project, the features 'Channel' and 'Region' will be excluded in the analysis \u2014 with focus instead on the six product categories recorded for customers. Run the code block below to load the wholesale customers dataset, along with a few of the necessary Python libraries required for this project. You will know the dataset loaded successfully if the size of the dataset is reported. # Import libraries necessary for this project import numpy as np import pandas as pd from IPython.display import display # Allows the use of display() for DataFrames import matplotlib.pyplot as plt # Import supplementary visualizations code visuals.py import visuals as vs import seaborn as sns # Pretty display for notebooks % matplotlib inline # Load the wholesale customers dataset try : data = pd . read_csv( \"customers.csv\" ) data . drop([ 'Region' , 'Channel' ], axis = 1 , inplace = True ) print ( \"Wholesale customers dataset has {} samples with {} features each.\" . format( * data . shape)) except : print ( \"Dataset could not be loaded. Is the dataset missing?\" ) Wholesale customers dataset has 440 samples with 6 features each.","title":"Getting Started"},{"location":"ml/Customer_segments/customer_segments/#data-exploration","text":"In this section, you will begin exploring the data through visualizations and code to understand how each feature is related to the others. You will observe a statistical description of the dataset, consider the relevance of each feature, and select a few sample data points from the dataset which you will track through the course of this project. Run the code block below to observe a statistical description of the dataset. Note that the dataset is composed of six important product categories: 'Fresh' , 'Milk' , 'Grocery' , 'Frozen' , 'Detergents_Paper' , and 'Delicatessen' . Consider what each category represents in terms of products you could purchase. Lets see what are the column names and their types data . info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 440 entries, 0 to 439 Data columns (total 6 columns): Fresh 440 non-null int64 Milk 440 non-null int64 Grocery 440 non-null int64 Frozen 440 non-null int64 Detergents_Paper 440 non-null int64 Delicatessen 440 non-null int64 dtypes: int64(6) memory usage: 20.7 KB Head part of the data data . head( 5 ) Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 0 12669 9656 7561 214 2674 1338 1 7057 9810 9568 1762 3293 1776 2 6353 8808 7684 2405 3516 7844 3 13265 1196 4221 6404 507 1788 4 22615 5410 7198 3915 1777 5185","title":"Data Exploration"},{"location":"ml/Customer_segments/customer_segments/#lets-look-how-each-feature-propagates-from-customer-to-customers","text":"from matplotlib import cm test_data = data . loc[ 0 : 200 ] test_data . plot(colormap = cm . cubehelix,figsize = ( 16 , 12 )) &lt;matplotlib.axes._subplots.AxesSubplot at 0x10de94950&gt; Fresh and grocery are really high demanding items! But this is not the global nature of the data. Detail will be discussed later.","title":"Lets look how each feature propagates from customer to customers."},{"location":"ml/Customer_segments/customer_segments/#lets-look-their-propertion-on-small-section-of-the-data","text":"test_data = data . loc[ 100 : 150 ] test_data . plot . barh(stacked = True ,figsize = ( 16 , 15 ) ); Customer at index 125 is really heavy consumer of fresh! Little bit of statistics of each columns: # Display a description of the dataset display(data . describe()) Fresh Milk Grocery Frozen Detergents_Paper Delicatessen count 440.000000 440.000000 440.000000 440.000000 440.000000 440.000000 mean 12000.297727 5796.265909 7951.277273 3071.931818 2881.493182 1524.870455 std 12647.328865 7380.377175 9503.162829 4854.673333 4767.854448 2820.105937 min 3.000000 55.000000 3.000000 25.000000 3.000000 3.000000 25% 3127.750000 1533.000000 2153.000000 742.250000 256.750000 408.250000 50% 8504.000000 3627.000000 4755.500000 1526.000000 816.500000 965.500000 75% 16933.750000 7190.250000 10655.750000 3554.250000 3922.000000 1820.250000 max 112151.000000 73498.000000 92780.000000 60869.000000 40827.000000 47943.000000 In general the mean for each feature seems much higher than the median(50%-Quartile), so this data seems somewhat skewed . We can visualize them as well in the histogram comming in next blocks. Histogram for individual data plt . figure(figsize = ( 14 , 14 )) bins = [ 100 , 100 , 100 , 100 , 100 , 100 ] colors = sns . color_palette( \"bright\" , 6 ) items = [ 'Fresh' , 'Milk' , 'Grocery' , 'Frozen' , 'Detergents_Paper' , 'Delicatessen' ] ks = [ 1 , 2 , 3 , 4 , 5 , 6 ] # plot individual histograms for color,item,k in zip (colors,items,ks): plt . subplot( 3 , 2 ,k) plt . title( \"plot of \" + item) sns . distplot(data[item],bins = 50 , kde = True , color = color) Most of these features have peak near zero to 1000. Grocery is pretty much flat among them. We can cut a section from each of these plots and compare between them as shown below: Putting all together # Plot all together plt . figure(figsize = ( 14 , 6 )) plt . title( \"distribution of all items\" ) for ibin,color,item in zip (bins,colors,items): sns . distplot(data[item],bins = ibin, kde = True , color = color) plt . legend([item for item in items]) &lt;matplotlib.legend.Legend at 0x1051439d0&gt; Peak of the distribution shifts to the right while moving from feature Lets make it much lear as shown below: Lets visualize most dynamic part of the data # view most dynamic part of the data plt . figure(figsize = ( 14 , 10 )) plt . title( \"Viewing most dynamic part of the data\" ) for ibin,color,item in zip (bins,colors,items): sns . distplot(data[data[item] <= 5000 ][item],bins = ibin, kde = True , color = color) plt . legend([item for item in items]) &lt;matplotlib.legend.Legend at 0x111c56050&gt; Wow! Look at this trend : Detergent -> Delicatessen-> Fresh-I -> Frozen -> Milk -> Grocery -> Fresh-II . Fresh has two peak representing two types of purcheser : household(low price) and might be resturents(high price) etc. Data skewed to the right as pridected before.","title":"Lets look their propertion on small section of the data"},{"location":"ml/Customer_segments/customer_segments/#implementation-selecting-samples","text":"To get a better understanding of the customers and how their data will transform through the analysis, it would be best to select a few sample data points and explore them in more detail. In the code block below, add three indices of your choice to the indices list which will represent the customers to track. It is suggested to try different sets of samples until you obtain customers that vary significantly from one another. # TODO: Select three indices of your choice you wish to sample from the dataset indices = [ 12 , 125 , 412 ] # Create a DataFrame of the chosen samples samples = pd . DataFrame(data . loc[indices], columns = data . keys()) . reset_index(drop = True ) print ( \"Chosen samples of wholesale customers dataset:\" ) display(samples) samples . plot . barh(stacked = True ,figsize = ( 10 , 2 ) ); # show devations of each feature from its mean measured by std mean_data = np . mean(data) std_data = np . std(data) deviation_samples = (samples - mean_data) / std_data print ( \" \\n Deviation of chosen samples of wholesale customers dataset in mean+deviation*std:\" ) display(deviation_samples) Chosen samples of wholesale customers dataset: Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 0 31714 12319 11757 287 3881 2931 1 76237 3473 7102 16538 778 918 2 97 3605 12400 98 2970 62 Deviation of chosen samples of wholesale customers dataset in mean+deviation*std: Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 0 1.560499 0.884800 0.400925 -0.574313 0.209873 0.499176 1 5.084854 -0.315148 -0.089470 2.776994 -0.441685 -0.215439 2 -0.942242 -0.297242 0.468664 -0.613289 0.018584 -0.519319","title":"Implementation: Selecting Samples"},{"location":"ml/Customer_segments/customer_segments/#quartile-visualization-of-sample","text":"plt . figure(figsize = ( 12 , 5 )) percentiles = data . rank(pct = True ) percentiles = 100 * percentiles . round(decimals = 3 ) percentiles = percentiles . iloc[indices] display(percentiles) print \"Quartile Visualization\" sns . heatmap(percentiles, vmin = 1 , vmax = 99 , annot = True ) Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 12 93.6 90.2 79.1 10.7 74.5 89.8 125 99.8 47.7 63.4 98.2 48.6 48.6 412 1.8 49.3 80.9 3.2 69.3 4.8 Quartile Visualization &lt;matplotlib.axes._subplots.AxesSubplot at 0x113465150&gt;","title":"Quartile Visualization of sample"},{"location":"ml/Customer_segments/customer_segments/#question-1","text":"Consider the total purchase cost of each product category and the statistical description of the dataset above for your sample customers. What kind of establishment (customer) could each of the three samples you've chosen represent? Hint: Examples of establishments include places like markets, cafes, and retailers, among many others. Avoid using names for establishments, such as saying \"McDonalds\" when describing a sample customer as a restaurant. Answer: There are three information hiden in samples which are (1) Total values: It helps to know typr of wholesale distributer (2) Mean value of each category and deviations: It helps to know types of distributer Lets go sample by sample: Sample-1: This customer a heavy consumer of Fresh items, milk and Delicatessen. Spending on Grocery is also above 75th quartile. Frozen is pretty much less and below 25th quartile and mean. It should be Supermarket , because all kinds of produces are sold much more than average value except the frozen. Sample-2: This customer is crazy at Fresh. It touched 100th quartile.Second most purchased items are frozens. Grocery is little below the mean value but pretty above the 50th quartile value. Rest of them are below 5oth quartile.This should be Greengrocery , be cause the main produce being sold is fresh produce and frozen while the total amount of produces is below average Sample-3: For this customer, first priority is Grocery which is above 75th quartile range and pretty above mean. Next priority os Detergent paper which is above 50th quartile range and also above the mean. Milk is below mean and median. Rest of the other are blow mean and 25th quartile range. It is Convenience store , because the main produce being sold is grocery, while the total amount of produces is below average","title":"Question 1"},{"location":"ml/Customer_segments/customer_segments/#implementation-feature-relevance","text":"One interesting thought to consider is if one (or more) of the six product categories is actually relevant for understanding customer purchasing. That is to say, is it possible to determine whether customers purchasing some amount of one category of products will necessarily purchase some proportional amount of another category of products? We can make this determination quite easily by training a supervised regression learner on a subset of the data with one feature removed, and then score how well that model can predict the removed feature. In the code block below, you will need to implement the following: - Assign new_data a copy of the data by removing a feature of your choice using the DataFrame.drop function. - Use sklearn.cross_validation.train_test_split to split the dataset into training and testing sets. - Use the removed feature as your target label. Set a test_size of 0.25 and set a random_state . - Import a decision tree regressor, set a random_state , and fit the learner to the training data. - Report the prediction score of the testing set using the regressor's score function. from sklearn.cross_validation import train_test_split from sklearn.tree import DecisionTreeRegressor # TODO: Make a copy of the DataFrame, using the 'drop' function to drop the given feature new_data = data . copy() targets = [ 'Fresh' , 'Milk' , 'Grocery' , 'Frozen' , 'Detergents_Paper' , 'Delicatessen' ] # train and evaluate for each feature scores = {} for target_feature in targets: label = new_data[target_feature] left_features = new_data . drop([target_feature], axis = 1 ) # TODO: Split the data into training and testing sets using the given feature as the target X_train, X_test, y_train, y_test = train_test_split(left_features, label, test_size = 0.25 , random_state = 1 ) # TODO: Create a decision tree regressor and fit it to the training set regressor = DecisionTreeRegressor(random_state = 1 ) regressor . fit(X_train, y_train) # TODO: Report the score of the prediction using the testing set score = regressor . score(X_test, y_test) scores[target_feature] = score # display features and scores result = pd . DataFrame(scores, index = [ 'Score' ]) display(result) Delicatessen Detergents_Paper Fresh Frozen Grocery Milk Score -0.429125 0.815241 -0.923374 -0.649574 0.795768 0.51585 plt . figure(figsize = ( 12 , 12 )) result . plot(kind = \"bar\" ,figsize = ( 8 , 8 )) plt . axhline( 0 , color = 'k' ) &lt;matplotlib.lines.Line2D at 0x113bc6350&gt; &lt;matplotlib.figure.Figure at 0x113bc6550&gt;","title":"Implementation: Feature Relevance"},{"location":"ml/Customer_segments/customer_segments/#question-2","text":"Which feature did you attempt to predict? What was the reported prediction score? Is this feature is necessary for identifying customers' spending habits? Hint: The coefficient of determination, R^2 , is scored between 0 and 1, with 1 being a perfect fit. A negative R^2 implies the model fails to fit the data. Answer: I am going to predict Detergents_Paper or Grocery . Reported prediction score for them are 0.815241 and 0.795768 respectively. No! this feature is less important for identifying customer's spending habits. The reason is following: A predication with higher score (close to 1.0) implies that the training features are likely to predicate the label feature. Alternalely, label feature is more dependent on the other features, which makes it unnecessary for identifying customer's spending habits. Predication with lower score (negative value) indicates the labeling feature is independent to others and necessary for learning algorithm.","title":"Question 2"},{"location":"ml/Customer_segments/customer_segments/#visualize-feature-distributions","text":"To get a better understanding of the dataset, we can construct a scatter matrix of each of the six product features present in the data. If you found that the feature you attempted to predict above is relevant for identifying a specific customer, then the scatter matrix below may not show any correlation between that feature and the others. Conversely, if you believe that feature is not relevant for identifying a specific customer, the scatter matrix might show a correlation between that feature and another feature in the data. Run the code block below to produce a scatter matrix. plt . figure(figsize = ( 16 , 21 )) sns . pairplot(data) &lt;seaborn.axisgrid.PairGrid at 0x113bc6c90&gt; &lt;matplotlib.figure.Figure at 0x113bc6b90&gt;","title":"Visualize Feature Distributions"},{"location":"ml/Customer_segments/customer_segments/#study-of-variance-covariance-of-the-data","text":"data . cov() Fresh Milk Grocery Frozen Detergents_Paper Delicatessen Fresh 1.599549e+08 9.381789e+06 -1.424713e+06 2.123665e+07 -6.147826e+06 8.727310e+06 Milk 9.381789e+06 5.446997e+07 5.108319e+07 4.442612e+06 2.328834e+07 8.457925e+06 Grocery -1.424713e+06 5.108319e+07 9.031010e+07 -1.854282e+06 4.189519e+07 5.507291e+06 Frozen 2.123665e+07 4.442612e+06 -1.854282e+06 2.356785e+07 -3.044325e+06 5.352342e+06 Detergents_Paper -6.147826e+06 2.328834e+07 4.189519e+07 -3.044325e+06 2.273244e+07 9.316807e+05 Delicatessen 8.727310e+06 8.457925e+06 5.507291e+06 5.352342e+06 9.316807e+05 7.952997e+06 plt . figure(figsize = ( 10 , 10 )) sns . heatmap(data . cov(),annot = True ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x115490890&gt; This plot of covariance gives us little bit more idea about possible correlation between variables. This is symmetric plot. Diagonal values represents the variance where as non diagonal values gives some information about correlation. For example we can look Detergents_Paper and Grocery . Lets look them in separate plot below:","title":"Study of variance covariance of the data"},{"location":"ml/Customer_segments/customer_segments/#plot-of-detergents_paper-with-grocery","text":"plt . figure(figsize = ( 8 , 8 )) sns . jointplot(x = \"Detergents_Paper\" , y = \"Grocery\" , data = data,size = 15 ,kind = 'reg' ); &lt;matplotlib.figure.Figure at 0x115ec98d0&gt;","title":"plot of Detergents_Paper with Grocery"},{"location":"ml/Customer_segments/customer_segments/#question-3","text":"Are there any pairs of features which exhibit some degree of correlation? Does this confirm or deny your suspicions about the relevance of the feature you attempted to predict? How is the data for those features distributed? Hint: Is the data normally distributed? Where do most of the data points lie? Answer: Are there any pairs of features which exhibit some degree of correlation? Yes! Detergents_Paper and Grocery are highly correlated. There may be little correlation between Detergents_Paper and Milk , Grocery and Milk as well. Does this confirm or deny your suspicions about the relevance of the feature you attempted to predict? Yes! It confirms my suspicions about the relevance of the feature in Question 2. How is the data for those features distributed? The data distribution plot looks more like a F distribution.","title":"Question 3"},{"location":"ml/Customer_segments/customer_segments/#data-preprocessing","text":"In this section, you will preprocess the data to create a better representation of customers by performing a scaling on the data and detecting (and optionally removing) outliers. Preprocessing data is often times a critical step in assuring that results you obtain from your analysis are significant and meaningful.","title":"Data Preprocessing"},{"location":"ml/Customer_segments/customer_segments/#implementation-feature-scaling","text":"If data is not normally distributed, especially if the mean and median vary significantly (indicating a large skew), it is most often appropriate to apply a non-linear scaling \u2014 particularly for financial data. One way to achieve this scaling is by using a Box-Cox test , which calculates the best power transformation of the data that reduces skewness. A simpler approach which can work in most cases would be applying the natural logarithm. In the code block below, you will need to implement the following: - Assign a copy of the data to log_data after applying logarithmic scaling. Use the np.log function for this. - Assign a copy of the sample data to log_samples after applying logarithmic scaling. Again, use np.log . # TODO: Scale the data using the natural logarithm log_data = np . log(data) # TODO: Scale the sample data using the natural logarithm log_samples = np . log(samples) # Produce a scatter matrix for each pair of newly-transformed features sns . pairplot(log_data) &lt;seaborn.axisgrid.PairGrid at 0x110d1cbd0&gt;","title":"Implementation: Feature Scaling"},{"location":"ml/Customer_segments/customer_segments/#observation","text":"After applying a natural logarithm scaling to the data, the distribution of each feature should appear much more normal. For any pairs of features you may have identified earlier as being correlated, observe here whether that correlation is still present (and whether it is now stronger or weaker than before). Run the code below to see how the sample data has changed after having the natural logarithm applied to it. # Display the log-transformed sample data display(log_samples) Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 0 10.364514 9.418898 9.372204 5.659482 8.263848 7.983099 1 11.241602 8.152774 8.868132 9.713416 6.656727 6.822197 2 4.574711 8.190077 9.425452 4.584967 7.996317 4.127134","title":"Observation"},{"location":"ml/Customer_segments/customer_segments/#implementation-outlier-detection","text":"Detecting outliers in the data is extremely important in the data preprocessing step of any analysis. The presence of outliers can often skew results which take into consideration these data points. There are many \"rules of thumb\" for what constitutes an outlier in a dataset. Here, we will use Tukey's Method for identfying outliers : An outlier step is calculated as 1.5 times the interquartile range (IQR). A data point with a feature that is beyond an outlier step outside of the IQR for that feature is considered abnormal. In the code block below, you will need to implement the following: - Assign the value of the 25th percentile for the given feature to Q1 . Use np.percentile for this. - Assign the value of the 75th percentile for the given feature to Q3 . Again, use np.percentile . - Assign the calculation of an outlier step for the given feature to step . - Optionally remove data points from the dataset by adding indices to the outliers list. NOTE: If you choose to remove any outliers, ensure that the sample data does not contain any of these points! Once you have performed this implementation, the dataset will be stored in the variable good_data . log_data[ 'sn' ] = log_data . index bad_indexes = {} # For each feature find the data points with extreme high or low values for feature in log_data . keys(): feature_data = log_data[feature] # TODO: Calculate Q1 (25th percentile of the data) for the given feature Q1 = np . percentile(feature_data, 25 ) # TODO: Calculate Q3 (75th percentile of the data) for the given feature Q3 = np . percentile(feature_data, 75 ) # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range) step = 1.5 * (Q3 - Q1) feature_outliers = log_data[ ~ ((feature_data >= Q1 - step) & (feature_data <= Q3 + step))] # Display the outliers print ( \"Data points considered outliers for the feature '{}':\" . format(feature)) display(feature_outliers) for i, r in feature_data . iteritems(): if not (Q1 - step <= r <= Q3 + step): if i not in bad_indexes: bad_indexes[i] = 1 else : bad_indexes[i] += 1 # OPTIONAL: Select the indices for data points you wish to remove outliers = [i for i, n in bad_indexes . items() if n > 1 ] # Remove the outliers, if any were specified good_data = log_data . drop(log_data . index[outliers]) . reset_index(drop = True ) outliars_data = log_data . iloc[log_data . index[outliers],:] Data points considered outliers for the feature 'Fresh': Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn 65 4.442651 9.950323 10.732651 3.583519 10.095388 7.260523 65 66 2.197225 7.335634 8.911530 5.164786 8.151333 3.295837 66 81 5.389072 9.163249 9.575192 5.645447 8.964184 5.049856 81 95 1.098612 7.979339 8.740657 6.086775 5.407172 6.563856 95 96 3.135494 7.869402 9.001839 4.976734 8.262043 5.379897 96 128 4.941642 9.087834 8.248791 4.955827 6.967909 1.098612 128 171 5.298317 10.160530 9.894245 6.478510 9.079434 8.740337 171 193 5.192957 8.156223 9.917982 6.865891 8.633731 6.501290 193 218 2.890372 8.923191 9.629380 7.158514 8.475746 8.759669 218 304 5.081404 8.917311 10.117510 6.424869 9.374413 7.787382 304 305 5.493061 9.468001 9.088399 6.683361 8.271037 5.351858 305 338 1.098612 5.808142 8.856661 9.655090 2.708050 6.309918 338 353 4.762174 8.742574 9.961898 5.429346 9.069007 7.013016 353 355 5.247024 6.588926 7.606885 5.501258 5.214936 4.844187 355 357 3.610918 7.150701 10.011086 4.919981 8.816853 4.700480 357 412 4.574711 8.190077 9.425452 4.584967 7.996317 4.127134 412 Data points considered outliers for the feature 'Milk': Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn 86 10.039983 11.205013 10.377047 6.894670 9.906981 6.805723 86 98 6.220590 4.718499 6.656727 6.796824 4.025352 4.882802 98 154 6.432940 4.007333 4.919981 4.317488 1.945910 2.079442 154 356 10.029503 4.897840 5.384495 8.057377 2.197225 6.306275 356 Data points considered outliers for the feature 'Grocery': Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn 75 9.923192 7.036148 1.098612 8.390949 1.098612 6.882437 75 154 6.432940 4.007333 4.919981 4.317488 1.945910 2.079442 154 Data points considered outliers for the feature 'Frozen': Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn 38 8.431853 9.663261 9.723703 3.496508 8.847360 6.070738 38 57 8.597297 9.203618 9.257892 3.637586 8.932213 7.156177 57 65 4.442651 9.950323 10.732651 3.583519 10.095388 7.260523 65 145 10.000569 9.034080 10.457143 3.737670 9.440738 8.396155 145 175 7.759187 8.967632 9.382106 3.951244 8.341887 7.436617 175 264 6.978214 9.177714 9.645041 4.110874 8.696176 7.142827 264 325 10.395650 9.728181 9.519735 11.016479 7.148346 8.632128 325 420 8.402007 8.569026 9.490015 3.218876 8.827321 7.239215 420 429 9.060331 7.467371 8.183118 3.850148 4.430817 7.824446 429 439 7.932721 7.437206 7.828038 4.174387 6.167516 3.951244 439 Data points considered outliers for the feature 'Detergents_Paper': Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn 75 9.923192 7.036148 1.098612 8.390949 1.098612 6.882437 75 161 9.428190 6.291569 5.645447 6.995766 1.098612 7.711101 161 Data points considered outliers for the feature 'Delicatessen': Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn 66 2.197225 7.335634 8.911530 5.164786 8.151333 3.295837 66 109 7.248504 9.724899 10.274568 6.511745 6.728629 1.098612 109 128 4.941642 9.087834 8.248791 4.955827 6.967909 1.098612 128 137 8.034955 8.997147 9.021840 6.493754 6.580639 3.583519 137 142 10.519646 8.875147 9.018332 8.004700 2.995732 1.098612 142 154 6.432940 4.007333 4.919981 4.317488 1.945910 2.079442 154 183 10.514529 10.690808 9.911952 10.505999 5.476464 10.777768 183 184 5.789960 6.822197 8.457443 4.304065 5.811141 2.397895 184 187 7.798933 8.987447 9.192075 8.743372 8.148735 1.098612 187 203 6.368187 6.529419 7.703459 6.150603 6.860664 2.890372 203 233 6.871091 8.513988 8.106515 6.842683 6.013715 1.945910 233 285 10.602965 6.461468 8.188689 6.948897 6.077642 2.890372 285 289 10.663966 5.655992 6.154858 7.235619 3.465736 3.091042 289 343 7.431892 8.848509 10.177932 7.283448 9.646593 3.610918 343 Data points considered outliers for the feature 'sn': Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn Outliars data Outliars indices [i for i, n in bad_indexes . items() if n > 1 ] [128, 154, 65, 66, 75] outliars_data Fresh Milk Grocery Frozen Detergents_Paper Delicatessen sn 128 4.941642 9.087834 8.248791 4.955827 6.967909 1.098612 128 154 6.432940 4.007333 4.919981 4.317488 1.945910 2.079442 154 65 4.442651 9.950323 10.732651 3.583519 10.095388 7.260523 65 66 2.197225 7.335634 8.911530 5.164786 8.151333 3.295837 66 75 9.923192 7.036148 1.098612 8.390949 1.098612 6.882437 75","title":"Implementation: Outlier Detection"},{"location":"ml/Customer_segments/customer_segments/#visualization-of-outliers","text":"def outlier_plotter (Feature): ax = good_data . plot(kind = 'scatter' ,\\ x = 'sn' ,\\ y = Feature,\\ color = 'LightGreen' ,\\ label = 'good_data' ,\\ s = 30 ) outliars_data . plot(kind = 'scatter' , \\ x = 'sn' ,\\ y = Feature,\\ color = 'Darkred' ,\\ label = 'outliars_data' ,\\ s = 60 ,ax = ax) outlier_plotter(Feature = 'Milk' ) outlier_plotter(Feature = 'Grocery' ) outlier_plotter(Feature = 'Fresh' ) outlier_plotter(Feature = 'Frozen' ) good_data = good_data . drop( 'sn' ,axis = 1 )","title":"Visualization of outliers"},{"location":"ml/Customer_segments/customer_segments/#question-4","text":"Are there any data points considered outliers for more than one feature based on the definition above? Should these data points be removed from the dataset? If any data points were added to the outliers list to be removed, explain why. Answer: Yes, There are data points considered outliers for more than one feature based on the definition above. Yes, they should be removed and the reason is that the data points at row [128, 154, 65, 66, 75] has more than one outlier features because they are more likely to be true outliers than others. Removing all of them could cause underfitting. So, for ones with one feature outlier, I decided to keep them as single outlier feature. It may be the desirable pattern in the datasets.","title":"Question 4"},{"location":"ml/Customer_segments/customer_segments/#feature-transformation","text":"In this section you will use principal component analysis (PCA) to draw conclusions about the underlying structure of the wholesale customer data. Since using PCA on a dataset calculates the dimensions which best maximize variance, we will find which compound combinations of features best describe customers.","title":"Feature Transformation"},{"location":"ml/Customer_segments/customer_segments/#implementation-pca","text":"Now that the data has been scaled to a more normal distribution and has had any necessary outliers removed, we can now apply PCA to the good_data to discover which dimensions about the data best maximize the variance of features involved. In addition to finding these dimensions, PCA will also report the explained variance ratio of each dimension \u2014 how much variance within the data is explained by that dimension alone. Note that a component (dimension) from PCA can be considered a new \"feature\" of the space, however it is a composition of the original features present in the data. In the code block below, you will need to implement the following: - Import sklearn.decomposition.PCA and assign the results of fitting PCA in six dimensions with good_data to pca . - Apply a PCA transformation of log_samples using pca.transform , and assign the results to pca_samples . from sklearn.decomposition import PCA # TODO: Apply PCA by fitting the good data with the same number of dimensions as features pca = PCA() pca . fit(good_data) # TODO: Transform log_samples using the PCA fit above pca_samples = pca . transform(log_samples) # Generate PCA results plot pca_results = vs . pca_results(good_data, pca) display(pca_results) Explained Variance Fresh Milk Grocery Frozen Detergents_Paper Delicatessen Dimension 1 0.4430 0.1675 -0.4014 -0.4381 0.1782 -0.7514 -0.1499 Dimension 2 0.2638 -0.6859 -0.1672 -0.0707 -0.5005 -0.0424 -0.4941 Dimension 3 0.1231 -0.6774 0.0402 -0.0195 0.3150 -0.2117 0.6286 Dimension 4 0.1012 -0.2043 0.0128 0.0557 0.7854 0.2096 -0.5423 Dimension 5 0.0485 -0.0026 0.7192 0.3554 -0.0331 -0.5582 -0.2092 Dimension 6 0.0204 0.0292 -0.5402 0.8205 0.0205 -0.1824 0.0197 print pca_results[ 'Explained Variance' ] . cumsum() Dimension 1 0.4430 Dimension 2 0.7068 Dimension 3 0.8299 Dimension 4 0.9311 Dimension 5 0.9796 Dimension 6 1.0000 Name: Explained Variance, dtype: float64","title":"Implementation: PCA"},{"location":"ml/Customer_segments/customer_segments/#question-5","text":"How much variance in the data is explained in total by the first and second principal component? What about the first four principal components? Using the visualization provided above, discuss what the first four dimensions best represent in terms of customer spending. Hint: A positive increase in a specific dimension corresponds with an increase of the positive-weighted features and a decrease of the negative-weighted features. The rate of increase or decrease is based on the indivdual feature weights. Answer: How much variance in the data is explained in total by the first and second principal component? Total variance by the first and second principal component: 0.7068 What about the first four principal components? Total variance by first four princile components: 0.9311 Using the visualization provided above, discuss what the first four dimensions best represent in terms of customer spending. Dimension 1: It shows large increases for features Milk, Grocery and Detergents_Paper, a small increase for Delicatessen, and small decreases for features Fresh and Frozen. Dimension 2: It shows large increases for Fresh, Frozen and Delicatessen, and small increase for Milk, Grocery and Detergents_Paper. Dimension 3: It shows large increases for Frozen and Delicatessen, and large decreases for Fresh and Detergents_Paper. Dimension 4: It shows large increases for Frozen and Detergents_Paper, and large a decrease for Fish and Delicatessen.","title":"Question 5"},{"location":"ml/Customer_segments/customer_segments/#observation_1","text":"Run the code below to see how the log-transformed sample data has changed after having a PCA transformation applied to it in six dimensions. Observe the numerical value for the first four dimensions of the sample points. Consider if this is consistent with your initial interpretation of the sample points. # Display sample log-data after having a PCA transformation applied display(pd . DataFrame(np . round(pca_samples, 4 ), columns = pca_results . index . values)) Dimension 1 Dimension 2 Dimension 3 Dimension 4 Dimension 5 Dimension 6 0 -2.2406 -1.2419 -1.0729 -1.9589 0.2160 -0.1782 1 0.7394 -2.9834 -0.8204 1.2945 0.1297 0.4712 2 -2.1528 5.3859 0.0930 0.4023 0.3577 0.3111","title":"Observation"},{"location":"ml/Customer_segments/customer_segments/#implementation-dimensionality-reduction","text":"When using principal component analysis, one of the main goals is to reduce the dimensionality of the data \u2014 in effect, reducing the complexity of the problem. Dimensionality reduction comes at a cost: Fewer dimensions used implies less of the total variance in the data is being explained. Because of this, the cumulative explained variance ratio is extremely important for knowing how many dimensions are necessary for the problem. Additionally, if a signifiant amount of variance is explained by only two or three dimensions, the reduced data can be visualized afterwards. In the code block below, you will need to implement the following: - Assign the results of fitting PCA in two dimensions with good_data to pca . - Apply a PCA transformation of good_data using pca.transform , and assign the results to reduced_data . - Apply a PCA transformation of log_samples using pca.transform , and assign the results to pca_samples . # TODO: Apply PCA by fitting the good data with only two dimensions pca = PCA(n_components = 2 ) pca . fit(good_data) # TODO: Transform the good data using the PCA fit above reduced_data = pca . transform(good_data) # TODO: Transform the sample log-data using the PCA fit above pca_samples = pca . transform(log_samples) # Create a DataFrame for the reduced data reduced_data = pd . DataFrame(reduced_data, columns = [ 'Dimension 1' , 'Dimension 2' ])","title":"Implementation: Dimensionality Reduction"},{"location":"ml/Customer_segments/customer_segments/#observation_2","text":"Run the code below to see how the log-transformed sample data has changed after having a PCA transformation applied to it using only two dimensions. Observe how the values for the first two dimensions remains unchanged when compared to a PCA transformation in six dimensions. # Display sample log-data after applying PCA transformation in two dimensions display(pd . DataFrame(np . round(pca_samples, 4 ), columns = [ 'Dimension 1' , 'Dimension 2' ])) # scatter plot of reduced features #pd.scatter_matrix(reduced_data, alpha = 0.3, figsize = (10,10), diagonal = 'kde'); sns . pairplot(reduced_data,size = 5 ) Dimension 1 Dimension 2 0 -2.2406 -1.2419 1 0.7394 -2.9834 2 -2.1528 5.3859 &lt;seaborn.axisgrid.PairGrid at 0x119715710&gt;","title":"Observation"},{"location":"ml/Customer_segments/customer_segments/#visualizing-a-biplot","text":"A biplot is a scatterplot where each data point is represented by its scores along the principal components. The axes are the principal components (in this case Dimension 1 and Dimension 2 ). In addition, the biplot shows the projection of the original features along the components. A biplot can help us interpret the reduced dimensions of the data, and discover relationships between the principal components and original features. Run the code cell below to produce a biplot of the reduced-dimension data. # Create a biplot vs . biplot(good_data, reduced_data, pca) &lt;matplotlib.axes._subplots.AxesSubplot at 0x119c34b10&gt;","title":"Visualizing a Biplot"},{"location":"ml/Customer_segments/customer_segments/#observation_3","text":"Once we have the original feature projections (in red), it is easier to interpret the relative position of each data point in the scatterplot. For instance, a point the lower right corner of the figure will likely correspond to a customer that spends a lot on 'Milk' , 'Grocery' and 'Detergents_Paper' , but not so much on the other product categories. From the biplot, which of the original features are most strongly correlated with the first component? What about those that are associated with the second component? Do these observations agree with the pca_results plot you obtained earlier?","title":"Observation"},{"location":"ml/Customer_segments/customer_segments/#clustering","text":"In this section, you will choose to use either a K-Means clustering algorithm or a Gaussian Mixture Model clustering algorithm to identify the various customer segments hidden in the data. You will then recover specific data points from the clusters to understand their significance by transforming them back into their original dimension and scale.","title":"Clustering"},{"location":"ml/Customer_segments/customer_segments/#question-6","text":"What are the advantages to using a K-Means clustering algorithm? What are the advantages to using a Gaussian Mixture Model clustering algorithm? Given your observations about the wholesale customer data so far, which of the two algorithms will you use and why? Answer: What are the advantages to using a K-Means clustering algorithm? It is simple to understand and easy to implement.In computational point of view, it is fast to run and always converge. It is suitable for searching convex clusters. What are the advantages to using a Gaussian Mixture Model clustering algorithm? It is the fastest algorithm for learning mixture models among other available models. In case of overlapping clusters, it has \"soft\" classification technique available. In GMM, there are well-studied statistical inference techniques available. Given your observations about the wholesale customer data so far, which of the two algorithms will you use and why? Gaussian Mixture Model clustering algorithm would be the most appropriate because from preious scatter graph of data sets with reduced features, there are two clusters that overlap with each other.","title":"Question 6"},{"location":"ml/Customer_segments/customer_segments/#implementation-creating-clusters","text":"Depending on the problem, the number of clusters that you expect to be in the data may already be known. When the number of clusters is not known a priori , there is no guarantee that a given number of clusters best segments the data, since it is unclear what structure exists in the data \u2014 if any. However, we can quantify the \"goodness\" of a clustering by calculating each data point's silhouette coefficient . The silhouette coefficient for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). Calculating the mean silhouette coefficient provides for a simple scoring method of a given clustering. In the code block below, you will need to implement the following: - Fit a clustering algorithm to the reduced_data and assign it to clusterer . - Predict the cluster for each data point in reduced_data using clusterer.predict and assign them to preds . - Find the cluster centers using the algorithm's respective attribute and assign them to centers . - Predict the cluster for each sample data point in pca_samples and assign them sample_preds . - Import sklearn.metrics.silhouette_score and calculate the silhouette score of reduced_data against preds . - Assign the silhouette score to score and print the result. from sklearn.mixture import GaussianMixture as GMM from sklearn.metrics import silhouette_score # method to find the best n_components def score_cluster (data, num_components): clusterer = GMM(n_components = num_components) clusterer . fit(data) preds = clusterer . predict(data) score = silhouette_score(data, preds) return score print ( \"Silhouette Score for different sizes\" ) silhouette_scores_matrix = pd . DataFrame(index = [ 'Score' ]) for size in range ( 2 , 11 ): silhouette_scores_matrix[size] = pd . Series(score_cluster(reduced_data, size),\\ index = silhouette_scores_matrix . index) display(silhouette_scores_matrix) best_n_components = 2 # Apply the selected clustering algorithm to the reduced data clusterer = GMM(n_components = best_n_components) clusterer . fit(reduced_data) # Predict the cluster for each data point preds = clusterer . predict(reduced_data) # Find the cluster centers centers = clusterer . means_ # Predict the cluster for each transformed sample data point sample_preds = clusterer . predict(pca_samples) # Calculate the mean silhouette coefficient for the number of clusters chosen score = silhouette_score(reduced_data, preds) print ( \"Best Score: {}, n_components={}\" . format(score, best_n_components)) Silhouette Score for different sizes 2 3 4 5 6 7 8 9 10 Score 0.421917 0.395557 0.279418 0.321519 0.25418 0.323427 0.309646 0.328185 0.319549 Best Score: 0.422324682646, n_components=2 silhouette_scores_matrix . plot(kind = \"barh\" ,figsize = ( 14 , 4 )) &lt;matplotlib.axes._subplots.AxesSubplot at 0x11ca59050&gt;","title":"Implementation: Creating Clusters"},{"location":"ml/Customer_segments/customer_segments/#question-7","text":"Report the silhouette score for several cluster numbers you tried. Of these, which number of clusters has the best silhouette score? Answer: It is shown in previous output of \" Implementation: Creating Clusters \". the Best Score: 0.42, for number of components=2.","title":"Question 7"},{"location":"ml/Customer_segments/customer_segments/#cluster-visualization","text":"Once you've chosen the optimal number of clusters for your clustering algorithm using the scoring metric above, you can now visualize the results by executing the code block below. Note that, for experimentation purposes, you are welcome to adjust the number of clusters for your clustering algorithm to see various visualizations. The final visualization provided should, however, correspond with the optimal number of clusters. # Display the results of the clustering from implementation vs . cluster_results(reduced_data, preds, centers, pca_samples)","title":"Cluster Visualization"},{"location":"ml/Customer_segments/customer_segments/#implementation-data-recovery","text":"Each cluster present in the visualization above has a central point. These centers (or means) are not specifically data points from the data, but rather the averages of all the data points predicted in the respective clusters. For the problem of creating customer segments, a cluster's center point corresponds to the average customer of that segment . Since the data is currently reduced in dimension and scaled by a logarithm, we can recover the representative customer spending from these data points by applying the inverse transformations. In the code block below, you will need to implement the following: - Apply the inverse transform to centers using pca.inverse_transform and assign the new centers to log_centers . - Apply the inverse function of np.log to log_centers using np.exp and assign the true centers to true_centers . # TODO: Inverse transform the centers log_centers = pca . inverse_transform(centers) # TODO: Exponentiate the centers true_centers = np . exp(log_centers) # Display the true centers segments = [ 'Segment {}' . format(i) for i in range ( 0 , len (centers))] true_centers = pd . DataFrame(np . round(true_centers), columns = data . keys()) true_centers . index = segments display(true_centers) #show segments in percentile newdata = data . append(true_centers) print ( \"Percentiles of the centers\" ) percent_centers = 100.0 * newdata . rank(axis = 0 , pct = True )\\ . loc[[ 'Segment 0' , 'Segment 1' ]]\\ . round(decimals = 3 ) display(percent_centers) Fresh Milk Grocery Frozen Detergents_Paper Delicatessen Segment 0 8939.0 2108.0 2758.0 2073.0 352.0 730.0 Segment 1 3567.0 7860.0 12249.0 873.0 4713.0 966.0 Percentiles of the centers Fresh Milk Grocery Frozen Detergents_Paper Delicatessen Segment 0 52.5 34.4 34.4 58.4 32.0 41.2 Segment 1 28.1 79.0 80.5 31.0 80.3 50.2","title":"Implementation: Data Recovery"},{"location":"ml/Customer_segments/customer_segments/#plot-of-true-center","text":"true_centers . plot(kind = \"barh\" ,figsize = ( 10 , 6 )) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1196ac210&gt;","title":"Plot of true center"},{"location":"ml/Customer_segments/customer_segments/#plot-of-percentile-centers","text":"percent_centers . plot(kind = \"barh\" ,figsize = ( 10 , 6 )) &lt;matplotlib.axes._subplots.AxesSubplot at 0x11b6d8950&gt;","title":"Plot of Percentile centers"},{"location":"ml/Customer_segments/customer_segments/#question-8","text":"Consider the total purchase cost of each product category for the representative data points above, and reference the statistical description of the dataset at the beginning of this project. What set of establishments could each of the customer segments represent? Hint: A customer who is assigned to 'Cluster X' should best identify with the establishments represented by the feature set of 'Segment X' . Answer: Segment 0 customers buy most Fresh, Frozen (> %50) and much Delicatessen (40%), it's mostly likely to be a convinence store or greegrocery . Segment 1 customers buy lots of Grocery, Milk and Detergents_Paper (> 70%), following by Delicatessen (~%50), it indicates it's a supermarket .","title":"Question 8"},{"location":"ml/Customer_segments/customer_segments/#question-9","text":"For each sample point, which customer segment from Question 8 best represents it? Are the predictions for each sample point consistent with this? Run the code block below to find which cluster each sample point is predicted to be. # Display the predictions for i, pred in enumerate (sample_preds): print ( \"Sample point\" , i, \"predicted to be in Cluster\" , pred) Sample point 0 predicted to be in Cluster 0 Sample point 1 predicted to be in Cluster 1 Sample point 2 predicted to be in Cluster 0 samples . plot . barh(stacked = True ,figsize = ( 10 , 2 ) ); Answer: For each sample point, which customer segment from Question 8 best represents it? It is shown in output of Question 9 . Are the predictions for each sample point consistent with this? Yes, more or less. The prediction agrees with previous guess that samples are Supermarket, Greengrocer, except for sample 3 as Convenience store. Sample 1 customers buy lots of produces in all category, which is close to Segment 0. Sample 2 customers buy mostly Fresh and frozen, which makes it close to Segment 1. Sample 3 customers buy mostly Grocery and Detergents_Paper, which makes it close to Segment 0. Samples Fresh Milk Grocery Frozen Detergents_Paper Delicatessen 0 1.560499 0.884800 0.400925 -0.574313 0.209873 1 5.084854 -0.315148 -0.089470 2.776994 -0.441685 2 -0.942242 -0.297242 0.468664 -0.613289 0.018584","title":"Question 9"},{"location":"ml/Customer_segments/customer_segments/#conclusion","text":"In this final section, you will investigate ways that you can make use of the clustered data. First, you will consider how the different groups of customers, the customer segments , may be affected differently by a specific delivery scheme. Next, you will consider how giving a label to each customer (which segment that customer belongs to) can provide for additional features about the customer data. Finally, you will compare the customer segments to a hidden variable present in the data, to see whether the clustering identified certain relationships.","title":"Conclusion"},{"location":"ml/Customer_segments/customer_segments/#question-10","text":"Companies will often run A/B tests when making small changes to their products or services to determine whether making that change will affect its customers positively or negatively. The wholesale distributor is considering changing its delivery service from currently 5 days a week to 3 days a week. However, the distributor will only make this change in delivery service for customers that react positively. How can the wholesale distributor use the customer segments to determine which customers, if any, would react positively to the change in delivery service? Hint: Can we assume the change affects all customers equally? How can we determine which group of customers it affects the most? Answer: The items that customer purchase like: Fresh, Milk and Frozen are time sensitive and customer may prefer faster and frequent dilivery. Clustering designed with focusing on these three features may give distributor more intution. One can perform the A/B test by selecting a random sample from each cluster and reducing the delivery frequency for each of them. Then a suvey of customer satisfaction could be made for further analysis. An equally sized group of customers with same shopping conditions except the delivery time selected from the remaining customers could be used for comparison. The A/B test data is prepared, interpreated and cross-validated by changing the delivery service to certain amount of test customers. The process can be iterated until certain model is verified or market goals are met.","title":"Question 10"},{"location":"ml/Customer_segments/customer_segments/#question-11","text":"Additional structure is derived from originally unlabeled data when using clustering techniques. Since each customer has a customer segment it best identifies with (depending on the clustering algorithm applied), we can consider 'customer segment' as an engineered feature for the data. Assume the wholesale distributor recently acquired ten new customers and each provided estimates for anticipated annual spending of each product category. Knowing these estimates, the wholesale distributor wants to classify each new customer to a customer segment to determine the most appropriate delivery service. How can the wholesale distributor label the new customers using only their estimated product spending and the customer segment data? *Hint: A supervised learner could be used to train on the original customers. What would be the target variable? Answer: The customer segment (cluster prediction) can be used as target and one can design the supervised learning model based on them. The model developed could be used to predict the new customer data.","title":"Question 11"},{"location":"ml/Customer_segments/customer_segments/#visualizing-underlying-distributions","text":"At the beginning of this project, it was discussed that the 'Channel' and 'Region' features would be excluded from the dataset so that the customer product categories were emphasized in the analysis. By reintroducing the 'Channel' feature to the dataset, an interesting structure emerges when considering the same PCA dimensionality reduction applied earlier to the original dataset. Run the code block below to see how each data point is labeled either 'HoReCa' (Hotel/Restaurant/Cafe) or 'Retail' the reduced space. In addition, you will find the sample points are circled in the plot, which will identify their labeling. # Display the clustering results based on 'Channel' data vs . channel_results(reduced_data, outliers, pca_samples)","title":"Visualizing Underlying Distributions"},{"location":"ml/Customer_segments/customer_segments/#question-12","text":"How well does the clustering algorithm and number of clusters you've chosen compare to this underlying distribution of Hotel/Restaurant/Cafe customers to Retailer customers? Are there customer segments that would be classified as purely 'Retailers' or 'Hotels/Restaurants/Cafes' by this distribution? Would you consider these classifications as consistent with your previous definition of the customer segments? Answer: How well does the clustering algorithm and number of clusters you've chosen compare to this underlying distribution of Hotel/Restaurant/Cafe customers to Retailer customers? The algorithm and the number of clusters chosen matches with the underlying distribution. Are there customer segments that would be classified as purely 'Retailers' or 'Hotels/Restaurants/Cafes' by this distribution? No, there will be overlap areas for two clusters. Although there is some overlap between the two groups in the middle of the distribution, segment 0 in the analysis is clearly \"Retail\", and segment 1 is clearly \"Hotel/Restaurants/Cafes\". Would you consider these classifications as consistent with your previous definition of the customer segments? It shows the previous definition of customer segments is consistent with the classification using the Channel feature. I believe the algorithm did a reasonable job of clustering these customers according to broad categories of business type. Note : Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to File -> Download as -> HTML (.html) . Include the finished document along with this notebook as your submission.","title":"Question 12"},{"location":"ml/boston_housing/boston_housing/","text":"Machine Learning Engineer Nanodegree Model Evaluation & Validation Project: Predicting Boston Housing Prices Welcome to the first project of the Machine Learning Engineer Nanodegree! In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with 'Implementation' in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a 'Question X' header. Carefully read each question and provide thorough answers in the following text boxes that begin with 'Answer:' . Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide. Note: Code and Markdown cells can be executed using the Shift + Enter keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode. Getting Started In this project, you will evaluate the performance and predictive power of a model that has been trained and tested on data collected from homes in suburbs of Boston, Massachusetts. A model trained on this data that is seen as a good fit could then be used to make certain predictions about a home \u2014 in particular, its monetary value. This model would prove to be invaluable for someone like a real estate agent who could make use of such information on a daily basis. The dataset for this project originates from the UCI Machine Learning Repository . The Boston housing data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts. For the purposes of this project, the following preprocessing steps have been made to the dataset: - 16 data points have an 'MEDV' value of 50.0. These data points likely contain missing or censored values and have been removed. - 1 data point has an 'RM' value of 8.78. This data point can be considered an outlier and has been removed. - The features 'RM' , 'LSTAT' , 'PTRATIO' , and 'MEDV' are essential. The remaining non-relevant features have been excluded. - The feature 'MEDV' has been multiplicatively scaled to account for 35 years of market inflation. Run the code cell below to load the Boston housing dataset, along with a few of the necessary Python libraries required for this project. You will know the dataset loaded successfully if the size of the dataset is reported. # Import libraries necessary for this project import numpy as np import pandas as pd from sklearn.cross_validation import ShuffleSplit # Import supplementary visualizations code visuals.py import visuals as vs # Pretty display for notebooks % matplotlib inline # Load the Boston housing dataset data = pd . read_csv( 'housing.csv' ) prices = data[ 'MEDV' ] features = data . drop( 'MEDV' , axis = 1 ) # Success print ( \"Boston housing dataset has {} data points with {} variables each.\" . format( * data . shape)) Boston housing dataset has 489 data points with 4 variables each. Part- I Data Exploration In this first section of this project, you will make a cursory investigation about the Boston housing data and provide your observations. Familiarizing yourself with the data through an explorative process is a fundamental practice to help you better understand and justify your results. Since the main goal of this project is to construct a working model which has the capability of predicting the value of houses, we will need to separate the dataset into features and the target variable . The features , 'RM' , 'LSTAT' , and 'PTRATIO' , give us quantitative information about each data point. The target variable , 'MEDV' , will be the variable we seek to predict. These are stored in features and prices , respectively. Implementation: Calculate Statistics For your very first coding implementation, you will calculate descriptive statistics about the Boston housing prices. Since numpy has already been imported for you, use this library to perform the necessary calculations. These statistics will be extremely important later on to analyze various prediction results from the constructed model. In the code cell below, you will need to implement the following: - Calculate the minimum, maximum, mean, median, and standard deviation of 'MEDV' , which is stored in prices . - Store each calculation in their respective variable. # TODO: Minimum price of the data minimum_price = np . min(prices) # TODO: Maximum price of the data maximum_price = np . max(prices) # TODO: Mean price of the data mean_price = np . mean(prices) # TODO: Median price of the data median_price = np . median(prices) # Extra: First abd Third Quartile first_quartile = np . percentile(prices, 25 ) third_quartile = np . percentile(prices, 75 ) # TODO: Standard deviation of prices of the data std_price = np . std(prices) # Show the calculated statistics print ( \"Statistics for Boston housing dataset: \\n \" ) print ( \"Minimum price: ${:,.2f}\" . format(minimum_price)) print ( \"Maximum price: ${:,.2f}\" . format(maximum_price)) print ( \"Mean price: ${:,.2f}\" . format(mean_price)) print ( \"Median price ${:,.2f}\" . format(median_price)) print ( \"First Quartile: ${:,.2f}\" . format(first_quartile )) print ( \"Third Quartile: ${:,.2f}\" . format(third_quartile )) print ( \"Standard deviation of prices: ${:,.2f}\" . format(std_price)) Statistics for Boston housing dataset: Minimum price: $105,000.00 Maximum price: $1,024,800.00 Mean price: $454,342.94 Median price $438,900.00 First Quartile: $350,700.00 Third Quartile: $518,700.00 Standard deviation of prices: $165,171.13 I love visualizing them import matplotlib.pyplot as plt plt . figure(figsize = ( 8 , 6 )) plt . hist(prices, bins = 20 , color = 'g' ) plt . axvline(mean_price, lw = 5 , c = 'b' , label = 'mean' ) plt . axvline(median_price, lw = 5 , c = 'y' , label = 'median' ) plt . axvline(first_quartile, lw = 5 , c = 'r' ,label = 'first_quartile' ) plt . axvline(third_quartile, lw = 5 , c = 'm' , label = 'third_quartile' ) plt . legend(bbox_to_anchor = ( 1.05 , 0.05 ),loc = 'lower left' ,borderaxespad = 0. ) &lt;matplotlib.legend.Legend at 0x11b06b400&gt; Question 1 - Feature Observation As a reminder, we are using three features from the Boston housing dataset: 'RM' , 'LSTAT' , and 'PTRATIO' . For each data point (neighborhood): - 'RM' is the average number of rooms among homes in the neighborhood. - 'LSTAT' is the percentage of homeowners in the neighborhood considered \"lower class\" (working poor). - 'PTRATIO' is the ratio of students to teachers in primary and secondary schools in the neighborhood. Using your intuition, for each of the three features above, do you think that an increase in the value of that feature would lead to an increase in the value of 'MEDV' or a decrease in the value of 'MEDV' ? Justify your answer for each. Hint: Would you expect a home that has an 'RM' value of 6 be worth more or less than a home that has an 'RM' value of 7? Answer: Increase in the value of a features sometime cross the limit for increase in the value of MEDV . Let me go one by one. House with large RM (more rooms) is in general more likely to cost more for the larger space. But small houses with many rooms may not impose same type of contribution on decision making. High LSTAT means the house has a poor neighborhood and likely to reduce the value of MEDV . This is because houses in rich neighborhood is correlated with high standard of self paied local security network, transportation and other extra infrastructure. Low PTRATIO implies that the community has rich education resources. This will raise the value of the hosue. But this is correlated with the family members and chance of having baby in near future. Checking my intution import matplotlib.pyplot as plt plt . figure(figsize = ( 20 , 5 )) for i, col in enumerate (features . columns): plt . subplot( 1 , 4 , i + 1 ) plt . plot(data[col], prices, 'o' ) plt . title(col) plt . xlabel(col) plt . ylabel( 'prices' ) Plot 1 and 2 are making a lot of sense. Plot 3 is little complicated. Part- II Developing a Model In this second section of the project, you will develop the tools and techniques necessary for a model to make a prediction. Being able to make accurate evaluations of each model's performance through the use of these tools and techniques helps to greatly reinforce the confidence in your predictions. Implementation: Define a Performance Metric It is difficult to measure the quality of a given model without quantifying its performance over training and testing. This is typically done using some type of performance metric, whether it is through calculating some type of error, the goodness of fit, or some other useful measurement. For this project, you will be calculating the coefficient of determination , R 2 , to quantify your model's performance. The coefficient of determination for a model is a useful statistic in regression analysis, as it often describes how \"good\" that model is at making predictions. The values for R 2 range from 0 to 1, which captures the percentage of squared correlation between the predicted and actual values of the target variable . A model with an R 2 of 0 is no better than a model that always predicts the mean of the target variable, whereas a model with an R 2 of 1 perfectly predicts the target variable. Any value between 0 and 1 indicates what percentage of the target variable, using this model, can be explained by the features . A model can be given a negative R 2 as well, which indicates that the model is arbitrarily worse than one that always predicts the mean of the target variable. For the performance_metric function in the code cell below, you will need to implement the following: - Use r2_score from sklearn.metrics to perform a performance calculation between y_true and y_predict . - Assign the performance score to the score variable. # Import 'r2_score' from sklearn.metrics import r2_score def performance_metric (y_true, y_predict): \"\"\" Calculates and returns the performance score between true and predicted values based on the metric chosen. \"\"\" #Calculate the performance score between 'y_true' and 'y_predict' score = r2_score(y_true, y_predict) # Return the score return score Question 2 - Goodness of Fit Assume that a dataset contains five data points and a model made the following predictions for the target variable: True Value Prediction 3.0 2.5 -0.5 0.0 2.0 2.1 7.0 7.8 4.2 5.3 Would you consider this model to have successfully captured the variation of the target variable? Why or why not? Run the code cell below to use the performance_metric function and calculate this model's coefficient of determination. # Calculate the performance of this model score = performance_metric([ 3 , - 0.5 , 2 , 7 , 4.2 ], [ 2.5 , 0.0 , 2.1 , 7.8 , 5.3 ]) print ( \"Model has a coefficient of determination, R^2, of {:.3f}.\" . format(score)) Model has a coefficient of determination, R^2, of 0.923. Answer: Would you consider this model to have successfully captured the variation of the target variable? Yes, this model have successfully captured the variation of the target variable. Since r2_score is 0.923, this implies that 92.3% of the variables are predictable. Why or why not? The r2_score provides a measure of how well future samples are likely to be predicted by the model compared to the case where all predictions are replaced by mean value of target variable. First of all, the meaning of different values of r2_score are : $ \\huge{R^{2} (y,\\hat{y}) = 1 - \\frac{\\sum {i=0}^{n {samples} -1} (y_{i} - \\hat{y} {i})^{2}}{\\sum {i=0}^{n_{samples} -1} (y_{i} - \\bar{y})^{2}}}$ Where $\\hat{y}$ = prediction, $\\bar{y}$ = mean value Special case: $R^{2} =0$, In this case model predictions are equivalent to all predictions replaced by mean value. Special case: $R^{2} =1$, In this case model prediction is equal to the exact value. This is an ideal case and may lead us to overfitting. Special case: $R^{2} < 0$, In this case model predictions are more worst and could not even exceed the score if we roughly predict all value to mean value. Thus, the value of $ R^{2}$ closer to 1 is the best value! Reference used: http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score Implementation: Shuffle and Split Data Your next implementation requires that you take the Boston housing dataset and split the data into training and testing subsets. Typically, the data is also shuffled into a random order when creating the training and testing subsets to remove any bias in the ordering of the dataset. For the code cell below, you will need to implement the following: - Use train_test_split from sklearn.cross_validation to shuffle and split the features and prices data into training and testing sets. - Split the data into 80% training and 20% testing. - Set the random_state for train_test_split to a value of your choice. This ensures results are consistent. - Assign the train and testing splits to X_train , X_test , y_train , and y_test . # Import 'train_test_split' from sklearn.cross_validation import train_test_split # Shuffle and split the data into training and testing subsets X_train, X_test, y_train, y_test = train_test_split(features, prices, test_size = 0.20 , random_state = 31 ) # Success print ( \"Training and testing split was successful.\" ) Training and testing split was successful. Question 3 - Training and Testing What is the benefit to splitting a dataset into some ratio of training and testing subsets for a learning algorithm? Hint: What could go wrong with not having a way to test your model? Answer: Theoretically, there could be infinite data and the best model with 99.99% accuracy disregarding the computational cost and time. Practically, this is not feasible. For a given set of data, there exist infinitely many models withinh the certain range of accuricy. Our intention is to find that best model which will have less and less error based on computational cost. If we use whole data as training data in the search of the best model, we will hit that model which will predict all data in traing set with high accuracy but we are left with nothing to check our model. We will be unknown with the accuracy of the model with the data which exist out there in the Universal set(which has infinite data where training data is only a subset of it). In short, test set being independent from training set, helps us better evluate the algorithms by detecting overfitting or underfitting in training subsets. Analyzing Model Performance In this third section of the project, you'll take a look at several models' learning and testing performances on various subsets of training data. Additionally, you'll investigate one particular algorithm with an increasing 'max_depth' parameter on the full training set to observe how model complexity affects performance. Graphing your model's performance based on varying criteria can be beneficial in the analysis process, such as visualizing behavior that may not have been apparent from the results alone. Learning Curves The following code cell produces four graphs for a decision tree model with different maximum depths. Each graph visualizes the learning curves of the model for both training and testing as the size of the training set is increased. Note that the shaded region of a learning curve denotes the uncertainty of that curve (measured as the standard deviation). The model is scored on both the training and testing sets using R 2 , the coefficient of determination. Run the code cell below and use these graphs to answer the following question. # Produce learning curves for varying training set sizes and maximum depths vs . ModelLearning(features, prices) Question 4 - Learning the Data Choose one of the graphs above and state the maximum depth for the model. What happens to the score of the training curve as more training points are added? What about the testing curve? Would having more training points benefit the model? Hint: Are the learning curves converging to particular scores? Answer: By looking gap between training and testing curves, as max_depth goes from 1 to 10 , this clearly indicates that algorithm goes from underfitting to overfitting. Graph with max_depth = 3 seems to be the best fit in between. So my analysis goes for max_depth = 3 . What happens to the score of the training curve as more training points are added? Adding more training points may reduce the noise in training score plot and it will be more smooth and almost horizontal after crossing certain limit along x-axis(Number of Training Points). For example at max_depth =3 , as training points increase, it does not suffer from overfitting or underfitting, so adding more training points will not affect the trend of convergence. What about the testing curve? It will behave similar to training curves. Would having more training points benefit the model? Eventhough training curve becomes smooth on increasing training data, it is evident form the convergence of training and testing curves that after certain number of training points it may not have much siginificance to improve accuracy with respect to computation cost. Complexity Curves The following code cell produces a graph for a decision tree model that has been trained and validated on the training data using different maximum depths. The graph produces two complexity curves \u2014 one for training and one for validation. Similar to the learning curves , the shaded regions of both the complexity curves denote the uncertainty in those curves, and the model is scored on both the training and validation sets using the performance_metric function. Run the code cell below and use this graph to answer the following two questions. vs . ModelComplexity(X_train, y_train) Question 5 - Bias-Variance Tradeoff When the model is trained with a maximum depth of 1, does the model suffer from high bias or from high variance? How about when the model is trained with a maximum depth of 10? What visual cues in the graph justify your conclusions? Hint: How do you know when a model is suffering from high bias or high variance? Answer: When the model is trained with a maximum depth of 1, does the model suffer from high bias or from high variance? Yes, it suffers from high bias. First of all the validation score and training score both are lower than desired value (~0.8). Sconldly, both curve are close to each other on score value which indicates there is underfitting. How about when the model is trained with a maximum depth of 10? Now, it suffers from high variance. The gap between validation score and training score clearly indicates there is overfitting. What visual cues in the graph justify your conclusions? Comparision with desired values (>0.8) and gap between training and testing curves are the important cues for the conclusion. Question 6 - Best-Guess Optimal Model Which maximum depth do you think results in a model that best generalizes to unseen data? What intuition lead you to this answer? Answer: The best maximum depth seems to be 3. Both training curve and validation curve reach maximum value of score with no sign of overfitting or underfitting. Part-III Evaluating Model Performance In this final section of the project, you will construct a model and make a prediction on the client's feature set using an optimized model from fit_model . Question 7 - Grid Search What is the grid search technique and how it can be applied to optimize a learning algorithm? Answer: Grid search is an exhaustive search algorithm which searches over all combinations of parameters we specify to find the optimum combination that yields the best performance. Due to its exhaustive search nature, grid search can be computationally expensive, especially when data size is large and model is complicated. Sometimes we resort to randomized search in this case to search only some combinations of the parameters. Following are the detail of two such algorithms: Algorithm Detail Implementation Difference GridSearchCV GridSearchCV implements with exhaustive search over specified parameter values for an estimator. The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid. fit , score , predict , predict_proba , decision_function , transform and inverse_transform In GridSearchCV , all parameter values are tried out. RandomizedSearchCV RandomizedSearchCV implements with Randomized search on hyper parameters. The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings. fit , score , predict , predict_proba , decision_function , transform and inverse_transform In contrast to GridSearchCV , not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter . References used to answer: 1. http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV 2. http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV Question 8 - Cross-Validation What is the k-fold cross-validation training technique? What benefit does this technique provide for grid search when optimizing a model? Hint: Much like the reasoning behind having a testing set, what could go wrong with using grid search without a cross-validated set? Answer: What is the k-fold cross-validation training technique? Practically, this is a method to extract the data resources from multiple direction to insure that the model trained is robust over any possible noise. Technically, it is achived by taking all data as training sets and testing sets via roation and averaging. This includes the following steps: Step-I: Data is divided into k folds of eqaully large chunks. Step-II: Iterated through the k chunks using the current chunk for validation and other k-1 chunks for training. Step -III: Average is taken over the validation scores to give a single validation score for the learning algorithm. What benefit does this technique provide for grid search when optimizing a model? Using all the examples in the dataset for both training and testing is the main advantage of k-fold cross-validation. This makes it more robust to variations of parameters in grid search. Grid search may overfit the validation set if certain evaluation data points are outliers. It can be eliminated by averaging over k-times evaluations using all data points. Implementation: Fitting a Model Your final implementation requires that you bring everything together and train a model using the decision tree algorithm . To ensure that you are producing an optimized model, you will train the model using the grid search technique to optimize the 'max_depth' parameter for the decision tree. The 'max_depth' parameter can be thought of as how many questions the decision tree algorithm is allowed to ask about the data before making a prediction. Decision trees are part of a class of algorithms called supervised learning algorithms . In addition, you will find your implementation is using ShuffleSplit() for an alternative form of cross-validation (see the 'cv_sets' variable). While it is not the K-Fold cross-validation technique you describe in Question 8 , this type of cross-validation technique is just as useful!. The ShuffleSplit() implementation below will create 10 ( 'n_iter' ) shuffled sets, and for each shuffle, 20% ( 'test_size' ) of the data will be used as the validation set . While you're working on your implementation, think about the contrasts and similarities it has to the K-fold cross-validation technique. For the fit_model function in the code cell below, you will need to implement the following: - Use DecisionTreeRegressor from sklearn.tree to create a decision tree regressor object. - Assign this object to the 'regressor' variable. - Create a dictionary for 'max_depth' with the values from 1 to 10, and assign this to the 'params' variable. - Use make_scorer from sklearn.metrics to create a scoring function object. - Pass the performance_metric function as a parameter to the object. - Assign this scoring function to the 'scoring_fnc' variable. - Use GridSearchCV from sklearn.grid_search to create a grid search object. - Pass the variables 'regressor' , 'params' , 'scoring_fnc' , and 'cv_sets' as parameters to the object. - Assign the GridSearchCV object to the 'grid' variable. # Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV' from sklearn.metrics import make_scorer from sklearn.tree import DecisionTreeRegressor from sklearn.grid_search import GridSearchCV def fit_model (X, y): \"\"\" Performs grid search over the 'max_depth' parameter for a decision tree regressor trained on the input data [X, y]. \"\"\" # Create cross-validation sets from the training data cv_sets = ShuffleSplit(X . shape[ 0 ], n_iter = 10 , test_size = 0.20 , random_state = 0 ) # Create a decision tree regressor object regressor = DecisionTreeRegressor() # Create a dictionary for the parameter 'max_depth' with a range from 1 to 10 params = { 'max_depth' :( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 )} # Transform 'performance_metric' into a scoring function using 'make_scorer' scoring_fnc = make_scorer(performance_metric) # Create the grid search object grid = GridSearchCV(regressor, params,scoring_fnc,cv = cv_sets) # Fit the grid search object to the data to compute the optimal model grid = grid . fit(X, y) # Return the optimal model after fitting the data return grid . best_estimator_ Making Predictions Once a model has been trained on a given set of data, it can now be used to make predictions on new sets of input data. In the case of a decision tree regressor , the model has learned what the best questions to ask about the input data are , and can respond with a prediction for the target variable . You can use these predictions to gain information about data where the value of the target variable is unknown \u2014 such as data the model was not trained on. Question 9 - Optimal Model What maximum depth does the optimal model have? How does this result compare to your guess in Question 6 ? Run the code block below to fit the decision tree regressor to the training data and produce an optimal model. # Fit the training data to the model using grid search reg = fit_model(X_train, y_train) # Produce the value for 'max_depth' print ( \"Parameter 'max_depth' is {} for the optimal model.\" \\ . format(reg . get_params()[ 'max_depth' ])) Parameter 'max_depth' is 3 for the optimal model. Answer: The optimal model has max_depth=4 , it is close to the result max_depth=3 in Question 6 . This difference could be as a result of different data used for in the graphical representation and the grid search. Arguably grid search should be more reliable. Making life less complicated by following - Occam's Razor principle(https://simple.wikipedia.org/wiki/Occam's_razor), which favours a simpler model to more complicated ones. :) Question 10 - Predicting Selling Prices Imagine that you were a real estate agent in the Boston area looking to use this model to help price homes owned by your clients that they wish to sell. You have collected the following information from three of your clients: Feature Client 1 Client 2 Client 3 Total number of rooms in home 5 rooms 4 rooms 8 rooms Neighborhood poverty level (as %) 17% 32% 3% Student-teacher ratio of nearby schools 15-to-1 22-to-1 12-to-1 What price would you recommend each client sell his/her home at? Do these prices seem reasonable given the values for the respective features? Hint: Use the statistics you calculated in the Data Exploration section to help justify your response. Run the code block below to have your optimized model make predictions for each client's home. # Produce a matrix for client data client_data = [[ 5 , 17 , 15 ], # Client 1 [ 4 , 32 , 22 ], # Client 2 [ 8 , 3 , 12 ]] # Client 3 # Show predictions for i, price in enumerate (reg . predict(client_data)): print ( \"Predicted selling price for Client {}'s home: ${:,.2f}\" . format(i + 1 , price)) Predicted selling price for Client 1's home: $369,680.77 Predicted selling price for Client 2's home: $274,568.97 Predicted selling price for Client 3's home: $903,150.00 Answer: 1. What price would you recommend each client sell his/her home at? Following are the prices recomended: Predicted selling price for Client 1's home: $410,792.31 Predicted selling price for Client 2's home: $234,054.55 Predicted selling price for Client 3's home: $920,010.00 Do these prices seem reasonable given the values for the respective features? Yes. The above prices are for average house(~410k), cheap house(~234k), and costy house(~920k) respectively by their deviation from mean_price measured in std_price.This is evident from histogram plot of prices below. The predicated prices are reasonable considering dataset statistics and our observation in Question 1 . The house price is likely to be in proportion to RM , and in inverse proportion to LSTAT or PTRATIO . As an example, for client 2 the deviations of three features all decrease the house price based on assumpution in Question 1 which cause it be cheaper than average house. import matplotlib.pyplot as plt plt . figure(figsize = ( 8 , 6 )) plt . hist(prices, bins = 20 ) mean_price = np . mean(prices) plt . axvline(mean_price, lw = 5 , c = 'g' ,label = \"mean\" ) for price in reg . predict(client_data): plt . axvline(price, lw = 5 , c = 'r' , label = \"client\" ) plt . legend(bbox_to_anchor = ( 1.05 , 0.05 ),loc = 'lower left' ,borderaxespad = 0. ) &lt;matplotlib.legend.Legend at 0x11bf42ba8&gt; Sensitivity An optimal model is not necessarily a robust model. Sometimes, a model is either too complex or too simple to sufficiently generalize to new data. Sometimes, a model could use a learning algorithm that is not appropriate for the structure of the data given. Other times, the data itself could be too noisy or contain too few samples to allow a model to adequately capture the target variable \u2014 i.e., the model is underfitted. Run the code cell below to run the fit_model function ten times with different training and testing sets to see how the prediction for a specific client changes with the data it's trained on. vs . PredictTrials(features, prices, fit_model, client_data) Trial 1: $391,183.33 Trial 2: $419,700.00 Trial 3: $415,800.00 Trial 4: $420,622.22 Trial 5: $418,377.27 Trial 6: $411,931.58 Trial 7: $399,663.16 Trial 8: $407,232.00 Trial 9: $351,577.61 Trial 10: $413,700.00 Range in prices: $69,044.61 The standard deviation of prices is 165,171.13 and range of price predicted is 69,044.61. This shows that the price predicted is consistent within 1 stadard deviation. Question 11 - Applicability In a few sentences, discuss whether the constructed model should or should not be used in a real-world setting. Hint: Some questions to answering: - How relevant today is data that was collected from 1978? - Are the features present in the data sufficient to describe a home? - Is the model robust enough to make consistent predictions? - Would data collected in an urban city like Boston be applicable in a rural city? Answer: The constructed model should not be used directly in a real-world setting. Following are the potential reasons: This data is from 1978.So, this model can not capture the present day scenario and can be biased with time changing. Dataset has a small number of samples. This may cause the model not to be robust enough to make consistent predictions. The features such as location,security, transportation, population etc are also likly to affect the house price. The features in this dataset may be insufficient to avoid underfitting, apart from room numbers, local incoming and education resources. There could be different models in different areas such as urban city or rural city. Newly built smart homes or IOT add technology may add new features not seen before in house pricing. Note : Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to File -> Download as -> HTML (.html) . Include the finished document along with this notebook as your submission.","title":"Boston Housing"},{"location":"ml/boston_housing/boston_housing/#machine-learning-engineer-nanodegree","text":"","title":"Machine Learning Engineer Nanodegree"},{"location":"ml/boston_housing/boston_housing/#model-evaluation-validation","text":"","title":"Model Evaluation &amp; Validation"},{"location":"ml/boston_housing/boston_housing/#project-predicting-boston-housing-prices","text":"Welcome to the first project of the Machine Learning Engineer Nanodegree! In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with 'Implementation' in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a 'Question X' header. Carefully read each question and provide thorough answers in the following text boxes that begin with 'Answer:' . Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide. Note: Code and Markdown cells can be executed using the Shift + Enter keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode.","title":"Project: Predicting Boston Housing Prices"},{"location":"ml/boston_housing/boston_housing/#getting-started","text":"In this project, you will evaluate the performance and predictive power of a model that has been trained and tested on data collected from homes in suburbs of Boston, Massachusetts. A model trained on this data that is seen as a good fit could then be used to make certain predictions about a home \u2014 in particular, its monetary value. This model would prove to be invaluable for someone like a real estate agent who could make use of such information on a daily basis. The dataset for this project originates from the UCI Machine Learning Repository . The Boston housing data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts. For the purposes of this project, the following preprocessing steps have been made to the dataset: - 16 data points have an 'MEDV' value of 50.0. These data points likely contain missing or censored values and have been removed. - 1 data point has an 'RM' value of 8.78. This data point can be considered an outlier and has been removed. - The features 'RM' , 'LSTAT' , 'PTRATIO' , and 'MEDV' are essential. The remaining non-relevant features have been excluded. - The feature 'MEDV' has been multiplicatively scaled to account for 35 years of market inflation. Run the code cell below to load the Boston housing dataset, along with a few of the necessary Python libraries required for this project. You will know the dataset loaded successfully if the size of the dataset is reported. # Import libraries necessary for this project import numpy as np import pandas as pd from sklearn.cross_validation import ShuffleSplit # Import supplementary visualizations code visuals.py import visuals as vs # Pretty display for notebooks % matplotlib inline # Load the Boston housing dataset data = pd . read_csv( 'housing.csv' ) prices = data[ 'MEDV' ] features = data . drop( 'MEDV' , axis = 1 ) # Success print ( \"Boston housing dataset has {} data points with {} variables each.\" . format( * data . shape)) Boston housing dataset has 489 data points with 4 variables each.","title":"Getting Started"},{"location":"ml/boston_housing/boston_housing/#part-i","text":"","title":"Part- I"},{"location":"ml/boston_housing/boston_housing/#data-exploration","text":"In this first section of this project, you will make a cursory investigation about the Boston housing data and provide your observations. Familiarizing yourself with the data through an explorative process is a fundamental practice to help you better understand and justify your results. Since the main goal of this project is to construct a working model which has the capability of predicting the value of houses, we will need to separate the dataset into features and the target variable . The features , 'RM' , 'LSTAT' , and 'PTRATIO' , give us quantitative information about each data point. The target variable , 'MEDV' , will be the variable we seek to predict. These are stored in features and prices , respectively.","title":"Data Exploration"},{"location":"ml/boston_housing/boston_housing/#implementation-calculate-statistics","text":"For your very first coding implementation, you will calculate descriptive statistics about the Boston housing prices. Since numpy has already been imported for you, use this library to perform the necessary calculations. These statistics will be extremely important later on to analyze various prediction results from the constructed model. In the code cell below, you will need to implement the following: - Calculate the minimum, maximum, mean, median, and standard deviation of 'MEDV' , which is stored in prices . - Store each calculation in their respective variable. # TODO: Minimum price of the data minimum_price = np . min(prices) # TODO: Maximum price of the data maximum_price = np . max(prices) # TODO: Mean price of the data mean_price = np . mean(prices) # TODO: Median price of the data median_price = np . median(prices) # Extra: First abd Third Quartile first_quartile = np . percentile(prices, 25 ) third_quartile = np . percentile(prices, 75 ) # TODO: Standard deviation of prices of the data std_price = np . std(prices) # Show the calculated statistics print ( \"Statistics for Boston housing dataset: \\n \" ) print ( \"Minimum price: ${:,.2f}\" . format(minimum_price)) print ( \"Maximum price: ${:,.2f}\" . format(maximum_price)) print ( \"Mean price: ${:,.2f}\" . format(mean_price)) print ( \"Median price ${:,.2f}\" . format(median_price)) print ( \"First Quartile: ${:,.2f}\" . format(first_quartile )) print ( \"Third Quartile: ${:,.2f}\" . format(third_quartile )) print ( \"Standard deviation of prices: ${:,.2f}\" . format(std_price)) Statistics for Boston housing dataset: Minimum price: $105,000.00 Maximum price: $1,024,800.00 Mean price: $454,342.94 Median price $438,900.00 First Quartile: $350,700.00 Third Quartile: $518,700.00 Standard deviation of prices: $165,171.13 I love visualizing them import matplotlib.pyplot as plt plt . figure(figsize = ( 8 , 6 )) plt . hist(prices, bins = 20 , color = 'g' ) plt . axvline(mean_price, lw = 5 , c = 'b' , label = 'mean' ) plt . axvline(median_price, lw = 5 , c = 'y' , label = 'median' ) plt . axvline(first_quartile, lw = 5 , c = 'r' ,label = 'first_quartile' ) plt . axvline(third_quartile, lw = 5 , c = 'm' , label = 'third_quartile' ) plt . legend(bbox_to_anchor = ( 1.05 , 0.05 ),loc = 'lower left' ,borderaxespad = 0. ) &lt;matplotlib.legend.Legend at 0x11b06b400&gt;","title":"Implementation: Calculate Statistics"},{"location":"ml/boston_housing/boston_housing/#question-1-feature-observation","text":"As a reminder, we are using three features from the Boston housing dataset: 'RM' , 'LSTAT' , and 'PTRATIO' . For each data point (neighborhood): - 'RM' is the average number of rooms among homes in the neighborhood. - 'LSTAT' is the percentage of homeowners in the neighborhood considered \"lower class\" (working poor). - 'PTRATIO' is the ratio of students to teachers in primary and secondary schools in the neighborhood. Using your intuition, for each of the three features above, do you think that an increase in the value of that feature would lead to an increase in the value of 'MEDV' or a decrease in the value of 'MEDV' ? Justify your answer for each. Hint: Would you expect a home that has an 'RM' value of 6 be worth more or less than a home that has an 'RM' value of 7? Answer: Increase in the value of a features sometime cross the limit for increase in the value of MEDV . Let me go one by one. House with large RM (more rooms) is in general more likely to cost more for the larger space. But small houses with many rooms may not impose same type of contribution on decision making. High LSTAT means the house has a poor neighborhood and likely to reduce the value of MEDV . This is because houses in rich neighborhood is correlated with high standard of self paied local security network, transportation and other extra infrastructure. Low PTRATIO implies that the community has rich education resources. This will raise the value of the hosue. But this is correlated with the family members and chance of having baby in near future. Checking my intution import matplotlib.pyplot as plt plt . figure(figsize = ( 20 , 5 )) for i, col in enumerate (features . columns): plt . subplot( 1 , 4 , i + 1 ) plt . plot(data[col], prices, 'o' ) plt . title(col) plt . xlabel(col) plt . ylabel( 'prices' ) Plot 1 and 2 are making a lot of sense. Plot 3 is little complicated.","title":"Question 1 - Feature Observation"},{"location":"ml/boston_housing/boston_housing/#part-ii","text":"","title":"Part- II"},{"location":"ml/boston_housing/boston_housing/#developing-a-model","text":"In this second section of the project, you will develop the tools and techniques necessary for a model to make a prediction. Being able to make accurate evaluations of each model's performance through the use of these tools and techniques helps to greatly reinforce the confidence in your predictions.","title":"Developing a Model"},{"location":"ml/boston_housing/boston_housing/#implementation-define-a-performance-metric","text":"It is difficult to measure the quality of a given model without quantifying its performance over training and testing. This is typically done using some type of performance metric, whether it is through calculating some type of error, the goodness of fit, or some other useful measurement. For this project, you will be calculating the coefficient of determination , R 2 , to quantify your model's performance. The coefficient of determination for a model is a useful statistic in regression analysis, as it often describes how \"good\" that model is at making predictions. The values for R 2 range from 0 to 1, which captures the percentage of squared correlation between the predicted and actual values of the target variable . A model with an R 2 of 0 is no better than a model that always predicts the mean of the target variable, whereas a model with an R 2 of 1 perfectly predicts the target variable. Any value between 0 and 1 indicates what percentage of the target variable, using this model, can be explained by the features . A model can be given a negative R 2 as well, which indicates that the model is arbitrarily worse than one that always predicts the mean of the target variable. For the performance_metric function in the code cell below, you will need to implement the following: - Use r2_score from sklearn.metrics to perform a performance calculation between y_true and y_predict . - Assign the performance score to the score variable. # Import 'r2_score' from sklearn.metrics import r2_score def performance_metric (y_true, y_predict): \"\"\" Calculates and returns the performance score between true and predicted values based on the metric chosen. \"\"\" #Calculate the performance score between 'y_true' and 'y_predict' score = r2_score(y_true, y_predict) # Return the score return score","title":"Implementation: Define a Performance Metric"},{"location":"ml/boston_housing/boston_housing/#question-2-goodness-of-fit","text":"Assume that a dataset contains five data points and a model made the following predictions for the target variable: True Value Prediction 3.0 2.5 -0.5 0.0 2.0 2.1 7.0 7.8 4.2 5.3 Would you consider this model to have successfully captured the variation of the target variable? Why or why not? Run the code cell below to use the performance_metric function and calculate this model's coefficient of determination. # Calculate the performance of this model score = performance_metric([ 3 , - 0.5 , 2 , 7 , 4.2 ], [ 2.5 , 0.0 , 2.1 , 7.8 , 5.3 ]) print ( \"Model has a coefficient of determination, R^2, of {:.3f}.\" . format(score)) Model has a coefficient of determination, R^2, of 0.923. Answer: Would you consider this model to have successfully captured the variation of the target variable? Yes, this model have successfully captured the variation of the target variable. Since r2_score is 0.923, this implies that 92.3% of the variables are predictable. Why or why not? The r2_score provides a measure of how well future samples are likely to be predicted by the model compared to the case where all predictions are replaced by mean value of target variable. First of all, the meaning of different values of r2_score are : $ \\huge{R^{2} (y,\\hat{y}) = 1 - \\frac{\\sum {i=0}^{n {samples} -1} (y_{i} - \\hat{y} {i})^{2}}{\\sum {i=0}^{n_{samples} -1} (y_{i} - \\bar{y})^{2}}}$ Where $\\hat{y}$ = prediction, $\\bar{y}$ = mean value Special case: $R^{2} =0$, In this case model predictions are equivalent to all predictions replaced by mean value. Special case: $R^{2} =1$, In this case model prediction is equal to the exact value. This is an ideal case and may lead us to overfitting. Special case: $R^{2} < 0$, In this case model predictions are more worst and could not even exceed the score if we roughly predict all value to mean value. Thus, the value of $ R^{2}$ closer to 1 is the best value! Reference used: http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score","title":"Question 2 - Goodness of Fit"},{"location":"ml/boston_housing/boston_housing/#implementation-shuffle-and-split-data","text":"Your next implementation requires that you take the Boston housing dataset and split the data into training and testing subsets. Typically, the data is also shuffled into a random order when creating the training and testing subsets to remove any bias in the ordering of the dataset. For the code cell below, you will need to implement the following: - Use train_test_split from sklearn.cross_validation to shuffle and split the features and prices data into training and testing sets. - Split the data into 80% training and 20% testing. - Set the random_state for train_test_split to a value of your choice. This ensures results are consistent. - Assign the train and testing splits to X_train , X_test , y_train , and y_test . # Import 'train_test_split' from sklearn.cross_validation import train_test_split # Shuffle and split the data into training and testing subsets X_train, X_test, y_train, y_test = train_test_split(features, prices, test_size = 0.20 , random_state = 31 ) # Success print ( \"Training and testing split was successful.\" ) Training and testing split was successful.","title":"Implementation: Shuffle and Split Data"},{"location":"ml/boston_housing/boston_housing/#question-3-training-and-testing","text":"What is the benefit to splitting a dataset into some ratio of training and testing subsets for a learning algorithm? Hint: What could go wrong with not having a way to test your model? Answer: Theoretically, there could be infinite data and the best model with 99.99% accuracy disregarding the computational cost and time. Practically, this is not feasible. For a given set of data, there exist infinitely many models withinh the certain range of accuricy. Our intention is to find that best model which will have less and less error based on computational cost. If we use whole data as training data in the search of the best model, we will hit that model which will predict all data in traing set with high accuracy but we are left with nothing to check our model. We will be unknown with the accuracy of the model with the data which exist out there in the Universal set(which has infinite data where training data is only a subset of it). In short, test set being independent from training set, helps us better evluate the algorithms by detecting overfitting or underfitting in training subsets.","title":"Question 3 - Training and Testing"},{"location":"ml/boston_housing/boston_housing/#analyzing-model-performance","text":"In this third section of the project, you'll take a look at several models' learning and testing performances on various subsets of training data. Additionally, you'll investigate one particular algorithm with an increasing 'max_depth' parameter on the full training set to observe how model complexity affects performance. Graphing your model's performance based on varying criteria can be beneficial in the analysis process, such as visualizing behavior that may not have been apparent from the results alone.","title":"Analyzing Model Performance"},{"location":"ml/boston_housing/boston_housing/#learning-curves","text":"The following code cell produces four graphs for a decision tree model with different maximum depths. Each graph visualizes the learning curves of the model for both training and testing as the size of the training set is increased. Note that the shaded region of a learning curve denotes the uncertainty of that curve (measured as the standard deviation). The model is scored on both the training and testing sets using R 2 , the coefficient of determination. Run the code cell below and use these graphs to answer the following question. # Produce learning curves for varying training set sizes and maximum depths vs . ModelLearning(features, prices)","title":"Learning Curves"},{"location":"ml/boston_housing/boston_housing/#question-4-learning-the-data","text":"Choose one of the graphs above and state the maximum depth for the model. What happens to the score of the training curve as more training points are added? What about the testing curve? Would having more training points benefit the model? Hint: Are the learning curves converging to particular scores? Answer: By looking gap between training and testing curves, as max_depth goes from 1 to 10 , this clearly indicates that algorithm goes from underfitting to overfitting. Graph with max_depth = 3 seems to be the best fit in between. So my analysis goes for max_depth = 3 . What happens to the score of the training curve as more training points are added? Adding more training points may reduce the noise in training score plot and it will be more smooth and almost horizontal after crossing certain limit along x-axis(Number of Training Points). For example at max_depth =3 , as training points increase, it does not suffer from overfitting or underfitting, so adding more training points will not affect the trend of convergence. What about the testing curve? It will behave similar to training curves. Would having more training points benefit the model? Eventhough training curve becomes smooth on increasing training data, it is evident form the convergence of training and testing curves that after certain number of training points it may not have much siginificance to improve accuracy with respect to computation cost.","title":"Question 4 - Learning the Data"},{"location":"ml/boston_housing/boston_housing/#complexity-curves","text":"The following code cell produces a graph for a decision tree model that has been trained and validated on the training data using different maximum depths. The graph produces two complexity curves \u2014 one for training and one for validation. Similar to the learning curves , the shaded regions of both the complexity curves denote the uncertainty in those curves, and the model is scored on both the training and validation sets using the performance_metric function. Run the code cell below and use this graph to answer the following two questions. vs . ModelComplexity(X_train, y_train)","title":"Complexity Curves"},{"location":"ml/boston_housing/boston_housing/#question-5-bias-variance-tradeoff","text":"When the model is trained with a maximum depth of 1, does the model suffer from high bias or from high variance? How about when the model is trained with a maximum depth of 10? What visual cues in the graph justify your conclusions? Hint: How do you know when a model is suffering from high bias or high variance? Answer: When the model is trained with a maximum depth of 1, does the model suffer from high bias or from high variance? Yes, it suffers from high bias. First of all the validation score and training score both are lower than desired value (~0.8). Sconldly, both curve are close to each other on score value which indicates there is underfitting. How about when the model is trained with a maximum depth of 10? Now, it suffers from high variance. The gap between validation score and training score clearly indicates there is overfitting. What visual cues in the graph justify your conclusions? Comparision with desired values (>0.8) and gap between training and testing curves are the important cues for the conclusion.","title":"Question 5 - Bias-Variance Tradeoff"},{"location":"ml/boston_housing/boston_housing/#question-6-best-guess-optimal-model","text":"Which maximum depth do you think results in a model that best generalizes to unseen data? What intuition lead you to this answer? Answer: The best maximum depth seems to be 3. Both training curve and validation curve reach maximum value of score with no sign of overfitting or underfitting.","title":"Question 6 - Best-Guess Optimal Model"},{"location":"ml/boston_housing/boston_housing/#part-iii","text":"","title":"Part-III"},{"location":"ml/boston_housing/boston_housing/#evaluating-model-performance","text":"In this final section of the project, you will construct a model and make a prediction on the client's feature set using an optimized model from fit_model .","title":"Evaluating Model Performance"},{"location":"ml/boston_housing/boston_housing/#question-7-grid-search","text":"What is the grid search technique and how it can be applied to optimize a learning algorithm? Answer: Grid search is an exhaustive search algorithm which searches over all combinations of parameters we specify to find the optimum combination that yields the best performance. Due to its exhaustive search nature, grid search can be computationally expensive, especially when data size is large and model is complicated. Sometimes we resort to randomized search in this case to search only some combinations of the parameters. Following are the detail of two such algorithms: Algorithm Detail Implementation Difference GridSearchCV GridSearchCV implements with exhaustive search over specified parameter values for an estimator. The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid. fit , score , predict , predict_proba , decision_function , transform and inverse_transform In GridSearchCV , all parameter values are tried out. RandomizedSearchCV RandomizedSearchCV implements with Randomized search on hyper parameters. The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings. fit , score , predict , predict_proba , decision_function , transform and inverse_transform In contrast to GridSearchCV , not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter . References used to answer: 1. http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV 2. http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV","title":"Question 7 - Grid Search"},{"location":"ml/boston_housing/boston_housing/#question-8-cross-validation","text":"What is the k-fold cross-validation training technique? What benefit does this technique provide for grid search when optimizing a model? Hint: Much like the reasoning behind having a testing set, what could go wrong with using grid search without a cross-validated set? Answer: What is the k-fold cross-validation training technique? Practically, this is a method to extract the data resources from multiple direction to insure that the model trained is robust over any possible noise. Technically, it is achived by taking all data as training sets and testing sets via roation and averaging. This includes the following steps: Step-I: Data is divided into k folds of eqaully large chunks. Step-II: Iterated through the k chunks using the current chunk for validation and other k-1 chunks for training. Step -III: Average is taken over the validation scores to give a single validation score for the learning algorithm. What benefit does this technique provide for grid search when optimizing a model? Using all the examples in the dataset for both training and testing is the main advantage of k-fold cross-validation. This makes it more robust to variations of parameters in grid search. Grid search may overfit the validation set if certain evaluation data points are outliers. It can be eliminated by averaging over k-times evaluations using all data points.","title":"Question 8 - Cross-Validation"},{"location":"ml/boston_housing/boston_housing/#implementation-fitting-a-model","text":"Your final implementation requires that you bring everything together and train a model using the decision tree algorithm . To ensure that you are producing an optimized model, you will train the model using the grid search technique to optimize the 'max_depth' parameter for the decision tree. The 'max_depth' parameter can be thought of as how many questions the decision tree algorithm is allowed to ask about the data before making a prediction. Decision trees are part of a class of algorithms called supervised learning algorithms . In addition, you will find your implementation is using ShuffleSplit() for an alternative form of cross-validation (see the 'cv_sets' variable). While it is not the K-Fold cross-validation technique you describe in Question 8 , this type of cross-validation technique is just as useful!. The ShuffleSplit() implementation below will create 10 ( 'n_iter' ) shuffled sets, and for each shuffle, 20% ( 'test_size' ) of the data will be used as the validation set . While you're working on your implementation, think about the contrasts and similarities it has to the K-fold cross-validation technique. For the fit_model function in the code cell below, you will need to implement the following: - Use DecisionTreeRegressor from sklearn.tree to create a decision tree regressor object. - Assign this object to the 'regressor' variable. - Create a dictionary for 'max_depth' with the values from 1 to 10, and assign this to the 'params' variable. - Use make_scorer from sklearn.metrics to create a scoring function object. - Pass the performance_metric function as a parameter to the object. - Assign this scoring function to the 'scoring_fnc' variable. - Use GridSearchCV from sklearn.grid_search to create a grid search object. - Pass the variables 'regressor' , 'params' , 'scoring_fnc' , and 'cv_sets' as parameters to the object. - Assign the GridSearchCV object to the 'grid' variable. # Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV' from sklearn.metrics import make_scorer from sklearn.tree import DecisionTreeRegressor from sklearn.grid_search import GridSearchCV def fit_model (X, y): \"\"\" Performs grid search over the 'max_depth' parameter for a decision tree regressor trained on the input data [X, y]. \"\"\" # Create cross-validation sets from the training data cv_sets = ShuffleSplit(X . shape[ 0 ], n_iter = 10 , test_size = 0.20 , random_state = 0 ) # Create a decision tree regressor object regressor = DecisionTreeRegressor() # Create a dictionary for the parameter 'max_depth' with a range from 1 to 10 params = { 'max_depth' :( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 )} # Transform 'performance_metric' into a scoring function using 'make_scorer' scoring_fnc = make_scorer(performance_metric) # Create the grid search object grid = GridSearchCV(regressor, params,scoring_fnc,cv = cv_sets) # Fit the grid search object to the data to compute the optimal model grid = grid . fit(X, y) # Return the optimal model after fitting the data return grid . best_estimator_","title":"Implementation: Fitting a Model"},{"location":"ml/boston_housing/boston_housing/#making-predictions","text":"Once a model has been trained on a given set of data, it can now be used to make predictions on new sets of input data. In the case of a decision tree regressor , the model has learned what the best questions to ask about the input data are , and can respond with a prediction for the target variable . You can use these predictions to gain information about data where the value of the target variable is unknown \u2014 such as data the model was not trained on.","title":"Making Predictions"},{"location":"ml/boston_housing/boston_housing/#question-9-optimal-model","text":"What maximum depth does the optimal model have? How does this result compare to your guess in Question 6 ? Run the code block below to fit the decision tree regressor to the training data and produce an optimal model. # Fit the training data to the model using grid search reg = fit_model(X_train, y_train) # Produce the value for 'max_depth' print ( \"Parameter 'max_depth' is {} for the optimal model.\" \\ . format(reg . get_params()[ 'max_depth' ])) Parameter 'max_depth' is 3 for the optimal model.","title":"Question 9 - Optimal Model"},{"location":"ml/boston_housing/boston_housing/#answer","text":"The optimal model has max_depth=4 , it is close to the result max_depth=3 in Question 6 . This difference could be as a result of different data used for in the graphical representation and the grid search. Arguably grid search should be more reliable. Making life less complicated by following - Occam's Razor principle(https://simple.wikipedia.org/wiki/Occam's_razor), which favours a simpler model to more complicated ones. :)","title":"Answer:"},{"location":"ml/boston_housing/boston_housing/#question-10-predicting-selling-prices","text":"Imagine that you were a real estate agent in the Boston area looking to use this model to help price homes owned by your clients that they wish to sell. You have collected the following information from three of your clients: Feature Client 1 Client 2 Client 3 Total number of rooms in home 5 rooms 4 rooms 8 rooms Neighborhood poverty level (as %) 17% 32% 3% Student-teacher ratio of nearby schools 15-to-1 22-to-1 12-to-1 What price would you recommend each client sell his/her home at? Do these prices seem reasonable given the values for the respective features? Hint: Use the statistics you calculated in the Data Exploration section to help justify your response. Run the code block below to have your optimized model make predictions for each client's home. # Produce a matrix for client data client_data = [[ 5 , 17 , 15 ], # Client 1 [ 4 , 32 , 22 ], # Client 2 [ 8 , 3 , 12 ]] # Client 3 # Show predictions for i, price in enumerate (reg . predict(client_data)): print ( \"Predicted selling price for Client {}'s home: ${:,.2f}\" . format(i + 1 , price)) Predicted selling price for Client 1's home: $369,680.77 Predicted selling price for Client 2's home: $274,568.97 Predicted selling price for Client 3's home: $903,150.00 Answer: 1. What price would you recommend each client sell his/her home at? Following are the prices recomended: Predicted selling price for Client 1's home: $410,792.31 Predicted selling price for Client 2's home: $234,054.55 Predicted selling price for Client 3's home: $920,010.00 Do these prices seem reasonable given the values for the respective features? Yes. The above prices are for average house(~410k), cheap house(~234k), and costy house(~920k) respectively by their deviation from mean_price measured in std_price.This is evident from histogram plot of prices below. The predicated prices are reasonable considering dataset statistics and our observation in Question 1 . The house price is likely to be in proportion to RM , and in inverse proportion to LSTAT or PTRATIO . As an example, for client 2 the deviations of three features all decrease the house price based on assumpution in Question 1 which cause it be cheaper than average house. import matplotlib.pyplot as plt plt . figure(figsize = ( 8 , 6 )) plt . hist(prices, bins = 20 ) mean_price = np . mean(prices) plt . axvline(mean_price, lw = 5 , c = 'g' ,label = \"mean\" ) for price in reg . predict(client_data): plt . axvline(price, lw = 5 , c = 'r' , label = \"client\" ) plt . legend(bbox_to_anchor = ( 1.05 , 0.05 ),loc = 'lower left' ,borderaxespad = 0. ) &lt;matplotlib.legend.Legend at 0x11bf42ba8&gt;","title":"Question 10 - Predicting Selling Prices"},{"location":"ml/boston_housing/boston_housing/#sensitivity","text":"An optimal model is not necessarily a robust model. Sometimes, a model is either too complex or too simple to sufficiently generalize to new data. Sometimes, a model could use a learning algorithm that is not appropriate for the structure of the data given. Other times, the data itself could be too noisy or contain too few samples to allow a model to adequately capture the target variable \u2014 i.e., the model is underfitted. Run the code cell below to run the fit_model function ten times with different training and testing sets to see how the prediction for a specific client changes with the data it's trained on. vs . PredictTrials(features, prices, fit_model, client_data) Trial 1: $391,183.33 Trial 2: $419,700.00 Trial 3: $415,800.00 Trial 4: $420,622.22 Trial 5: $418,377.27 Trial 6: $411,931.58 Trial 7: $399,663.16 Trial 8: $407,232.00 Trial 9: $351,577.61 Trial 10: $413,700.00 Range in prices: $69,044.61 The standard deviation of prices is 165,171.13 and range of price predicted is 69,044.61. This shows that the price predicted is consistent within 1 stadard deviation.","title":"Sensitivity"},{"location":"ml/boston_housing/boston_housing/#question-11-applicability","text":"In a few sentences, discuss whether the constructed model should or should not be used in a real-world setting. Hint: Some questions to answering: - How relevant today is data that was collected from 1978? - Are the features present in the data sufficient to describe a home? - Is the model robust enough to make consistent predictions? - Would data collected in an urban city like Boston be applicable in a rural city? Answer: The constructed model should not be used directly in a real-world setting. Following are the potential reasons: This data is from 1978.So, this model can not capture the present day scenario and can be biased with time changing. Dataset has a small number of samples. This may cause the model not to be robust enough to make consistent predictions. The features such as location,security, transportation, population etc are also likly to affect the house price. The features in this dataset may be insufficient to avoid underfitting, apart from room numbers, local incoming and education resources. There could be different models in different areas such as urban city or rural city. Newly built smart homes or IOT add technology may add new features not seen before in house pricing. Note : Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to File -> Download as -> HTML (.html) . Include the finished document along with this notebook as your submission.","title":"Question 11 - Applicability"},{"location":"ml/finding_donors/finding_donors/","text":"Machine Learning Engineer Nanodegree Supervised Learning Project: Finding Donors for CharityML Welcome to the second project of the Machine Learning Engineer Nanodegree! In this notebook, some template code has already been provided for you, and it will be your job to implement the additional functionality necessary to successfully complete this project. Sections that begin with 'Implementation' in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a 'Question X' header. Carefully read each question and provide thorough answers in the following text boxes that begin with 'Answer:' . Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide. Note: Code and Markdown cells can be executed using the Shift + Enter keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode. Getting Started In this project, you will employ several supervised algorithms of your choice to accurately model individuals' income using data collected from the 1994 U.S. Census. You will then choose the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data. Your goal with this implementation is to construct a model that accurately predicts whether an individual makes more than $50,000. This sort of task can arise in a non-profit setting, where organizations survive on donations. Understanding an individual's income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with. While it can be difficult to determine an individual's general income bracket directly from public sources, we can (as we will see) infer this value from other publically available features. The dataset for this project originates from the UCI Machine Learning Repository . The datset was donated by Ron Kohavi and Barry Becker, after being published in the article \"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid\" . You can find the article by Ron Kohavi online . The data we investigate here consists of small changes to the original dataset, such as removing the 'fnlwgt' feature and records with missing or ill-formatted entries. Exploring the Data Run the code cell below to load necessary Python libraries and load the census data. Note that the last column from this dataset, 'income' , will be our target label (whether an individual makes more than, or at most, $50,000 annually). All other columns are features about each individual in the census database. # Import libraries necessary for this project import numpy as np import pandas as pd from time import time from IPython.display import display # Allows the use of display() for DataFrames # Import supplementary visualization code visuals.py import visuals as vs # Pretty display for notebooks % matplotlib inline # Load the Census dataset data = pd . read_csv( \"census.csv\" ) # Success - Display the first record display(data . head(n = 2 )) print ( 'shape of data =' , data . shape) age workclass education_level education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country income 0 39 State-gov Bachelors 13.0 Never-married Adm-clerical Not-in-family White Male 2174.0 0.0 40.0 United-States <=50K 1 50 Self-emp-not-inc Bachelors 13.0 Married-civ-spouse Exec-managerial Husband White Male 0.0 0.0 13.0 United-States <=50K ('shape of data =', (45222, 14)) Data info data . info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 45222 entries, 0 to 45221 Data columns (total 14 columns): age 45222 non-null int64 workclass 45222 non-null object education_level 45222 non-null object education-num 45222 non-null float64 marital-status 45222 non-null object occupation 45222 non-null object relationship 45222 non-null object race 45222 non-null object sex 45222 non-null object capital-gain 45222 non-null float64 capital-loss 45222 non-null float64 hours-per-week 45222 non-null float64 native-country 45222 non-null object income 45222 non-null object dtypes: float64(4), int64(1), object(9) memory usage: 4.8+ MB data . describe() age education-num capital-gain capital-loss hours-per-week count 45222.000000 45222.000000 45222.000000 45222.000000 45222.000000 mean 38.547941 10.118460 1101.430344 88.595418 40.938017 std 13.217870 2.552881 7506.430084 404.956092 12.007508 min 17.000000 1.000000 0.000000 0.000000 1.000000 25% 28.000000 9.000000 0.000000 0.000000 40.000000 50% 37.000000 10.000000 0.000000 0.000000 40.000000 75% 47.000000 13.000000 0.000000 0.000000 45.000000 max 90.000000 16.000000 99999.000000 4356.000000 99.000000 Histogram import matplotlib.pyplot as plt plt . figure(figsize = ( 14 , 10 )) plt . subplot( 2 , 3 , 1 ) plt . title( \"distribution of age\" ) data[ 'age' ] . hist(bins = 100 ) plt . subplot( 2 , 3 , 2 ) plt . title( \"distribution of education-num\" ) data[ 'education-num' ] . hist(bins = 40 ) plt . subplot( 2 , 3 , 3 ) plt . title( \"distribution of capital-gain\" ) data[ 'capital-gain' ] . hist(bins = 100 ) plt . subplot( 2 , 3 , 4 ) plt . title( \"distribution of hours-per-week\" ) data[ 'hours-per-week' ] . hist(bins = 50 ) plt . subplot( 2 , 3 , 5 ) plt . title( \"distribution of capital-loss\" ) data[ 'capital-loss' ] . hist(bins = 50 ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x13ab99890&gt; Pairplot: Lets look little more insight about dependency or independency of income with other variables like age, hours per week etc. import seaborn as sns plt . figure(figsize = ( 16 , 21 )) sns . set() sns . pairplot(data, hue = \"income\" ) &lt;seaborn.axisgrid.PairGrid at 0x13bb2b290&gt; &lt;matplotlib.figure.Figure at 0x13b569490&gt; It shows high earning is in general for older age, large education number and hours per week etc. This might be because of knowledge and experience. Implementation: Data Exploration A cursory investigation of the dataset will determine how many individuals fit into either group, and will tell us about the percentage of these individuals making more than \\$50,000. In the code cell below, you will need to compute the following: - The total number of records, 'n_records' - The number of individuals making more than \\$50,000 annually, 'n_greater_50k' . - The number of individuals making at most \\$50,000 annually, 'n_at_most_50k' . - The percentage of individuals making more than \\$50,000 annually, 'greater_percent' . Hint: You may need to look at the table above to understand how the 'income' entries are formatted. # Total number of records n_records = data . shape[ 0 ] # Number of records where individual's income is more than $50,000 n_greater_50k = data[(data . income == '>50K' )] . shape[ 0 ] # Number of records where individual's income is at most $50,000 n_at_most_50k = data[(data . income == '<=50K' )] . shape[ 0 ] # Percentage of individuals whose income is more than $50,000 greater_percent = (n_greater_50k / float (n_records)) * 100.0 # Print the results print ( \"Total number of records: {}\" . format(n_records)) print ( \"Individuals making more than $50,000: {}\" . format(n_greater_50k)) print ( \"Individuals making at most $50,000: {}\" . format(n_at_most_50k)) print ( \"Percentage of individuals making more than $50,000: {:.2f}%\" . format(greater_percent)) Total number of records: 45222 Individuals making more than $50,000: 11208 Individuals making at most $50,000: 34014 Percentage of individuals making more than $50,000: 24.78% I love to plot them in pi-chart. import matplotlib.pyplot as plt plt . figure(figsize = ( 4 , 4 )) labels = 'High Earning' , 'Low Earning' sizes = [n_greater_50k, n_at_most_50k] colors = [ 'yellowgreen' , 'gold' ] explode = ( 0 , 0.1 ) plt . pie(sizes, explode = explode, labels = labels, colors = colors, autopct = ' %1.1f%% ' , shadow = True , startangle = 90 ) plt . axis( 'equal' ) plt . title( 'Income' ) plt . show() 1. Income versus merital status and sex: Lets look the distribution of count over different merital status and sex: sns . countplot(y = \"marital-status\" , hue = \"income\" , data = data, palette = \"Greens_d\" ); Plot below shows the count of individuals making less than or more than 50K, with a breakdown by the features sex and marital-status. import seaborn as sns plt . figure(figsize = ( 16 , 21 )) sns . set(style = \"whitegrid\" , color_codes = True ) sns . factorplot( \"sex\" , col = \"marital-status\" , data = data, hue = 'income' , kind = \"count\" , col_wrap = 2 ); &lt;matplotlib.figure.Figure at 0x13f1333d0&gt; Naturally, married families are more stable, happy and show more in count and also earn much! 2. Income versus age and hours per week I am pretty much interested to see income with respect to age plt . figure(figsize = ( 15 , 21 )) plt . subplot( 1 , 2 , 1 ) sns . countplot(y = \"age\" , hue = \"income\" , data = data, palette = \"Greens_d\" ); plt . subplot( 1 , 2 , 2 ) sns . countplot(y = \"hours-per-week\" , hue = \"income\" , data = data, palette = \"Greens_d\" ); Wow! Look at the profile of count distribution over age, young generation are going crazy. Large no of young people are working hard to earn less money. They should! Because they have to learn. If you want to earn more, you need to wait patiently and keep learning skills till 40. :) :) From hours per week count, why is that 40 hours so much special? This data is little bit biased. We prefer to say 10, 15, 20, 25 etc. But do not bother to say exact value like 13.5 hours or 21 hours. Working place, office etc set working hours like 10 hrs, 20 hrs etc thats why people say like that. I have a question to my reviewer: Should I add some normal noise to this data(hours per week) during preprocessing to get more robust model? Is there any relation of age with working hours? Lets see it below. plt . figure(figsize = ( 15 , 21 )) sns . jointplot(x = \"age\" , y = \"hours-per-week\" , data = data,size = 15 ,kind = 'reg' ); &lt;matplotlib.figure.Figure at 0x1459e1410&gt; I see as the age grows people work more hours, (Do they sleep less ? or take no rest? no idea! There is not data for sleeping habit). Happyness and calmness definitely play a role to decide for donation! 3. Income versus education number and eduction level First lets see how are these two related. plt . figure(figsize = ( 10 , 10 )) sns . barplot(x = \"education-num\" , y = \"education_level\" , data = data); Our guess came true. Doctorate has highest education number. Second heighest is Prof-school and pre school is the lowest one. Lets see distribution of count of income over different education level and education number. plt . figure(figsize = ( 16 , 16 )) plt . subplot( 1 , 2 , 1 ) sns . countplot(y = \"education-num\" , hue = \"income\" , data = data, palette = \"Greens_d\" ); plt . subplot( 1 , 2 , 2 ) sns . countplot(y = \"education_level\" , hue = \"income\" , data = data, palette = \"Greens_d\" ); WOw! Look at that HS-grad which has education number = 9 . These are the majority of the total population. Interestingly Bachelors (education num = 13) have heighest count on earning much compaired to others! I am going to skip two more features (capital- gain and capital-loss). Which are being discussed in Data Preparation topics Preparing the Data Before data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured \u2014 this is typically known as preprocessing . Fortunately, for this dataset, there are no invalid or missing entries we must deal with, however, there are some qualities about certain features that must be adjusted. This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms. Transforming Skewed Continuous Features A dataset may sometimes contain at least one feature whose values tend to lie near a single number, but will also have a non-trivial number of vastly larger or smaller values than that single number. Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized. With the census dataset two features fit this description: ' capital-gain' and 'capital-loss' . Run the code cell below to plot a histogram of these two features. Note the range of the values present and how they are distributed. # Split the data into features and target label income_raw = data[ 'income' ] features_raw = data . drop( 'income' , axis = 1 ) # Visualize skewed continuous features of original data vs . distribution(data) For highly-skewed feature distributions such as 'capital-gain' and 'capital-loss' , it is common practice to apply a logarithmic transformation on the data so that the very large and very small values do not negatively affect the performance of a learning algorithm. Using a logarithmic transformation significantly reduces the range of values caused by outliers. Care must be taken when applying this transformation however: The logarithm of 0 is undefined, so we must translate the values by a small amount above 0 to apply the the logarithm successfully. Run the code cell below to perform a transformation on the data and visualize the results. Again, note the range of values and how they are distributed. # Log-transform the skewed features skewed = [ 'capital-gain' , 'capital-loss' ] features_raw[skewed] = data[skewed] . apply( lambda x: np . log(x + 1 )) # Visualize the new log distributions vs . distribution(features_raw, transformed = True ) Normalizing Numerical Features In addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each feature's distribution (such as 'capital-gain' or 'capital-loss' above); however, normalization ensures that each feature is treated equally when applying supervised learners. Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning, as exampled below. Run the code cell below to normalize each numerical feature. We will use sklearn.preprocessing.MinMaxScaler for this. # Import sklearn.preprocessing.StandardScaler from sklearn.preprocessing import MinMaxScaler # Initialize a scaler, then apply it to the features scaler = MinMaxScaler() numerical = [ 'age' , 'education-num' , 'capital-gain' , 'capital-loss' , 'hours-per-week' ] features_raw[numerical] = scaler . fit_transform(data[numerical]) # Show an example of a record with scaling applied display(features_raw . head(n = 2 )) age workclass education_level education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country 0 0.301370 State-gov Bachelors 0.8 Never-married Adm-clerical Not-in-family White Male 0.02174 0.0 0.397959 United-States 1 0.452055 Self-emp-not-inc Bachelors 0.8 Married-civ-spouse Exec-managerial Husband White Male 0.00000 0.0 0.122449 United-States Implementation: Data Preprocessing From the table in Exploring the Data above, we can see there are several features for each record that are non-numeric. Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called categorical variables ) be converted. One popular way to convert categorical variables is by using the one-hot encoding scheme. One-hot encoding creates a \"dummy\" variable for each possible category of each non-numeric feature. For example, assume someFeature has three possible entries: A , B , or C . We then encode this feature into someFeature_A , someFeature_B and someFeature_C . someFeature someFeature_A someFeature_B someFeature_C 0 B 0 1 0 1 C ----> one-hot encode ----> 0 0 1 2 A 1 0 0 Additionally, as with the non-numeric features, we need to convert the non-numeric target label, 'income' to numerical values for the learning algorithm to work. Since there are only two possible categories for this label (\"<=50K\" and \">50K\"), we can avoid using one-hot encoding and simply encode these two categories as 0 and 1 , respectively. In code cell below, you will need to implement the following: - Use pandas.get_dummies() to perform one-hot encoding on the 'features_raw' data. - Convert the target label 'income_raw' to numerical entries. - Set records with \"<=50K\" to 0 and records with \">50K\" to 1 . # One-hot encode the 'features_raw' data using pandas.get_dummies() features = pd . get_dummies(features_raw) # Encode the 'income_raw' data to numerical values income = income_raw . apply( lambda x: 1 if x == '>50K' else 0 ) # Alternate method # pd.get_dummies(income_raw)['>50K'] # pd.get_dummies(income_raw, drop_first=True) # Print the number of features after one-hot encoding encoded = list (features . columns) print ( \"{} total features after one-hot encoding.\" . format( len (encoded))) # Uncomment the following line to see the encoded feature names #print encoded 103 total features after one-hot encoding. features . shape (45222, 103) Shuffle and Split Data Now all categorical variables have been converted into numerical features, and all numerical features have been normalized. As always, we will now split the data (both features and their labels) into training and test sets. 80% of the data will be used for training and 20% for testing. Run the code cell below to perform this split. # Import train_test_split from sklearn.model_selection import train_test_split from sklearn.utils import shuffle #shuffle the data features, income = shuffle(features, income) # Split the 'features' and 'income' data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(features, income, test_size = 0.2 , random_state = 0 ) # Show the results of the split print ( \"Training set has {} samples.\" . format(X_train . shape[ 0 ])) print ( \"Testing set has {} samples.\" . format(X_test . shape[ 0 ])) Training set has 36177 samples. Testing set has 9045 samples. Evaluating Model Performance In this section, we will investigate four different algorithms, and determine which is best at modeling the data. Three of these algorithms will be supervised learners of your choice, and the fourth algorithm is known as a naive predictor . Metrics and the Naive Predictor CharityML , equipped with their research, knows individuals that make more than \\$50,000 are most likely to donate to their charity. Because of this, CharityML is particularly interested in predicting who makes more than \\$50,000 accurately. It would seem that using accuracy as a metric for evaluating a particular model's performace would be appropriate. Additionally, identifying someone that does not make more than \\$50,000 as someone who does would be detrimental to CharityML , since they are looking to find individuals willing to donate. Therefore, a model's ability to precisely predict those that make more than \\$50,000 is more important than the model's ability to recall those individuals. We can use F-beta score as a metric that considers both precision and recall: $$ F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{\\left( \\beta^2 \\cdot precision \\right) + recall} $$ In particular, when $\\beta = 0.5$, more emphasis is placed on precision. This is called the F$_{0.5}$ score (or F-score for simplicity). Looking at the distribution of classes (those who make at most \\$50,000, and those who make more), it's clear most individuals do not make more than \\$50,000. This can greatly affect accuracy , since we could simply say \"this person does not make more than \\$50,000\" and generally be right, without ever looking at the data! Making such a statement would be called naive , since we have not considered any information to substantiate the claim. It is always important to consider the naive prediction for your data, to help establish a benchmark for whether a model is performing well. That been said, using that prediction would be pointless: If we predicted all people made less than \\$50,000, CharityML would identify no one as donors. Question 1 - Naive Predictor Performace If we chose a model that always predicted an individual made more than \\$50,000, what would that model's accuracy and F-score be on this dataset? Note: You must use the code cell below and assign your results to 'accuracy' and 'fscore' to be used later. Answer Since this model always predicts individual making more than \\$50,000, True Positive(TP) = greater_percentage*no_of_observations False Positive(FP) = no_of_observations - True Positive False Negative(FN) = 0 accuracy = $\\large{\\frac{TP}{TP+FP}}$ = greater_percent/100 recall = $\\large{\\frac{TP}{TP+FN}}$ = 1.0 # Accuracy accuracy = greater_percent / 100.0 # Calculate F-score with beta = 0.5 beta = 0.5 recall = 1.0 fscore = ( 1 + beta ** 2 ) * accuracy * recall / (beta ** 2 * accuracy + recall) # Print results print ( \"Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\" . format(accuracy, fscore)) Naive Predictor: [Accuracy score: 0.2478, F-score: 0.2917] accuracy 0.2478439697492371 Supervised Learning Models The following supervised learning models are currently available in scikit-learn that you may choose from: - Gaussian Naive Bayes (GaussianNB) - Decision Trees - Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting) - K-Nearest Neighbors (KNeighbors) - Stochastic Gradient Descent Classifier (SGDC) - Support Vector Machines (SVM) - Logistic Regression Question 2 - Model Application List three of the supervised learning models above that are appropriate for this problem that you will test on the census data. For each model chosen - Describe one real-world application in industry where the model can be applied. (You may need to do research for this \u2014 give references!) - What are the strengths of the model; when does it perform well? - What are the weaknesses of the model; when does it perform poorly? - What makes this model a good candidate for the problem, given what you know about the data? Answer: model real-world application strength weakness why it's a good candidate Gaussian Naive Bayes natural languages processing (such as spam classification and text document classification) even it being simple, It works well in many complex real-world situation.it relies on the independence assumption which means that each distribution can be independently estimated as a one-dimensional distribution and there is no decoupling between them. This helps to reduce the problems stemming from the curse of dimensionality, such as the need for data sets that scale exponentially with the number of features. Since it relies on the independence assumption; it only works well with limited amount of features and This model classifies datasets with complex relationships. Naive Bayes classifiers are also highly scalable because it requires a number of parameters linear in the number of variables in a learning problem. model real-world application strength weakness why it's a good candidate Support Vector Machine knowledge-based systems such as image recognition and hand-written digit recognition, widely applied in the biological and other sciences It has both linear classification and non-linear classification using what is called the kernel trick. It can map inputs into high-dimensional feature spaces and captures complex relationships in dataset. A special property of SVM is that they simultaneously minimize the empirical classification error and maximize the geometric margin and hence make them more robust among other classifiers. It is problematic for learning with data points and many features. It requirs full labelling of data. It shows computational inefficiency specifically the complex parameters such as C (regularization) and kernels are hard to interpret It's powerful in capturing complex relationships in data automatically. It can be optimized by fine tuning parameters. model real-world application strength weakness why it's a good candidate Logistic Regression medical (disease diagonastics) and economics (mortage), social science(voter prediction), natural language processing, business (prediction of a customer's propensity to purchase a product) It is simple, fast, efficient for small dataset with limited features. It is more robust: the independent variables don\u2019t have to be normally distributed, or have equal variance in each group. It may handle nonlinear effects. It cannot capture complex relationship in data, need fine tune of feature population and selection to avoid underfitting or overfitting. It is the baseline algorithms in most framework. It is widely used in many applications. Implementation - Creating a Training and Predicting Pipeline To properly evaluate the performance of each model you've chosen, it's important that you create a training and predicting pipeline that allows you to quickly and effectively train models using various sizes of training data and perform predictions on the testing data. Your implementation here will be used in the following section. In the code block below, you will need to implement the following: - Import fbeta_score and accuracy_score from sklearn.metrics . - Fit the learner to the sampled training data and record the training time. - Perform predictions on the test data X_test , and also on the first 300 training points X_train[:300] . - Record the total prediction time. - Calculate the accuracy score for both the training subset and testing set. - Calculate the F-score for both the training subset and testing set. - Make sure that you set the beta parameter! # Import two metrics from sklearn - fbeta_score and accuracy_score from sklearn.metrics import fbeta_score, accuracy_score from time import time beta = 0.5 def train_predict (learner, sample_size, X_train, y_train, X_test, y_test): ''' inputs: learner: the learning algorithm to be trained and predicted on sample_size: the size of samples (number) to be drawn from training set X_train: features training set y_train: income training set X_test: features testing set y_test: income testing set ''' results = {} # Training sets has been modified the by sample size to train model # with different number of samples.Otherwise training time for each model would be constant X_train = X_train[:sample_size] y_train = y_train[:sample_size] # TODO: Fit the learner to the training data using slicing with 'sample_size' start = time() # Get start time --------> learner . fit(X_train, y_train) end = time() # Get end time <-------- # TODO: Calculate the training time results[ 'train_time' ] = end - start # TODO: Get the predictions on the test set, # then get predictions on the first 300 training samples start = time() # Get start time -------> predictions_test = learner . predict(X_test) predictions_train = learner . predict(X_train[: 300 ]) end = time() # Get end time <------- # TODO: Calculate the total prediction time results[ 'pred_time' ] = end - start # TODO: Compute accuracy on the first 300 training samples results[ 'acc_train' ] = accuracy_score(y_train[: 300 ], predictions_train) # TODO: Compute accuracy on test set results[ 'acc_test' ] = accuracy_score(y_test, predictions_test) # TODO: Compute F-score on the the first 300 training samples results[ 'f_train' ] = fbeta_score(y_train[: 300 ], predictions_train, beta = beta) # TODO: Compute F-score on the test set results[ 'f_test' ] = fbeta_score(y_test, predictions_test, beta = beta) # Success print ( \"{} trained on {} samples.\" . format(learner . __class__ . __name__ , sample_size)) # Return the results return results Implementation: Initial Model Evaluation In the code cell, you will need to implement the following: - Import the three supervised learning models you've discussed in the previous section. - Initialize the three models and store them in 'clf_A' , 'clf_B' , and 'clf_C' . - Use a 'random_state' for each model you use, if provided. - Note: Use the default settings for each model \u2014 you will tune one specific model in a later section. - Calculate the number of records equal to 1%, 10%, and 100% of the training data. - Store those values in 'samples_1' , 'samples_10' , and 'samples_100' respectively. Note: Depending on which algorithms you chose, the following implementation may take some time to run! # TODO: Import the three supervised learning models from sklearn from sklearn.linear_model import LogisticRegression, SGDClassifier from sklearn.svm import LinearSVC, SVC from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.tree import DecisionTreeClassifier # TODO: Initialize the three models clf_A = LogisticRegression(random_state = 101 ) clf_B = LinearSVC(random_state = 102 ) clf_C = GaussianNB() # TODO: Calculate the number of samples for 1%, 10%, and 100% of the training data n_train = len (y_train) samples_1 = int (n_train * 0.01 ) samples_10 = int (n_train * 0.1 ) samples_100 = n_train # Collect results on the learners results = {} for clf in [clf_A, clf_B, clf_C]: clf_name = clf . __class__ . __name__ results[clf_name] = {} for i, samples in enumerate ([samples_1, samples_10, samples_100]): results[clf_name][i] = \\ train_predict(clf, samples, X_train, y_train, X_test, y_test) # Run metrics visualization for the three supervised learning models chosen vs . evaluate(results, accuracy, fscore) LogisticRegression trained on 361 samples. LogisticRegression trained on 3617 samples. LogisticRegression trained on 36177 samples. LinearSVC trained on 361 samples. LinearSVC trained on 3617 samples. LinearSVC trained on 36177 samples. GaussianNB trained on 361 samples. GaussianNB trained on 3617 samples. GaussianNB trained on 36177 samples. for i in results . items(): print i[ 0 ] display(pd . DataFrame(i[ 1 ]) . rename(columns = { 0 : '1%' , 1 : '10%' , 2 : '100%' })) LinearSVC 1% 10% 100% acc_test 0.812604 0.842012 0.847319 acc_train 0.876667 0.860000 0.843333 f_test 0.622364 0.689428 0.705203 f_train 0.734463 0.705521 0.668790 pred_time 0.004078 0.004321 0.004121 train_time 0.003039 0.045014 0.769056 LogisticRegression 1% 10% 100% acc_test 0.817247 0.838474 0.847208 acc_train 0.876667 0.850000 0.840000 f_test 0.632853 0.682881 0.703769 f_train 0.745342 0.683230 0.660377 pred_time 0.004678 0.004482 0.004177 train_time 0.002453 0.020018 0.333923 GaussianNB 1% 10% 100% acc_test 0.489331 0.350138 0.598231 acc_train 0.483333 0.333333 0.593333 f_test 0.359942 0.317909 0.425515 f_train 0.360825 0.302102 0.407268 pred_time 0.029218 0.024140 0.023861 train_time 0.001960 0.009882 0.090051 Improving Results In this final section, you will choose from the three supervised learning models the best model to use on the student data. You will then perform a grid search optimization for the model over the entire training set ( X_train and y_train ) by tuning at least one parameter to improve upon the untuned model's F-score. Question 3 - Choosing the Best Model Based on the evaluation you performed earlier, in one to two paragraphs, explain to CharityML which of the three models you believe to be most appropriate for the task of identifying individuals that make more than \\$50,000. Hint: Your answer should include discussion of the metrics, prediction/training time, and the algorithm's suitability for the data. Answer: I conclude Logistic Regression as the most appropriate model with respect to both accuracy and computation time. It has shown the best balance between training/testing speed, and accuracy/f1 scores of test data with different volumes. It also shows a little overfitting problem as there's no large gap between training scores and testing scores. The Support Vector Machine has a slightly higer accuracy and f-score, but long training time makes it difficult for tuning hyperparameters later. In case of extra computation cost could not be paid to get little increase in performance, SVM would be droped. Naive Bayers with Gaussian model shows the fastest training speed and least variance of scores between training and testing, however, it may have undefitting problem and need more training samples or iterations. This model is rejected simply because it has small accuracy. Question 4 - Describing the Model in Layman's Terms In one to two paragraphs, explain to CharityML , in layman's terms, how the final model chosen is supposed to work. Be sure that you are describing the major qualities of the model, such as how the model is trained and how the model makes a prediction. Avoid using advanced mathematical or technical jargon, such as describing equations or discussing the algorithm implementation. Answer: Logistic Regression is my final model. Logistic Regression first learns from the training data. To learn a model from the data it utilizes weights. These weights are initially assigned as random numbers. As the training steps keep consuming training data, it modifies those weights with respect to the label of the data. At the end it uses classification function to classify data. It is like a model which tries to remember the class lebel of the data by observing different data of same class. Overfitting and underfitting are like over memorization and lack of information. If the features are plotted in a 2-D map, logistic regression draws stright lines as boundaries to seperate features into different classifictions. The detailed steps of how Logistic Regression works is as follows: Outline of Model: It first multiplies features with weights and add them up, then a classification function is applied to the sum. If the function output is large enough, then the classifier would positively relate it with a certain label. Performance checking: One need an objective score to measure how the classifier performs. During training, the predictions are compared to true labels to provide feedback. The weights are updated in order to get a better score in following predictions. Testing step: For prediction, it applies the weights, multiplication, summation, and function to the testing sample to get the final prediction. Implementation: Model Tuning Fine tune the chosen model. Use grid search ( GridSearchCV ) with at least one important parameter tuned with at least 3 different values. You will need to use the entire training set for this. In the code cell below, you will need to implement the following: - Import sklearn.grid_search.GridSearchCV and sklearn.metrics.make_scorer . - Initialize the classifier you've chosen and store it in clf . - Set a random_state if one is available to the same state you set before. - Create a dictionary of parameters you wish to tune for the chosen model. - Example: parameters = {'parameter' : [list of values]} . - Note: Avoid tuning the max_features parameter of your learner if that parameter is available! - Use make_scorer to create an fbeta_score scoring object (with $\\beta = 0.5$). - Perform grid search on the classifier clf using the 'scorer' , and store it in grid_obj . - Fit the grid search object to the training data ( X_train , y_train ), and store it in grid_fit . Note: Depending on the algorithm chosen and the parameter list, the following implementation may take some time to run! # TODO: Import 'GridSearchCV', 'make_scorer', and any other necessary libraries from sklearn.model_selection import GridSearchCV from sklearn.metrics import make_scorer # TODO: Initialize the classifier clf = LogisticRegression() # TODO: Create the parameters list you wish to tune parameters = { 'solver' : [ 'newton-cg' , 'lbfgs' , 'sag' ], 'C' : np . logspace( - 2 , 2 , 13 ), 'random_state' : [ 101 , 101 , 101 ]} # TODO: Make an fbeta_score scoring object scorer = make_scorer(fbeta_score, beta = beta) # TODO: Perform grid search on the classifier using 'scorer' as the scoring method grid_obj = GridSearchCV(clf, parameters, cv = 4 , scoring = scorer) # TODO: Fit the grid search object to the training data and find the optimal parameters grid_fit = grid_obj . fit(X_train, y_train) # Get the estimator best_clf = grid_fit . best_estimator_ # Make predictions using the unoptimized and model predictions = (clf . fit(X_train, y_train)) . predict(X_test) best_predictions = best_clf . predict(X_test) # Report the before-and-afterscores print ( \"Unoptimized model \\n ------\" ) print ( \"Accuracy score on testing data: {:.4f}\" . format(accuracy_score(y_test, predictions))) print ( \"F-score on testing data: {:.4f}\" . format(fbeta_score(y_test, predictions, beta = 0.5 ))) print ( \" \\n Optimized Model \\n ------\" ) print ( \"Final accuracy score on the testing data: {:.4f}\" . format(accuracy_score(y_test, best_predictions))) print ( \"Final F-score on the testing data: {:.4f}\" . format(fbeta_score(y_test, best_predictions, beta = 0.5 ))) Unoptimized model ------ Accuracy score on testing data: 0.8472 F-score on testing data: 0.7038 Optimized Model ------ Final accuracy score on the testing data: 0.8478 Final F-score on the testing data: 0.7046 Question 5 - Final Model Evaluation What is your optimized model's accuracy and F-score on the testing data? Are these scores better or worse than the unoptimized model? How do the results from your optimized model compare to the naive predictor benchmarks you found earlier in Question 1 ? Note: Fill in the table below with your results, and then provide discussion in the Answer box. Results: Metric Benchmark Predictor Unoptimized Model Optimized Model Accuracy Score 0.2478 0.8483 0.8494 F-score 0.2917 0.6993 0.7008 Answer: The optimized model have better accuracy and F-score than unoptimized model, the improvement is small as the grid search of parameters is coarse. The optimized model has much larger accuracy and F-score compared to the benchmark predicator (of naive all positive model). Feature Importance An important task when performing supervised learning on a dataset like the census data we study here is determining which features provide the most predictive power. By focusing on the relationship between only a few crucial features and the target label we simplify our understanding of the phenomenon, which is most always a useful thing to do. In the case of this project, that means we wish to identify a small number of features that most strongly predict whether an individual makes at most or more than \\$50,000. Choose a scikit-learn classifier (e.g., adaboost, random forests) that has a feature_importance_ attribute, which is a function that ranks the importance of features according to the chosen classifier. In the next python cell fit this classifier to training set and use this attribute to determine the top 5 most important features for the census dataset. Question 6 - Feature Relevance Observation When Exploring the Data , it was shown there are thirteen available features for each individual on record in the census data. Of these thirteen records, which five features do you believe to be most important for prediction, and in what order would you rank them and why? Answer: The top five features could be age, income, occupation, education_level, marital-status , in decreasing order of importance. This is based on the assumptions that a naive profile of generous donor could be an adult person who has moderate level income, occupation, education background and happy family. Hence these are corresponding to factors of rule in society, wealth, career, education background, and family rule. Implementation - Extracting Feature Importance Choose a scikit-learn supervised learning algorithm that has a feature_importance_ attribute availble for it. This attribute is a function that ranks the importance of each feature when making predictions based on the chosen algorithm. In the code cell below, you will need to implement the following: - Import a supervised learning model from sklearn if it is different from the three used earlier. - Train the supervised model on the entire training set. - Extract the feature importances using '.feature_importances_' . # TODO: Import a supervised learning model that has 'feature_importances_' from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier # TODO: Train the supervised model on the training set model = GradientBoostingClassifier() . fit(X_train, y_train) # TODO: Extract the feature importances importances = model . feature_importances_ # Plot vs . feature_plot(importances, X_train, y_train) Question 7 - Extracting Feature Importance Observe the visualization created above which displays the five most relevant features for predicting if an individual makes at most or above \\$50,000. How do these five features compare to the five features you discussed in Question 6 ? If you were close to the same answer, how does this visualization confirm your thoughts? If you were not close, why do you think these features are more relevant? Answer: How do these five features compare to the five features you discussed in Question 6? Features in Question 6 have mached at 2 places (age and marital status) and one is related (education-num)I missed two (capital gain and capital loss). If you were close to the same answer, how does this visualization confirm your thoughts? Since the trained model gives good scores and quantative weights of feature importances, the importances given by ensemble classifier is more convincing. If you were not close, why do you think these features are more relevant? According to the model ranking, capital loss/gain (investment) are more important than income and career (salary), and education number is more important than education level. Feature Selection How does a model perform if we only use a subset of all the available features in the data? With less features required to train, the expectation is that training and prediction time is much lower \u2014 at the cost of performance metrics. From the visualization above, we see that the top five most important features contribute more than half of the importance of all features present in the data. This hints that we can attempt to reduce the feature space and simplify the information required for the model to learn. The code cell below will use the same optimized model you found earlier, and train it on the same training set with only the top five important features . # Import functionality for cloning a model from sklearn.base import clone # Reduce the feature space X_train_reduced = X_train[X_train . columns . values[(np . argsort(importances)[:: - 1 ])[: 5 ]]] X_test_reduced = X_test[X_test . columns . values[(np . argsort(importances)[:: - 1 ])[: 5 ]]] # Train on the \"best\" model found from grid search earlier clf = (clone(best_clf)) . fit(X_train_reduced, y_train) # Make new predictions reduced_predictions = clf . predict(X_test_reduced) # Report scores from the final model using both versions of data print ( \"Final Model trained on full data \\n ------\" ) print ( \"Accuracy on testing data: {:.4f}\" . format(accuracy_score(y_test, best_predictions))) print ( \"F-score on testing data: {:.4f}\" . format(fbeta_score(y_test, best_predictions, beta = 0.5 ))) print ( \" \\n Final Model trained on reduced data \\n ------\" ) print ( \"Accuracy on testing data: {:.4f}\" . format(accuracy_score(y_test, reduced_predictions))) print ( \"F-score on testing data: {:.4f}\" . format(fbeta_score(y_test, reduced_predictions, beta = 0.5 ))) Final Model trained on full data ------ Accuracy on testing data: 0.8478 F-score on testing data: 0.7046 Final Model trained on reduced data ------ Accuracy on testing data: 0.8363 F-score on testing data: 0.6791 Question 8 - Effects of Feature Selection How does the final model's F-score and accuracy score on the reduced data using only five features compare to those same scores when all features are used? If training time was a factor, would you consider using the reduced data as your training set? Answer: Yes! if time or computation power is of high priority, the reduced features are good choices. The scores of training top 5 features are slightly smaller than the full features, yet it only takes less than 1/10 time for training and testing. Little gain in accuracy by spending much in computation cust could be stupidity! Note : Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to File -> Download as -> HTML (.html) . Include the finished document along with this notebook as your submission. references - sklearn : Choosing the right estimator from - sklearn : Classifier comparison - wikipedia - Assumptions of Logistic regression","title":"Finding Donors"},{"location":"ml/finding_donors/finding_donors/#machine-learning-engineer-nanodegree","text":"","title":"Machine Learning Engineer Nanodegree"},{"location":"ml/finding_donors/finding_donors/#supervised-learning","text":"","title":"Supervised Learning"},{"location":"ml/finding_donors/finding_donors/#project-finding-donors-for-charityml","text":"Welcome to the second project of the Machine Learning Engineer Nanodegree! In this notebook, some template code has already been provided for you, and it will be your job to implement the additional functionality necessary to successfully complete this project. Sections that begin with 'Implementation' in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a 'Question X' header. Carefully read each question and provide thorough answers in the following text boxes that begin with 'Answer:' . Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide. Note: Code and Markdown cells can be executed using the Shift + Enter keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode.","title":"Project: Finding Donors for CharityML"},{"location":"ml/finding_donors/finding_donors/#getting-started","text":"In this project, you will employ several supervised algorithms of your choice to accurately model individuals' income using data collected from the 1994 U.S. Census. You will then choose the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data. Your goal with this implementation is to construct a model that accurately predicts whether an individual makes more than $50,000. This sort of task can arise in a non-profit setting, where organizations survive on donations. Understanding an individual's income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with. While it can be difficult to determine an individual's general income bracket directly from public sources, we can (as we will see) infer this value from other publically available features. The dataset for this project originates from the UCI Machine Learning Repository . The datset was donated by Ron Kohavi and Barry Becker, after being published in the article \"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid\" . You can find the article by Ron Kohavi online . The data we investigate here consists of small changes to the original dataset, such as removing the 'fnlwgt' feature and records with missing or ill-formatted entries.","title":"Getting Started"},{"location":"ml/finding_donors/finding_donors/#exploring-the-data","text":"Run the code cell below to load necessary Python libraries and load the census data. Note that the last column from this dataset, 'income' , will be our target label (whether an individual makes more than, or at most, $50,000 annually). All other columns are features about each individual in the census database. # Import libraries necessary for this project import numpy as np import pandas as pd from time import time from IPython.display import display # Allows the use of display() for DataFrames # Import supplementary visualization code visuals.py import visuals as vs # Pretty display for notebooks % matplotlib inline # Load the Census dataset data = pd . read_csv( \"census.csv\" ) # Success - Display the first record display(data . head(n = 2 )) print ( 'shape of data =' , data . shape) age workclass education_level education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country income 0 39 State-gov Bachelors 13.0 Never-married Adm-clerical Not-in-family White Male 2174.0 0.0 40.0 United-States <=50K 1 50 Self-emp-not-inc Bachelors 13.0 Married-civ-spouse Exec-managerial Husband White Male 0.0 0.0 13.0 United-States <=50K ('shape of data =', (45222, 14))","title":"Exploring the Data"},{"location":"ml/finding_donors/finding_donors/#data-info","text":"data . info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 45222 entries, 0 to 45221 Data columns (total 14 columns): age 45222 non-null int64 workclass 45222 non-null object education_level 45222 non-null object education-num 45222 non-null float64 marital-status 45222 non-null object occupation 45222 non-null object relationship 45222 non-null object race 45222 non-null object sex 45222 non-null object capital-gain 45222 non-null float64 capital-loss 45222 non-null float64 hours-per-week 45222 non-null float64 native-country 45222 non-null object income 45222 non-null object dtypes: float64(4), int64(1), object(9) memory usage: 4.8+ MB data . describe() age education-num capital-gain capital-loss hours-per-week count 45222.000000 45222.000000 45222.000000 45222.000000 45222.000000 mean 38.547941 10.118460 1101.430344 88.595418 40.938017 std 13.217870 2.552881 7506.430084 404.956092 12.007508 min 17.000000 1.000000 0.000000 0.000000 1.000000 25% 28.000000 9.000000 0.000000 0.000000 40.000000 50% 37.000000 10.000000 0.000000 0.000000 40.000000 75% 47.000000 13.000000 0.000000 0.000000 45.000000 max 90.000000 16.000000 99999.000000 4356.000000 99.000000","title":"Data info"},{"location":"ml/finding_donors/finding_donors/#histogram","text":"import matplotlib.pyplot as plt plt . figure(figsize = ( 14 , 10 )) plt . subplot( 2 , 3 , 1 ) plt . title( \"distribution of age\" ) data[ 'age' ] . hist(bins = 100 ) plt . subplot( 2 , 3 , 2 ) plt . title( \"distribution of education-num\" ) data[ 'education-num' ] . hist(bins = 40 ) plt . subplot( 2 , 3 , 3 ) plt . title( \"distribution of capital-gain\" ) data[ 'capital-gain' ] . hist(bins = 100 ) plt . subplot( 2 , 3 , 4 ) plt . title( \"distribution of hours-per-week\" ) data[ 'hours-per-week' ] . hist(bins = 50 ) plt . subplot( 2 , 3 , 5 ) plt . title( \"distribution of capital-loss\" ) data[ 'capital-loss' ] . hist(bins = 50 ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x13ab99890&gt;","title":"Histogram"},{"location":"ml/finding_donors/finding_donors/#pairplot","text":"Lets look little more insight about dependency or independency of income with other variables like age, hours per week etc. import seaborn as sns plt . figure(figsize = ( 16 , 21 )) sns . set() sns . pairplot(data, hue = \"income\" ) &lt;seaborn.axisgrid.PairGrid at 0x13bb2b290&gt; &lt;matplotlib.figure.Figure at 0x13b569490&gt; It shows high earning is in general for older age, large education number and hours per week etc. This might be because of knowledge and experience.","title":"Pairplot:"},{"location":"ml/finding_donors/finding_donors/#implementation-data-exploration","text":"A cursory investigation of the dataset will determine how many individuals fit into either group, and will tell us about the percentage of these individuals making more than \\$50,000. In the code cell below, you will need to compute the following: - The total number of records, 'n_records' - The number of individuals making more than \\$50,000 annually, 'n_greater_50k' . - The number of individuals making at most \\$50,000 annually, 'n_at_most_50k' . - The percentage of individuals making more than \\$50,000 annually, 'greater_percent' . Hint: You may need to look at the table above to understand how the 'income' entries are formatted. # Total number of records n_records = data . shape[ 0 ] # Number of records where individual's income is more than $50,000 n_greater_50k = data[(data . income == '>50K' )] . shape[ 0 ] # Number of records where individual's income is at most $50,000 n_at_most_50k = data[(data . income == '<=50K' )] . shape[ 0 ] # Percentage of individuals whose income is more than $50,000 greater_percent = (n_greater_50k / float (n_records)) * 100.0 # Print the results print ( \"Total number of records: {}\" . format(n_records)) print ( \"Individuals making more than $50,000: {}\" . format(n_greater_50k)) print ( \"Individuals making at most $50,000: {}\" . format(n_at_most_50k)) print ( \"Percentage of individuals making more than $50,000: {:.2f}%\" . format(greater_percent)) Total number of records: 45222 Individuals making more than $50,000: 11208 Individuals making at most $50,000: 34014 Percentage of individuals making more than $50,000: 24.78% I love to plot them in pi-chart. import matplotlib.pyplot as plt plt . figure(figsize = ( 4 , 4 )) labels = 'High Earning' , 'Low Earning' sizes = [n_greater_50k, n_at_most_50k] colors = [ 'yellowgreen' , 'gold' ] explode = ( 0 , 0.1 ) plt . pie(sizes, explode = explode, labels = labels, colors = colors, autopct = ' %1.1f%% ' , shadow = True , startangle = 90 ) plt . axis( 'equal' ) plt . title( 'Income' ) plt . show()","title":"Implementation: Data Exploration"},{"location":"ml/finding_donors/finding_donors/#1-income-versus-merital-status-and-sex","text":"Lets look the distribution of count over different merital status and sex: sns . countplot(y = \"marital-status\" , hue = \"income\" , data = data, palette = \"Greens_d\" ); Plot below shows the count of individuals making less than or more than 50K, with a breakdown by the features sex and marital-status. import seaborn as sns plt . figure(figsize = ( 16 , 21 )) sns . set(style = \"whitegrid\" , color_codes = True ) sns . factorplot( \"sex\" , col = \"marital-status\" , data = data, hue = 'income' , kind = \"count\" , col_wrap = 2 ); &lt;matplotlib.figure.Figure at 0x13f1333d0&gt; Naturally, married families are more stable, happy and show more in count and also earn much!","title":"1. Income versus merital status and sex:"},{"location":"ml/finding_donors/finding_donors/#2-income-versus-age-and-hours-per-week","text":"I am pretty much interested to see income with respect to age plt . figure(figsize = ( 15 , 21 )) plt . subplot( 1 , 2 , 1 ) sns . countplot(y = \"age\" , hue = \"income\" , data = data, palette = \"Greens_d\" ); plt . subplot( 1 , 2 , 2 ) sns . countplot(y = \"hours-per-week\" , hue = \"income\" , data = data, palette = \"Greens_d\" ); Wow! Look at the profile of count distribution over age, young generation are going crazy. Large no of young people are working hard to earn less money. They should! Because they have to learn. If you want to earn more, you need to wait patiently and keep learning skills till 40. :) :) From hours per week count, why is that 40 hours so much special? This data is little bit biased. We prefer to say 10, 15, 20, 25 etc. But do not bother to say exact value like 13.5 hours or 21 hours. Working place, office etc set working hours like 10 hrs, 20 hrs etc thats why people say like that. I have a question to my reviewer: Should I add some normal noise to this data(hours per week) during preprocessing to get more robust model? Is there any relation of age with working hours? Lets see it below. plt . figure(figsize = ( 15 , 21 )) sns . jointplot(x = \"age\" , y = \"hours-per-week\" , data = data,size = 15 ,kind = 'reg' ); &lt;matplotlib.figure.Figure at 0x1459e1410&gt; I see as the age grows people work more hours, (Do they sleep less ? or take no rest? no idea! There is not data for sleeping habit). Happyness and calmness definitely play a role to decide for donation!","title":"2. Income versus age and hours per week"},{"location":"ml/finding_donors/finding_donors/#3-income-versus-education-number-and-eduction-level","text":"First lets see how are these two related. plt . figure(figsize = ( 10 , 10 )) sns . barplot(x = \"education-num\" , y = \"education_level\" , data = data); Our guess came true. Doctorate has highest education number. Second heighest is Prof-school and pre school is the lowest one. Lets see distribution of count of income over different education level and education number. plt . figure(figsize = ( 16 , 16 )) plt . subplot( 1 , 2 , 1 ) sns . countplot(y = \"education-num\" , hue = \"income\" , data = data, palette = \"Greens_d\" ); plt . subplot( 1 , 2 , 2 ) sns . countplot(y = \"education_level\" , hue = \"income\" , data = data, palette = \"Greens_d\" ); WOw! Look at that HS-grad which has education number = 9 . These are the majority of the total population. Interestingly Bachelors (education num = 13) have heighest count on earning much compaired to others! I am going to skip two more features (capital- gain and capital-loss). Which are being discussed in Data Preparation topics","title":"3. Income versus education number and eduction level"},{"location":"ml/finding_donors/finding_donors/#preparing-the-data","text":"Before data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured \u2014 this is typically known as preprocessing . Fortunately, for this dataset, there are no invalid or missing entries we must deal with, however, there are some qualities about certain features that must be adjusted. This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms.","title":"Preparing the Data"},{"location":"ml/finding_donors/finding_donors/#transforming-skewed-continuous-features","text":"A dataset may sometimes contain at least one feature whose values tend to lie near a single number, but will also have a non-trivial number of vastly larger or smaller values than that single number. Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized. With the census dataset two features fit this description: ' capital-gain' and 'capital-loss' . Run the code cell below to plot a histogram of these two features. Note the range of the values present and how they are distributed. # Split the data into features and target label income_raw = data[ 'income' ] features_raw = data . drop( 'income' , axis = 1 ) # Visualize skewed continuous features of original data vs . distribution(data) For highly-skewed feature distributions such as 'capital-gain' and 'capital-loss' , it is common practice to apply a logarithmic transformation on the data so that the very large and very small values do not negatively affect the performance of a learning algorithm. Using a logarithmic transformation significantly reduces the range of values caused by outliers. Care must be taken when applying this transformation however: The logarithm of 0 is undefined, so we must translate the values by a small amount above 0 to apply the the logarithm successfully. Run the code cell below to perform a transformation on the data and visualize the results. Again, note the range of values and how they are distributed. # Log-transform the skewed features skewed = [ 'capital-gain' , 'capital-loss' ] features_raw[skewed] = data[skewed] . apply( lambda x: np . log(x + 1 )) # Visualize the new log distributions vs . distribution(features_raw, transformed = True )","title":"Transforming Skewed Continuous Features"},{"location":"ml/finding_donors/finding_donors/#normalizing-numerical-features","text":"In addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each feature's distribution (such as 'capital-gain' or 'capital-loss' above); however, normalization ensures that each feature is treated equally when applying supervised learners. Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning, as exampled below. Run the code cell below to normalize each numerical feature. We will use sklearn.preprocessing.MinMaxScaler for this. # Import sklearn.preprocessing.StandardScaler from sklearn.preprocessing import MinMaxScaler # Initialize a scaler, then apply it to the features scaler = MinMaxScaler() numerical = [ 'age' , 'education-num' , 'capital-gain' , 'capital-loss' , 'hours-per-week' ] features_raw[numerical] = scaler . fit_transform(data[numerical]) # Show an example of a record with scaling applied display(features_raw . head(n = 2 )) age workclass education_level education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country 0 0.301370 State-gov Bachelors 0.8 Never-married Adm-clerical Not-in-family White Male 0.02174 0.0 0.397959 United-States 1 0.452055 Self-emp-not-inc Bachelors 0.8 Married-civ-spouse Exec-managerial Husband White Male 0.00000 0.0 0.122449 United-States","title":"Normalizing Numerical Features"},{"location":"ml/finding_donors/finding_donors/#implementation-data-preprocessing","text":"From the table in Exploring the Data above, we can see there are several features for each record that are non-numeric. Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called categorical variables ) be converted. One popular way to convert categorical variables is by using the one-hot encoding scheme. One-hot encoding creates a \"dummy\" variable for each possible category of each non-numeric feature. For example, assume someFeature has three possible entries: A , B , or C . We then encode this feature into someFeature_A , someFeature_B and someFeature_C . someFeature someFeature_A someFeature_B someFeature_C 0 B 0 1 0 1 C ----> one-hot encode ----> 0 0 1 2 A 1 0 0 Additionally, as with the non-numeric features, we need to convert the non-numeric target label, 'income' to numerical values for the learning algorithm to work. Since there are only two possible categories for this label (\"<=50K\" and \">50K\"), we can avoid using one-hot encoding and simply encode these two categories as 0 and 1 , respectively. In code cell below, you will need to implement the following: - Use pandas.get_dummies() to perform one-hot encoding on the 'features_raw' data. - Convert the target label 'income_raw' to numerical entries. - Set records with \"<=50K\" to 0 and records with \">50K\" to 1 . # One-hot encode the 'features_raw' data using pandas.get_dummies() features = pd . get_dummies(features_raw) # Encode the 'income_raw' data to numerical values income = income_raw . apply( lambda x: 1 if x == '>50K' else 0 ) # Alternate method # pd.get_dummies(income_raw)['>50K'] # pd.get_dummies(income_raw, drop_first=True) # Print the number of features after one-hot encoding encoded = list (features . columns) print ( \"{} total features after one-hot encoding.\" . format( len (encoded))) # Uncomment the following line to see the encoded feature names #print encoded 103 total features after one-hot encoding. features . shape (45222, 103)","title":"Implementation: Data Preprocessing"},{"location":"ml/finding_donors/finding_donors/#shuffle-and-split-data","text":"Now all categorical variables have been converted into numerical features, and all numerical features have been normalized. As always, we will now split the data (both features and their labels) into training and test sets. 80% of the data will be used for training and 20% for testing. Run the code cell below to perform this split. # Import train_test_split from sklearn.model_selection import train_test_split from sklearn.utils import shuffle #shuffle the data features, income = shuffle(features, income) # Split the 'features' and 'income' data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(features, income, test_size = 0.2 , random_state = 0 ) # Show the results of the split print ( \"Training set has {} samples.\" . format(X_train . shape[ 0 ])) print ( \"Testing set has {} samples.\" . format(X_test . shape[ 0 ])) Training set has 36177 samples. Testing set has 9045 samples.","title":"Shuffle and Split Data"},{"location":"ml/finding_donors/finding_donors/#evaluating-model-performance","text":"In this section, we will investigate four different algorithms, and determine which is best at modeling the data. Three of these algorithms will be supervised learners of your choice, and the fourth algorithm is known as a naive predictor .","title":"Evaluating Model Performance"},{"location":"ml/finding_donors/finding_donors/#metrics-and-the-naive-predictor","text":"CharityML , equipped with their research, knows individuals that make more than \\$50,000 are most likely to donate to their charity. Because of this, CharityML is particularly interested in predicting who makes more than \\$50,000 accurately. It would seem that using accuracy as a metric for evaluating a particular model's performace would be appropriate. Additionally, identifying someone that does not make more than \\$50,000 as someone who does would be detrimental to CharityML , since they are looking to find individuals willing to donate. Therefore, a model's ability to precisely predict those that make more than \\$50,000 is more important than the model's ability to recall those individuals. We can use F-beta score as a metric that considers both precision and recall: $$ F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{\\left( \\beta^2 \\cdot precision \\right) + recall} $$ In particular, when $\\beta = 0.5$, more emphasis is placed on precision. This is called the F$_{0.5}$ score (or F-score for simplicity). Looking at the distribution of classes (those who make at most \\$50,000, and those who make more), it's clear most individuals do not make more than \\$50,000. This can greatly affect accuracy , since we could simply say \"this person does not make more than \\$50,000\" and generally be right, without ever looking at the data! Making such a statement would be called naive , since we have not considered any information to substantiate the claim. It is always important to consider the naive prediction for your data, to help establish a benchmark for whether a model is performing well. That been said, using that prediction would be pointless: If we predicted all people made less than \\$50,000, CharityML would identify no one as donors.","title":"Metrics and the Naive Predictor"},{"location":"ml/finding_donors/finding_donors/#question-1-naive-predictor-performace","text":"If we chose a model that always predicted an individual made more than \\$50,000, what would that model's accuracy and F-score be on this dataset? Note: You must use the code cell below and assign your results to 'accuracy' and 'fscore' to be used later. Answer Since this model always predicts individual making more than \\$50,000, True Positive(TP) = greater_percentage*no_of_observations False Positive(FP) = no_of_observations - True Positive False Negative(FN) = 0 accuracy = $\\large{\\frac{TP}{TP+FP}}$ = greater_percent/100 recall = $\\large{\\frac{TP}{TP+FN}}$ = 1.0 # Accuracy accuracy = greater_percent / 100.0 # Calculate F-score with beta = 0.5 beta = 0.5 recall = 1.0 fscore = ( 1 + beta ** 2 ) * accuracy * recall / (beta ** 2 * accuracy + recall) # Print results print ( \"Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\" . format(accuracy, fscore)) Naive Predictor: [Accuracy score: 0.2478, F-score: 0.2917] accuracy 0.2478439697492371","title":"Question 1 - Naive Predictor Performace"},{"location":"ml/finding_donors/finding_donors/#supervised-learning-models","text":"The following supervised learning models are currently available in scikit-learn that you may choose from: - Gaussian Naive Bayes (GaussianNB) - Decision Trees - Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting) - K-Nearest Neighbors (KNeighbors) - Stochastic Gradient Descent Classifier (SGDC) - Support Vector Machines (SVM) - Logistic Regression","title":"Supervised Learning Models"},{"location":"ml/finding_donors/finding_donors/#question-2-model-application","text":"List three of the supervised learning models above that are appropriate for this problem that you will test on the census data. For each model chosen - Describe one real-world application in industry where the model can be applied. (You may need to do research for this \u2014 give references!) - What are the strengths of the model; when does it perform well? - What are the weaknesses of the model; when does it perform poorly? - What makes this model a good candidate for the problem, given what you know about the data?","title":"Question 2 - Model Application"},{"location":"ml/finding_donors/finding_donors/#answer","text":"model real-world application strength weakness why it's a good candidate Gaussian Naive Bayes natural languages processing (such as spam classification and text document classification) even it being simple, It works well in many complex real-world situation.it relies on the independence assumption which means that each distribution can be independently estimated as a one-dimensional distribution and there is no decoupling between them. This helps to reduce the problems stemming from the curse of dimensionality, such as the need for data sets that scale exponentially with the number of features. Since it relies on the independence assumption; it only works well with limited amount of features and This model classifies datasets with complex relationships. Naive Bayes classifiers are also highly scalable because it requires a number of parameters linear in the number of variables in a learning problem. model real-world application strength weakness why it's a good candidate Support Vector Machine knowledge-based systems such as image recognition and hand-written digit recognition, widely applied in the biological and other sciences It has both linear classification and non-linear classification using what is called the kernel trick. It can map inputs into high-dimensional feature spaces and captures complex relationships in dataset. A special property of SVM is that they simultaneously minimize the empirical classification error and maximize the geometric margin and hence make them more robust among other classifiers. It is problematic for learning with data points and many features. It requirs full labelling of data. It shows computational inefficiency specifically the complex parameters such as C (regularization) and kernels are hard to interpret It's powerful in capturing complex relationships in data automatically. It can be optimized by fine tuning parameters. model real-world application strength weakness why it's a good candidate Logistic Regression medical (disease diagonastics) and economics (mortage), social science(voter prediction), natural language processing, business (prediction of a customer's propensity to purchase a product) It is simple, fast, efficient for small dataset with limited features. It is more robust: the independent variables don\u2019t have to be normally distributed, or have equal variance in each group. It may handle nonlinear effects. It cannot capture complex relationship in data, need fine tune of feature population and selection to avoid underfitting or overfitting. It is the baseline algorithms in most framework. It is widely used in many applications.","title":"Answer:"},{"location":"ml/finding_donors/finding_donors/#implementation-creating-a-training-and-predicting-pipeline","text":"To properly evaluate the performance of each model you've chosen, it's important that you create a training and predicting pipeline that allows you to quickly and effectively train models using various sizes of training data and perform predictions on the testing data. Your implementation here will be used in the following section. In the code block below, you will need to implement the following: - Import fbeta_score and accuracy_score from sklearn.metrics . - Fit the learner to the sampled training data and record the training time. - Perform predictions on the test data X_test , and also on the first 300 training points X_train[:300] . - Record the total prediction time. - Calculate the accuracy score for both the training subset and testing set. - Calculate the F-score for both the training subset and testing set. - Make sure that you set the beta parameter! # Import two metrics from sklearn - fbeta_score and accuracy_score from sklearn.metrics import fbeta_score, accuracy_score from time import time beta = 0.5 def train_predict (learner, sample_size, X_train, y_train, X_test, y_test): ''' inputs: learner: the learning algorithm to be trained and predicted on sample_size: the size of samples (number) to be drawn from training set X_train: features training set y_train: income training set X_test: features testing set y_test: income testing set ''' results = {} # Training sets has been modified the by sample size to train model # with different number of samples.Otherwise training time for each model would be constant X_train = X_train[:sample_size] y_train = y_train[:sample_size] # TODO: Fit the learner to the training data using slicing with 'sample_size' start = time() # Get start time --------> learner . fit(X_train, y_train) end = time() # Get end time <-------- # TODO: Calculate the training time results[ 'train_time' ] = end - start # TODO: Get the predictions on the test set, # then get predictions on the first 300 training samples start = time() # Get start time -------> predictions_test = learner . predict(X_test) predictions_train = learner . predict(X_train[: 300 ]) end = time() # Get end time <------- # TODO: Calculate the total prediction time results[ 'pred_time' ] = end - start # TODO: Compute accuracy on the first 300 training samples results[ 'acc_train' ] = accuracy_score(y_train[: 300 ], predictions_train) # TODO: Compute accuracy on test set results[ 'acc_test' ] = accuracy_score(y_test, predictions_test) # TODO: Compute F-score on the the first 300 training samples results[ 'f_train' ] = fbeta_score(y_train[: 300 ], predictions_train, beta = beta) # TODO: Compute F-score on the test set results[ 'f_test' ] = fbeta_score(y_test, predictions_test, beta = beta) # Success print ( \"{} trained on {} samples.\" . format(learner . __class__ . __name__ , sample_size)) # Return the results return results","title":"Implementation - Creating a Training and Predicting Pipeline"},{"location":"ml/finding_donors/finding_donors/#implementation-initial-model-evaluation","text":"In the code cell, you will need to implement the following: - Import the three supervised learning models you've discussed in the previous section. - Initialize the three models and store them in 'clf_A' , 'clf_B' , and 'clf_C' . - Use a 'random_state' for each model you use, if provided. - Note: Use the default settings for each model \u2014 you will tune one specific model in a later section. - Calculate the number of records equal to 1%, 10%, and 100% of the training data. - Store those values in 'samples_1' , 'samples_10' , and 'samples_100' respectively. Note: Depending on which algorithms you chose, the following implementation may take some time to run! # TODO: Import the three supervised learning models from sklearn from sklearn.linear_model import LogisticRegression, SGDClassifier from sklearn.svm import LinearSVC, SVC from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.tree import DecisionTreeClassifier # TODO: Initialize the three models clf_A = LogisticRegression(random_state = 101 ) clf_B = LinearSVC(random_state = 102 ) clf_C = GaussianNB() # TODO: Calculate the number of samples for 1%, 10%, and 100% of the training data n_train = len (y_train) samples_1 = int (n_train * 0.01 ) samples_10 = int (n_train * 0.1 ) samples_100 = n_train # Collect results on the learners results = {} for clf in [clf_A, clf_B, clf_C]: clf_name = clf . __class__ . __name__ results[clf_name] = {} for i, samples in enumerate ([samples_1, samples_10, samples_100]): results[clf_name][i] = \\ train_predict(clf, samples, X_train, y_train, X_test, y_test) # Run metrics visualization for the three supervised learning models chosen vs . evaluate(results, accuracy, fscore) LogisticRegression trained on 361 samples. LogisticRegression trained on 3617 samples. LogisticRegression trained on 36177 samples. LinearSVC trained on 361 samples. LinearSVC trained on 3617 samples. LinearSVC trained on 36177 samples. GaussianNB trained on 361 samples. GaussianNB trained on 3617 samples. GaussianNB trained on 36177 samples. for i in results . items(): print i[ 0 ] display(pd . DataFrame(i[ 1 ]) . rename(columns = { 0 : '1%' , 1 : '10%' , 2 : '100%' })) LinearSVC 1% 10% 100% acc_test 0.812604 0.842012 0.847319 acc_train 0.876667 0.860000 0.843333 f_test 0.622364 0.689428 0.705203 f_train 0.734463 0.705521 0.668790 pred_time 0.004078 0.004321 0.004121 train_time 0.003039 0.045014 0.769056 LogisticRegression 1% 10% 100% acc_test 0.817247 0.838474 0.847208 acc_train 0.876667 0.850000 0.840000 f_test 0.632853 0.682881 0.703769 f_train 0.745342 0.683230 0.660377 pred_time 0.004678 0.004482 0.004177 train_time 0.002453 0.020018 0.333923 GaussianNB 1% 10% 100% acc_test 0.489331 0.350138 0.598231 acc_train 0.483333 0.333333 0.593333 f_test 0.359942 0.317909 0.425515 f_train 0.360825 0.302102 0.407268 pred_time 0.029218 0.024140 0.023861 train_time 0.001960 0.009882 0.090051","title":"Implementation: Initial Model Evaluation"},{"location":"ml/finding_donors/finding_donors/#improving-results","text":"In this final section, you will choose from the three supervised learning models the best model to use on the student data. You will then perform a grid search optimization for the model over the entire training set ( X_train and y_train ) by tuning at least one parameter to improve upon the untuned model's F-score.","title":"Improving Results"},{"location":"ml/finding_donors/finding_donors/#question-3-choosing-the-best-model","text":"Based on the evaluation you performed earlier, in one to two paragraphs, explain to CharityML which of the three models you believe to be most appropriate for the task of identifying individuals that make more than \\$50,000. Hint: Your answer should include discussion of the metrics, prediction/training time, and the algorithm's suitability for the data. Answer: I conclude Logistic Regression as the most appropriate model with respect to both accuracy and computation time. It has shown the best balance between training/testing speed, and accuracy/f1 scores of test data with different volumes. It also shows a little overfitting problem as there's no large gap between training scores and testing scores. The Support Vector Machine has a slightly higer accuracy and f-score, but long training time makes it difficult for tuning hyperparameters later. In case of extra computation cost could not be paid to get little increase in performance, SVM would be droped. Naive Bayers with Gaussian model shows the fastest training speed and least variance of scores between training and testing, however, it may have undefitting problem and need more training samples or iterations. This model is rejected simply because it has small accuracy.","title":"Question 3 - Choosing the Best Model"},{"location":"ml/finding_donors/finding_donors/#question-4-describing-the-model-in-laymans-terms","text":"In one to two paragraphs, explain to CharityML , in layman's terms, how the final model chosen is supposed to work. Be sure that you are describing the major qualities of the model, such as how the model is trained and how the model makes a prediction. Avoid using advanced mathematical or technical jargon, such as describing equations or discussing the algorithm implementation. Answer: Logistic Regression is my final model. Logistic Regression first learns from the training data. To learn a model from the data it utilizes weights. These weights are initially assigned as random numbers. As the training steps keep consuming training data, it modifies those weights with respect to the label of the data. At the end it uses classification function to classify data. It is like a model which tries to remember the class lebel of the data by observing different data of same class. Overfitting and underfitting are like over memorization and lack of information. If the features are plotted in a 2-D map, logistic regression draws stright lines as boundaries to seperate features into different classifictions. The detailed steps of how Logistic Regression works is as follows: Outline of Model: It first multiplies features with weights and add them up, then a classification function is applied to the sum. If the function output is large enough, then the classifier would positively relate it with a certain label. Performance checking: One need an objective score to measure how the classifier performs. During training, the predictions are compared to true labels to provide feedback. The weights are updated in order to get a better score in following predictions. Testing step: For prediction, it applies the weights, multiplication, summation, and function to the testing sample to get the final prediction.","title":"Question 4 - Describing the Model in Layman's Terms"},{"location":"ml/finding_donors/finding_donors/#implementation-model-tuning","text":"Fine tune the chosen model. Use grid search ( GridSearchCV ) with at least one important parameter tuned with at least 3 different values. You will need to use the entire training set for this. In the code cell below, you will need to implement the following: - Import sklearn.grid_search.GridSearchCV and sklearn.metrics.make_scorer . - Initialize the classifier you've chosen and store it in clf . - Set a random_state if one is available to the same state you set before. - Create a dictionary of parameters you wish to tune for the chosen model. - Example: parameters = {'parameter' : [list of values]} . - Note: Avoid tuning the max_features parameter of your learner if that parameter is available! - Use make_scorer to create an fbeta_score scoring object (with $\\beta = 0.5$). - Perform grid search on the classifier clf using the 'scorer' , and store it in grid_obj . - Fit the grid search object to the training data ( X_train , y_train ), and store it in grid_fit . Note: Depending on the algorithm chosen and the parameter list, the following implementation may take some time to run! # TODO: Import 'GridSearchCV', 'make_scorer', and any other necessary libraries from sklearn.model_selection import GridSearchCV from sklearn.metrics import make_scorer # TODO: Initialize the classifier clf = LogisticRegression() # TODO: Create the parameters list you wish to tune parameters = { 'solver' : [ 'newton-cg' , 'lbfgs' , 'sag' ], 'C' : np . logspace( - 2 , 2 , 13 ), 'random_state' : [ 101 , 101 , 101 ]} # TODO: Make an fbeta_score scoring object scorer = make_scorer(fbeta_score, beta = beta) # TODO: Perform grid search on the classifier using 'scorer' as the scoring method grid_obj = GridSearchCV(clf, parameters, cv = 4 , scoring = scorer) # TODO: Fit the grid search object to the training data and find the optimal parameters grid_fit = grid_obj . fit(X_train, y_train) # Get the estimator best_clf = grid_fit . best_estimator_ # Make predictions using the unoptimized and model predictions = (clf . fit(X_train, y_train)) . predict(X_test) best_predictions = best_clf . predict(X_test) # Report the before-and-afterscores print ( \"Unoptimized model \\n ------\" ) print ( \"Accuracy score on testing data: {:.4f}\" . format(accuracy_score(y_test, predictions))) print ( \"F-score on testing data: {:.4f}\" . format(fbeta_score(y_test, predictions, beta = 0.5 ))) print ( \" \\n Optimized Model \\n ------\" ) print ( \"Final accuracy score on the testing data: {:.4f}\" . format(accuracy_score(y_test, best_predictions))) print ( \"Final F-score on the testing data: {:.4f}\" . format(fbeta_score(y_test, best_predictions, beta = 0.5 ))) Unoptimized model ------ Accuracy score on testing data: 0.8472 F-score on testing data: 0.7038 Optimized Model ------ Final accuracy score on the testing data: 0.8478 Final F-score on the testing data: 0.7046","title":"Implementation: Model Tuning"},{"location":"ml/finding_donors/finding_donors/#question-5-final-model-evaluation","text":"What is your optimized model's accuracy and F-score on the testing data? Are these scores better or worse than the unoptimized model? How do the results from your optimized model compare to the naive predictor benchmarks you found earlier in Question 1 ? Note: Fill in the table below with your results, and then provide discussion in the Answer box.","title":"Question 5 - Final Model Evaluation"},{"location":"ml/finding_donors/finding_donors/#results","text":"Metric Benchmark Predictor Unoptimized Model Optimized Model Accuracy Score 0.2478 0.8483 0.8494 F-score 0.2917 0.6993 0.7008 Answer: The optimized model have better accuracy and F-score than unoptimized model, the improvement is small as the grid search of parameters is coarse. The optimized model has much larger accuracy and F-score compared to the benchmark predicator (of naive all positive model).","title":"Results:"},{"location":"ml/finding_donors/finding_donors/#feature-importance","text":"An important task when performing supervised learning on a dataset like the census data we study here is determining which features provide the most predictive power. By focusing on the relationship between only a few crucial features and the target label we simplify our understanding of the phenomenon, which is most always a useful thing to do. In the case of this project, that means we wish to identify a small number of features that most strongly predict whether an individual makes at most or more than \\$50,000. Choose a scikit-learn classifier (e.g., adaboost, random forests) that has a feature_importance_ attribute, which is a function that ranks the importance of features according to the chosen classifier. In the next python cell fit this classifier to training set and use this attribute to determine the top 5 most important features for the census dataset.","title":"Feature Importance"},{"location":"ml/finding_donors/finding_donors/#question-6-feature-relevance-observation","text":"When Exploring the Data , it was shown there are thirteen available features for each individual on record in the census data. Of these thirteen records, which five features do you believe to be most important for prediction, and in what order would you rank them and why? Answer: The top five features could be age, income, occupation, education_level, marital-status , in decreasing order of importance. This is based on the assumptions that a naive profile of generous donor could be an adult person who has moderate level income, occupation, education background and happy family. Hence these are corresponding to factors of rule in society, wealth, career, education background, and family rule.","title":"Question 6 - Feature Relevance Observation"},{"location":"ml/finding_donors/finding_donors/#implementation-extracting-feature-importance","text":"Choose a scikit-learn supervised learning algorithm that has a feature_importance_ attribute availble for it. This attribute is a function that ranks the importance of each feature when making predictions based on the chosen algorithm. In the code cell below, you will need to implement the following: - Import a supervised learning model from sklearn if it is different from the three used earlier. - Train the supervised model on the entire training set. - Extract the feature importances using '.feature_importances_' . # TODO: Import a supervised learning model that has 'feature_importances_' from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier # TODO: Train the supervised model on the training set model = GradientBoostingClassifier() . fit(X_train, y_train) # TODO: Extract the feature importances importances = model . feature_importances_ # Plot vs . feature_plot(importances, X_train, y_train)","title":"Implementation - Extracting Feature Importance"},{"location":"ml/finding_donors/finding_donors/#question-7-extracting-feature-importance","text":"Observe the visualization created above which displays the five most relevant features for predicting if an individual makes at most or above \\$50,000. How do these five features compare to the five features you discussed in Question 6 ? If you were close to the same answer, how does this visualization confirm your thoughts? If you were not close, why do you think these features are more relevant? Answer: How do these five features compare to the five features you discussed in Question 6? Features in Question 6 have mached at 2 places (age and marital status) and one is related (education-num)I missed two (capital gain and capital loss). If you were close to the same answer, how does this visualization confirm your thoughts? Since the trained model gives good scores and quantative weights of feature importances, the importances given by ensemble classifier is more convincing. If you were not close, why do you think these features are more relevant? According to the model ranking, capital loss/gain (investment) are more important than income and career (salary), and education number is more important than education level.","title":"Question 7 - Extracting Feature Importance"},{"location":"ml/finding_donors/finding_donors/#feature-selection","text":"How does a model perform if we only use a subset of all the available features in the data? With less features required to train, the expectation is that training and prediction time is much lower \u2014 at the cost of performance metrics. From the visualization above, we see that the top five most important features contribute more than half of the importance of all features present in the data. This hints that we can attempt to reduce the feature space and simplify the information required for the model to learn. The code cell below will use the same optimized model you found earlier, and train it on the same training set with only the top five important features . # Import functionality for cloning a model from sklearn.base import clone # Reduce the feature space X_train_reduced = X_train[X_train . columns . values[(np . argsort(importances)[:: - 1 ])[: 5 ]]] X_test_reduced = X_test[X_test . columns . values[(np . argsort(importances)[:: - 1 ])[: 5 ]]] # Train on the \"best\" model found from grid search earlier clf = (clone(best_clf)) . fit(X_train_reduced, y_train) # Make new predictions reduced_predictions = clf . predict(X_test_reduced) # Report scores from the final model using both versions of data print ( \"Final Model trained on full data \\n ------\" ) print ( \"Accuracy on testing data: {:.4f}\" . format(accuracy_score(y_test, best_predictions))) print ( \"F-score on testing data: {:.4f}\" . format(fbeta_score(y_test, best_predictions, beta = 0.5 ))) print ( \" \\n Final Model trained on reduced data \\n ------\" ) print ( \"Accuracy on testing data: {:.4f}\" . format(accuracy_score(y_test, reduced_predictions))) print ( \"F-score on testing data: {:.4f}\" . format(fbeta_score(y_test, reduced_predictions, beta = 0.5 ))) Final Model trained on full data ------ Accuracy on testing data: 0.8478 F-score on testing data: 0.7046 Final Model trained on reduced data ------ Accuracy on testing data: 0.8363 F-score on testing data: 0.6791","title":"Feature Selection"},{"location":"ml/finding_donors/finding_donors/#question-8-effects-of-feature-selection","text":"How does the final model's F-score and accuracy score on the reduced data using only five features compare to those same scores when all features are used? If training time was a factor, would you consider using the reduced data as your training set? Answer: Yes! if time or computation power is of high priority, the reduced features are good choices. The scores of training top 5 features are slightly smaller than the full features, yet it only takes less than 1/10 time for training and testing. Little gain in accuracy by spending much in computation cust could be stupidity! Note : Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to File -> Download as -> HTML (.html) . Include the finished document along with this notebook as your submission. references - sklearn : Choosing the right estimator from - sklearn : Classifier comparison - wikipedia - Assumptions of Logistic regression","title":"Question 8 - Effects of Feature Selection"},{"location":"ml/perceptron/dlnd-your-first-neural-network/","text":"Your first neural network In this project, you'll build your first neural network and use it to predict daily bike rental ridership. We've provided some of the code, but left the implementation of the neural network up to you (for the most part). After you've submitted this project, feel free to explore the data and the model more. % matplotlib inline % config InlineBackend . figure_format = 'retina' import numpy as np import pandas as pd import matplotlib.pyplot as plt Load and prepare the data A critical step in working with neural networks is preparing the data correctly. Variables on different scales make it difficult for the network to efficiently learn the correct weights. Below, we've written the code to load and prepare the data. You'll learn more about this soon! data_path = 'Bike-Sharing-Dataset/hour.csv' rides = pd . read_csv(data_path) rides . head() instant dteday season yr mnth hr holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt 0 1 2011-01-01 1 0 1 0 0 6 0 1 0.24 0.2879 0.81 0.0 3 13 16 1 2 2011-01-01 1 0 1 1 0 6 0 1 0.22 0.2727 0.80 0.0 8 32 40 2 3 2011-01-01 1 0 1 2 0 6 0 1 0.22 0.2727 0.80 0.0 5 27 32 3 4 2011-01-01 1 0 1 3 0 6 0 1 0.24 0.2879 0.75 0.0 3 10 13 4 5 2011-01-01 1 0 1 4 0 6 0 1 0.24 0.2879 0.75 0.0 0 1 1 Checking out the data This dataset has the number of riders for each hour of each day from January 1 2011 to December 31 2012. The number of riders is split between casual and registered, summed up in the cnt column. You can see the first few rows of the data above. Below is a plot showing the number of bike riders over the first 10 days in the data set. You can see the hourly rentals here. This data is pretty complicated! The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week. Looking at the data above, we also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders. You'll be trying to capture all this with your model. rides[: 24 * 10 ] . plot(x = 'dteday' , y = 'cnt' ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x10cfe1dd8&gt; Dummy variables Here we have some categorical variables like season, weather, month. To include these in our model, we'll need to make binary dummy variables. This is simple to do with Pandas thanks to get_dummies() . dummy_fields = [ 'season' , 'weathersit' , 'mnth' , 'hr' , 'weekday' ] for each in dummy_fields: dummies = pd . get_dummies(rides[each], prefix = each, drop_first = False ) rides = pd . concat([rides, dummies], axis = 1 ) fields_to_drop = [ 'instant' , 'dteday' , 'season' , 'weathersit' , 'weekday' , 'atemp' , 'mnth' , 'workingday' , 'hr' ] data = rides . drop(fields_to_drop, axis = 1 ) data . head() yr holiday temp hum windspeed casual registered cnt season_1 season_2 ... hr_21 hr_22 hr_23 weekday_0 weekday_1 weekday_2 weekday_3 weekday_4 weekday_5 weekday_6 0 0 0 0.24 0.81 0.0 3 13 16 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1 0 0 0.22 0.80 0.0 8 32 40 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 2 0 0 0.22 0.80 0.0 5 27 32 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 3 0 0 0.24 0.75 0.0 3 10 13 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 4 0 0 0.24 0.75 0.0 0 1 1 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 5 rows \u00d7 59 columns Scaling target variables To make training the network easier, we'll standardize each of the continuous variables. That is, we'll shift and scale the variables such that they have zero mean and a standard deviation of 1. The scaling factors are saved so we can go backwards when we use the network for predictions. quant_features = [ 'casual' , 'registered' , 'cnt' , 'temp' , 'hum' , 'windspeed' ] # Store scalings in a dictionary so we can convert back later scaled_features = {} for each in quant_features: mean, std = data[each] . mean(), data[each] . std() scaled_features[each] = [mean, std] data . loc[:, each] = (data[each] - mean) / std Splitting the data into training, testing, and validation sets We'll save the last 21 days of the data to use as a test set after we've trained the network. We'll use this set to make predictions and compare them with the actual number of riders. # Save the last 21 days test_data = data[ - 21 * 24 :] data = data[: - 21 * 24 ] # Separate the data into features and targets target_fields = [ 'cnt' , 'casual' , 'registered' ] features, targets = data . drop(target_fields, axis = 1 ), data[target_fields] test_features, test_targets = test_data . drop(target_fields, axis = 1 ), test_data[target_fields] We'll split the data into two sets, one for training and one for validating as the network is being trained. Since this is time series data, we'll train on historical data, then try to predict on future data (the validation set). # Hold out the last 60 days of the remaining data as a validation set train_features, train_targets = features[: - 60 * 24 ], targets[: - 60 * 24 ] val_features, val_targets = features[ - 60 * 24 :], targets[ - 60 * 24 :] Time to build the network Below you'll build your network. We've built out the structure and the backwards pass. You'll implement the forward pass through the network. You'll also set the hyperparameters: the learning rate, the number of hidden units, and the number of training passes. The network has two layers, a hidden layer and an output layer. The hidden layer will use the sigmoid function for activations. The output layer has only one node and is used for the regression, the output of the node is the same as the input of the node. That is, the activation function is $f(x)=x$. A function that takes the input signal and generates an output signal, but takes into account the threshold, is called an activation function. We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer. This process is called forward propagation . We use the weights to propagate signals forward from the input to the output layers in a neural network. We use the weights to also propagate error backwards from the output back into the network to update our weights. This is called backpropagation . Hint: You'll need the derivative of the output activation function ($f(x) = x$) for the backpropagation implementation. If you aren't familiar with calculus, this function is equivalent to the equation $y = x$. What is the slope of that equation? That is the derivative of $f(x)$. Below, you have these tasks: 1. Implement the sigmoid function to use as the activation function. Set self.activation_function in __init__ to your sigmoid function. 2. Implement the forward pass in the train method. 3. Implement the backpropagation algorithm in the train method, including calculating the output error. 4. Implement the forward pass in the run method. def sigmoid (x): return 1 / ( 1 + np . exp( - x)) def MSE (y, Y): return np . mean((y - Y) ** 2 ) class NeuralNetwork ( object ): def __init__ ( self , input_nodes, hidden_nodes, output_nodes, learning_rate): # Set number of nodes in input, hidden and output layers. self . input_nodes = input_nodes self . hidden_nodes = hidden_nodes self . output_nodes = output_nodes # Initialize weights self . weights_input_to_hidden = np . random . normal( 0.0 , self . hidden_nodes **- 0.5 , ( self . hidden_nodes, self . input_nodes)) self . weights_hidden_to_output = np . random . normal( 0.0 , self . output_nodes **- 0.5 , ( self . output_nodes, self . hidden_nodes)) self . lr = learning_rate #### Set this to your implemented sigmoid function #### # Activation function is the sigmoid function #self.activation_function = lambda x: 1 / ( 1 + np.exp(-x)) self . activation_function = sigmoid # Gradient of Sigmoid curve def sigmoid_derivative ( self , z): return z * ( 1 - z) def train ( self , inputs_list, targets_list, verbose): # Convert inputs list to 2d array inputs = np . array(inputs_list, ndmin = 2 ) . T targets = np . array(targets_list, ndmin = 2 ) . T results = {} if verbose: results[ \"inputs\" ] = inputs results[ \"targets\" ] = targets results[ \"o-weights_input_to_hidden\" ] = self . weights_input_to_hidden results[ \"o-weights_hidden_to_output\" ] = self . weights_hidden_to_output #### Implement the forward pass here #### ### Forward pass ### # TODO: Hidden layer hidden_inputs = np . dot( self . weights_input_to_hidden,inputs) # signals into hidden layer hidden_outputs = self . activation_function(hidden_inputs) # signals from hidden layer # TODO: Output layer final_inputs = np . dot( self . weights_hidden_to_output,hidden_outputs) # signals into final output layer final_outputs = final_inputs # signals from final output layer if verbose: results[ \"hidden_inputs\" ] = hidden_inputs results[ \"hidden_outputs\" ] = hidden_outputs results[ \"final_inputs\" ] = final_inputs results[ \"final_outputs\" ] = final_outputs #### Implement the backward pass here #### ### Backward pass ### # TODO: Output error # Output layer error is the difference between desired target and actual output. output_errors = (targets - final_outputs ) # TODO: Backpropagated error # errors propagated to the hidden layer hidden_grad = self . sigmoid_derivative(hidden_outputs) # hidden layer gradients hidden_errors = np . dot( self . weights_hidden_to_output . T,output_errors) if verbose: results[ \"output_errors\" ] = output_errors results[ \"hidden_grad\" ] = hidden_grad results[ \"hidden_errors\" ] = hidden_errors # TODO: Update the weights # update hidden-to-output weights with gradient descent step self . weights_hidden_to_output += self . lr * \\ np . dot(output_errors,hidden_outputs . T) # update input-to-hidden weights with gradient descent step self . weights_input_to_hidden += self . lr * \\ np . dot(hidden_errors * hidden_grad,inputs . T) if verbose: results[ \"weights_hidden_to_output\" ] = self . weights_hidden_to_output results[ \"weights_input_to_hidden\" ] = self . weights_input_to_hidden results[ \"Error\" ] = MSE(final_outputs,targets) return results def run ( self , inputs_list): # Run a forward pass through the network inputs = np . array(inputs_list, ndmin = 2 ) . T #### Implement the forward pass here #### # TODO: Hidden layer hidden_inputs = np . dot( self . weights_input_to_hidden,inputs) # signals into hidden layer hidden_outputs = self . activation_function(hidden_inputs) # signals from hidden layer # TODO: Output layer final_inputs = np . dot( self . weights_hidden_to_output,hidden_outputs) # signals into final output layer final_outputs = final_inputs # signals from final output layer #print(self.weights_input_to_hidden) return final_outputs My Network Checker Following is the detail theory for input dim =3, hidden layer = 2 and output layer = 1 Feed farward: 1. Hidden input = input_to_hidden_weights * input $$\\left[ {\\begin{array}{c} z_{1}^{2} \\ z_{2}^{2} \\ \\end{array} } \\right] = \\left[ {\\begin{array}{cc} W_{11}^{1} & W_{12}^{1} & W_{13}^{1} \\ W_{21}^{1} & W_{22}^{1} & W_{23}^{1}\\ \\end{array} } \\right]\\left[ {\\begin{array}{c} x_{1} \\ x_{2} \\ x_{3}\\ \\end{array} } \\right] = \\left[ {\\begin{array}{c} W_{11}^{1}x_1 + W_{12}^{1}x_2 + W_{13}^{1}x_3 \\ W_{21}^{1}x_1 + W_{22}^{1}x_2 + W_{23}^{1}x_3 \\ \\end{array} } \\right] $$ 2. Hidden Output = activation(hidden input) $$ \\left[ {\\begin{array}{c} a_{1}^{2} \\ a_{2}^{2} \\ \\end{array} } \\right] = \\left[ {\\begin{array}{c} \\sigma(z^{2} {1}) \\ \\sigma(z^{2} {2}) \\ \\end{array} } \\right] $$ 3. Final input = hidden_to_output_weights* hidden output $$ z_{1}^{3} = \\left[ {\\begin{array}{c} W_{11}^{2} & W_{12}^{2} \\ \\end{array} } \\right]\\left[ {\\begin{array}{c} a_{1}^{2} \\ a_{2}^{2} \\ \\end{array} } \\right] = [ W_{11}^{2}a_1^{2} + W_{12}^{2}a_2^{2}] $$ 4. Final output = activation(final input) $$ a^{3} {1} = \\sigma(z {1}^{3}) $$ inputs = [ 0.5 , - 0.2 , 0.1 ] targets = [ 0.4 ] test_w_i_h = np . array([[ 0.1 , 0.4 , - 0.3 ], [ - 0.2 , 0.5 , 0.2 ]]) test_w_h_o = np . array([[ 0.3 , - 0.1 ]]) NN = NeuralNetwork( 3 , 2 , 1 , 0.5 ) NN . weights_input_to_hidden = test_w_i_h . copy() NN . weights_hidden_to_output = test_w_h_o . copy() NN . train(inputs, targets, verbose = True ) {'Error': 0.090006456864883497, 'final_inputs': array([[ 0.09998924]]), 'final_outputs': array([[ 0.09998924]]), 'hidden_errors': array([[ 0.09000323], [-0.03000108]]), 'hidden_grad': array([[ 0.24977513], [ 0.24798589]]), 'hidden_inputs': array([[-0.06], [-0.18]]), 'hidden_outputs': array([[ 0.4850045 ], [ 0.45512111]]), 'inputs': array([[ 0.5], [-0.2], [ 0.1]]), 'o-weights_hidden_to_output': array([[ 0.37275328, -0.03172939]]), 'o-weights_input_to_hidden': array([[ 0.10562014, 0.39775194, -0.29887597], [-0.20185996, 0.50074398, 0.19962801]]), 'output_errors': array([[ 0.30001076]]), 'targets': array([[ 0.4]]), 'weights_hidden_to_output': array([[ 0.37275328, -0.03172939]]), 'weights_input_to_hidden': array([[ 0.10562014, 0.39775194, -0.29887597], [-0.20185996, 0.50074398, 0.19962801]])} Training the network Here you'll set the hyperparameters for the network. The strategy here is to find hyperparameters such that the error on the training set is low, but you're not overfitting to the data. If you train the network too long or have too many hidden nodes, it can become overly specific to the training set and will fail to generalize to the validation set. That is, the loss on the validation set will start increasing as the training set loss drops. You'll also be using a method know as Stochastic Gradient Descent (SGD) to train the network. The idea is that for each training pass, you grab a random sample of the data instead of using the whole data set. You use many more training passes than with normal gradient descent, but each pass is much faster. This ends up training the network more efficiently. You'll learn more about SGD later. Choose the number of epochs This is the number of times the dataset will pass through the network, each time updating the weights. As the number of epochs increases, the network becomes better and better at predicting the targets in the training set. You'll need to choose enough epochs to train the network well but not too many or you'll be overfitting. Choose the learning rate This scales the size of weight updates. If this is too big, the weights tend to explode and the network fails to fit the data. A good choice to start at is 0.1. If the network has problems fitting the data, try reducing the learning rate. Note that the lower the learning rate, the smaller the steps are in the weight updates and the longer it takes for the neural network to converge. Choose the number of hidden nodes The more hidden nodes you have, the more accurate predictions the model will make. Try a few different numbers and see how it affects the performance. You can look at the losses dictionary for a metric of the network performance. If the number of hidden units is too low, then the model won't have enough space to learn and if it is too high there are too many options for the direction that the learning can take. The trick here is to find the right balance in number of hidden units you choose. train_features . shape (15435, 56) import sys ### Set the hyperparameters here ### epochs = 1000 learning_rate = 0.1 hidden_nodes = 28 output_nodes = 1 N_i = train_features . shape[ 1 ] network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate) losses = { 'train' :[], 'validation' :[]} for e in range (epochs): # Go through a random batch of 128 records from the training data set batch = np . random . choice(train_features . index, size = 128 ) for record, target in zip (train_features . ix[batch] . values, train_targets . ix[batch][ 'cnt' ]): network . train(record, target, verbose = False ) # Printing out the training progress train_loss = MSE(network . run(train_features), train_targets[ 'cnt' ] . values) val_loss = MSE(network . run(val_features), val_targets[ 'cnt' ] . values) #print(epochs,train_loss,val_loss) sys . stdout . write( \" \\r Progress: \" + str ( 100 * e / float (epochs))[: 4 ] \\ + \"% ... Training loss: \" + str (train_loss)[: 5 ] \\ + \" ... Validation loss: \" + str (val_loss)[: 5 ]) losses[ 'train' ] . append(train_loss) losses[ 'validation' ] . append(val_loss) Progress: 99.9% ... Training loss: 0.058 ... Validation loss: 0.147 plt . plot(losses[ 'train' ], label = 'Training loss' ) plt . plot(losses[ 'validation' ], label = 'Validation loss' ) plt . legend() plt . ylim(ymax = 1.5 ) plt . show() Check out your predictions Here, use the test data to view how well your network is modeling the data. If something is completely wrong here, make sure each step in your network is implemented correctly. fig, ax = plt . subplots(figsize = ( 8 , 4 )) mean, std = scaled_features[ 'cnt' ] predictions = network . run(test_features) * std + mean ax . plot(predictions[ 0 ], label = 'Prediction' ) ax . plot((test_targets[ 'cnt' ] * std + mean) . values, label = 'Data' ) ax . set_xlim(right = len (predictions)) ax . legend() dates = pd . to_datetime(rides . ix[test_data . index][ 'dteday' ]) dates = dates . apply( lambda d: d . strftime( '%b %d ' )) ax . set_xticks(np . arange( len (dates))[ 12 :: 24 ]) _ = ax . set_xticklabels(dates[ 12 :: 24 ], rotation = 45 ) Thinking about your results Answer these questions about your results. How well does the model predict the data? Where does it fail? Why does it fail where it does? Note: You can edit the text in this cell by double clicking on it. When you want to render the text, press control + enter Your answer below Unit tests Run these unit tests to check the correctness of your network implementation. These tests must all be successful to pass the project. import unittest inputs = [ 0.5 , - 0.2 , 0.1 ] targets = [ 0.4 ] test_w_i_h = np . array([[ 0.1 , 0.4 , - 0.3 ], [ - 0.2 , 0.5 , 0.2 ]]) test_w_h_o = np . array([[ 0.3 , - 0.1 ]]) class TestMethods (unittest . TestCase): ########## # Unit tests for data loading ########## def test_data_path ( self ): # Test that file path to dataset has been unaltered self . assertTrue(data_path . lower() == 'bike-sharing-dataset/hour.csv' ) def test_data_loaded ( self ): # Test that data frame loaded self . assertTrue( isinstance (rides, pd . DataFrame)) ########## # Unit tests for network functionality ########## def test_activation ( self ): network = NeuralNetwork( 3 , 2 , 1 , 0.5 ) # Test that the activation function is a sigmoid self . assertTrue(np . all(network . activation_function( 0.5 ) == 1 / ( 1 + np . exp( - 0.5 )))) def test_train ( self ): # Test that weights are updated correctly on training network = NeuralNetwork( 3 , 2 , 1 , 0.5 ) network . weights_input_to_hidden = test_w_i_h . copy() network . weights_hidden_to_output = test_w_h_o . copy() network . train(inputs, targets,verbose = False ) self . assertTrue(np . allclose(network . weights_hidden_to_output, np . array([[ 0.37275328 , - 0.03172939 ]]))) self . assertTrue(np . allclose(network . weights_input_to_hidden, np . array([[ 0.10562014 , 0.39775194 , - 0.29887597 ], [ - 0.20185996 , 0.50074398 , 0.19962801 ]]))) def test_run ( self ): # Test correctness of run method network = NeuralNetwork( 3 , 2 , 1 , 0.5 ) network . weights_input_to_hidden = test_w_i_h . copy() network . weights_hidden_to_output = test_w_h_o . copy() self . assertTrue(np . allclose(network . run(inputs), 0.09998924 )) suite = unittest . TestLoader() . loadTestsFromModule(TestMethods()) unittest . TextTestRunner() . run(suite) ..... ---------------------------------------------------------------------- Ran 5 tests in 0.006s OK &lt;unittest.runner.TextTestResult run=5 errors=0 failures=0&gt; Some Fun Resources: Image:Neural Networks by Simon Following is the detail of (input =3, hidden=2,output =1) THANK YOU!","title":"Perceptron"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#your-first-neural-network","text":"In this project, you'll build your first neural network and use it to predict daily bike rental ridership. We've provided some of the code, but left the implementation of the neural network up to you (for the most part). After you've submitted this project, feel free to explore the data and the model more. % matplotlib inline % config InlineBackend . figure_format = 'retina' import numpy as np import pandas as pd import matplotlib.pyplot as plt","title":"Your first neural network"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#load-and-prepare-the-data","text":"A critical step in working with neural networks is preparing the data correctly. Variables on different scales make it difficult for the network to efficiently learn the correct weights. Below, we've written the code to load and prepare the data. You'll learn more about this soon! data_path = 'Bike-Sharing-Dataset/hour.csv' rides = pd . read_csv(data_path) rides . head() instant dteday season yr mnth hr holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt 0 1 2011-01-01 1 0 1 0 0 6 0 1 0.24 0.2879 0.81 0.0 3 13 16 1 2 2011-01-01 1 0 1 1 0 6 0 1 0.22 0.2727 0.80 0.0 8 32 40 2 3 2011-01-01 1 0 1 2 0 6 0 1 0.22 0.2727 0.80 0.0 5 27 32 3 4 2011-01-01 1 0 1 3 0 6 0 1 0.24 0.2879 0.75 0.0 3 10 13 4 5 2011-01-01 1 0 1 4 0 6 0 1 0.24 0.2879 0.75 0.0 0 1 1","title":"Load and prepare the data"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#checking-out-the-data","text":"This dataset has the number of riders for each hour of each day from January 1 2011 to December 31 2012. The number of riders is split between casual and registered, summed up in the cnt column. You can see the first few rows of the data above. Below is a plot showing the number of bike riders over the first 10 days in the data set. You can see the hourly rentals here. This data is pretty complicated! The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week. Looking at the data above, we also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders. You'll be trying to capture all this with your model. rides[: 24 * 10 ] . plot(x = 'dteday' , y = 'cnt' ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x10cfe1dd8&gt;","title":"Checking out the data"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#dummy-variables","text":"Here we have some categorical variables like season, weather, month. To include these in our model, we'll need to make binary dummy variables. This is simple to do with Pandas thanks to get_dummies() . dummy_fields = [ 'season' , 'weathersit' , 'mnth' , 'hr' , 'weekday' ] for each in dummy_fields: dummies = pd . get_dummies(rides[each], prefix = each, drop_first = False ) rides = pd . concat([rides, dummies], axis = 1 ) fields_to_drop = [ 'instant' , 'dteday' , 'season' , 'weathersit' , 'weekday' , 'atemp' , 'mnth' , 'workingday' , 'hr' ] data = rides . drop(fields_to_drop, axis = 1 ) data . head() yr holiday temp hum windspeed casual registered cnt season_1 season_2 ... hr_21 hr_22 hr_23 weekday_0 weekday_1 weekday_2 weekday_3 weekday_4 weekday_5 weekday_6 0 0 0 0.24 0.81 0.0 3 13 16 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1 0 0 0.22 0.80 0.0 8 32 40 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 2 0 0 0.22 0.80 0.0 5 27 32 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 3 0 0 0.24 0.75 0.0 3 10 13 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 4 0 0 0.24 0.75 0.0 0 1 1 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 5 rows \u00d7 59 columns","title":"Dummy variables"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#scaling-target-variables","text":"To make training the network easier, we'll standardize each of the continuous variables. That is, we'll shift and scale the variables such that they have zero mean and a standard deviation of 1. The scaling factors are saved so we can go backwards when we use the network for predictions. quant_features = [ 'casual' , 'registered' , 'cnt' , 'temp' , 'hum' , 'windspeed' ] # Store scalings in a dictionary so we can convert back later scaled_features = {} for each in quant_features: mean, std = data[each] . mean(), data[each] . std() scaled_features[each] = [mean, std] data . loc[:, each] = (data[each] - mean) / std","title":"Scaling target variables"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#splitting-the-data-into-training-testing-and-validation-sets","text":"We'll save the last 21 days of the data to use as a test set after we've trained the network. We'll use this set to make predictions and compare them with the actual number of riders. # Save the last 21 days test_data = data[ - 21 * 24 :] data = data[: - 21 * 24 ] # Separate the data into features and targets target_fields = [ 'cnt' , 'casual' , 'registered' ] features, targets = data . drop(target_fields, axis = 1 ), data[target_fields] test_features, test_targets = test_data . drop(target_fields, axis = 1 ), test_data[target_fields] We'll split the data into two sets, one for training and one for validating as the network is being trained. Since this is time series data, we'll train on historical data, then try to predict on future data (the validation set). # Hold out the last 60 days of the remaining data as a validation set train_features, train_targets = features[: - 60 * 24 ], targets[: - 60 * 24 ] val_features, val_targets = features[ - 60 * 24 :], targets[ - 60 * 24 :]","title":"Splitting the data into training, testing, and validation sets"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#time-to-build-the-network","text":"Below you'll build your network. We've built out the structure and the backwards pass. You'll implement the forward pass through the network. You'll also set the hyperparameters: the learning rate, the number of hidden units, and the number of training passes. The network has two layers, a hidden layer and an output layer. The hidden layer will use the sigmoid function for activations. The output layer has only one node and is used for the regression, the output of the node is the same as the input of the node. That is, the activation function is $f(x)=x$. A function that takes the input signal and generates an output signal, but takes into account the threshold, is called an activation function. We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer. This process is called forward propagation . We use the weights to propagate signals forward from the input to the output layers in a neural network. We use the weights to also propagate error backwards from the output back into the network to update our weights. This is called backpropagation . Hint: You'll need the derivative of the output activation function ($f(x) = x$) for the backpropagation implementation. If you aren't familiar with calculus, this function is equivalent to the equation $y = x$. What is the slope of that equation? That is the derivative of $f(x)$. Below, you have these tasks: 1. Implement the sigmoid function to use as the activation function. Set self.activation_function in __init__ to your sigmoid function. 2. Implement the forward pass in the train method. 3. Implement the backpropagation algorithm in the train method, including calculating the output error. 4. Implement the forward pass in the run method. def sigmoid (x): return 1 / ( 1 + np . exp( - x)) def MSE (y, Y): return np . mean((y - Y) ** 2 ) class NeuralNetwork ( object ): def __init__ ( self , input_nodes, hidden_nodes, output_nodes, learning_rate): # Set number of nodes in input, hidden and output layers. self . input_nodes = input_nodes self . hidden_nodes = hidden_nodes self . output_nodes = output_nodes # Initialize weights self . weights_input_to_hidden = np . random . normal( 0.0 , self . hidden_nodes **- 0.5 , ( self . hidden_nodes, self . input_nodes)) self . weights_hidden_to_output = np . random . normal( 0.0 , self . output_nodes **- 0.5 , ( self . output_nodes, self . hidden_nodes)) self . lr = learning_rate #### Set this to your implemented sigmoid function #### # Activation function is the sigmoid function #self.activation_function = lambda x: 1 / ( 1 + np.exp(-x)) self . activation_function = sigmoid # Gradient of Sigmoid curve def sigmoid_derivative ( self , z): return z * ( 1 - z) def train ( self , inputs_list, targets_list, verbose): # Convert inputs list to 2d array inputs = np . array(inputs_list, ndmin = 2 ) . T targets = np . array(targets_list, ndmin = 2 ) . T results = {} if verbose: results[ \"inputs\" ] = inputs results[ \"targets\" ] = targets results[ \"o-weights_input_to_hidden\" ] = self . weights_input_to_hidden results[ \"o-weights_hidden_to_output\" ] = self . weights_hidden_to_output #### Implement the forward pass here #### ### Forward pass ### # TODO: Hidden layer hidden_inputs = np . dot( self . weights_input_to_hidden,inputs) # signals into hidden layer hidden_outputs = self . activation_function(hidden_inputs) # signals from hidden layer # TODO: Output layer final_inputs = np . dot( self . weights_hidden_to_output,hidden_outputs) # signals into final output layer final_outputs = final_inputs # signals from final output layer if verbose: results[ \"hidden_inputs\" ] = hidden_inputs results[ \"hidden_outputs\" ] = hidden_outputs results[ \"final_inputs\" ] = final_inputs results[ \"final_outputs\" ] = final_outputs #### Implement the backward pass here #### ### Backward pass ### # TODO: Output error # Output layer error is the difference between desired target and actual output. output_errors = (targets - final_outputs ) # TODO: Backpropagated error # errors propagated to the hidden layer hidden_grad = self . sigmoid_derivative(hidden_outputs) # hidden layer gradients hidden_errors = np . dot( self . weights_hidden_to_output . T,output_errors) if verbose: results[ \"output_errors\" ] = output_errors results[ \"hidden_grad\" ] = hidden_grad results[ \"hidden_errors\" ] = hidden_errors # TODO: Update the weights # update hidden-to-output weights with gradient descent step self . weights_hidden_to_output += self . lr * \\ np . dot(output_errors,hidden_outputs . T) # update input-to-hidden weights with gradient descent step self . weights_input_to_hidden += self . lr * \\ np . dot(hidden_errors * hidden_grad,inputs . T) if verbose: results[ \"weights_hidden_to_output\" ] = self . weights_hidden_to_output results[ \"weights_input_to_hidden\" ] = self . weights_input_to_hidden results[ \"Error\" ] = MSE(final_outputs,targets) return results def run ( self , inputs_list): # Run a forward pass through the network inputs = np . array(inputs_list, ndmin = 2 ) . T #### Implement the forward pass here #### # TODO: Hidden layer hidden_inputs = np . dot( self . weights_input_to_hidden,inputs) # signals into hidden layer hidden_outputs = self . activation_function(hidden_inputs) # signals from hidden layer # TODO: Output layer final_inputs = np . dot( self . weights_hidden_to_output,hidden_outputs) # signals into final output layer final_outputs = final_inputs # signals from final output layer #print(self.weights_input_to_hidden) return final_outputs","title":"Time to build the network"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#my-network-checker","text":"Following is the detail theory for input dim =3, hidden layer = 2 and output layer = 1","title":"My Network Checker"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#feed-farward","text":"","title":"Feed farward:"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#1-hidden-input-input_to_hidden_weights-input","text":"$$\\left[ {\\begin{array}{c} z_{1}^{2} \\ z_{2}^{2} \\ \\end{array} } \\right] = \\left[ {\\begin{array}{cc} W_{11}^{1} & W_{12}^{1} & W_{13}^{1} \\ W_{21}^{1} & W_{22}^{1} & W_{23}^{1}\\ \\end{array} } \\right]\\left[ {\\begin{array}{c} x_{1} \\ x_{2} \\ x_{3}\\ \\end{array} } \\right] = \\left[ {\\begin{array}{c} W_{11}^{1}x_1 + W_{12}^{1}x_2 + W_{13}^{1}x_3 \\ W_{21}^{1}x_1 + W_{22}^{1}x_2 + W_{23}^{1}x_3 \\ \\end{array} } \\right] $$","title":"1. Hidden input = input_to_hidden_weights * input"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#2-hidden-output-activationhidden-input","text":"$$ \\left[ {\\begin{array}{c} a_{1}^{2} \\ a_{2}^{2} \\ \\end{array} } \\right] = \\left[ {\\begin{array}{c} \\sigma(z^{2} {1}) \\ \\sigma(z^{2} {2}) \\ \\end{array} } \\right] $$","title":"2. Hidden Output  = activation(hidden input)"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#3-final-input-hidden_to_output_weights-hidden-output","text":"$$ z_{1}^{3} = \\left[ {\\begin{array}{c} W_{11}^{2} & W_{12}^{2} \\ \\end{array} } \\right]\\left[ {\\begin{array}{c} a_{1}^{2} \\ a_{2}^{2} \\ \\end{array} } \\right] = [ W_{11}^{2}a_1^{2} + W_{12}^{2}a_2^{2}] $$","title":"3. Final input   = hidden_to_output_weights* hidden output"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#4-final-output-activationfinal-input","text":"$$ a^{3} {1} = \\sigma(z {1}^{3}) $$ inputs = [ 0.5 , - 0.2 , 0.1 ] targets = [ 0.4 ] test_w_i_h = np . array([[ 0.1 , 0.4 , - 0.3 ], [ - 0.2 , 0.5 , 0.2 ]]) test_w_h_o = np . array([[ 0.3 , - 0.1 ]]) NN = NeuralNetwork( 3 , 2 , 1 , 0.5 ) NN . weights_input_to_hidden = test_w_i_h . copy() NN . weights_hidden_to_output = test_w_h_o . copy() NN . train(inputs, targets, verbose = True ) {'Error': 0.090006456864883497, 'final_inputs': array([[ 0.09998924]]), 'final_outputs': array([[ 0.09998924]]), 'hidden_errors': array([[ 0.09000323], [-0.03000108]]), 'hidden_grad': array([[ 0.24977513], [ 0.24798589]]), 'hidden_inputs': array([[-0.06], [-0.18]]), 'hidden_outputs': array([[ 0.4850045 ], [ 0.45512111]]), 'inputs': array([[ 0.5], [-0.2], [ 0.1]]), 'o-weights_hidden_to_output': array([[ 0.37275328, -0.03172939]]), 'o-weights_input_to_hidden': array([[ 0.10562014, 0.39775194, -0.29887597], [-0.20185996, 0.50074398, 0.19962801]]), 'output_errors': array([[ 0.30001076]]), 'targets': array([[ 0.4]]), 'weights_hidden_to_output': array([[ 0.37275328, -0.03172939]]), 'weights_input_to_hidden': array([[ 0.10562014, 0.39775194, -0.29887597], [-0.20185996, 0.50074398, 0.19962801]])}","title":"4. Final output = activation(final input)"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#training-the-network","text":"Here you'll set the hyperparameters for the network. The strategy here is to find hyperparameters such that the error on the training set is low, but you're not overfitting to the data. If you train the network too long or have too many hidden nodes, it can become overly specific to the training set and will fail to generalize to the validation set. That is, the loss on the validation set will start increasing as the training set loss drops. You'll also be using a method know as Stochastic Gradient Descent (SGD) to train the network. The idea is that for each training pass, you grab a random sample of the data instead of using the whole data set. You use many more training passes than with normal gradient descent, but each pass is much faster. This ends up training the network more efficiently. You'll learn more about SGD later.","title":"Training the network"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#choose-the-number-of-epochs","text":"This is the number of times the dataset will pass through the network, each time updating the weights. As the number of epochs increases, the network becomes better and better at predicting the targets in the training set. You'll need to choose enough epochs to train the network well but not too many or you'll be overfitting.","title":"Choose the number of epochs"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#choose-the-learning-rate","text":"This scales the size of weight updates. If this is too big, the weights tend to explode and the network fails to fit the data. A good choice to start at is 0.1. If the network has problems fitting the data, try reducing the learning rate. Note that the lower the learning rate, the smaller the steps are in the weight updates and the longer it takes for the neural network to converge.","title":"Choose the learning rate"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#choose-the-number-of-hidden-nodes","text":"The more hidden nodes you have, the more accurate predictions the model will make. Try a few different numbers and see how it affects the performance. You can look at the losses dictionary for a metric of the network performance. If the number of hidden units is too low, then the model won't have enough space to learn and if it is too high there are too many options for the direction that the learning can take. The trick here is to find the right balance in number of hidden units you choose. train_features . shape (15435, 56) import sys ### Set the hyperparameters here ### epochs = 1000 learning_rate = 0.1 hidden_nodes = 28 output_nodes = 1 N_i = train_features . shape[ 1 ] network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate) losses = { 'train' :[], 'validation' :[]} for e in range (epochs): # Go through a random batch of 128 records from the training data set batch = np . random . choice(train_features . index, size = 128 ) for record, target in zip (train_features . ix[batch] . values, train_targets . ix[batch][ 'cnt' ]): network . train(record, target, verbose = False ) # Printing out the training progress train_loss = MSE(network . run(train_features), train_targets[ 'cnt' ] . values) val_loss = MSE(network . run(val_features), val_targets[ 'cnt' ] . values) #print(epochs,train_loss,val_loss) sys . stdout . write( \" \\r Progress: \" + str ( 100 * e / float (epochs))[: 4 ] \\ + \"% ... Training loss: \" + str (train_loss)[: 5 ] \\ + \" ... Validation loss: \" + str (val_loss)[: 5 ]) losses[ 'train' ] . append(train_loss) losses[ 'validation' ] . append(val_loss) Progress: 99.9% ... Training loss: 0.058 ... Validation loss: 0.147 plt . plot(losses[ 'train' ], label = 'Training loss' ) plt . plot(losses[ 'validation' ], label = 'Validation loss' ) plt . legend() plt . ylim(ymax = 1.5 ) plt . show()","title":"Choose the number of hidden nodes"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#check-out-your-predictions","text":"Here, use the test data to view how well your network is modeling the data. If something is completely wrong here, make sure each step in your network is implemented correctly. fig, ax = plt . subplots(figsize = ( 8 , 4 )) mean, std = scaled_features[ 'cnt' ] predictions = network . run(test_features) * std + mean ax . plot(predictions[ 0 ], label = 'Prediction' ) ax . plot((test_targets[ 'cnt' ] * std + mean) . values, label = 'Data' ) ax . set_xlim(right = len (predictions)) ax . legend() dates = pd . to_datetime(rides . ix[test_data . index][ 'dteday' ]) dates = dates . apply( lambda d: d . strftime( '%b %d ' )) ax . set_xticks(np . arange( len (dates))[ 12 :: 24 ]) _ = ax . set_xticklabels(dates[ 12 :: 24 ], rotation = 45 )","title":"Check out your predictions"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#thinking-about-your-results","text":"Answer these questions about your results. How well does the model predict the data? Where does it fail? Why does it fail where it does? Note: You can edit the text in this cell by double clicking on it. When you want to render the text, press control + enter","title":"Thinking about your results"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#your-answer-below","text":"","title":"Your answer below"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#unit-tests","text":"Run these unit tests to check the correctness of your network implementation. These tests must all be successful to pass the project. import unittest inputs = [ 0.5 , - 0.2 , 0.1 ] targets = [ 0.4 ] test_w_i_h = np . array([[ 0.1 , 0.4 , - 0.3 ], [ - 0.2 , 0.5 , 0.2 ]]) test_w_h_o = np . array([[ 0.3 , - 0.1 ]]) class TestMethods (unittest . TestCase): ########## # Unit tests for data loading ########## def test_data_path ( self ): # Test that file path to dataset has been unaltered self . assertTrue(data_path . lower() == 'bike-sharing-dataset/hour.csv' ) def test_data_loaded ( self ): # Test that data frame loaded self . assertTrue( isinstance (rides, pd . DataFrame)) ########## # Unit tests for network functionality ########## def test_activation ( self ): network = NeuralNetwork( 3 , 2 , 1 , 0.5 ) # Test that the activation function is a sigmoid self . assertTrue(np . all(network . activation_function( 0.5 ) == 1 / ( 1 + np . exp( - 0.5 )))) def test_train ( self ): # Test that weights are updated correctly on training network = NeuralNetwork( 3 , 2 , 1 , 0.5 ) network . weights_input_to_hidden = test_w_i_h . copy() network . weights_hidden_to_output = test_w_h_o . copy() network . train(inputs, targets,verbose = False ) self . assertTrue(np . allclose(network . weights_hidden_to_output, np . array([[ 0.37275328 , - 0.03172939 ]]))) self . assertTrue(np . allclose(network . weights_input_to_hidden, np . array([[ 0.10562014 , 0.39775194 , - 0.29887597 ], [ - 0.20185996 , 0.50074398 , 0.19962801 ]]))) def test_run ( self ): # Test correctness of run method network = NeuralNetwork( 3 , 2 , 1 , 0.5 ) network . weights_input_to_hidden = test_w_i_h . copy() network . weights_hidden_to_output = test_w_h_o . copy() self . assertTrue(np . allclose(network . run(inputs), 0.09998924 )) suite = unittest . TestLoader() . loadTestsFromModule(TestMethods()) unittest . TextTestRunner() . run(suite) ..... ---------------------------------------------------------------------- Ran 5 tests in 0.006s OK &lt;unittest.runner.TextTestResult run=5 errors=0 failures=0&gt;","title":"Unit tests"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#some-fun-resources","text":"Image:Neural Networks by Simon","title":"Some Fun Resources:"},{"location":"ml/perceptron/dlnd-your-first-neural-network/#following-is-the-detail-of-input-3-hidden2output-1","text":"THANK YOU!","title":"Following is the detail of (input =3, hidden=2,output =1)"},{"location":"ml/vehicle-detection/CARND-Project-5/","text":"Udacity Self-Driving Car Nanodegree CarND-Vehicle-Detection The goals / steps of this project are the following: Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector. Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing. Implement a sliding-window technique and use your trained classifier to search for vehicles in images. Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles. Estimate a bounding box for vehicles detected. Imports import matplotlib.image as mpimg import matplotlib.pyplot as plt import numpy as np import cv2 import glob from skimage.feature import hog from sklearn.preprocessing import StandardScaler from sklearn.cross_validation import train_test_split import time from sklearn.svm import LinearSVC import matplotlib.image as mpimg #import seaborn as sns from mpl_toolkits.mplot3d import Axes3D from scipy.ndimage.measurements import label import pickle % matplotlib inline from scipy.ndimage.measurements import label from collections import deque from moviepy.editor import VideoFileClip from IPython.display import HTML import warnings with warnings . catch_warnings(): warnings . filterwarnings( \"ignore\" ,category = DeprecationWarning ) Manual Vehicle Detection Drawing rectangles over an image image = mpimg . imread( 'car.jpg' ) # Define a function that takes an image, a list of bounding boxes, # and optional color tuple and line thickness as inputs # then draws boxes in that color on the output def draw_boxes (img, bboxes, color = ( 0 , 0 , 255 ), thick = 6 ): # Make a copy of the image draw_img = np . copy(img) # Iterate through the bounding boxes for bbox in bboxes: # Draw a rectangle given bbox coordinates cv2 . rectangle(draw_img, bbox[ 0 ], bbox[ 1 ], color, thick) # Return the image copy with boxes drawn return draw_img # Here are the bounding boxes I used bboxes = [(( 275 , 572 ), ( 380 , 510 )), (( 488 , 563 ), ( 549 , 518 )), (( 554 , 543 ), ( 582 , 522 )), (( 601 , 555 ), ( 646 , 522 )), (( 657 , 545 ), ( 685 , 517 )), (( 849 , 678 ), ( 1135 , 512 ))] result = draw_boxes(image, bboxes) plt . imshow(result) &lt;matplotlib.image.AxesImage at 0x11d760908&gt; Templet Matching # Here is your draw_boxes function from the previous exercise def draw_boxes (img, bboxes, color = ( 0 , 0 , 255 ), thick = 6 ): # Make a copy of the image imcopy = np . copy(img) # Iterate through the bounding boxes for bbox in bboxes: # Draw a rectangle given bbox coordinates cv2 . rectangle(imcopy, bbox[ 0 ], bbox[ 1 ], color, thick) # Return the image copy with boxes drawn return imcopy # Define a function to search for template matches # and return a list of bounding boxes def find_matches (img, template_list): # Define an empty list to take bbox coords bbox_list = [] # Define matching method # Other options include: cv2.TM_CCORR_NORMED', 'cv2.TM_CCOEFF', 'cv2.TM_CCORR', # 'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED' method = cv2 . TM_CCOEFF_NORMED # Iterate through template list for temp in template_list: # Read in templates one by one tmp = mpimg . imread(temp) # Use cv2.matchTemplate() to search the image result = cv2 . matchTemplate(img, tmp, method) # Use cv2.minMaxLoc() to extract the location of the best match min_val, max_val, min_loc, max_loc = cv2 . minMaxLoc(result) # Determine a bounding box for the match w, h = (tmp . shape[ 1 ], tmp . shape[ 0 ]) if method in [cv2 . TM_SQDIFF, cv2 . TM_SQDIFF_NORMED]: top_left = min_loc else : top_left = max_loc bottom_right = (top_left[ 0 ] + w, top_left[ 1 ] + h) # Append bbox position to list bbox_list . append((top_left, bottom_right)) # Return the list of bounding boxes return bbox_list Application over a sample image image = mpimg . imread( 'car.jpg' ) #image = mpimg.imread('temp-matching-example-2.jpg') templist = [ 'cutouts/cutout1.jpg' , 'cutouts/cutout2.jpg' , 'cutouts/cutout3.jpg' , 'cutouts/cutout4.jpg' , 'cutouts/cutout5.jpg' , 'cutouts/cutout6.jpg' ] bboxes = find_matches(image, templist) result = draw_boxes(image, bboxes) plt . imshow(result) &lt;matplotlib.image.AxesImage at 0x11e16a978&gt; Getting familiar with Data # Read in our vehicles and non-vehicles cars = glob . glob( 'data/vehicles/**/*.png' ) notcars = glob . glob( 'data/non-vehicles/**/*.png' ) # Define a function to return some characteristics of the dataset def data_look (car_list, notcar_list): data_dict = {} # Define a key in data_dict \"n_cars\" and store the number of car images data_dict[ \"n_cars\" ] = len (car_list) # Define a key \"n_notcars\" and store the number of notcar images data_dict[ \"n_notcars\" ] = len (notcar_list) # Read in a test image, either car or notcar example_img = mpimg . imread(car_list[ 0 ]) # Define a key \"image_shape\" and store the test image shape 3-tuple data_dict[ \"image_shape\" ] = example_img . shape # Define a key \"data_type\" and store the data type of the test image. data_dict[ \"data_type\" ] = example_img . dtype # Return data_dict return data_dict data_info = data_look(cars, notcars) print ( 'Your function returned a count of' , data_info[ \"n_cars\" ], ' cars and' , data_info[ \"n_notcars\" ], ' non-cars' ) print ( 'of size: ' ,data_info[ \"image_shape\" ], ' and data type:' , data_info[ \"data_type\" ]) Your function returned a count of 8792 cars and 8968 non-cars of size: (64, 64, 3) and data type: float32 fig = plt . figure(figsize = ( 10 , 10 )) count = 1 for k in range ( 12 ): # Generate a random index to look at a car image if k % 2 == 0 : ind = np . random . randint( 0 , len (cars)) Img = mpimg . imread(cars[ind]) #fig-1 fig . add_subplot( 3 , 4 ,count) plt . imshow(Img) plt . title( 'Car' ) count += 1 else : ind = np . random . randint( 0 , len (notcars)) Img = mpimg . imread(notcars[ind]) #fig-1 fig . add_subplot( 3 , 4 ,count) plt . imshow(Img) plt . title( 'NotCar ' ) count += 1 Feature Extraction class FeatureExtraction ( object ): def __init__ ( self ,hist_bins,hist_bins_range,spatial_size,\\ orient,pix_per_cell,cell_per_block): self . hist_bins = hist_bins # hist_bins=32 self . hist_bins_range = hist_bins_range # hist_bins_range = (0, 256) self . spatial_size = spatial_size # spatial_size=(32, 32) self . orient = orient # orient=9 self . pix_per_cell = pix_per_cell # pix_per_cell=8 self . cell_per_block = cell_per_block # cell_per_block=2 def bin_spatial ( self ,img): # Use cv2.resize().ravel() to create the feature vector features = cv2 . resize(img, self . spatial_size) . ravel() # Return the feature vector return features def convert_color ( self ,img, cspace): if cspace == 'BGR' : return cv2 . cvtColor(img, cv2 . COLOR_RGB2BGR) if cspace == 'YCrCb' : return cv2 . cvtColor(img, cv2 . COLOR_RGB2YCrCb) if cspace == 'HSV' : return cv2 . cvtColor(img, cv2 . COLOR_RGB2HSV) if cspace == 'LUV' : return cv2 . cvtColor(img, cv2 . COLOR_RGB2LUV) if cspace == 'HLS' : return cv2 . cvtColor(img, cv2 . COLOR_RGB2HLS) if cspace == 'YUV' : return cv2 . cvtColor(img, cv2 . COLOR_RGB2YUV) '''Define a function to compute color histogram features ''' def color_hist ( self ,img,plot_info = False ): # Compute the histogram of the color channels separately ch1_hist = np . histogram(img[:,:, 0 ], bins = self . hist_bins, range = self . hist_bins_range) ch2_hist = np . histogram(img[:,:, 1 ], bins = self . hist_bins, range = self . hist_bins_range) ch3_hist = np . histogram(img[:,:, 2 ], bins = self . hist_bins, range = self . hist_bins_range) bin_edges = ch1_hist[ 1 ] bin_centers = (bin_edges[ 1 :] + bin_edges[ 0 : len (bin_edges) - 1 ]) / 2 # Concatenate the histograms into a single feature vector hist_features = np . concatenate((ch1_hist[ 0 ], ch2_hist[ 0 ], ch3_hist[ 0 ])) # Return the individual histograms, bin_centers and feature vector if plot_info: return ch1_hist,ch2_hist,ch3_hist, bin_centers, hist_features else : return hist_features ''' Define a function to extract features from a list of images Have this function call bin_spatial() and color_hist()''' def extract_color_features ( self ,imgs, cspace = 'RGB' ): # Create a list to append feature vectors to features = [] # Iterate through the list of images for file in imgs: # Read in each one by one image = mpimg . imread( file ) # apply color conversion if other than 'RGB' if cspace != 'RGB' : feature_image = self . convert_color(image,cspace) else : feature_image = np . copy(image) # Apply bin_spatial() to get spatial color features spatial_features = self . bin_spatial(feature_image) # Apply color_hist() also with a color space option now hist_features = self . color_hist(feature_image) # Append the new feature vector to the features list features . append(np . hstack((spatial_features, hist_features))) # Return list of feature vectors return features def get_hog_features ( self ,img,vis,feature_vec): if vis == True : features, hog_image = hog(img, orientations = self . orient, pixels_per_cell = ( self . pix_per_cell, self . pix_per_cell), cells_per_block = ( self . cell_per_block, self . cell_per_block), transform_sqrt = False , visualise = True , feature_vector = feature_vec) return features, hog_image else : features = hog(img, orientations = self . orient, pixels_per_cell = ( self . pix_per_cell, self . pix_per_cell), cells_per_block = ( self . cell_per_block, self . cell_per_block), transform_sqrt = False , visualise = False , feature_vector = feature_vec) return features ''' Define a function to extract features from a list of images Have this function call bin_spatial() and color_hist()''' def extract_hog_features ( self ,imgs, cspace = 'RGB' , hog_channel = 0 ): # Create a list to append feature vectors to features = [] # Iterate through the list of images for file in imgs: # Read in each one by one image = mpimg . imread( file ) # apply color conversion if other than 'RGB' if cspace != 'RGB' : feature_image = self . convert_color(image,cspace) else : feature_image = np . copy(image) # Call get_hog_features() with vis=False, feature_vec=True if hog_channel == 'ALL' : hog_features = [] for channel in range (feature_image . shape[ 2 ]): hog_features . append( self . get_hog_features(feature_image[:,:,channel], vis = False , feature_vec = True )) hog_features = np . ravel(hog_features) else : hog_features = self . get_hog_features(feature_image[:,:,hog_channel], vis = False , feature_vec = True ) # Append the new feature vector to the features list features . append(hog_features) # Return list of feature vectors return features Application of some feature extraction techniques colorspace = 'RGB' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb orient = 9 pix_per_cell = 8 cell_per_block = 2 hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\" hist_bins = 32 hist_bins_range = ( 0 , 256 ) spatial_size = ( 32 , 32 ) FE = FeatureExtraction(hist_bins = hist_bins,\\ hist_bins_range = hist_bins_range,\\ spatial_size = spatial_size,\\ orient = orient,\\ pix_per_cell = pix_per_cell,\\ cell_per_block = cell_per_block) Exploration of different chanals for hist feature vecture fig = plt . figure(figsize = ( 14 , 22 )) count = 1 n1,n2 = 6 , 4 for k in range ( 6 ): if k % 2 == 0 : ind = np . random . randint( 0 , len (cars)) Img = cv2 . imread(cars[ind]) else : ind = np . random . randint( 0 , len (notcars)) Img = cv2 . imread(notcars[ind]) Img = cv2 . cvtColor(Img, cv2 . COLOR_BGR2RGB) ch1_hist,ch2_hist,ch3_hist, bincen, hist_features\\ = FE . color_hist(Img,plot_info = True ) #fig-1 fig . add_subplot(n1,n2,count) plt . imshow(Img) plt . title( ' Example Image' , fontsize = 10 ) count += 1 #fig-2 fig . add_subplot(n1,n2,count) plt . bar(bincen,ch1_hist[ 0 ]) plt . xlim( 0 , 256 ) plt . title( 'R Histogram' ) count += 1 #fig-3 fig . add_subplot(n1,n2,count) plt . bar(bincen,ch2_hist[ 0 ]) plt . xlim( 0 , 256 ) plt . title( 'G Histogram' ) count += 1 #fig-4 fig . add_subplot(n1,n2,count) plt . bar(bincen,ch3_hist[ 0 ]) plt . xlim( 0 , 256 ) plt . title( 'B Histogram' ) count += 1 Plot of concateneted hist feature vector Exploring bin_spatial feature vectors fig = plt . figure(figsize = ( 16 , 20 )) count = 1 n1,n2 = 6 , 2 for k in range ( 6 ): if k % 2 == 0 : ind = np . random . randint( 0 , len (cars)) Img = cv2 . imread(cars[ind]) else : ind = np . random . randint( 0 , len (notcars)) Img = cv2 . imread(notcars[ind]) Img = cv2 . cvtColor(Img, cv2 . COLOR_BGR2RGB) features = FE . bin_spatial(Img) #fig-1 fig . add_subplot(n1,n2,count) plt . imshow(Img) plt . title( 'vehicle Image' , fontsize = 8 ) count += 1 #fig-5 fig . add_subplot( 6 , 2 ,count) plt . hist(features,bins = 255 ) plt . xlim( 0 , 256 ) plt . title( 'spatial_feature' ) count += 1 Exploration of color spaces Img = cv2 . imread( \"000275.png\" ) Img = cv2 . cvtColor(Img, cv2 . COLOR_BGR2RGB) count = 1 n1,n2 = 3 , 2 convs = [ 'BGR' , 'HSV' , 'LUV' , 'HLS' , 'YUV' , 'YCrCb' ] fig = plt . figure(figsize = ( 14 , 10 )) for conv in convs: Img2 = FE . convert_color(Img, conv) #fig-1 fig . add_subplot(n1,n2,count) plt . imshow(Img2) plt . title(conv, fontsize = 15 ) count += 1 Img = cv2 . imread( \"000275.png\" ) Img = cv2 . cvtColor(Img, cv2 . COLOR_BGR2RGB) count = 1 n1,n2 = 6 , 4 convs = [ 'BGR' , 'HSV' , 'LUV' , 'HLS' , 'YUV' , 'YCrCb' ] fig = plt . figure(figsize = ( 16 , 22 )) for conv in convs: Img = FE . convert_color(Img, conv) ch1_hist,ch2_hist,ch3_hist, bincen, hist_features\\ = FE . color_hist(Img,plot_info = True ) #fig-1 fig . add_subplot(n1,n2,count) plt . imshow(Img) plt . title(conv + ' Example Image' , fontsize = 10 ) count += 1 #fig-2 fig . add_subplot(n1,n2,count) plt . bar(bincen,ch1_hist[ 0 ]) plt . xlim( 0 , 256 ) plt . title(conv[ 0 ] + ' Histogram' ) count += 1 #fig-3 fig . add_subplot(n1,n2,count) plt . bar(bincen,ch2_hist[ 0 ]) plt . xlim( 0 , 256 ) plt . title(conv[ 1 ] + ' Histogram' ) count += 1 #fig-4 fig . add_subplot(n1,n2,count) plt . bar(bincen,ch3_hist[ 0 ]) plt . xlim( 0 , 256 ) plt . title(conv[ 2 ] + ' Histogram' ) count += 1 3-D plot of color space def plot3d (pixels, colors_rgb, axis_labels = list ( \"RGB\" ), axis_limits = [( 0 , 255 ), ( 0 , 255 ), ( 0 , 255 )]): \"\"\"Plot pixels in 3D.\"\"\" # Create figure and 3D axes fig = plt . figure(figsize = ( 6 , 6 )) ax = Axes3D(fig) # Set axis limits ax . set_xlim( * axis_limits[ 0 ]) ax . set_ylim( * axis_limits[ 1 ]) ax . set_zlim( * axis_limits[ 2 ]) # Set axis labels and sizes ax . tick_params(axis = 'both' , which = 'major' , labelsize = 14 , pad = 8 ) ax . set_xlabel(axis_labels[ 0 ], fontsize = 16 , labelpad = 16 ) ax . set_ylabel(axis_labels[ 1 ], fontsize = 16 , labelpad = 16 ) ax . set_zlabel(axis_labels[ 2 ], fontsize = 16 , labelpad = 16 ) # Plot pixel values with colors given in colors_rgb ax . scatter( pixels[:, :, 0 ] . ravel(), pixels[:, :, 1 ] . ravel(), pixels[:, :, 2 ] . ravel(), c = colors_rgb . reshape(( - 1 , 3 )), edgecolors = 'none' ) return ax # return Axes3D object for further manipulation Exploring 3D color space Img = cv2 . imread( \"000275.png\" ) scale = max (Img . shape[ 0 ], Img . shape[ 1 ], 64 ) / 64 # at most 64 rows and columns Img = cv2 . resize(Img, (np . int(Img . shape[ 1 ] / scale),\\ np . int(Img . shape[ 0 ] / scale)),\\ interpolation = cv2 . INTER_NEAREST) Img_RGB = cv2 . cvtColor(Img, cv2 . COLOR_BGR2RGB) img_small_rgb = Img_RGB / 255. convs = [ 'BGR' , 'HSV' , 'LUV' , 'HLS' , 'YUV' , 'YCrCb' ] for conv in convs: Img2 = FE . convert_color(Img, conv) #fig-1 plot3d(Img2, img_small_rgb,axis_labels = list (conv)) plt . title(conv, fontsize = 15 ) plt . show() Hog feature visualizations fig = plt . figure(figsize = ( 10 , 10 )) count = 1 for k in range ( 6 ): # Generate a random index to look at a car image if k % 2 == 0 : ind = np . random . randint( 0 , len (cars)) Img = mpimg . imread(cars[ind]) else : ind = np . random . randint( 0 , len (notcars)) Img = mpimg . imread(notcars[ind]) gray = cv2 . cvtColor(Img, cv2 . COLOR_RGB2GRAY) features, hog_image = FE . get_hog_features(gray,vis = True , feature_vec = True ) #fig-1 fig . add_subplot( 3 , 4 ,count) plt . imshow(Img) plt . title( 'Example Image' ) count += 1 #fig-1 fig . add_subplot( 3 , 4 ,count) plt . imshow(hog_image,cmap = 'gray' ) plt . title( 'HOG Visualization' ) count += 1 Application of feature Normalization X_car = FE . extract_color_features(cars,cspace = colorspace) X_notcar = FE . extract_color_features(notcars, cspace = colorspace) X = np . vstack((X_car,X_notcar)) . astype(np . float64) X_scaler = StandardScaler() . fit(X) # Apply the scaler to X scaled_X = X_scaler . transform(X) fig = plt . figure(figsize = ( 14 , 10 )) count = 1 for k in range ( 1 , 5 ): # Generate a random index to look at a car image if k % 2 == 0 : ind = np . random . randint( 0 , len (cars)) Img = mpimg . imread(cars[ind]) else : ind = np . random . randint( 0 , len (notcars)) Img = mpimg . imread(notcars[ind]) plt . subplot( 4 , 3 ,count) plt . imshow(Img) plt . title( 'Original Image' ) count += 1 plt . subplot( 4 , 3 ,count) plt . plot(X[ind]) plt . title( 'Raw Features' ) count += 1 plt . subplot( 4 , 3 ,count) plt . plot(scaled_X[ind]) plt . title( 'Normalized Features' ) count += 1 fig . tight_layout() Classifier Extraction of all required features # Reduce the sample size because HOG features are slow to compute # The quiz evaluator times out after 13s of CPU time #sample_size = 500 #cars = cars[0:sample_size] #notcars = notcars[0:sample_size] ### TODO: Tweak these parameters and see how the results change. colorspace = 'YCrCb' #'RGB' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb orient = 9 pix_per_cell = 8 cell_per_block = 2 hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\" hist_bins = 32 hist_bins_range = ( 0 , 256 ) spatial_size = ( 32 , 32 ) t = time . time() FE = FeatureExtraction(hist_bins = hist_bins,\\ hist_bins_range = hist_bins_range,\\ spatial_size = spatial_size,\\ orient = orient,\\ pix_per_cell = pix_per_cell,\\ cell_per_block = cell_per_block) color_car_features = FE . extract_color_features(cars, cspace = colorspace) color_notcar_features = FE . extract_color_features(notcars, cspace = colorspace) hog_car_features = FE . extract_hog_features(cars, cspace = colorspace, hog_channel = hog_channel) hog_notcar_features = FE . extract_hog_features(notcars, cspace = colorspace, hog_channel = hog_channel) t2 = time . time() print ( round (t2 - t, 2 ), 'Seconds to extract HOG features...' ) 232.56 Seconds to extract HOG features... Stacking all feature to single vector # Create an array stack of feature vectors X_car = np . hstack((color_car_features,hog_car_features)) . astype(np . float64) X_notcar = np . hstack((color_notcar_features, hog_notcar_features)) . astype(np . float64) X = np . vstack((X_car,X_notcar)) . astype(np . float64) X_scaler = StandardScaler() . fit(X) # Apply the scaler to X scaled_X = X_scaler . transform(X) # Define the labels vector y = np . hstack((np . ones( len (hog_car_features)), np . zeros( len (hog_notcar_features)))) Train and test split from sklearn.utils import shuffle scaled_X, y = shuffle(scaled_X, y ) # Split up data into randomized training and test sets rand_state = np . random . randint( 0 , 100 ) X_train, X_test, y_train, y_test = train_test_split( scaled_X, y, test_size = 0.2 , random_state = rand_state) print ( 'Using:' ,orient, 'orientations' ,pix_per_cell, 'pixels per cell and' , cell_per_block, 'cells per block' ) print ( 'Feature vector length:' , len (X_train[ 0 ])) Using: 9 orientations 8 pixels per cell and 2 cells per block Feature vector length: 8460 Training Linear SVC # Use a linear SVC svc = LinearSVC() # Check the training time for the SVC t = time . time() svc . fit(X_train, y_train) t2 = time . time() print ( round (t2 - t, 2 ), 'Seconds to train SVC...' ) # Check the score of the SVC print ( 'Test Accuracy of SVC = ' , round (svc . score(X_test, y_test), 4 )) # Check the prediction time for a single sample t = time . time() n_predict = 10 print ( 'My SVC predicts: ' , svc . predict(X_test[ 0 :n_predict])) print ( 'For these' ,n_predict, 'labels: ' , y_test[ 0 :n_predict]) t2 = time . time() print ( round (t2 - t, 5 ), 'Seconds to predict' , n_predict, 'labels with SVC' ) 392.39 Seconds to train SVC... Test Accuracy of SVC = 0.9904 My SVC predicts: [ 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.] For these 10 labels: [ 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.] 2.42908 Seconds to predict 10 labels with SVC Saving model for later use import pickle as pickle dist_pickle = {} dist_pickle[ \"svc\" ] = svc dist_pickle[ \"scaler\" ] = X_scaler dist_pickle[ \"orient\" ] = orient dist_pickle[ \"pix_per_cell\" ] = pix_per_cell dist_pickle[ \"cell_per_block\" ] = cell_per_block dist_pickle[ \"spatial_size\" ] = spatial_size dist_pickle[ \"hist_bins\" ] = hist_bins with open ( 'svc_pickle.p' , mode = 'wb' ) as f: pickle . dump(dist_pickle, f) Applying classifier to an image def find_cars (img, ystart, ystop, scale, svc,\\ X_scaler, color_space, orient,\\ pix_per_cell, cell_per_block,\\ hog_channel,spatial_size, hist_bins): FE = FeatureExtraction(hist_bins = hist_bins,\\ hist_bins_range = hist_bins_range,\\ spatial_size = spatial_size,\\ orient = orient,\\ pix_per_cell = pix_per_cell,\\ cell_per_block = cell_per_block) draw_img = np . copy(img) img = img . astype(np . float32) / 255 img_tosearch = img[ystart:ystop, :, :] ctrans_tosearch = FE . convert_color(img_tosearch, color_space) if scale != 1 : imshape = ctrans_tosearch . shape ctrans_tosearch = cv2 . resize(ctrans_tosearch,\\ (np . int(imshape[ 1 ] / scale), np . int(imshape[ 0 ] / scale))) ch1 = ctrans_tosearch[:, :, 0 ] ch2 = ctrans_tosearch[:, :, 1 ] ch3 = ctrans_tosearch[:, :, 2 ] # Define blocks and steps as above nxblocks = (ch1 . shape[ 1 ] // pix_per_cell) - 1 nyblocks = (ch1 . shape[ 0 ] // pix_per_cell) - 1 nfeat_per_block = orient * cell_per_block ** 2 # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell window = 64 nblocks_per_window = (window // pix_per_cell) - 1 cells_per_step = 2 # Instead of overlap, define how many cells to step nxsteps = (nxblocks - nblocks_per_window) // cells_per_step nysteps = (nyblocks - nblocks_per_window) // cells_per_step # Compute individual channel HOG features for the entire image if hog_channel == 'ALL' : hog1 = FE . get_hog_features(ch1,vis = False , feature_vec = False ) hog2 = FE . get_hog_features(ch2,vis = False , feature_vec = False ) hog3 = FE . get_hog_features(ch3,vis = False , feature_vec = False ) else : hog1 = FE . get_hog_features(hog_channel,vis = False , feature_vec = False ) for xb in range (nxsteps): for yb in range (nysteps): ypos = yb * cells_per_step xpos = xb * cells_per_step # Extract HOG for this patch if hog_channel == 'ALL' : hog_feat1 = hog1[ypos:ypos + nblocks_per_window,\\ xpos:xpos + nblocks_per_window] . ravel() hog_feat2 = hog2[ypos:ypos + nblocks_per_window,\\ xpos:xpos + nblocks_per_window] . ravel() hog_feat3 = hog3[ypos:ypos + nblocks_per_window,\\ xpos:xpos + nblocks_per_window] . ravel() hog_features = np . hstack((hog_feat1, hog_feat2, hog_feat3)) else : hog_features = hog1[ypos:ypos + nblocks_per_window,\\ xpos:xpos + nblocks_per_window] . ravel() xleft = xpos * pix_per_cell ytop = ypos * pix_per_cell # Extract the image patch subimg = cv2 . resize(ctrans_tosearch[ytop:ytop + \\ window, xleft:xleft + window], ( 64 , 64 )) # Get color features spatial_features = FE . bin_spatial(subimg) hist_features = FE . color_hist(subimg) # Scale features and make a prediction test_features = X_scaler . transform( np . hstack((spatial_features, hist_features, hog_features)) . reshape( 1 , - 1 )) # test_features = X_scaler.transform(np.hstack((shape_feat, hist_feat)).reshape(1, -1)) test_prediction = svc . predict(test_features) if test_prediction == 1 : xbox_left = np . int(xleft * scale) ytop_draw = np . int(ytop * scale) win_draw = np . int(window * scale) cv2 . rectangle(draw_img, (xbox_left, ytop_draw + ystart), (xbox_left + win_draw, ytop_draw + win_draw + ystart), ( 0 , 0 , 255 ), 6 ) return draw_img directory = 'test_images/*.jpg' globDir = glob . glob(directory) fig = plt . figure(figsize = ( 16 , 14 )) n1 = 4 n2 = 4 color_space = 'YCrCb' ystart = 400 ystop = 656 scale = 1.5 count = 1 for k,img in enumerate (globDir): Img = mpimg . imread(img) out_img = find_cars(Img, ystart, ystop, scale,\\ svc, X_scaler, color_space,\\ orient, pix_per_cell, cell_per_block,\\ hog_channel, spatial_size, hist_bins) #fig-1 fig . add_subplot(n1,n2,count) plt . imshow(Img) plt . title( 'Original Image' ) count += 1 #fig-2 fig . add_subplot(n1,n2,count) plt . imshow(out_img) plt . title( 'Box-image' ) count += 1 Utilities Sliding window def slide_window (img, x_start_stop = [ None , None ],\\ y_start_stop = [ None , None ],\\ xy_window = ( 64 , 64 ),\\ xy_overlap = ( 0.5 , 0.5 )): # If x and/or y start/stop positions not defined, set to image size if x_start_stop[ 0 ] == None : x_start_stop[ 0 ] = 0 if x_start_stop[ 1 ] == None : x_start_stop[ 1 ] = img . shape[ 1 ] if y_start_stop[ 0 ] == None : y_start_stop[ 0 ] = 0 if y_start_stop[ 1 ] == None : y_start_stop[ 1 ] = img . shape[ 0 ] # Compute the span of the region to be searched xspan = x_start_stop[ 1 ] - x_start_stop[ 0 ] yspan = y_start_stop[ 1 ] - y_start_stop[ 0 ] # Compute the number of pixels per step in x/y nx_pix_per_step = np . int(xy_window[ 0 ] * ( 1 - xy_overlap[ 0 ])) ny_pix_per_step = np . int(xy_window[ 1 ] * ( 1 - xy_overlap[ 1 ])) # Compute the number of windows in x/y nx_buffer = np . int(xy_window[ 0 ] * (xy_overlap[ 0 ])) ny_buffer = np . int(xy_window[ 1 ] * (xy_overlap[ 1 ])) nx_windows = np . int((xspan - nx_buffer) / nx_pix_per_step) ny_windows = np . int((yspan - ny_buffer) / ny_pix_per_step) # Initialize a list to append window positions to window_list = [] # Loop through finding x and y window positions # Note: you could vectorize this step, but in practice # you'll be considering windows one by one with your # classifier, so looping makes sense for ys in range (ny_windows): for xs in range (nx_windows): # Calculate window position startx = xs * nx_pix_per_step + x_start_stop[ 0 ] endx = startx + xy_window[ 0 ] starty = ys * ny_pix_per_step + y_start_stop[ 0 ] endy = starty + xy_window[ 1 ] # Append window position to list window_list . append(((startx, starty), (endx, endy))) # Return the list of windows return window_list image = mpimg . imread( 'car.jpg' ) windows = slide_window(image, x_start_stop = [ None , None ], y_start_stop = [ None , None ], xy_window = ( 128 , 128 ), xy_overlap = ( 0.5 , 0.5 )) window_img = draw_boxes(image, windows, color = ( 0 , 0 , 255 ), thick = 6 ) plt . imshow(window_img) &lt;matplotlib.image.AxesImage at 0x1239f9a58&gt; Multi scale window def draw_multi_scale_windows (img, ystart, ystop, scale): draw_img = np . copy(img) img = img . astype(np . float32) / 255 img_tosearch = img[ystart:ystop, :, :] imshape = img_tosearch . shape img_tosearch = cv2 . resize(img_tosearch,\\ (np . int(imshape[ 1 ] / scale),\\ np . int(imshape[ 0 ] / scale))) # Define blocks and steps as above nxblocks = (img_tosearch . shape[ 1 ] // pix_per_cell) - 1 nyblocks = (img_tosearch . shape[ 0 ] // pix_per_cell) - 1 nfeat_per_block = orient * cell_per_block ** 2 window = 64 nblocks_per_window = (window // pix_per_cell) - 1 cells_per_step = 2 # Instead of overlap, define how many cells to step nxsteps = (nxblocks - nblocks_per_window) // cells_per_step nysteps = (nyblocks - nblocks_per_window) // cells_per_step rect_start = None rect_end = None for xb in range (nxsteps + 1 ): for yb in range (nysteps + 1 ): ypos = yb * cells_per_step xpos = xb * cells_per_step xleft = xpos * pix_per_cell ytop = ypos * pix_per_cell xbox_left = np . int(xleft * scale) ytop_draw = np . int(ytop * scale) win_draw = np . int(window * scale) rect_start = (xbox_left, ytop_draw + ystart) rect_end = (xbox_left + win_draw, ytop_draw + win_draw + ystart) cv2 . rectangle(draw_img, rect_start, rect_end, ( 0 , 0 , 255 ), 6 ) cv2 . rectangle(draw_img, rect_start, rect_end, ( 255 , 0 , 0 ), 6 ) return draw_img img = mpimg . imread( 'test_images/test6.jpg' ) plt . figure(figsize = ( 15 , 10 )) subpltcount = 1 for (ystart, ystop, scale) in [( 360 , 560 , 1.5 ), ( 400 , 600 , 1.8 ), ( 440 , 700 , 2.5 )]: plt . subplot( 1 , 3 ,subpltcount) plt . imshow(draw_multi_scale_windows(img, ystart, ystop, scale)) plt . title( 'Scale %s ' % subpltcount) plt . xticks([]) plt . yticks([]) subpltcount += 1 Vehicle Detecton Pipeline class DetectVehicle ( object ): def __init__ ( self ,param_dict): self . param_dict = param_dict self . svc = self . param_dict[ 'svc' ] self . X_scalar = self . param_dict[ 'X_scalar' ] self . orient = self . param_dict[ 'orient' ] self . pix_per_cell = self . param_dict[ 'pix_per_cell' ] self . cell_per_block = self . param_dict[ 'cell_per_block' ] self . hist_bins = self . param_dict[ 'hist_bins' ] self . spatial_size = self . param_dict[ 'spatial_size' ] self . FE = FeatureExtraction(hist_bins = self . hist_bins,\\ hist_bins_range = ( 0 , 256 ),\\ spatial_size = self . spatial_size,\\ orient = self . orient,\\ pix_per_cell = self . pix_per_cell,\\ cell_per_block = self . cell_per_block) self . threshold = 1.0 self . heatmap = None # Heat Image for the Last Three Frames self . heat_images = deque(maxlen = 3 ) # Current Frame Count self . frame_count = 0 self . full_frame_processing_interval = 4 # Xstart self . xstart = 600 # Various Scales self . ystart_ystop_scale = [( 360 , 560 , 1.5 ), ( 400 , 600 , 1.8 ), ( 440 , 700 , 2.5 )] # Kernal For Dilation self . kernel = np . ones(( 50 , 50 )) self . image = None def find_cars ( self ,image,vid = True ,vis = False ): self . image = image box_list = [] draw_img = np . copy( self . image) img = self . image . astype(np . float32) / 255 if vid: # video if self . frame_count % self . full_frame_processing_interval == 0 : mask = np . ones_like(img[:, :, 0 ]) else : mask = np . sum(np . array( self . heat_images), axis = 0 ) mask[(mask > 0 )] = 1 mask = cv2 . dilate(mask, self . kernel, iterations = 1 ) self . frame_count += 1 for ( self . ystart, self . ystop, self . scale) in self . ystart_ystop_scale: nonzero = mask . nonzero() nonzeroy = np . array(nonzero[ 0 ]) nonzerox = np . array(nonzero[ 1 ]) if len (nonzeroy) != 0 : self . ystart = max (np . min(nonzeroy), self . ystart) self . ystop = min (np . max(nonzeroy), self . ystop) if len (nonzeroy) != 0 : x_start = max (np . min(nonzerox), self . xstart) x_stop = np . max(nonzerox) else : continue if x_stop <= x_start or self . ystop <= self . ystart: continue ibox_list = self . window_search(img) for k in range ( len (ibox_list)): box_list . append(ibox_list[k]) # Add heat to each box in box list self . add_heat_and_threshold(draw_img, box_list) # Find final boxes from heatmap using label function labels = label( self . heatmap) draw_img = self . draw_labeled_bboxes(draw_img, labels) return draw_img else : # picture for ( self . ystart, self . ystop, self . scale) in self . ystart_ystop_scale: ibox_list = self . window_search(img) for k in range ( len (ibox_list)): box_list . append(ibox_list[k]) if vis: # visualize picture with all boaxes found return box_list else : # visualize only threshold boxes draw_image = self . window_search(img) # Add heat to each box in box list self . add_heat_and_threshold(draw_img, box_list) # Find final boxes from heatmap using label function labels = label( self . heatmap) draw_img = self . draw_labeled_bboxes(draw_img, labels) return [draw_img, self . heatmap] def window_search ( self ,img): box_list = [] img_tosearch = img[ self . ystart: self . ystop,:,:] ctrans_tosearch = FE . convert_color(img_tosearch, cspace = 'YCrCb' ) if self . scale != 1 : imshape = ctrans_tosearch . shape ctrans_tosearch = cv2 . resize(ctrans_tosearch, (np . int(imshape[ 1 ] / self . scale), np . int(imshape[ 0 ] / self . scale))) ch1 = ctrans_tosearch[:,:, 0 ] ch2 = ctrans_tosearch[:,:, 1 ] ch3 = ctrans_tosearch[:,:, 2 ] # Define blocks and steps as above nxblocks = (ch1 . shape[ 1 ] // self . pix_per_cell) - self . cell_per_block + 1 nyblocks = (ch1 . shape[ 0 ] // self . pix_per_cell) - self . cell_per_block + 1 nfeat_per_block = self . orient * self . cell_per_block ** 2 # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell window = 64 nblocks_per_window = (window // self . pix_per_cell) - self . cell_per_block + 1 cells_per_step = 2 # Instead of overlap, define how many cells to step nxsteps = (nxblocks - nblocks_per_window) // cells_per_step nysteps = (nyblocks - nblocks_per_window) // cells_per_step # Compute individual channel HOG features for the entire image hog1 = FE . get_hog_features(ch1,vis = False ,feature_vec = False ) hog2 = FE . get_hog_features(ch2,vis = False ,feature_vec = False ) hog3 = FE . get_hog_features(ch3,vis = False ,feature_vec = False ) for xb in range (nxsteps): for yb in range (nysteps): ypos = yb * cells_per_step xpos = xb * cells_per_step # Extract HOG for this patch hog_feat1 = hog1[ypos:ypos + nblocks_per_window, xpos:xpos + nblocks_per_window] . ravel() hog_feat2 = hog2[ypos:ypos + nblocks_per_window, xpos:xpos + nblocks_per_window] . ravel() hog_feat3 = hog3[ypos:ypos + nblocks_per_window, xpos:xpos + nblocks_per_window] . ravel() hog_features = np . hstack((hog_feat1, hog_feat2, hog_feat3)) xleft = xpos * self . pix_per_cell ytop = ypos * self . pix_per_cell # Extract the image patch subimg = cv2 . resize(ctrans_tosearch[ytop:ytop + window,\\ xleft:xleft + window], ( 64 , 64 )) # Get color features spatial_features = self . FE . bin_spatial(subimg) hist_features = self . FE . color_hist(subimg) color_features = np . hstack((spatial_features, hist_features)) X = np . hstack((color_features,hog_features)) . reshape( 1 , - 1 ) test_features = X_scaler . transform(X) test_prediction = self . svc . predict(test_features) if test_prediction == 1 : xbox_left = np . int(xleft * self . scale) ytop_draw = np . int(ytop * self . scale) win_draw = np . int(window * self . scale) box_list . append(((xbox_left, ytop_draw + self . ystart),\\ (xbox_left + win_draw, ytop_draw + \\ win_draw + self . ystart))) return box_list def add_heat_and_threshold ( self ,draw_img, bbox_list): # Iterate through list of bboxes h_map = np . zeros_like(draw_img[:,:, 0 ]) . astype(np . float) for box in bbox_list: # Add += 1 for all pixels inside each bbox # Assuming each \"box\" takes the form ((x1, y1), (x2, y2)) h_map[box[ 0 ][ 1 ]:box[ 1 ][ 1 ], box[ 0 ][ 0 ]:box[ 1 ][ 0 ]] += 1 self . heat_images . append(h_map) self . heatmap = np . sum(np . array( self . heat_images),axis = 0 ) # Return thresholded map self . heatmap[ self . heatmap <= self . threshold] = 0 # Return updated heatmap return def draw_labeled_bboxes ( self ,img, labels): # Iterate through all detected cars for car_number in range ( 1 , labels[ 1 ] + 1 ): # Find pixels with each car_number label value nonzero = (labels[ 0 ] == car_number) . nonzero() # Identify x and y values of those pixels nonzeroy = np . array(nonzero[ 0 ]) nonzerox = np . array(nonzero[ 1 ]) # Define a bounding box based on min/max x and y bbox = ((np . min(nonzerox), np . min(nonzeroy)),\\ (np . max(nonzerox), np . max(nonzeroy))) # Draw the box on the image cv2 . rectangle(img, bbox[ 0 ], bbox[ 1 ], ( 0 , 0 , 255 ), 6 ) # Return the image return img Application of sliding window search Parameter setting dist_pickle = pickle . load( open ( \"svc_pickle.p\" , \"rb\" ) ) param_dict = {} param_dict[ 'ystart' ] = 400 param_dict[ 'ystop' ] = 656 param_dict[ 'scale' ] = 1.5 param_dict[ 'svc' ] = dist_pickle[ \"svc\" ] param_dict[ 'X_scalar' ] = dist_pickle[ \"scaler\" ] param_dict[ 'orient' ] = dist_pickle[ \"orient\" ] param_dict[ 'pix_per_cell' ] = dist_pickle[ \"pix_per_cell\" ] param_dict[ 'cell_per_block' ] = dist_pickle[ \"cell_per_block\" ] param_dict[ 'spatial_size' ] = ( 32 , 32 ) param_dict[ 'hist_bins' ] = 32 directory = 'test_images/*.jpg' globDir = glob . glob(directory) fig = plt . figure(figsize = ( 18 , 24 )) DV = DetectVehicle(param_dict) n1 = 8 n2 = 4 count = 1 for k,img in enumerate (globDir): Img = mpimg . imread(img) boxes = DV . find_cars(Img,vid = False ,vis = True ) [draw_img_,heatmap] = DV . find_cars(Img,vid = False ,vis = False ) #fig-1 fig . add_subplot(n1,n2,count) plt . imshow(Img) plt . title( 'Original Image' ) count += 1 #fig-2 fig . add_subplot(n1,n2,count) box_Img = draw_boxes(Img, boxes) plt . imshow(box_Img) plt . title( 'boxes-image' ) count += 1 #fig-3 fig . add_subplot(n1,n2,count) plt . imshow(DV . heat_images[ 0 ]) plt . title( 'Heatmap-sample-1' ) count += 1 #fig-5 fig . add_subplot(n1,n2,count) plt . imshow(DV . heatmap) plt . title( 'Heatmap-threshold' ) count += 1 directory = 'test_images/*.jpg' globDir = glob . glob(directory) fig = plt . figure(figsize = ( 14 , 24 )) DV = DetectVehicle(param_dict) count = 1 for k,img in enumerate (globDir): Img = mpimg . imread(img) [draw_img,heatmap] = DV . find_cars(Img,vid = False ,vis = False ) #fig-1 fig . add_subplot( 8 , 3 ,count) plt . imshow(Img) plt . title( 'Original Image' ) count += 1 #fig-2 fig . add_subplot( 8 , 3 ,count) plt . imshow(heatmap) plt . title( 'threshold-Heatmap' ) count += 1 #fig-3 fig . add_subplot( 8 , 3 ,count) plt . imshow(draw_img) plt . title( 'Vehicle Detection' ) count += 1 Video Detector dist_pickle = pickle . load( open ( \"svc_pickle.p\" , \"rb\" ) ) param_dict = {} param_dict[ 'ystart' ] = 400 param_dict[ 'ystop' ] = 656 param_dict[ 'scale' ] = 1.5 param_dict[ 'svc' ] = dist_pickle[ \"svc\" ] param_dict[ 'X_scalar' ] = dist_pickle[ \"scaler\" ] param_dict[ 'orient' ] = dist_pickle[ \"orient\" ] param_dict[ 'pix_per_cell' ] = dist_pickle[ \"pix_per_cell\" ] param_dict[ 'cell_per_block' ] = dist_pickle[ \"cell_per_block\" ] param_dict[ 'spatial_size' ] = ( 32 , 32 ) param_dict[ 'hist_bins' ] = 32 # Read in image similar to one shown above DV = DetectVehicle(param_dict) white_output = 'test_video_output.mp4' clip = VideoFileClip( \"test_video.mp4\" ) #.subclip(t_start=30,t_end=35) white_clip = clip . fl_image(DV . find_cars) % time white_clip . write_videofile(white_output, audio = False ) HTML( \"\"\" <video width=\"960\" height=\"540\" controls> <source src=\"{0}\"> </video> \"\"\" . format(white_output)) [MoviePy] &gt;&gt;&gt;&gt; Building video test_video_output.mp4 [MoviePy] Writing video test_video_output.mp4 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 38/39 [00:21&lt;00:00, 1.64it/s] [MoviePy] Done. [MoviePy] &gt;&gt;&gt;&gt; Video ready: test_video_output.mp4 CPU times: user 19.3 s, sys: 1.79 s, total: 21.1 s Wall time: 22.2 s white_output = 'project_video_output.mp4' clip = VideoFileClip( \"project_video.mp4\" ) #.subclip(t_start=30,t_end=35) white_clip = clip . fl_image(DV . find_cars) % time white_clip . write_videofile(white_output, audio = False ) HTML( \"\"\" <video width=\"960\" height=\"540\" controls> <source src=\"{0}\"> </video> \"\"\" . format(white_output)) [MoviePy] &gt;&gt;&gt;&gt; Building video project_video_output.mp4 [MoviePy] Writing video project_video_output.mp4 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 1260/1261 [10:04&lt;00:00, 1.94it/s] [MoviePy] Done. [MoviePy] &gt;&gt;&gt;&gt; Video ready: project_video_output.mp4 CPU times: user 9min 8s, sys: 51.8 s, total: 10min Wall time: 10min 5s Both lane and vehicle detection white_output = 'project_video2_output.mp4' clip = VideoFileClip( \"project_video2.mp4\" ) #.subclip(t_start=30,t_end=35) white_clip = clip . fl_image(DV . find_cars) % time white_clip . write_videofile(white_output, audio = False ) HTML( \"\"\" <video width=\"960\" height=\"540\" controls> <source src=\"{0}\"> </video> \"\"\" . format(white_output)) [MoviePy] &gt;&gt;&gt;&gt; Building video project_video2_output.mp4 [MoviePy] Writing video project_video2_output.mp4 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 1260/1261 [10:34&lt;00:00, 1.96it/s] [MoviePy] Done. [MoviePy] &gt;&gt;&gt;&gt; Video ready: project_video2_output.mp4 CPU times: user 9min 40s, sys: 54.3 s, total: 10min 35s Wall time: 10min 34s","title":"Vehicle Detection"},{"location":"ml/vehicle-detection/CARND-Project-5/#udacity-self-driving-car-nanodegree","text":"","title":"Udacity Self-Driving Car Nanodegree"},{"location":"ml/vehicle-detection/CARND-Project-5/#carnd-vehicle-detection","text":"","title":"CarND-Vehicle-Detection"},{"location":"ml/vehicle-detection/CARND-Project-5/#the-goals-steps-of-this-project-are-the-following","text":"Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector. Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing. Implement a sliding-window technique and use your trained classifier to search for vehicles in images. Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles. Estimate a bounding box for vehicles detected.","title":"The goals / steps of this project are the following:"},{"location":"ml/vehicle-detection/CARND-Project-5/#imports","text":"import matplotlib.image as mpimg import matplotlib.pyplot as plt import numpy as np import cv2 import glob from skimage.feature import hog from sklearn.preprocessing import StandardScaler from sklearn.cross_validation import train_test_split import time from sklearn.svm import LinearSVC import matplotlib.image as mpimg #import seaborn as sns from mpl_toolkits.mplot3d import Axes3D from scipy.ndimage.measurements import label import pickle % matplotlib inline from scipy.ndimage.measurements import label from collections import deque from moviepy.editor import VideoFileClip from IPython.display import HTML import warnings with warnings . catch_warnings(): warnings . filterwarnings( \"ignore\" ,category = DeprecationWarning )","title":"Imports"},{"location":"ml/vehicle-detection/CARND-Project-5/#manual-vehicle-detection","text":"","title":"Manual Vehicle Detection"},{"location":"ml/vehicle-detection/CARND-Project-5/#drawing-rectangles-over-an-image","text":"image = mpimg . imread( 'car.jpg' ) # Define a function that takes an image, a list of bounding boxes, # and optional color tuple and line thickness as inputs # then draws boxes in that color on the output def draw_boxes (img, bboxes, color = ( 0 , 0 , 255 ), thick = 6 ): # Make a copy of the image draw_img = np . copy(img) # Iterate through the bounding boxes for bbox in bboxes: # Draw a rectangle given bbox coordinates cv2 . rectangle(draw_img, bbox[ 0 ], bbox[ 1 ], color, thick) # Return the image copy with boxes drawn return draw_img # Here are the bounding boxes I used bboxes = [(( 275 , 572 ), ( 380 , 510 )), (( 488 , 563 ), ( 549 , 518 )), (( 554 , 543 ), ( 582 , 522 )), (( 601 , 555 ), ( 646 , 522 )), (( 657 , 545 ), ( 685 , 517 )), (( 849 , 678 ), ( 1135 , 512 ))] result = draw_boxes(image, bboxes) plt . imshow(result) &lt;matplotlib.image.AxesImage at 0x11d760908&gt;","title":"Drawing rectangles over an image"},{"location":"ml/vehicle-detection/CARND-Project-5/#templet-matching","text":"# Here is your draw_boxes function from the previous exercise def draw_boxes (img, bboxes, color = ( 0 , 0 , 255 ), thick = 6 ): # Make a copy of the image imcopy = np . copy(img) # Iterate through the bounding boxes for bbox in bboxes: # Draw a rectangle given bbox coordinates cv2 . rectangle(imcopy, bbox[ 0 ], bbox[ 1 ], color, thick) # Return the image copy with boxes drawn return imcopy # Define a function to search for template matches # and return a list of bounding boxes def find_matches (img, template_list): # Define an empty list to take bbox coords bbox_list = [] # Define matching method # Other options include: cv2.TM_CCORR_NORMED', 'cv2.TM_CCOEFF', 'cv2.TM_CCORR', # 'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED' method = cv2 . TM_CCOEFF_NORMED # Iterate through template list for temp in template_list: # Read in templates one by one tmp = mpimg . imread(temp) # Use cv2.matchTemplate() to search the image result = cv2 . matchTemplate(img, tmp, method) # Use cv2.minMaxLoc() to extract the location of the best match min_val, max_val, min_loc, max_loc = cv2 . minMaxLoc(result) # Determine a bounding box for the match w, h = (tmp . shape[ 1 ], tmp . shape[ 0 ]) if method in [cv2 . TM_SQDIFF, cv2 . TM_SQDIFF_NORMED]: top_left = min_loc else : top_left = max_loc bottom_right = (top_left[ 0 ] + w, top_left[ 1 ] + h) # Append bbox position to list bbox_list . append((top_left, bottom_right)) # Return the list of bounding boxes return bbox_list","title":"Templet Matching"},{"location":"ml/vehicle-detection/CARND-Project-5/#application-over-a-sample-image","text":"image = mpimg . imread( 'car.jpg' ) #image = mpimg.imread('temp-matching-example-2.jpg') templist = [ 'cutouts/cutout1.jpg' , 'cutouts/cutout2.jpg' , 'cutouts/cutout3.jpg' , 'cutouts/cutout4.jpg' , 'cutouts/cutout5.jpg' , 'cutouts/cutout6.jpg' ] bboxes = find_matches(image, templist) result = draw_boxes(image, bboxes) plt . imshow(result) &lt;matplotlib.image.AxesImage at 0x11e16a978&gt;","title":"Application over a sample image"},{"location":"ml/vehicle-detection/CARND-Project-5/#getting-familiar-with-data","text":"# Read in our vehicles and non-vehicles cars = glob . glob( 'data/vehicles/**/*.png' ) notcars = glob . glob( 'data/non-vehicles/**/*.png' ) # Define a function to return some characteristics of the dataset def data_look (car_list, notcar_list): data_dict = {} # Define a key in data_dict \"n_cars\" and store the number of car images data_dict[ \"n_cars\" ] = len (car_list) # Define a key \"n_notcars\" and store the number of notcar images data_dict[ \"n_notcars\" ] = len (notcar_list) # Read in a test image, either car or notcar example_img = mpimg . imread(car_list[ 0 ]) # Define a key \"image_shape\" and store the test image shape 3-tuple data_dict[ \"image_shape\" ] = example_img . shape # Define a key \"data_type\" and store the data type of the test image. data_dict[ \"data_type\" ] = example_img . dtype # Return data_dict return data_dict data_info = data_look(cars, notcars) print ( 'Your function returned a count of' , data_info[ \"n_cars\" ], ' cars and' , data_info[ \"n_notcars\" ], ' non-cars' ) print ( 'of size: ' ,data_info[ \"image_shape\" ], ' and data type:' , data_info[ \"data_type\" ]) Your function returned a count of 8792 cars and 8968 non-cars of size: (64, 64, 3) and data type: float32 fig = plt . figure(figsize = ( 10 , 10 )) count = 1 for k in range ( 12 ): # Generate a random index to look at a car image if k % 2 == 0 : ind = np . random . randint( 0 , len (cars)) Img = mpimg . imread(cars[ind]) #fig-1 fig . add_subplot( 3 , 4 ,count) plt . imshow(Img) plt . title( 'Car' ) count += 1 else : ind = np . random . randint( 0 , len (notcars)) Img = mpimg . imread(notcars[ind]) #fig-1 fig . add_subplot( 3 , 4 ,count) plt . imshow(Img) plt . title( 'NotCar ' ) count += 1","title":"Getting familiar with Data"},{"location":"ml/vehicle-detection/CARND-Project-5/#feature-extraction","text":"class FeatureExtraction ( object ): def __init__ ( self ,hist_bins,hist_bins_range,spatial_size,\\ orient,pix_per_cell,cell_per_block): self . hist_bins = hist_bins # hist_bins=32 self . hist_bins_range = hist_bins_range # hist_bins_range = (0, 256) self . spatial_size = spatial_size # spatial_size=(32, 32) self . orient = orient # orient=9 self . pix_per_cell = pix_per_cell # pix_per_cell=8 self . cell_per_block = cell_per_block # cell_per_block=2 def bin_spatial ( self ,img): # Use cv2.resize().ravel() to create the feature vector features = cv2 . resize(img, self . spatial_size) . ravel() # Return the feature vector return features def convert_color ( self ,img, cspace): if cspace == 'BGR' : return cv2 . cvtColor(img, cv2 . COLOR_RGB2BGR) if cspace == 'YCrCb' : return cv2 . cvtColor(img, cv2 . COLOR_RGB2YCrCb) if cspace == 'HSV' : return cv2 . cvtColor(img, cv2 . COLOR_RGB2HSV) if cspace == 'LUV' : return cv2 . cvtColor(img, cv2 . COLOR_RGB2LUV) if cspace == 'HLS' : return cv2 . cvtColor(img, cv2 . COLOR_RGB2HLS) if cspace == 'YUV' : return cv2 . cvtColor(img, cv2 . COLOR_RGB2YUV) '''Define a function to compute color histogram features ''' def color_hist ( self ,img,plot_info = False ): # Compute the histogram of the color channels separately ch1_hist = np . histogram(img[:,:, 0 ], bins = self . hist_bins, range = self . hist_bins_range) ch2_hist = np . histogram(img[:,:, 1 ], bins = self . hist_bins, range = self . hist_bins_range) ch3_hist = np . histogram(img[:,:, 2 ], bins = self . hist_bins, range = self . hist_bins_range) bin_edges = ch1_hist[ 1 ] bin_centers = (bin_edges[ 1 :] + bin_edges[ 0 : len (bin_edges) - 1 ]) / 2 # Concatenate the histograms into a single feature vector hist_features = np . concatenate((ch1_hist[ 0 ], ch2_hist[ 0 ], ch3_hist[ 0 ])) # Return the individual histograms, bin_centers and feature vector if plot_info: return ch1_hist,ch2_hist,ch3_hist, bin_centers, hist_features else : return hist_features ''' Define a function to extract features from a list of images Have this function call bin_spatial() and color_hist()''' def extract_color_features ( self ,imgs, cspace = 'RGB' ): # Create a list to append feature vectors to features = [] # Iterate through the list of images for file in imgs: # Read in each one by one image = mpimg . imread( file ) # apply color conversion if other than 'RGB' if cspace != 'RGB' : feature_image = self . convert_color(image,cspace) else : feature_image = np . copy(image) # Apply bin_spatial() to get spatial color features spatial_features = self . bin_spatial(feature_image) # Apply color_hist() also with a color space option now hist_features = self . color_hist(feature_image) # Append the new feature vector to the features list features . append(np . hstack((spatial_features, hist_features))) # Return list of feature vectors return features def get_hog_features ( self ,img,vis,feature_vec): if vis == True : features, hog_image = hog(img, orientations = self . orient, pixels_per_cell = ( self . pix_per_cell, self . pix_per_cell), cells_per_block = ( self . cell_per_block, self . cell_per_block), transform_sqrt = False , visualise = True , feature_vector = feature_vec) return features, hog_image else : features = hog(img, orientations = self . orient, pixels_per_cell = ( self . pix_per_cell, self . pix_per_cell), cells_per_block = ( self . cell_per_block, self . cell_per_block), transform_sqrt = False , visualise = False , feature_vector = feature_vec) return features ''' Define a function to extract features from a list of images Have this function call bin_spatial() and color_hist()''' def extract_hog_features ( self ,imgs, cspace = 'RGB' , hog_channel = 0 ): # Create a list to append feature vectors to features = [] # Iterate through the list of images for file in imgs: # Read in each one by one image = mpimg . imread( file ) # apply color conversion if other than 'RGB' if cspace != 'RGB' : feature_image = self . convert_color(image,cspace) else : feature_image = np . copy(image) # Call get_hog_features() with vis=False, feature_vec=True if hog_channel == 'ALL' : hog_features = [] for channel in range (feature_image . shape[ 2 ]): hog_features . append( self . get_hog_features(feature_image[:,:,channel], vis = False , feature_vec = True )) hog_features = np . ravel(hog_features) else : hog_features = self . get_hog_features(feature_image[:,:,hog_channel], vis = False , feature_vec = True ) # Append the new feature vector to the features list features . append(hog_features) # Return list of feature vectors return features","title":"Feature Extraction"},{"location":"ml/vehicle-detection/CARND-Project-5/#application-of-some-feature-extraction-techniques","text":"colorspace = 'RGB' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb orient = 9 pix_per_cell = 8 cell_per_block = 2 hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\" hist_bins = 32 hist_bins_range = ( 0 , 256 ) spatial_size = ( 32 , 32 ) FE = FeatureExtraction(hist_bins = hist_bins,\\ hist_bins_range = hist_bins_range,\\ spatial_size = spatial_size,\\ orient = orient,\\ pix_per_cell = pix_per_cell,\\ cell_per_block = cell_per_block)","title":"Application of some feature extraction techniques"},{"location":"ml/vehicle-detection/CARND-Project-5/#exploration-of-different-chanals-for-hist-feature-vecture","text":"fig = plt . figure(figsize = ( 14 , 22 )) count = 1 n1,n2 = 6 , 4 for k in range ( 6 ): if k % 2 == 0 : ind = np . random . randint( 0 , len (cars)) Img = cv2 . imread(cars[ind]) else : ind = np . random . randint( 0 , len (notcars)) Img = cv2 . imread(notcars[ind]) Img = cv2 . cvtColor(Img, cv2 . COLOR_BGR2RGB) ch1_hist,ch2_hist,ch3_hist, bincen, hist_features\\ = FE . color_hist(Img,plot_info = True ) #fig-1 fig . add_subplot(n1,n2,count) plt . imshow(Img) plt . title( ' Example Image' , fontsize = 10 ) count += 1 #fig-2 fig . add_subplot(n1,n2,count) plt . bar(bincen,ch1_hist[ 0 ]) plt . xlim( 0 , 256 ) plt . title( 'R Histogram' ) count += 1 #fig-3 fig . add_subplot(n1,n2,count) plt . bar(bincen,ch2_hist[ 0 ]) plt . xlim( 0 , 256 ) plt . title( 'G Histogram' ) count += 1 #fig-4 fig . add_subplot(n1,n2,count) plt . bar(bincen,ch3_hist[ 0 ]) plt . xlim( 0 , 256 ) plt . title( 'B Histogram' ) count += 1","title":"Exploration of different chanals for hist feature vecture"},{"location":"ml/vehicle-detection/CARND-Project-5/#plot-of-concateneted-hist-feature-vector","text":"","title":"Plot of concateneted hist feature vector"},{"location":"ml/vehicle-detection/CARND-Project-5/#exploring-bin_spatial-feature-vectors","text":"fig = plt . figure(figsize = ( 16 , 20 )) count = 1 n1,n2 = 6 , 2 for k in range ( 6 ): if k % 2 == 0 : ind = np . random . randint( 0 , len (cars)) Img = cv2 . imread(cars[ind]) else : ind = np . random . randint( 0 , len (notcars)) Img = cv2 . imread(notcars[ind]) Img = cv2 . cvtColor(Img, cv2 . COLOR_BGR2RGB) features = FE . bin_spatial(Img) #fig-1 fig . add_subplot(n1,n2,count) plt . imshow(Img) plt . title( 'vehicle Image' , fontsize = 8 ) count += 1 #fig-5 fig . add_subplot( 6 , 2 ,count) plt . hist(features,bins = 255 ) plt . xlim( 0 , 256 ) plt . title( 'spatial_feature' ) count += 1","title":"Exploring bin_spatial feature vectors"},{"location":"ml/vehicle-detection/CARND-Project-5/#exploration-of-color-spaces","text":"Img = cv2 . imread( \"000275.png\" ) Img = cv2 . cvtColor(Img, cv2 . COLOR_BGR2RGB) count = 1 n1,n2 = 3 , 2 convs = [ 'BGR' , 'HSV' , 'LUV' , 'HLS' , 'YUV' , 'YCrCb' ] fig = plt . figure(figsize = ( 14 , 10 )) for conv in convs: Img2 = FE . convert_color(Img, conv) #fig-1 fig . add_subplot(n1,n2,count) plt . imshow(Img2) plt . title(conv, fontsize = 15 ) count += 1 Img = cv2 . imread( \"000275.png\" ) Img = cv2 . cvtColor(Img, cv2 . COLOR_BGR2RGB) count = 1 n1,n2 = 6 , 4 convs = [ 'BGR' , 'HSV' , 'LUV' , 'HLS' , 'YUV' , 'YCrCb' ] fig = plt . figure(figsize = ( 16 , 22 )) for conv in convs: Img = FE . convert_color(Img, conv) ch1_hist,ch2_hist,ch3_hist, bincen, hist_features\\ = FE . color_hist(Img,plot_info = True ) #fig-1 fig . add_subplot(n1,n2,count) plt . imshow(Img) plt . title(conv + ' Example Image' , fontsize = 10 ) count += 1 #fig-2 fig . add_subplot(n1,n2,count) plt . bar(bincen,ch1_hist[ 0 ]) plt . xlim( 0 , 256 ) plt . title(conv[ 0 ] + ' Histogram' ) count += 1 #fig-3 fig . add_subplot(n1,n2,count) plt . bar(bincen,ch2_hist[ 0 ]) plt . xlim( 0 , 256 ) plt . title(conv[ 1 ] + ' Histogram' ) count += 1 #fig-4 fig . add_subplot(n1,n2,count) plt . bar(bincen,ch3_hist[ 0 ]) plt . xlim( 0 , 256 ) plt . title(conv[ 2 ] + ' Histogram' ) count += 1","title":"Exploration of color spaces"},{"location":"ml/vehicle-detection/CARND-Project-5/#3-d-plot-of-color-space","text":"def plot3d (pixels, colors_rgb, axis_labels = list ( \"RGB\" ), axis_limits = [( 0 , 255 ), ( 0 , 255 ), ( 0 , 255 )]): \"\"\"Plot pixels in 3D.\"\"\" # Create figure and 3D axes fig = plt . figure(figsize = ( 6 , 6 )) ax = Axes3D(fig) # Set axis limits ax . set_xlim( * axis_limits[ 0 ]) ax . set_ylim( * axis_limits[ 1 ]) ax . set_zlim( * axis_limits[ 2 ]) # Set axis labels and sizes ax . tick_params(axis = 'both' , which = 'major' , labelsize = 14 , pad = 8 ) ax . set_xlabel(axis_labels[ 0 ], fontsize = 16 , labelpad = 16 ) ax . set_ylabel(axis_labels[ 1 ], fontsize = 16 , labelpad = 16 ) ax . set_zlabel(axis_labels[ 2 ], fontsize = 16 , labelpad = 16 ) # Plot pixel values with colors given in colors_rgb ax . scatter( pixels[:, :, 0 ] . ravel(), pixels[:, :, 1 ] . ravel(), pixels[:, :, 2 ] . ravel(), c = colors_rgb . reshape(( - 1 , 3 )), edgecolors = 'none' ) return ax # return Axes3D object for further manipulation","title":"3-D plot of color space"},{"location":"ml/vehicle-detection/CARND-Project-5/#exploring-3d-color-space","text":"Img = cv2 . imread( \"000275.png\" ) scale = max (Img . shape[ 0 ], Img . shape[ 1 ], 64 ) / 64 # at most 64 rows and columns Img = cv2 . resize(Img, (np . int(Img . shape[ 1 ] / scale),\\ np . int(Img . shape[ 0 ] / scale)),\\ interpolation = cv2 . INTER_NEAREST) Img_RGB = cv2 . cvtColor(Img, cv2 . COLOR_BGR2RGB) img_small_rgb = Img_RGB / 255. convs = [ 'BGR' , 'HSV' , 'LUV' , 'HLS' , 'YUV' , 'YCrCb' ] for conv in convs: Img2 = FE . convert_color(Img, conv) #fig-1 plot3d(Img2, img_small_rgb,axis_labels = list (conv)) plt . title(conv, fontsize = 15 ) plt . show()","title":"Exploring 3D color space"},{"location":"ml/vehicle-detection/CARND-Project-5/#hog-feature-visualizations","text":"fig = plt . figure(figsize = ( 10 , 10 )) count = 1 for k in range ( 6 ): # Generate a random index to look at a car image if k % 2 == 0 : ind = np . random . randint( 0 , len (cars)) Img = mpimg . imread(cars[ind]) else : ind = np . random . randint( 0 , len (notcars)) Img = mpimg . imread(notcars[ind]) gray = cv2 . cvtColor(Img, cv2 . COLOR_RGB2GRAY) features, hog_image = FE . get_hog_features(gray,vis = True , feature_vec = True ) #fig-1 fig . add_subplot( 3 , 4 ,count) plt . imshow(Img) plt . title( 'Example Image' ) count += 1 #fig-1 fig . add_subplot( 3 , 4 ,count) plt . imshow(hog_image,cmap = 'gray' ) plt . title( 'HOG Visualization' ) count += 1","title":"Hog feature visualizations"},{"location":"ml/vehicle-detection/CARND-Project-5/#application-of-feature-normalization","text":"X_car = FE . extract_color_features(cars,cspace = colorspace) X_notcar = FE . extract_color_features(notcars, cspace = colorspace) X = np . vstack((X_car,X_notcar)) . astype(np . float64) X_scaler = StandardScaler() . fit(X) # Apply the scaler to X scaled_X = X_scaler . transform(X) fig = plt . figure(figsize = ( 14 , 10 )) count = 1 for k in range ( 1 , 5 ): # Generate a random index to look at a car image if k % 2 == 0 : ind = np . random . randint( 0 , len (cars)) Img = mpimg . imread(cars[ind]) else : ind = np . random . randint( 0 , len (notcars)) Img = mpimg . imread(notcars[ind]) plt . subplot( 4 , 3 ,count) plt . imshow(Img) plt . title( 'Original Image' ) count += 1 plt . subplot( 4 , 3 ,count) plt . plot(X[ind]) plt . title( 'Raw Features' ) count += 1 plt . subplot( 4 , 3 ,count) plt . plot(scaled_X[ind]) plt . title( 'Normalized Features' ) count += 1 fig . tight_layout()","title":"Application of feature Normalization"},{"location":"ml/vehicle-detection/CARND-Project-5/#classifier","text":"","title":"Classifier"},{"location":"ml/vehicle-detection/CARND-Project-5/#extraction-of-all-required-features","text":"# Reduce the sample size because HOG features are slow to compute # The quiz evaluator times out after 13s of CPU time #sample_size = 500 #cars = cars[0:sample_size] #notcars = notcars[0:sample_size] ### TODO: Tweak these parameters and see how the results change. colorspace = 'YCrCb' #'RGB' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb orient = 9 pix_per_cell = 8 cell_per_block = 2 hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\" hist_bins = 32 hist_bins_range = ( 0 , 256 ) spatial_size = ( 32 , 32 ) t = time . time() FE = FeatureExtraction(hist_bins = hist_bins,\\ hist_bins_range = hist_bins_range,\\ spatial_size = spatial_size,\\ orient = orient,\\ pix_per_cell = pix_per_cell,\\ cell_per_block = cell_per_block) color_car_features = FE . extract_color_features(cars, cspace = colorspace) color_notcar_features = FE . extract_color_features(notcars, cspace = colorspace) hog_car_features = FE . extract_hog_features(cars, cspace = colorspace, hog_channel = hog_channel) hog_notcar_features = FE . extract_hog_features(notcars, cspace = colorspace, hog_channel = hog_channel) t2 = time . time() print ( round (t2 - t, 2 ), 'Seconds to extract HOG features...' ) 232.56 Seconds to extract HOG features...","title":"Extraction of all required features"},{"location":"ml/vehicle-detection/CARND-Project-5/#stacking-all-feature-to-single-vector","text":"# Create an array stack of feature vectors X_car = np . hstack((color_car_features,hog_car_features)) . astype(np . float64) X_notcar = np . hstack((color_notcar_features, hog_notcar_features)) . astype(np . float64) X = np . vstack((X_car,X_notcar)) . astype(np . float64) X_scaler = StandardScaler() . fit(X) # Apply the scaler to X scaled_X = X_scaler . transform(X) # Define the labels vector y = np . hstack((np . ones( len (hog_car_features)), np . zeros( len (hog_notcar_features))))","title":"Stacking all feature to single vector"},{"location":"ml/vehicle-detection/CARND-Project-5/#train-and-test-split","text":"from sklearn.utils import shuffle scaled_X, y = shuffle(scaled_X, y ) # Split up data into randomized training and test sets rand_state = np . random . randint( 0 , 100 ) X_train, X_test, y_train, y_test = train_test_split( scaled_X, y, test_size = 0.2 , random_state = rand_state) print ( 'Using:' ,orient, 'orientations' ,pix_per_cell, 'pixels per cell and' , cell_per_block, 'cells per block' ) print ( 'Feature vector length:' , len (X_train[ 0 ])) Using: 9 orientations 8 pixels per cell and 2 cells per block Feature vector length: 8460","title":"Train and test split"},{"location":"ml/vehicle-detection/CARND-Project-5/#training-linear-svc","text":"# Use a linear SVC svc = LinearSVC() # Check the training time for the SVC t = time . time() svc . fit(X_train, y_train) t2 = time . time() print ( round (t2 - t, 2 ), 'Seconds to train SVC...' ) # Check the score of the SVC print ( 'Test Accuracy of SVC = ' , round (svc . score(X_test, y_test), 4 )) # Check the prediction time for a single sample t = time . time() n_predict = 10 print ( 'My SVC predicts: ' , svc . predict(X_test[ 0 :n_predict])) print ( 'For these' ,n_predict, 'labels: ' , y_test[ 0 :n_predict]) t2 = time . time() print ( round (t2 - t, 5 ), 'Seconds to predict' , n_predict, 'labels with SVC' ) 392.39 Seconds to train SVC... Test Accuracy of SVC = 0.9904 My SVC predicts: [ 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.] For these 10 labels: [ 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.] 2.42908 Seconds to predict 10 labels with SVC","title":"Training Linear SVC"},{"location":"ml/vehicle-detection/CARND-Project-5/#saving-model-for-later-use","text":"import pickle as pickle dist_pickle = {} dist_pickle[ \"svc\" ] = svc dist_pickle[ \"scaler\" ] = X_scaler dist_pickle[ \"orient\" ] = orient dist_pickle[ \"pix_per_cell\" ] = pix_per_cell dist_pickle[ \"cell_per_block\" ] = cell_per_block dist_pickle[ \"spatial_size\" ] = spatial_size dist_pickle[ \"hist_bins\" ] = hist_bins with open ( 'svc_pickle.p' , mode = 'wb' ) as f: pickle . dump(dist_pickle, f)","title":"Saving model for later use"},{"location":"ml/vehicle-detection/CARND-Project-5/#applying-classifier-to-an-image","text":"def find_cars (img, ystart, ystop, scale, svc,\\ X_scaler, color_space, orient,\\ pix_per_cell, cell_per_block,\\ hog_channel,spatial_size, hist_bins): FE = FeatureExtraction(hist_bins = hist_bins,\\ hist_bins_range = hist_bins_range,\\ spatial_size = spatial_size,\\ orient = orient,\\ pix_per_cell = pix_per_cell,\\ cell_per_block = cell_per_block) draw_img = np . copy(img) img = img . astype(np . float32) / 255 img_tosearch = img[ystart:ystop, :, :] ctrans_tosearch = FE . convert_color(img_tosearch, color_space) if scale != 1 : imshape = ctrans_tosearch . shape ctrans_tosearch = cv2 . resize(ctrans_tosearch,\\ (np . int(imshape[ 1 ] / scale), np . int(imshape[ 0 ] / scale))) ch1 = ctrans_tosearch[:, :, 0 ] ch2 = ctrans_tosearch[:, :, 1 ] ch3 = ctrans_tosearch[:, :, 2 ] # Define blocks and steps as above nxblocks = (ch1 . shape[ 1 ] // pix_per_cell) - 1 nyblocks = (ch1 . shape[ 0 ] // pix_per_cell) - 1 nfeat_per_block = orient * cell_per_block ** 2 # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell window = 64 nblocks_per_window = (window // pix_per_cell) - 1 cells_per_step = 2 # Instead of overlap, define how many cells to step nxsteps = (nxblocks - nblocks_per_window) // cells_per_step nysteps = (nyblocks - nblocks_per_window) // cells_per_step # Compute individual channel HOG features for the entire image if hog_channel == 'ALL' : hog1 = FE . get_hog_features(ch1,vis = False , feature_vec = False ) hog2 = FE . get_hog_features(ch2,vis = False , feature_vec = False ) hog3 = FE . get_hog_features(ch3,vis = False , feature_vec = False ) else : hog1 = FE . get_hog_features(hog_channel,vis = False , feature_vec = False ) for xb in range (nxsteps): for yb in range (nysteps): ypos = yb * cells_per_step xpos = xb * cells_per_step # Extract HOG for this patch if hog_channel == 'ALL' : hog_feat1 = hog1[ypos:ypos + nblocks_per_window,\\ xpos:xpos + nblocks_per_window] . ravel() hog_feat2 = hog2[ypos:ypos + nblocks_per_window,\\ xpos:xpos + nblocks_per_window] . ravel() hog_feat3 = hog3[ypos:ypos + nblocks_per_window,\\ xpos:xpos + nblocks_per_window] . ravel() hog_features = np . hstack((hog_feat1, hog_feat2, hog_feat3)) else : hog_features = hog1[ypos:ypos + nblocks_per_window,\\ xpos:xpos + nblocks_per_window] . ravel() xleft = xpos * pix_per_cell ytop = ypos * pix_per_cell # Extract the image patch subimg = cv2 . resize(ctrans_tosearch[ytop:ytop + \\ window, xleft:xleft + window], ( 64 , 64 )) # Get color features spatial_features = FE . bin_spatial(subimg) hist_features = FE . color_hist(subimg) # Scale features and make a prediction test_features = X_scaler . transform( np . hstack((spatial_features, hist_features, hog_features)) . reshape( 1 , - 1 )) # test_features = X_scaler.transform(np.hstack((shape_feat, hist_feat)).reshape(1, -1)) test_prediction = svc . predict(test_features) if test_prediction == 1 : xbox_left = np . int(xleft * scale) ytop_draw = np . int(ytop * scale) win_draw = np . int(window * scale) cv2 . rectangle(draw_img, (xbox_left, ytop_draw + ystart), (xbox_left + win_draw, ytop_draw + win_draw + ystart), ( 0 , 0 , 255 ), 6 ) return draw_img directory = 'test_images/*.jpg' globDir = glob . glob(directory) fig = plt . figure(figsize = ( 16 , 14 )) n1 = 4 n2 = 4 color_space = 'YCrCb' ystart = 400 ystop = 656 scale = 1.5 count = 1 for k,img in enumerate (globDir): Img = mpimg . imread(img) out_img = find_cars(Img, ystart, ystop, scale,\\ svc, X_scaler, color_space,\\ orient, pix_per_cell, cell_per_block,\\ hog_channel, spatial_size, hist_bins) #fig-1 fig . add_subplot(n1,n2,count) plt . imshow(Img) plt . title( 'Original Image' ) count += 1 #fig-2 fig . add_subplot(n1,n2,count) plt . imshow(out_img) plt . title( 'Box-image' ) count += 1","title":"Applying classifier to an image"},{"location":"ml/vehicle-detection/CARND-Project-5/#utilities","text":"","title":"Utilities"},{"location":"ml/vehicle-detection/CARND-Project-5/#sliding-window","text":"def slide_window (img, x_start_stop = [ None , None ],\\ y_start_stop = [ None , None ],\\ xy_window = ( 64 , 64 ),\\ xy_overlap = ( 0.5 , 0.5 )): # If x and/or y start/stop positions not defined, set to image size if x_start_stop[ 0 ] == None : x_start_stop[ 0 ] = 0 if x_start_stop[ 1 ] == None : x_start_stop[ 1 ] = img . shape[ 1 ] if y_start_stop[ 0 ] == None : y_start_stop[ 0 ] = 0 if y_start_stop[ 1 ] == None : y_start_stop[ 1 ] = img . shape[ 0 ] # Compute the span of the region to be searched xspan = x_start_stop[ 1 ] - x_start_stop[ 0 ] yspan = y_start_stop[ 1 ] - y_start_stop[ 0 ] # Compute the number of pixels per step in x/y nx_pix_per_step = np . int(xy_window[ 0 ] * ( 1 - xy_overlap[ 0 ])) ny_pix_per_step = np . int(xy_window[ 1 ] * ( 1 - xy_overlap[ 1 ])) # Compute the number of windows in x/y nx_buffer = np . int(xy_window[ 0 ] * (xy_overlap[ 0 ])) ny_buffer = np . int(xy_window[ 1 ] * (xy_overlap[ 1 ])) nx_windows = np . int((xspan - nx_buffer) / nx_pix_per_step) ny_windows = np . int((yspan - ny_buffer) / ny_pix_per_step) # Initialize a list to append window positions to window_list = [] # Loop through finding x and y window positions # Note: you could vectorize this step, but in practice # you'll be considering windows one by one with your # classifier, so looping makes sense for ys in range (ny_windows): for xs in range (nx_windows): # Calculate window position startx = xs * nx_pix_per_step + x_start_stop[ 0 ] endx = startx + xy_window[ 0 ] starty = ys * ny_pix_per_step + y_start_stop[ 0 ] endy = starty + xy_window[ 1 ] # Append window position to list window_list . append(((startx, starty), (endx, endy))) # Return the list of windows return window_list image = mpimg . imread( 'car.jpg' ) windows = slide_window(image, x_start_stop = [ None , None ], y_start_stop = [ None , None ], xy_window = ( 128 , 128 ), xy_overlap = ( 0.5 , 0.5 )) window_img = draw_boxes(image, windows, color = ( 0 , 0 , 255 ), thick = 6 ) plt . imshow(window_img) &lt;matplotlib.image.AxesImage at 0x1239f9a58&gt;","title":"Sliding window"},{"location":"ml/vehicle-detection/CARND-Project-5/#multi-scale-window","text":"def draw_multi_scale_windows (img, ystart, ystop, scale): draw_img = np . copy(img) img = img . astype(np . float32) / 255 img_tosearch = img[ystart:ystop, :, :] imshape = img_tosearch . shape img_tosearch = cv2 . resize(img_tosearch,\\ (np . int(imshape[ 1 ] / scale),\\ np . int(imshape[ 0 ] / scale))) # Define blocks and steps as above nxblocks = (img_tosearch . shape[ 1 ] // pix_per_cell) - 1 nyblocks = (img_tosearch . shape[ 0 ] // pix_per_cell) - 1 nfeat_per_block = orient * cell_per_block ** 2 window = 64 nblocks_per_window = (window // pix_per_cell) - 1 cells_per_step = 2 # Instead of overlap, define how many cells to step nxsteps = (nxblocks - nblocks_per_window) // cells_per_step nysteps = (nyblocks - nblocks_per_window) // cells_per_step rect_start = None rect_end = None for xb in range (nxsteps + 1 ): for yb in range (nysteps + 1 ): ypos = yb * cells_per_step xpos = xb * cells_per_step xleft = xpos * pix_per_cell ytop = ypos * pix_per_cell xbox_left = np . int(xleft * scale) ytop_draw = np . int(ytop * scale) win_draw = np . int(window * scale) rect_start = (xbox_left, ytop_draw + ystart) rect_end = (xbox_left + win_draw, ytop_draw + win_draw + ystart) cv2 . rectangle(draw_img, rect_start, rect_end, ( 0 , 0 , 255 ), 6 ) cv2 . rectangle(draw_img, rect_start, rect_end, ( 255 , 0 , 0 ), 6 ) return draw_img img = mpimg . imread( 'test_images/test6.jpg' ) plt . figure(figsize = ( 15 , 10 )) subpltcount = 1 for (ystart, ystop, scale) in [( 360 , 560 , 1.5 ), ( 400 , 600 , 1.8 ), ( 440 , 700 , 2.5 )]: plt . subplot( 1 , 3 ,subpltcount) plt . imshow(draw_multi_scale_windows(img, ystart, ystop, scale)) plt . title( 'Scale %s ' % subpltcount) plt . xticks([]) plt . yticks([]) subpltcount += 1","title":"Multi scale window"},{"location":"ml/vehicle-detection/CARND-Project-5/#vehicle-detecton-pipeline","text":"class DetectVehicle ( object ): def __init__ ( self ,param_dict): self . param_dict = param_dict self . svc = self . param_dict[ 'svc' ] self . X_scalar = self . param_dict[ 'X_scalar' ] self . orient = self . param_dict[ 'orient' ] self . pix_per_cell = self . param_dict[ 'pix_per_cell' ] self . cell_per_block = self . param_dict[ 'cell_per_block' ] self . hist_bins = self . param_dict[ 'hist_bins' ] self . spatial_size = self . param_dict[ 'spatial_size' ] self . FE = FeatureExtraction(hist_bins = self . hist_bins,\\ hist_bins_range = ( 0 , 256 ),\\ spatial_size = self . spatial_size,\\ orient = self . orient,\\ pix_per_cell = self . pix_per_cell,\\ cell_per_block = self . cell_per_block) self . threshold = 1.0 self . heatmap = None # Heat Image for the Last Three Frames self . heat_images = deque(maxlen = 3 ) # Current Frame Count self . frame_count = 0 self . full_frame_processing_interval = 4 # Xstart self . xstart = 600 # Various Scales self . ystart_ystop_scale = [( 360 , 560 , 1.5 ), ( 400 , 600 , 1.8 ), ( 440 , 700 , 2.5 )] # Kernal For Dilation self . kernel = np . ones(( 50 , 50 )) self . image = None def find_cars ( self ,image,vid = True ,vis = False ): self . image = image box_list = [] draw_img = np . copy( self . image) img = self . image . astype(np . float32) / 255 if vid: # video if self . frame_count % self . full_frame_processing_interval == 0 : mask = np . ones_like(img[:, :, 0 ]) else : mask = np . sum(np . array( self . heat_images), axis = 0 ) mask[(mask > 0 )] = 1 mask = cv2 . dilate(mask, self . kernel, iterations = 1 ) self . frame_count += 1 for ( self . ystart, self . ystop, self . scale) in self . ystart_ystop_scale: nonzero = mask . nonzero() nonzeroy = np . array(nonzero[ 0 ]) nonzerox = np . array(nonzero[ 1 ]) if len (nonzeroy) != 0 : self . ystart = max (np . min(nonzeroy), self . ystart) self . ystop = min (np . max(nonzeroy), self . ystop) if len (nonzeroy) != 0 : x_start = max (np . min(nonzerox), self . xstart) x_stop = np . max(nonzerox) else : continue if x_stop <= x_start or self . ystop <= self . ystart: continue ibox_list = self . window_search(img) for k in range ( len (ibox_list)): box_list . append(ibox_list[k]) # Add heat to each box in box list self . add_heat_and_threshold(draw_img, box_list) # Find final boxes from heatmap using label function labels = label( self . heatmap) draw_img = self . draw_labeled_bboxes(draw_img, labels) return draw_img else : # picture for ( self . ystart, self . ystop, self . scale) in self . ystart_ystop_scale: ibox_list = self . window_search(img) for k in range ( len (ibox_list)): box_list . append(ibox_list[k]) if vis: # visualize picture with all boaxes found return box_list else : # visualize only threshold boxes draw_image = self . window_search(img) # Add heat to each box in box list self . add_heat_and_threshold(draw_img, box_list) # Find final boxes from heatmap using label function labels = label( self . heatmap) draw_img = self . draw_labeled_bboxes(draw_img, labels) return [draw_img, self . heatmap] def window_search ( self ,img): box_list = [] img_tosearch = img[ self . ystart: self . ystop,:,:] ctrans_tosearch = FE . convert_color(img_tosearch, cspace = 'YCrCb' ) if self . scale != 1 : imshape = ctrans_tosearch . shape ctrans_tosearch = cv2 . resize(ctrans_tosearch, (np . int(imshape[ 1 ] / self . scale), np . int(imshape[ 0 ] / self . scale))) ch1 = ctrans_tosearch[:,:, 0 ] ch2 = ctrans_tosearch[:,:, 1 ] ch3 = ctrans_tosearch[:,:, 2 ] # Define blocks and steps as above nxblocks = (ch1 . shape[ 1 ] // self . pix_per_cell) - self . cell_per_block + 1 nyblocks = (ch1 . shape[ 0 ] // self . pix_per_cell) - self . cell_per_block + 1 nfeat_per_block = self . orient * self . cell_per_block ** 2 # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell window = 64 nblocks_per_window = (window // self . pix_per_cell) - self . cell_per_block + 1 cells_per_step = 2 # Instead of overlap, define how many cells to step nxsteps = (nxblocks - nblocks_per_window) // cells_per_step nysteps = (nyblocks - nblocks_per_window) // cells_per_step # Compute individual channel HOG features for the entire image hog1 = FE . get_hog_features(ch1,vis = False ,feature_vec = False ) hog2 = FE . get_hog_features(ch2,vis = False ,feature_vec = False ) hog3 = FE . get_hog_features(ch3,vis = False ,feature_vec = False ) for xb in range (nxsteps): for yb in range (nysteps): ypos = yb * cells_per_step xpos = xb * cells_per_step # Extract HOG for this patch hog_feat1 = hog1[ypos:ypos + nblocks_per_window, xpos:xpos + nblocks_per_window] . ravel() hog_feat2 = hog2[ypos:ypos + nblocks_per_window, xpos:xpos + nblocks_per_window] . ravel() hog_feat3 = hog3[ypos:ypos + nblocks_per_window, xpos:xpos + nblocks_per_window] . ravel() hog_features = np . hstack((hog_feat1, hog_feat2, hog_feat3)) xleft = xpos * self . pix_per_cell ytop = ypos * self . pix_per_cell # Extract the image patch subimg = cv2 . resize(ctrans_tosearch[ytop:ytop + window,\\ xleft:xleft + window], ( 64 , 64 )) # Get color features spatial_features = self . FE . bin_spatial(subimg) hist_features = self . FE . color_hist(subimg) color_features = np . hstack((spatial_features, hist_features)) X = np . hstack((color_features,hog_features)) . reshape( 1 , - 1 ) test_features = X_scaler . transform(X) test_prediction = self . svc . predict(test_features) if test_prediction == 1 : xbox_left = np . int(xleft * self . scale) ytop_draw = np . int(ytop * self . scale) win_draw = np . int(window * self . scale) box_list . append(((xbox_left, ytop_draw + self . ystart),\\ (xbox_left + win_draw, ytop_draw + \\ win_draw + self . ystart))) return box_list def add_heat_and_threshold ( self ,draw_img, bbox_list): # Iterate through list of bboxes h_map = np . zeros_like(draw_img[:,:, 0 ]) . astype(np . float) for box in bbox_list: # Add += 1 for all pixels inside each bbox # Assuming each \"box\" takes the form ((x1, y1), (x2, y2)) h_map[box[ 0 ][ 1 ]:box[ 1 ][ 1 ], box[ 0 ][ 0 ]:box[ 1 ][ 0 ]] += 1 self . heat_images . append(h_map) self . heatmap = np . sum(np . array( self . heat_images),axis = 0 ) # Return thresholded map self . heatmap[ self . heatmap <= self . threshold] = 0 # Return updated heatmap return def draw_labeled_bboxes ( self ,img, labels): # Iterate through all detected cars for car_number in range ( 1 , labels[ 1 ] + 1 ): # Find pixels with each car_number label value nonzero = (labels[ 0 ] == car_number) . nonzero() # Identify x and y values of those pixels nonzeroy = np . array(nonzero[ 0 ]) nonzerox = np . array(nonzero[ 1 ]) # Define a bounding box based on min/max x and y bbox = ((np . min(nonzerox), np . min(nonzeroy)),\\ (np . max(nonzerox), np . max(nonzeroy))) # Draw the box on the image cv2 . rectangle(img, bbox[ 0 ], bbox[ 1 ], ( 0 , 0 , 255 ), 6 ) # Return the image return img","title":"Vehicle Detecton Pipeline"},{"location":"ml/vehicle-detection/CARND-Project-5/#application-of-sliding-window-search","text":"","title":"Application of sliding window search"},{"location":"ml/vehicle-detection/CARND-Project-5/#parameter-setting","text":"dist_pickle = pickle . load( open ( \"svc_pickle.p\" , \"rb\" ) ) param_dict = {} param_dict[ 'ystart' ] = 400 param_dict[ 'ystop' ] = 656 param_dict[ 'scale' ] = 1.5 param_dict[ 'svc' ] = dist_pickle[ \"svc\" ] param_dict[ 'X_scalar' ] = dist_pickle[ \"scaler\" ] param_dict[ 'orient' ] = dist_pickle[ \"orient\" ] param_dict[ 'pix_per_cell' ] = dist_pickle[ \"pix_per_cell\" ] param_dict[ 'cell_per_block' ] = dist_pickle[ \"cell_per_block\" ] param_dict[ 'spatial_size' ] = ( 32 , 32 ) param_dict[ 'hist_bins' ] = 32 directory = 'test_images/*.jpg' globDir = glob . glob(directory) fig = plt . figure(figsize = ( 18 , 24 )) DV = DetectVehicle(param_dict) n1 = 8 n2 = 4 count = 1 for k,img in enumerate (globDir): Img = mpimg . imread(img) boxes = DV . find_cars(Img,vid = False ,vis = True ) [draw_img_,heatmap] = DV . find_cars(Img,vid = False ,vis = False ) #fig-1 fig . add_subplot(n1,n2,count) plt . imshow(Img) plt . title( 'Original Image' ) count += 1 #fig-2 fig . add_subplot(n1,n2,count) box_Img = draw_boxes(Img, boxes) plt . imshow(box_Img) plt . title( 'boxes-image' ) count += 1 #fig-3 fig . add_subplot(n1,n2,count) plt . imshow(DV . heat_images[ 0 ]) plt . title( 'Heatmap-sample-1' ) count += 1 #fig-5 fig . add_subplot(n1,n2,count) plt . imshow(DV . heatmap) plt . title( 'Heatmap-threshold' ) count += 1 directory = 'test_images/*.jpg' globDir = glob . glob(directory) fig = plt . figure(figsize = ( 14 , 24 )) DV = DetectVehicle(param_dict) count = 1 for k,img in enumerate (globDir): Img = mpimg . imread(img) [draw_img,heatmap] = DV . find_cars(Img,vid = False ,vis = False ) #fig-1 fig . add_subplot( 8 , 3 ,count) plt . imshow(Img) plt . title( 'Original Image' ) count += 1 #fig-2 fig . add_subplot( 8 , 3 ,count) plt . imshow(heatmap) plt . title( 'threshold-Heatmap' ) count += 1 #fig-3 fig . add_subplot( 8 , 3 ,count) plt . imshow(draw_img) plt . title( 'Vehicle Detection' ) count += 1","title":"Parameter setting"},{"location":"ml/vehicle-detection/CARND-Project-5/#video-detector","text":"dist_pickle = pickle . load( open ( \"svc_pickle.p\" , \"rb\" ) ) param_dict = {} param_dict[ 'ystart' ] = 400 param_dict[ 'ystop' ] = 656 param_dict[ 'scale' ] = 1.5 param_dict[ 'svc' ] = dist_pickle[ \"svc\" ] param_dict[ 'X_scalar' ] = dist_pickle[ \"scaler\" ] param_dict[ 'orient' ] = dist_pickle[ \"orient\" ] param_dict[ 'pix_per_cell' ] = dist_pickle[ \"pix_per_cell\" ] param_dict[ 'cell_per_block' ] = dist_pickle[ \"cell_per_block\" ] param_dict[ 'spatial_size' ] = ( 32 , 32 ) param_dict[ 'hist_bins' ] = 32 # Read in image similar to one shown above DV = DetectVehicle(param_dict) white_output = 'test_video_output.mp4' clip = VideoFileClip( \"test_video.mp4\" ) #.subclip(t_start=30,t_end=35) white_clip = clip . fl_image(DV . find_cars) % time white_clip . write_videofile(white_output, audio = False ) HTML( \"\"\" <video width=\"960\" height=\"540\" controls> <source src=\"{0}\"> </video> \"\"\" . format(white_output)) [MoviePy] &gt;&gt;&gt;&gt; Building video test_video_output.mp4 [MoviePy] Writing video test_video_output.mp4 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 38/39 [00:21&lt;00:00, 1.64it/s] [MoviePy] Done. [MoviePy] &gt;&gt;&gt;&gt; Video ready: test_video_output.mp4 CPU times: user 19.3 s, sys: 1.79 s, total: 21.1 s Wall time: 22.2 s white_output = 'project_video_output.mp4' clip = VideoFileClip( \"project_video.mp4\" ) #.subclip(t_start=30,t_end=35) white_clip = clip . fl_image(DV . find_cars) % time white_clip . write_videofile(white_output, audio = False ) HTML( \"\"\" <video width=\"960\" height=\"540\" controls> <source src=\"{0}\"> </video> \"\"\" . format(white_output)) [MoviePy] &gt;&gt;&gt;&gt; Building video project_video_output.mp4 [MoviePy] Writing video project_video_output.mp4 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 1260/1261 [10:04&lt;00:00, 1.94it/s] [MoviePy] Done. [MoviePy] &gt;&gt;&gt;&gt; Video ready: project_video_output.mp4 CPU times: user 9min 8s, sys: 51.8 s, total: 10min Wall time: 10min 5s","title":"Video Detector"},{"location":"ml/vehicle-detection/CARND-Project-5/#both-lane-and-vehicle-detection","text":"white_output = 'project_video2_output.mp4' clip = VideoFileClip( \"project_video2.mp4\" ) #.subclip(t_start=30,t_end=35) white_clip = clip . fl_image(DV . find_cars) % time white_clip . write_videofile(white_output, audio = False ) HTML( \"\"\" <video width=\"960\" height=\"540\" controls> <source src=\"{0}\"> </video> \"\"\" . format(white_output)) [MoviePy] &gt;&gt;&gt;&gt; Building video project_video2_output.mp4 [MoviePy] Writing video project_video2_output.mp4 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 1260/1261 [10:34&lt;00:00, 1.96it/s] [MoviePy] Done. [MoviePy] &gt;&gt;&gt;&gt; Video ready: project_video2_output.mp4 CPU times: user 9min 40s, sys: 54.3 s, total: 10min 35s Wall time: 10min 34s","title":"Both lane and vehicle detection"},{"location":"rl/smartcab/","text":"Machine Learning Engineer Nanodegree Reinforcement Learning Project: Train a Smartcab How to Drive Install This project requires Python 2.7 with the pygame library installed Code Template code is provided in the smartcab/agent.py python file. Additional supporting python code can be found in smartcab/enviroment.py , smartcab/planner.py , and smartcab/simulator.py . Supporting images for the graphical user interface can be found in the images folder. While some code has already been implemented to get you started, you will need to implement additional functionality for the LearningAgent class in agent.py when requested to successfully complete the project. Run In a terminal or command window, navigate to the top-level project directory smartcab/ (that contains this README) and run one of the following commands: python smartcab/agent.py python -m smartcab.agent This will run the agent.py file and execute your agent code.","title":"Machine Learning Engineer Nanodegree"},{"location":"rl/smartcab/#machine-learning-engineer-nanodegree","text":"","title":"Machine Learning Engineer Nanodegree"},{"location":"rl/smartcab/#reinforcement-learning","text":"","title":"Reinforcement Learning"},{"location":"rl/smartcab/#project-train-a-smartcab-how-to-drive","text":"","title":"Project: Train a Smartcab How to Drive"},{"location":"rl/smartcab/#install","text":"This project requires Python 2.7 with the pygame library installed","title":"Install"},{"location":"rl/smartcab/#code","text":"Template code is provided in the smartcab/agent.py python file. Additional supporting python code can be found in smartcab/enviroment.py , smartcab/planner.py , and smartcab/simulator.py . Supporting images for the graphical user interface can be found in the images folder. While some code has already been implemented to get you started, you will need to implement additional functionality for the LearningAgent class in agent.py when requested to successfully complete the project.","title":"Code"},{"location":"rl/smartcab/#run","text":"In a terminal or command window, navigate to the top-level project directory smartcab/ (that contains this README) and run one of the following commands: python smartcab/agent.py python -m smartcab.agent This will run the agent.py file and execute your agent code.","title":"Run"},{"location":"rl/smartcab/smartcab/","text":"Machine Learning Engineer Nanodegree Reinforcement Learning Project: Train a Smartcab to Drive Welcome to the fourth project of the Machine Learning Engineer Nanodegree! In this notebook, template code has already been provided for you to aid in your analysis of the Smartcab and your implemented learning algorithm. You will not need to modify the included code beyond what is requested. There will be questions that you must answer which relate to the project and the visualizations provided in the notebook. Each section where you will answer a question is preceded by a 'Question X' header. Carefully read each question and provide thorough answers in the following text boxes that begin with 'Answer:' . Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide in agent.py . Note: Code and Markdown cells can be executed using the Shift + Enter keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode. Getting Started In this project, you will work towards constructing an optimized Q-Learning driving agent that will navigate a Smartcab through its environment towards a goal. Since the Smartcab is expected to drive passengers from one location to another, the driving agent will be evaluated on two very important metrics: Safety and Reliability . A driving agent that gets the Smartcab to its destination while running red lights or narrowly avoiding accidents would be considered unsafe . Similarly, a driving agent that frequently fails to reach the destination in time would be considered unreliable . Maximizing the driving agent's safety and reliability would ensure that Smartcabs have a permanent place in the transportation industry. Safety and Reliability are measured using a letter-grade system as follows: Grade Safety Reliability A+ Agent commits no traffic violations, and always chooses the correct action. Agent reaches the destination in time for 100% of trips. A Agent commits few minor traffic violations, such as failing to move on a green light. Agent reaches the destination on time for at least 90% of trips. B Agent commits frequent minor traffic violations, such as failing to move on a green light. Agent reaches the destination on time for at least 80% of trips. C Agent commits at least one major traffic violation, such as driving through a red light. Agent reaches the destination on time for at least 70% of trips. D Agent causes at least one minor accident, such as turning left on green with oncoming traffic. Agent reaches the destination on time for at least 60% of trips. F Agent causes at least one major accident, such as driving through a red light with cross-traffic. Agent fails to reach the destination on time for at least 60% of trips. To assist evaluating these important metrics, you will need to load visualization code that will be used later on in the project. Run the code cell below to import this code which is required for your analysis. # Import the visualization code import visuals as vs # Pretty display for notebooks % matplotlib inline Understand the World Before starting to work on implementing your driving agent, it's necessary to first understand the world (environment) which the Smartcab and driving agent work in. One of the major components to building a self-learning agent is understanding the characteristics about the agent, which includes how the agent operates. To begin, simply run the agent.py agent code exactly how it is -- no need to make any additions whatsoever. Let the resulting simulation run for some time to see the various working components. Note that in the visual simulation (if enabled), the white vehicle is the Smartcab . Question 1 In a few sentences, describe what you observe during the simulation when running the default agent.py agent code. Some things you could consider: - Does the Smartcab move at all during the simulation? - What kind of rewards is the driving agent receiving? - How does the light changing color affect the rewards? Hint: From the /smartcab/ top-level directory (where this notebook is located), run the command 'python smartcab/agent.py' Answer: The Smartcab doesn't move, while other cars moving according to the traffic light signals. Between trials, smartcab is repositioned to different locations. The Smartcab gets rewards based on the policy violation. For example, Positive rewards, if there is a green light with other traffic. Negative rewards, if there is a green light without other traffic. Positive rewards, if there is a red light etc. There are several types of violations: No violation Minor violation Major violation Minor accident Major accident The example of a violation rule: ``` # 1. Agent wants to drive forward: if action == 'forward': if light != 'green': # Running red light violation = 2 # Major violation if inputs['left'] == 'forward' or inputs['right'] == 'forward': # Cross traffic violation = 4 # Accident # 2. Agent wants to drive left: elif action == 'left': if light != 'green': # Running a red light violation = 2 # Major violation if inputs['left'] == 'forward' or inputs['right'] == 'forward': # Cross traffic violation = 4 # Accident elif inputs['oncoming'] == 'right': # Oncoming car turning right violation = 4 # Accident else: # Green light if inputs['oncoming'] == 'right' or inputs['oncoming'] == 'forward': # Incoming traffic violation = 3 # Accident else: # Valid move! heading = (heading[1], -heading[0]) # 3. Agent wants to drive right: elif action == 'right': if light != 'green' and inputs['left'] == 'forward': # Cross traffic violation = 3 # Accident else: # Valid move! heading = (-heading[1], heading[0]) # 4. Agent wants to perform no action: elif action == None: if light == 'green' and inputs['oncoming'] != 'left': # No oncoming traffic violation = 1 # Minor violation ``` If there is no policy violation the agent get rewarded based on the rules # Did the agent attempt a valid move? if violation == 0: if action == agent.get_next_waypoint(): # Was it the correct action? reward += 2 - penalty # (2, 1) elif action == None and light != 'green': # Was the agent stuck at a red light? reward += 2 - penalty # (2, 1) else: # Valid but incorrect reward += 1 - penalty # (1, 0) # Move the agent if action is not None: location = ((location[0] + heading[0] - self.bounds[0]) % (self.bounds[2] - self.bounds[0] + 1) + self.bounds[0], (location[1] + heading[1] - self.bounds[1]) % (self.bounds[3] - self.bounds[1] + 1) + self.bounds[1]) # wrap-around state['location'] = location state['heading'] = heading otherwise these rules will be applied. # Agent attempted invalid move else: if violation == 1: # Minor violation reward += -5 elif violation == 2: # Major violation reward += -10 elif violation == 3: # Minor accident reward += -20 elif violation == 4: # Major accident reward += -40 Understand the Code In addition to understanding the world, it is also necessary to understand the code itself that governs how the world, simulation, and so on operate. Attempting to create a driving agent would be difficult without having at least explored the \"hidden\" devices that make everything work. In the /smartcab/ top-level directory, there are two folders: /logs/ (which will be used later) and /smartcab/ . Open the /smartcab/ folder and explore each Python file included, then answer the following question. Question 2 In the agent.py Python file, choose three flags that can be set and explain how they change the simulation. In the environment.py Python file, what Environment class function is called when an agent performs an action? In the simulator.py Python file, what is the difference between the 'render_text()' function and the 'render()' function? In the planner.py Python file, will the 'next_waypoint() function consider the North-South or East-West direction first? Answer: In the agent.py Python file, choose three flags that can be set and explain how they change the simulation. learning (True/False): This indicates whether the agent is learning or not. It is to force the driving agent to use Q-learning. epsilon (float): This is the random exploration factor. It has continuous value, which is default to 1. If it is test set it to 0. While choosing action, if it is learning it has to choose random action with 'epsilon' probability, if it is not learning, choose a random action. Thus it is probability which defines how often the agent acts randomly in order to avoid dead ends. alpha (float): This is the learning factor. This parameter tunes the rewards recieved from the learning steps. It has continuous value , default is 0.5, if it is test set it to 0. In the environment.py Python file, what Environment class function is called when an agent performs an action? def act(self, agent, action) : This function considers an action and performs it if it is legal and recieve a reward for the agent based on traffic rules. In the simulator.py Python file, what is the difference between the 'render_text()' function and the 'render()' function? render_text(self, trial, testing=False) This is the non-GUI render display of the simulation. This sends trial data to be displayed in the terminal. render(self, trial, testing=False) : This is the GUI render display of the simulation. Supplementary trial data can be found from render_text. First one is used to generate the text (render_text) and the second for displaying it (render) In the planner.py Python file, will the 'next_waypoint() function consider the North-South or East-West direction first? - East-West direction is considered first. It checks if destination is cardinally East or West of location first, and then North or South location. Implement a Basic Driving Agent The first step to creating an optimized Q-Learning driving agent is getting the agent to actually take valid actions. In this case, a valid action is one of None , (do nothing) 'Left' (turn left), 'Right' (turn right), or 'Forward' (go forward). For your first implementation, navigate to the 'choose_action()' agent function and make the driving agent randomly choose one of these actions. Note that you have access to several class variables that will help you write this functionality, such as 'self.learning' and 'self.valid_actions' . Once implemented, run the agent file and simulation briefly to confirm that your driving agent is taking a random action each time step. Basic Agent Simulation Results To obtain results from the initial simulation, you will need to adjust following flags: - 'enforce_deadline' - Set this to True to force the driving agent to capture whether it reaches the destination in time. - 'update_delay' - Set this to a small value (such as 0.01 ) to reduce the time between steps in each trial. - 'log_metrics' - Set this to True to log the simluation results as a .csv file in /logs/ . - 'n_test' - Set this to '10' to perform 10 testing trials. Optionally, you may disable to the visual simulation (which can make the trials go faster) by setting the 'display' flag to False . Flags that have been set here should be returned to their default setting when debugging. It is important that you understand what each flag does and how it affects the simulation! Once you have successfully completed the initial simulation (there should have been 20 training trials and 10 testing trials), run the code cell below to visualize the results. Note that log files are overwritten when identical simulations are run, so be careful with what log file is being loaded! Run the agent.py file after setting the flags from projects/smartcab folder instead of projects/smartcab/smartcab. # Load the 'sim_no-learning' log file from the initial simulation results vs . plot_trials( 'sim_no-learning.csv' ) Question 3 Using the visualization above that was produced from your initial simulation, provide an analysis and make several observations about the driving agent. Be sure that you are making at least one observation about each panel present in the visualization. Some things you could consider: - How frequently is the driving agent making bad decisions? How many of those bad decisions cause accidents? - Given that the agent is driving randomly, does the rate of reliabilty make sense? - What kind of rewards is the agent receiving for its actions? Do the rewards suggest it has been penalized heavily? - As the number of trials increases, does the outcome of results change significantly? - Would this Smartcab be considered safe and/or reliable for its passengers? Why or why not? Answer: How frequently is the driving agent making bad decisions? How many of those bad decisions cause accidents? The driving agent makes bad decisions in 30% to 40% of all cases. Minor accidents take about 1% to 5%, while major accidents are close to 5% to 10% of all cases. Given that the agent is driving randomly, does the rate of reliabilty make sense? The reliability means a driving agent that frequently reaches the destination in time. Since the agent is acting randomly, the result is also random and not very much meaningful. What kind of rewards is the agent receiving for its actions? Do the rewards suggest it has been penalized heavily? The agent is getting negative rewards on average. The agents is not making bad decision all the time, but the rolling average is always negative. From above plot, Reward per actioon decreased from -4 to -6, it suggests it has been penalized heavily. As the number of trials increases, does the outcome of results change significantly? - No, the result stay relatively the same. The reason is our agent is very random and doen't know the constains and is not able to learn from failures and successes. Would this Smartcab be considered safe and/or reliable for its passengers? Why or why not? No, as the both ratings suggest, the Smartcab is neither reliable nor safe, since the cab didn't get to the destination, and make a lot of violations. Inform the Driving Agent The second step to creating an optimized Q-learning driving agent is defining a set of states that the agent can occupy in the environment. Depending on the input, sensory data, and additional variables available to the driving agent, a set of states can be defined for the agent so that it can eventually learn what action it should take when occupying a state. The condition of 'if state then action' for each state is called a policy , and is ultimately what the driving agent is expected to learn. Without defining states, the driving agent would never understand which action is most optimal -- or even what environmental variables and conditions it cares about! Identify States Inspecting the 'build_state()' agent function shows that the driving agent is given the following data from the environment: - 'waypoint' , which is the direction the Smartcab should drive leading to the destination, relative to the Smartcab 's heading. - 'inputs' , which is the sensor data from the Smartcab . It includes - 'light' , the color of the light. - 'left' , the intended direction of travel for a vehicle to the Smartcab 's left. Returns None if no vehicle is present. - 'right' , the intended direction of travel for a vehicle to the Smartcab 's right. Returns None if no vehicle is present. - 'oncoming' , the intended direction of travel for a vehicle across the intersection from the Smartcab . Returns None if no vehicle is present. - 'deadline' , which is the number of actions remaining for the Smartcab to reach the destination before running out of time. Question 4 Which features available to the agent are most relevant for learning both safety and efficiency ? Why are these features appropriate for modeling the Smartcab in the environment? If you did not choose some features, why are those features not appropriate? Answer: all the features are appropriate for learning safety and efficiency (reliability). waypoint, light, oncoming, right, left are the most relevant ones. - Knowing the waypoint is how the smartcab get to the destination. - Light can let the smartcab to know whether it should stop or drive at the intersections. - Oncoming, left, rigth are relevant because the smartcab should not drive into other cars. - Deadline seems to be relevant at first glance, since it forces the agent to reach the destination as soon as possible. Unfortunately it could be a source of problems. For instance the agent could break the rules in order to reach the destination faster when it's near the final point. So, we won't use it.Deadline is not appropriate, since knowing deadline won't help smartcab get to destination. Too many features will make it hard to learn a good policy. Define a State Space When defining a set of states that the agent can occupy, it is necessary to consider the size of the state space. That is to say, if you expect the driving agent to learn a policy for each state, you would need to have an optimal action for every state the agent can occupy. If the number of all possible states is very large, it might be the case that the driving agent never learns what to do in some states, which can lead to uninformed decisions. For example, consider a case where the following features are used to define the state of the Smartcab : ('is_raining', 'is_foggy', 'is_red_light', 'turn_left', 'no_traffic', 'previous_turn_left', 'time_of_day') . How frequently would the agent occupy a state like (False, True, True, True, False, False, '3AM') ? Without a near-infinite amount of time for training, it's doubtful the agent would ever learn the proper action! Question 5 If a state is defined using the features you've selected from Question 4 , what would be the size of the state space? Given what you know about the evironment and how it is simulated, do you think the driving agent could learn a policy for each possible state within a reasonable number of training trials? Hint: Consider the combinations of features to calculate the total number of states! Answer: The total number of states is defined by the product of the number of possible states for each feature. We estimate the total number of states as: WAYPOINT x LIGHTS_STATES x INPUT[LEFT] x INPUT[ONCOMING] WAIPOINT: 3 (left, right, forward) LIGHTS_STATES: 2 (green, red) INPUT[LEFT]: 4 (None, left, right, forwards) INPUT[ONCOMING]: 4 (None, left, right, forwards) Therefore, the total number of states is 96. Figuring out the policy for each possible state will take some time, because the Smartcab should visit each state and try different actions to learn the reward policy. The agent won't learn a policy for each possible state. That will take too much time to train. However, the agent doesn't need to do that to get a reasonable good performance. Update the Driving Agent State For your second implementation, navigate to the 'build_state()' agent function. With the justification you've provided in Question 4 , you will now set the 'state' variable to a tuple of all the features necessary for Q-Learning. Confirm your driving agent is updating its state by running the agent file and simulation briefly and note whether the state is displaying. If the visual simulation is used, confirm that the updated state corresponds with what is seen in the simulation. Note: Remember to reset simulation flags to their default setting when making this observation! Implement a Q-Learning Driving Agent The third step to creating an optimized Q-Learning agent is to begin implementing the functionality of Q-Learning itself. The concept of Q-Learning is fairly straightforward: For every state the agent visits, create an entry in the Q-table for all state-action pairs available. Then, when the agent encounters a state and performs an action, update the Q-value associated with that state-action pair based on the reward received and the interative update rule implemented. Of course, additional benefits come from Q-Learning, such that we can have the agent choose the best action for each state based on the Q-values of each state-action pair possible. For this project, you will be implementing a decaying, $\\epsilon$ -greedy Q-learning algorithm with no discount factor. Follow the implementation instructions under each TODO in the agent functions. Note that the agent attribute self.Q is a dictionary: This is how the Q-table will be formed. Each state will be a key of the self.Q dictionary, and each value will then be another dictionary that holds the action and Q-value . Here is an example: { 'state-1': { 'action-1' : Qvalue-1, 'action-2' : Qvalue-2, ... }, 'state-2': { 'action-1' : Qvalue-1, ... }, ... } Furthermore, note that you are expected to use a decaying $\\epsilon$ (exploration) factor . Hence, as the number of trials increases, $\\epsilon$ should decrease towards 0. This is because the agent is expected to learn from its behavior and begin acting on its learned behavior. Additionally, The agent will be tested on what it has learned after $\\epsilon$ has passed a certain threshold (the default threshold is 0.01). For the initial Q-Learning implementation, you will be implementing a linear decaying function for $\\epsilon$. Q-Learning Simulation Results To obtain results from the initial Q-Learning implementation, you will need to adjust the following flags and setup: - 'enforce_deadline' - Set this to True to force the driving agent to capture whether it reaches the destination in time. - 'update_delay' - Set this to a small value (such as 0.01 ) to reduce the time between steps in each trial. - 'log_metrics' - Set this to True to log the simluation results as a .csv file and the Q-table as a .txt file in /logs/ . - 'n_test' - Set this to '10' to perform 10 testing trials. - 'learning' - Set this to 'True' to tell the driving agent to use your Q-Learning implementation. In addition, use the following decay function for $\\epsilon$: $$ \\epsilon_{t+1} = \\epsilon_{t} - 0.05, \\hspace{10px}\\textrm{for trial number } t$$ If you have difficulty getting your implementation to work, try setting the 'verbose' flag to True to help debug. Flags that have been set here should be returned to their default setting when debugging. It is important that you understand what each flag does and how it affects the simulation! Once you have successfully completed the initial Q-Learning simulation, run the code cell below to visualize the results. Note that log files are overwritten when identical simulations are run, so be careful with what log file is being loaded! # Load the 'sim_default-learning' file from the default Q-Learning simulation vs . plot_trials( 'sim_default-learning.csv' ) Question 6 Using the visualization above that was produced from your default Q-Learning simulation, provide an analysis and make observations about the driving agent like in Question 3 . Note that the simulation should have also produced the Q-table in a text file which can help you make observations about the agent's learning. Some additional things you could consider: - Are there any observations that are similar between the basic driving agent and the default Q-Learning agent? - Approximately how many training trials did the driving agent require before testing? Does that number make sense given the epsilon-tolerance? - Is the decaying function you implemented for $\\epsilon$ (the exploration factor) accurately represented in the parameters panel? - As the number of training trials increased, did the number of bad actions decrease? Did the average reward increase? - How does the safety and reliability rating compare to the initial driving agent? Answer: Are there any observations that are similar between the basic driving agent and the default Q-Learning agent? Even though we see some improvements, the ratings haven't changed. In both scenarios, the agent fail both test. It's still dangerous to use the Smartcab in the environment. Approximately how many training trials did the driving agent require before testing? Does that number make sense given the epsilon-tolerance? The Smartcab needs 20 trials for training before testing. This makes sense since we use decay function self.epsilon = self.epsilon - 0.05 , which reach the tolerence value after 20 trials. The test runs after the epsilon value becomes less than 0.001 (tolerance). Taking into account the default value ($\\epsilon$ =1) we can easily check that 20 trials is our threshold indeed (1-20x0.05 < 0.001) Is the decaying function you implemented for $\\epsilon$ (the exploration factor) accurately represented in the parameters panel? Yes, we can see a linear function with a negative slope. It starts with $\\epsilon$ =1 and ends up with $\\epsilon$ =0 after 20 trials. As the number of training trials increased, did the number of bad actions decrease? Did the average reward increase? Fortunatelly, the total number of bad actions has decreased therefore the total amount of good actions has increased. At the end of the training the average reward value has gone up to -4 from -2. The number of bad actions decreased from ~33% to ~18%. How does the safety and reliability rating compare to the initial driving agent? The agent still gets Fs. Unfortunately the rolling rate of reliability is fluctuating around 35% with no evidences of any improvement. Compare to basic driving agent, default Q-Learning agent has improved success rate, though it's still failed, we can understand that from logs, to reflect this rating as well we need more no.of trianing trials before testing, and reliability improved and showed here as Rating B. Improve the Q-Learning Driving Agent The third step to creating an optimized Q-Learning agent is to perform the optimization! Now that the Q-Learning algorithm is implemented and the driving agent is successfully learning, it's necessary to tune settings and adjust learning paramaters so the driving agent learns both safety and efficiency . Typically this step will require a lot of trial and error, as some settings will invariably make the learning worse. One thing to keep in mind is the act of learning itself and the time that this takes: In theory, we could allow the agent to learn for an incredibly long amount of time; however, another goal of Q-Learning is to transition from experimenting with unlearned behavior to acting on learned behavior . For example, always allowing the agent to perform a random action during training (if $\\epsilon = 1$ and never decays) will certainly make it learn , but never let it act . When improving on your Q-Learning implementation, consider the impliciations it creates and whether it is logistically sensible to make a particular adjustment. Improved Q-Learning Simulation Results To obtain results from the initial Q-Learning implementation, you will need to adjust the following flags and setup: - 'enforce_deadline' - Set this to True to force the driving agent to capture whether it reaches the destination in time. - 'update_delay' - Set this to a small value (such as 0.01 ) to reduce the time between steps in each trial. - 'log_metrics' - Set this to True to log the simluation results as a .csv file and the Q-table as a .txt file in /logs/ . - 'learning' - Set this to 'True' to tell the driving agent to use your Q-Learning implementation. - 'optimized' - Set this to 'True' to tell the driving agent you are performing an optimized version of the Q-Learning implementation. Additional flags that can be adjusted as part of optimizing the Q-Learning agent: - 'n_test' - Set this to some positive number (previously 10) to perform that many testing trials. - 'alpha' - Set this to a real number between 0 - 1 to adjust the learning rate of the Q-Learning algorithm. - 'epsilon' - Set this to a real number between 0 - 1 to adjust the starting exploration factor of the Q-Learning algorithm. - 'tolerance' - set this to some small value larger than 0 (default was 0.05) to set the epsilon threshold for testing. Furthermore, use a decaying function of your choice for $\\epsilon$ (the exploration factor). Note that whichever function you use, it must decay to 'tolerance' at a reasonable rate . The Q-Learning agent will not begin testing until this occurs. Some example decaying functions (for $t$, the number of trials): $$ \\epsilon = a^t, \\textrm{for } 0 < a < 1 \\hspace{50px}\\epsilon = \\frac{1}{t^2}\\hspace{50px}\\epsilon = e^{-at}, \\textrm{for } 0 < a < 1 \\hspace{50px} \\epsilon = \\cos(at), \\textrm{for } 0 < a < 1$$ You may also use a decaying function for $\\alpha$ (the learning rate) if you so choose, however this is typically less common. If you do so, be sure that it adheres to the inequality $0 \\leq \\alpha \\leq 1$. If you have difficulty getting your implementation to work, try setting the 'verbose' flag to True to help debug. Flags that have been set here should be returned to their default setting when debugging. It is important that you understand what each flag does and how it affects the simulation! Once you have successfully completed the improved Q-Learning simulation, run the code cell below to visualize the results. Note that log files are overwritten when identical simulations are run, so be careful with what log file is being loaded! Visualizing some decay functions import numpy as np import math as math import pandas as pd import matplotlib.pyplot as plt from matplotlib import cm class decay ( object ): def __init__ ( self ,t,a): self . t = t self . a = a def fun1 ( self ): return 1.0 - ( self . t * 0.01 ) def fun2 ( self ): return self . a ** ( self . t) def fun3 ( self ): return np . exp( - self . a * self . t) def fun4 ( self ): return np . cos( self . a * self . t) def fun5 ( self ): return self . a / np . sqrt( self . t) - 0.5 def fun6 ( self ): return 1.0 / (( self . a * self . t) ** 3 ) a1 = np . arange( 0.001 , 1.0 , 0.01 ) x = np . arange( 1 , 100.0 , 1.0 ) plt . figure(figsize = ( 12 , 10 )) for ia in a1: plt . subplot( 2 , 3 , 1 ) plt . plot(x,decay(x,ia) . fun1()) plt . subplot( 2 , 3 , 2 ) plt . plot(x,decay(x,ia) . fun2()) plt . subplot( 2 , 3 , 3 ) plt . plot(x,decay(x,ia) . fun3()) a2 = np . arange( 0.001 , 0.009 , 0.0005 ) for ia in a2: plt . subplot( 2 , 3 , 4 ) plt . plot(x,decay(x,ia) . fun4()) plt . subplot( 2 , 3 , 5 ) plt . plot(x,decay(x,ia) . fun5()) plt . subplot( 2 , 3 , 6 ) plt . plot(x,decay(x,ia) . fun6()) Choice of different decay functions Case-I: model epsilon tolerance alpha a safety reliability trainings linear $$\\epsilon = \\epsilon -0.005\\hspace{25px}$$ 0.0001 0.06 - A+ A+ 200 # Load the 'sim_improved-learning' file from the improved Q-Learning simulation vs . plot_trials( 'sim_improved-learning1.csv' ) Case-II: model epsilon tolerance alpha a safety reliability trainings exponential $$\\epsilon = e^{-at}\\hspace{50px}$$ 0.0001 0.06 0.01 A+ B ~700 # Load the 'sim_improved-learning' file from the improved Q-Learning simulation # Linear function self.epsilon = math.exp(1)**(-a*self.trials) a =0.01 vs . plot_trials( 'sim_improved-learning2.csv' ) Case:III: model epsilon tolerance alpha a safety reliability trainings exponential $$\\epsilon = a^t\\hspace{25px}$$ 0.0001 0.06 0.99 A+ B ~700 # Load the 'sim_improved-learning' file from the improved Q-Learning simulation # Linear function self.epsilon = a**self.trials a =0.99 vs . plot_trials( 'sim_improved-learning3.csv' ) Case-IV: | model |epsilon |tolerance |alpha | a | safety |reliability |trainings | |---|---|---|---|---|---|---|---| | | trigonometric | $$ \\epsilon = \\cos(at) +0.5\\hspace{50px}$$| 0.01 | 0.06 | 0.001 | A+ | A | ~2000 | # Load the 'sim_improved-learning' file from the improved Q-Learning simulation # Linear function self.epsilon = math.cos(a*self.trials)+0.5 a=0.001 tolerance=0.01 vs . plot_trials( 'sim_improved-learning5.csv' ) Question 7 Using the visualization above that was produced from your improved Q-Learning simulation, provide a final analysis and make observations about the improved driving agent like in Question 6 . Questions you should answer: - What decaying function was used for epsilon (the exploration factor)? - Approximately how many training trials were needed for your agent before begining testing? - What epsilon-tolerance and alpha (learning rate) did you use? Why did you use them? - How much improvement was made with this Q-Learner when compared to the default Q-Learner from the previous section? - Would you say that the Q-Learner results show that your driving agent successfully learned an appropriate policy? - Are you satisfied with the safety and reliability ratings of the Smartcab ? Answer: Following is the summary table for all models used: model epsilon tolerance alpha a safety reliability trainings linear $$\\epsilon = \\epsilon -0.005\\hspace{25px}$$ 0.0001 0.06 - A+ A+ 200 exponential $$\\epsilon = e^{-at}\\hspace{50px}$$ 0.0001 0.06 0.01 A+ B ~700 exponential $$\\epsilon = a^t\\hspace{25px}$$ 0.0001 0.06 0.99 A+ B ~700 trigonometric $$ \\epsilon = \\cos(at) +0.5\\hspace{50px}$$ 0.01 0.06 0.001 A+ A ~2000 What decaying function was used for epsilon (the exploration factor)? Different decay function used asr listed in the chart above. Approximately how many training trials were needed for your agent before begining testing? From above chart it ranges from 200 to 2000. How much improvement was made with this Q-Learner when compared to the default Q-Learner from the previous section? There is a tremendous increase in terms safety and reliability. The Smartcab managed to achieve A/A+, A+/B, A+/B A+/A grades respectively, comparing to F in case of the default Q-Learner. Would you say that the Q-Learner results show that your driving agent successfully learned an appropriate policy? Taking into account the rewards, the linear and exponential decay functions got into a positive reward sector after some training examples. Other functions slowly achive that success. In each of the case driving agent is learning successfully. Are you satisfied with the safety and reliability ratings of the Smartcab? Yes, I am satisfied with the learning rate and only partially with the safety and reliability ratings, there is a room for improvement here because I have got B+ some where. Obviously the Smartcab is not ready for usage in real world, it still causes road accidents. Define an Optimal Policy Sometimes, the answer to the important question \"what am I trying to get my agent to learn?\" only has a theoretical answer and cannot be concretely described. Here, however, you can concretely define what it is the agent is trying to learn, and that is the U.S. right-of-way traffic laws. Since these laws are known information, you can further define, for each state the Smartcab is occupying, the optimal action for the driving agent based on these laws. In that case, we call the set of optimal state-action pairs an optimal policy . Hence, unlike some theoretical answers, it is clear whether the agent is acting \"incorrectly\" not only by the reward (penalty) it receives, but also by pure observation. If the agent drives through a red light, we both see it receive a negative reward but also know that it is not the correct behavior. This can be used to your advantage for verifying whether the policy your driving agent has learned is the correct one, or if it is a suboptimal policy . Question 8 Provide a few examples (using the states you've defined) of what an optimal policy for this problem would look like. Afterwards, investigate the 'sim_improved-learning.txt' text file to see the results of your improved Q-Learning algorithm. For each state that has been recorded from the simulation, is the policy (the action with the highest value) correct for the given state? Are there any states where the policy is different than what would be expected from an optimal policy? Provide an example of a state and all state-action rewards recorded, and explain why it is the correct policy. Answer: In most cases the policy does make sense. The state dictionary is defined like this: state = (inputs['light'],\\ inputs['oncoming'],\\ inputs['right'],\\ inputs['left'],\\ waypoint) Example-1 : Consider following state-action-reward ('green', None, 'forward', 'right', 'left')---> left -- forward : 0.13 -- None : -0.78 -- right : 0.17 -- left : 0.67 Here traffic light is green , there is no upcomimg traffic and the waypoint is left . So , moving to left has highest reward and going left is optimal policy. Example-2 : Consider following state-action-reward ('green', 'right', None, None, 'right')---> right -- forward : 0.41 -- None : -3.46 -- right : 1.62 -- left : -12.22 Here traffic light is green , the waypoint is right . Moving to right has highest reward and going right is optimal policy. Example-3 : Consider following state-action-reward ('green', 'right', 'left', None, 'right')--> forward -- forward : 0.68 -- None : -1.34 -- right : 0.42 -- left : -2.34 Here traffic light is green , the waypoint is right . But policy shows going forward has maximum reward. While the correct action would be going to right . So , this is a sub-optimal policy. This will be corected while agent gets more steps to learn. Optional: Future Rewards - Discount Factor, 'gamma' Curiously, as part of the Q-Learning algorithm, you were asked to not use the discount factor, 'gamma' in the implementation. Including future rewards in the algorithm is used to aid in propogating positive rewards backwards from a future state to the current state. Essentially, if the driving agent is given the option to make several actions to arrive at different states, including future rewards will bias the agent towards states that could provide even more rewards. An example of this would be the driving agent moving towards a goal: With all actions and rewards equal, moving towards the goal would theoretically yield better rewards if there is an additional reward for reaching the goal. However, even though in this project, the driving agent is trying to reach a destination in the allotted time, including future rewards will not benefit the agent. In fact, if the agent were given many trials to learn, it could negatively affect Q-values! Optional Question 9 There are two characteristics about the project that invalidate the use of future rewards in the Q-Learning algorithm. One characteristic has to do with the Smartcab itself, and the other has to do with the environment. Can you figure out what they are and why future rewards won't work for this project? Answer: Note : Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to File -> Download as -> HTML (.html) . Include the finished document along with this notebook as your submission.","title":"SmartCab"},{"location":"rl/smartcab/smartcab/#machine-learning-engineer-nanodegree","text":"","title":"Machine Learning Engineer Nanodegree"},{"location":"rl/smartcab/smartcab/#reinforcement-learning","text":"","title":"Reinforcement Learning"},{"location":"rl/smartcab/smartcab/#project-train-a-smartcab-to-drive","text":"Welcome to the fourth project of the Machine Learning Engineer Nanodegree! In this notebook, template code has already been provided for you to aid in your analysis of the Smartcab and your implemented learning algorithm. You will not need to modify the included code beyond what is requested. There will be questions that you must answer which relate to the project and the visualizations provided in the notebook. Each section where you will answer a question is preceded by a 'Question X' header. Carefully read each question and provide thorough answers in the following text boxes that begin with 'Answer:' . Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide in agent.py . Note: Code and Markdown cells can be executed using the Shift + Enter keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode.","title":"Project: Train a Smartcab to Drive"},{"location":"rl/smartcab/smartcab/#getting-started","text":"In this project, you will work towards constructing an optimized Q-Learning driving agent that will navigate a Smartcab through its environment towards a goal. Since the Smartcab is expected to drive passengers from one location to another, the driving agent will be evaluated on two very important metrics: Safety and Reliability . A driving agent that gets the Smartcab to its destination while running red lights or narrowly avoiding accidents would be considered unsafe . Similarly, a driving agent that frequently fails to reach the destination in time would be considered unreliable . Maximizing the driving agent's safety and reliability would ensure that Smartcabs have a permanent place in the transportation industry. Safety and Reliability are measured using a letter-grade system as follows: Grade Safety Reliability A+ Agent commits no traffic violations, and always chooses the correct action. Agent reaches the destination in time for 100% of trips. A Agent commits few minor traffic violations, such as failing to move on a green light. Agent reaches the destination on time for at least 90% of trips. B Agent commits frequent minor traffic violations, such as failing to move on a green light. Agent reaches the destination on time for at least 80% of trips. C Agent commits at least one major traffic violation, such as driving through a red light. Agent reaches the destination on time for at least 70% of trips. D Agent causes at least one minor accident, such as turning left on green with oncoming traffic. Agent reaches the destination on time for at least 60% of trips. F Agent causes at least one major accident, such as driving through a red light with cross-traffic. Agent fails to reach the destination on time for at least 60% of trips. To assist evaluating these important metrics, you will need to load visualization code that will be used later on in the project. Run the code cell below to import this code which is required for your analysis. # Import the visualization code import visuals as vs # Pretty display for notebooks % matplotlib inline","title":"Getting Started"},{"location":"rl/smartcab/smartcab/#understand-the-world","text":"Before starting to work on implementing your driving agent, it's necessary to first understand the world (environment) which the Smartcab and driving agent work in. One of the major components to building a self-learning agent is understanding the characteristics about the agent, which includes how the agent operates. To begin, simply run the agent.py agent code exactly how it is -- no need to make any additions whatsoever. Let the resulting simulation run for some time to see the various working components. Note that in the visual simulation (if enabled), the white vehicle is the Smartcab .","title":"Understand the World"},{"location":"rl/smartcab/smartcab/#question-1","text":"In a few sentences, describe what you observe during the simulation when running the default agent.py agent code. Some things you could consider: - Does the Smartcab move at all during the simulation? - What kind of rewards is the driving agent receiving? - How does the light changing color affect the rewards? Hint: From the /smartcab/ top-level directory (where this notebook is located), run the command 'python smartcab/agent.py' Answer: The Smartcab doesn't move, while other cars moving according to the traffic light signals. Between trials, smartcab is repositioned to different locations. The Smartcab gets rewards based on the policy violation. For example, Positive rewards, if there is a green light with other traffic. Negative rewards, if there is a green light without other traffic. Positive rewards, if there is a red light etc. There are several types of violations: No violation Minor violation Major violation Minor accident Major accident The example of a violation rule: ``` # 1. Agent wants to drive forward: if action == 'forward': if light != 'green': # Running red light violation = 2 # Major violation if inputs['left'] == 'forward' or inputs['right'] == 'forward': # Cross traffic violation = 4 # Accident # 2. Agent wants to drive left: elif action == 'left': if light != 'green': # Running a red light violation = 2 # Major violation if inputs['left'] == 'forward' or inputs['right'] == 'forward': # Cross traffic violation = 4 # Accident elif inputs['oncoming'] == 'right': # Oncoming car turning right violation = 4 # Accident else: # Green light if inputs['oncoming'] == 'right' or inputs['oncoming'] == 'forward': # Incoming traffic violation = 3 # Accident else: # Valid move! heading = (heading[1], -heading[0]) # 3. Agent wants to drive right: elif action == 'right': if light != 'green' and inputs['left'] == 'forward': # Cross traffic violation = 3 # Accident else: # Valid move! heading = (-heading[1], heading[0]) # 4. Agent wants to perform no action: elif action == None: if light == 'green' and inputs['oncoming'] != 'left': # No oncoming traffic violation = 1 # Minor violation ``` If there is no policy violation the agent get rewarded based on the rules # Did the agent attempt a valid move? if violation == 0: if action == agent.get_next_waypoint(): # Was it the correct action? reward += 2 - penalty # (2, 1) elif action == None and light != 'green': # Was the agent stuck at a red light? reward += 2 - penalty # (2, 1) else: # Valid but incorrect reward += 1 - penalty # (1, 0) # Move the agent if action is not None: location = ((location[0] + heading[0] - self.bounds[0]) % (self.bounds[2] - self.bounds[0] + 1) + self.bounds[0], (location[1] + heading[1] - self.bounds[1]) % (self.bounds[3] - self.bounds[1] + 1) + self.bounds[1]) # wrap-around state['location'] = location state['heading'] = heading otherwise these rules will be applied. # Agent attempted invalid move else: if violation == 1: # Minor violation reward += -5 elif violation == 2: # Major violation reward += -10 elif violation == 3: # Minor accident reward += -20 elif violation == 4: # Major accident reward += -40","title":"Question 1"},{"location":"rl/smartcab/smartcab/#understand-the-code","text":"In addition to understanding the world, it is also necessary to understand the code itself that governs how the world, simulation, and so on operate. Attempting to create a driving agent would be difficult without having at least explored the \"hidden\" devices that make everything work. In the /smartcab/ top-level directory, there are two folders: /logs/ (which will be used later) and /smartcab/ . Open the /smartcab/ folder and explore each Python file included, then answer the following question.","title":"Understand the Code"},{"location":"rl/smartcab/smartcab/#question-2","text":"In the agent.py Python file, choose three flags that can be set and explain how they change the simulation. In the environment.py Python file, what Environment class function is called when an agent performs an action? In the simulator.py Python file, what is the difference between the 'render_text()' function and the 'render()' function? In the planner.py Python file, will the 'next_waypoint() function consider the North-South or East-West direction first? Answer: In the agent.py Python file, choose three flags that can be set and explain how they change the simulation. learning (True/False): This indicates whether the agent is learning or not. It is to force the driving agent to use Q-learning. epsilon (float): This is the random exploration factor. It has continuous value, which is default to 1. If it is test set it to 0. While choosing action, if it is learning it has to choose random action with 'epsilon' probability, if it is not learning, choose a random action. Thus it is probability which defines how often the agent acts randomly in order to avoid dead ends. alpha (float): This is the learning factor. This parameter tunes the rewards recieved from the learning steps. It has continuous value , default is 0.5, if it is test set it to 0. In the environment.py Python file, what Environment class function is called when an agent performs an action? def act(self, agent, action) : This function considers an action and performs it if it is legal and recieve a reward for the agent based on traffic rules. In the simulator.py Python file, what is the difference between the 'render_text()' function and the 'render()' function? render_text(self, trial, testing=False) This is the non-GUI render display of the simulation. This sends trial data to be displayed in the terminal. render(self, trial, testing=False) : This is the GUI render display of the simulation. Supplementary trial data can be found from render_text. First one is used to generate the text (render_text) and the second for displaying it (render) In the planner.py Python file, will the 'next_waypoint() function consider the North-South or East-West direction first? - East-West direction is considered first. It checks if destination is cardinally East or West of location first, and then North or South location.","title":"Question 2"},{"location":"rl/smartcab/smartcab/#implement-a-basic-driving-agent","text":"The first step to creating an optimized Q-Learning driving agent is getting the agent to actually take valid actions. In this case, a valid action is one of None , (do nothing) 'Left' (turn left), 'Right' (turn right), or 'Forward' (go forward). For your first implementation, navigate to the 'choose_action()' agent function and make the driving agent randomly choose one of these actions. Note that you have access to several class variables that will help you write this functionality, such as 'self.learning' and 'self.valid_actions' . Once implemented, run the agent file and simulation briefly to confirm that your driving agent is taking a random action each time step.","title":"Implement a Basic Driving Agent"},{"location":"rl/smartcab/smartcab/#basic-agent-simulation-results","text":"To obtain results from the initial simulation, you will need to adjust following flags: - 'enforce_deadline' - Set this to True to force the driving agent to capture whether it reaches the destination in time. - 'update_delay' - Set this to a small value (such as 0.01 ) to reduce the time between steps in each trial. - 'log_metrics' - Set this to True to log the simluation results as a .csv file in /logs/ . - 'n_test' - Set this to '10' to perform 10 testing trials. Optionally, you may disable to the visual simulation (which can make the trials go faster) by setting the 'display' flag to False . Flags that have been set here should be returned to their default setting when debugging. It is important that you understand what each flag does and how it affects the simulation! Once you have successfully completed the initial simulation (there should have been 20 training trials and 10 testing trials), run the code cell below to visualize the results. Note that log files are overwritten when identical simulations are run, so be careful with what log file is being loaded! Run the agent.py file after setting the flags from projects/smartcab folder instead of projects/smartcab/smartcab. # Load the 'sim_no-learning' log file from the initial simulation results vs . plot_trials( 'sim_no-learning.csv' )","title":"Basic Agent Simulation Results"},{"location":"rl/smartcab/smartcab/#question-3","text":"Using the visualization above that was produced from your initial simulation, provide an analysis and make several observations about the driving agent. Be sure that you are making at least one observation about each panel present in the visualization. Some things you could consider: - How frequently is the driving agent making bad decisions? How many of those bad decisions cause accidents? - Given that the agent is driving randomly, does the rate of reliabilty make sense? - What kind of rewards is the agent receiving for its actions? Do the rewards suggest it has been penalized heavily? - As the number of trials increases, does the outcome of results change significantly? - Would this Smartcab be considered safe and/or reliable for its passengers? Why or why not? Answer: How frequently is the driving agent making bad decisions? How many of those bad decisions cause accidents? The driving agent makes bad decisions in 30% to 40% of all cases. Minor accidents take about 1% to 5%, while major accidents are close to 5% to 10% of all cases. Given that the agent is driving randomly, does the rate of reliabilty make sense? The reliability means a driving agent that frequently reaches the destination in time. Since the agent is acting randomly, the result is also random and not very much meaningful. What kind of rewards is the agent receiving for its actions? Do the rewards suggest it has been penalized heavily? The agent is getting negative rewards on average. The agents is not making bad decision all the time, but the rolling average is always negative. From above plot, Reward per actioon decreased from -4 to -6, it suggests it has been penalized heavily. As the number of trials increases, does the outcome of results change significantly? - No, the result stay relatively the same. The reason is our agent is very random and doen't know the constains and is not able to learn from failures and successes. Would this Smartcab be considered safe and/or reliable for its passengers? Why or why not? No, as the both ratings suggest, the Smartcab is neither reliable nor safe, since the cab didn't get to the destination, and make a lot of violations.","title":"Question 3"},{"location":"rl/smartcab/smartcab/#inform-the-driving-agent","text":"The second step to creating an optimized Q-learning driving agent is defining a set of states that the agent can occupy in the environment. Depending on the input, sensory data, and additional variables available to the driving agent, a set of states can be defined for the agent so that it can eventually learn what action it should take when occupying a state. The condition of 'if state then action' for each state is called a policy , and is ultimately what the driving agent is expected to learn. Without defining states, the driving agent would never understand which action is most optimal -- or even what environmental variables and conditions it cares about!","title":"Inform the Driving Agent"},{"location":"rl/smartcab/smartcab/#identify-states","text":"Inspecting the 'build_state()' agent function shows that the driving agent is given the following data from the environment: - 'waypoint' , which is the direction the Smartcab should drive leading to the destination, relative to the Smartcab 's heading. - 'inputs' , which is the sensor data from the Smartcab . It includes - 'light' , the color of the light. - 'left' , the intended direction of travel for a vehicle to the Smartcab 's left. Returns None if no vehicle is present. - 'right' , the intended direction of travel for a vehicle to the Smartcab 's right. Returns None if no vehicle is present. - 'oncoming' , the intended direction of travel for a vehicle across the intersection from the Smartcab . Returns None if no vehicle is present. - 'deadline' , which is the number of actions remaining for the Smartcab to reach the destination before running out of time.","title":"Identify States"},{"location":"rl/smartcab/smartcab/#question-4","text":"Which features available to the agent are most relevant for learning both safety and efficiency ? Why are these features appropriate for modeling the Smartcab in the environment? If you did not choose some features, why are those features not appropriate? Answer: all the features are appropriate for learning safety and efficiency (reliability). waypoint, light, oncoming, right, left are the most relevant ones. - Knowing the waypoint is how the smartcab get to the destination. - Light can let the smartcab to know whether it should stop or drive at the intersections. - Oncoming, left, rigth are relevant because the smartcab should not drive into other cars. - Deadline seems to be relevant at first glance, since it forces the agent to reach the destination as soon as possible. Unfortunately it could be a source of problems. For instance the agent could break the rules in order to reach the destination faster when it's near the final point. So, we won't use it.Deadline is not appropriate, since knowing deadline won't help smartcab get to destination. Too many features will make it hard to learn a good policy.","title":"Question 4"},{"location":"rl/smartcab/smartcab/#define-a-state-space","text":"When defining a set of states that the agent can occupy, it is necessary to consider the size of the state space. That is to say, if you expect the driving agent to learn a policy for each state, you would need to have an optimal action for every state the agent can occupy. If the number of all possible states is very large, it might be the case that the driving agent never learns what to do in some states, which can lead to uninformed decisions. For example, consider a case where the following features are used to define the state of the Smartcab : ('is_raining', 'is_foggy', 'is_red_light', 'turn_left', 'no_traffic', 'previous_turn_left', 'time_of_day') . How frequently would the agent occupy a state like (False, True, True, True, False, False, '3AM') ? Without a near-infinite amount of time for training, it's doubtful the agent would ever learn the proper action!","title":"Define a State Space"},{"location":"rl/smartcab/smartcab/#question-5","text":"If a state is defined using the features you've selected from Question 4 , what would be the size of the state space? Given what you know about the evironment and how it is simulated, do you think the driving agent could learn a policy for each possible state within a reasonable number of training trials? Hint: Consider the combinations of features to calculate the total number of states! Answer: The total number of states is defined by the product of the number of possible states for each feature. We estimate the total number of states as: WAYPOINT x LIGHTS_STATES x INPUT[LEFT] x INPUT[ONCOMING] WAIPOINT: 3 (left, right, forward) LIGHTS_STATES: 2 (green, red) INPUT[LEFT]: 4 (None, left, right, forwards) INPUT[ONCOMING]: 4 (None, left, right, forwards) Therefore, the total number of states is 96. Figuring out the policy for each possible state will take some time, because the Smartcab should visit each state and try different actions to learn the reward policy. The agent won't learn a policy for each possible state. That will take too much time to train. However, the agent doesn't need to do that to get a reasonable good performance.","title":"Question 5"},{"location":"rl/smartcab/smartcab/#update-the-driving-agent-state","text":"For your second implementation, navigate to the 'build_state()' agent function. With the justification you've provided in Question 4 , you will now set the 'state' variable to a tuple of all the features necessary for Q-Learning. Confirm your driving agent is updating its state by running the agent file and simulation briefly and note whether the state is displaying. If the visual simulation is used, confirm that the updated state corresponds with what is seen in the simulation. Note: Remember to reset simulation flags to their default setting when making this observation!","title":"Update the Driving Agent State"},{"location":"rl/smartcab/smartcab/#implement-a-q-learning-driving-agent","text":"The third step to creating an optimized Q-Learning agent is to begin implementing the functionality of Q-Learning itself. The concept of Q-Learning is fairly straightforward: For every state the agent visits, create an entry in the Q-table for all state-action pairs available. Then, when the agent encounters a state and performs an action, update the Q-value associated with that state-action pair based on the reward received and the interative update rule implemented. Of course, additional benefits come from Q-Learning, such that we can have the agent choose the best action for each state based on the Q-values of each state-action pair possible. For this project, you will be implementing a decaying, $\\epsilon$ -greedy Q-learning algorithm with no discount factor. Follow the implementation instructions under each TODO in the agent functions. Note that the agent attribute self.Q is a dictionary: This is how the Q-table will be formed. Each state will be a key of the self.Q dictionary, and each value will then be another dictionary that holds the action and Q-value . Here is an example: { 'state-1': { 'action-1' : Qvalue-1, 'action-2' : Qvalue-2, ... }, 'state-2': { 'action-1' : Qvalue-1, ... }, ... } Furthermore, note that you are expected to use a decaying $\\epsilon$ (exploration) factor . Hence, as the number of trials increases, $\\epsilon$ should decrease towards 0. This is because the agent is expected to learn from its behavior and begin acting on its learned behavior. Additionally, The agent will be tested on what it has learned after $\\epsilon$ has passed a certain threshold (the default threshold is 0.01). For the initial Q-Learning implementation, you will be implementing a linear decaying function for $\\epsilon$.","title":"Implement a Q-Learning Driving Agent"},{"location":"rl/smartcab/smartcab/#q-learning-simulation-results","text":"To obtain results from the initial Q-Learning implementation, you will need to adjust the following flags and setup: - 'enforce_deadline' - Set this to True to force the driving agent to capture whether it reaches the destination in time. - 'update_delay' - Set this to a small value (such as 0.01 ) to reduce the time between steps in each trial. - 'log_metrics' - Set this to True to log the simluation results as a .csv file and the Q-table as a .txt file in /logs/ . - 'n_test' - Set this to '10' to perform 10 testing trials. - 'learning' - Set this to 'True' to tell the driving agent to use your Q-Learning implementation. In addition, use the following decay function for $\\epsilon$: $$ \\epsilon_{t+1} = \\epsilon_{t} - 0.05, \\hspace{10px}\\textrm{for trial number } t$$ If you have difficulty getting your implementation to work, try setting the 'verbose' flag to True to help debug. Flags that have been set here should be returned to their default setting when debugging. It is important that you understand what each flag does and how it affects the simulation! Once you have successfully completed the initial Q-Learning simulation, run the code cell below to visualize the results. Note that log files are overwritten when identical simulations are run, so be careful with what log file is being loaded! # Load the 'sim_default-learning' file from the default Q-Learning simulation vs . plot_trials( 'sim_default-learning.csv' )","title":"Q-Learning Simulation Results"},{"location":"rl/smartcab/smartcab/#question-6","text":"Using the visualization above that was produced from your default Q-Learning simulation, provide an analysis and make observations about the driving agent like in Question 3 . Note that the simulation should have also produced the Q-table in a text file which can help you make observations about the agent's learning. Some additional things you could consider: - Are there any observations that are similar between the basic driving agent and the default Q-Learning agent? - Approximately how many training trials did the driving agent require before testing? Does that number make sense given the epsilon-tolerance? - Is the decaying function you implemented for $\\epsilon$ (the exploration factor) accurately represented in the parameters panel? - As the number of training trials increased, did the number of bad actions decrease? Did the average reward increase? - How does the safety and reliability rating compare to the initial driving agent? Answer: Are there any observations that are similar between the basic driving agent and the default Q-Learning agent? Even though we see some improvements, the ratings haven't changed. In both scenarios, the agent fail both test. It's still dangerous to use the Smartcab in the environment. Approximately how many training trials did the driving agent require before testing? Does that number make sense given the epsilon-tolerance? The Smartcab needs 20 trials for training before testing. This makes sense since we use decay function self.epsilon = self.epsilon - 0.05 , which reach the tolerence value after 20 trials. The test runs after the epsilon value becomes less than 0.001 (tolerance). Taking into account the default value ($\\epsilon$ =1) we can easily check that 20 trials is our threshold indeed (1-20x0.05 < 0.001) Is the decaying function you implemented for $\\epsilon$ (the exploration factor) accurately represented in the parameters panel? Yes, we can see a linear function with a negative slope. It starts with $\\epsilon$ =1 and ends up with $\\epsilon$ =0 after 20 trials. As the number of training trials increased, did the number of bad actions decrease? Did the average reward increase? Fortunatelly, the total number of bad actions has decreased therefore the total amount of good actions has increased. At the end of the training the average reward value has gone up to -4 from -2. The number of bad actions decreased from ~33% to ~18%. How does the safety and reliability rating compare to the initial driving agent? The agent still gets Fs. Unfortunately the rolling rate of reliability is fluctuating around 35% with no evidences of any improvement. Compare to basic driving agent, default Q-Learning agent has improved success rate, though it's still failed, we can understand that from logs, to reflect this rating as well we need more no.of trianing trials before testing, and reliability improved and showed here as Rating B.","title":"Question 6"},{"location":"rl/smartcab/smartcab/#improve-the-q-learning-driving-agent","text":"The third step to creating an optimized Q-Learning agent is to perform the optimization! Now that the Q-Learning algorithm is implemented and the driving agent is successfully learning, it's necessary to tune settings and adjust learning paramaters so the driving agent learns both safety and efficiency . Typically this step will require a lot of trial and error, as some settings will invariably make the learning worse. One thing to keep in mind is the act of learning itself and the time that this takes: In theory, we could allow the agent to learn for an incredibly long amount of time; however, another goal of Q-Learning is to transition from experimenting with unlearned behavior to acting on learned behavior . For example, always allowing the agent to perform a random action during training (if $\\epsilon = 1$ and never decays) will certainly make it learn , but never let it act . When improving on your Q-Learning implementation, consider the impliciations it creates and whether it is logistically sensible to make a particular adjustment.","title":"Improve the Q-Learning Driving Agent"},{"location":"rl/smartcab/smartcab/#improved-q-learning-simulation-results","text":"To obtain results from the initial Q-Learning implementation, you will need to adjust the following flags and setup: - 'enforce_deadline' - Set this to True to force the driving agent to capture whether it reaches the destination in time. - 'update_delay' - Set this to a small value (such as 0.01 ) to reduce the time between steps in each trial. - 'log_metrics' - Set this to True to log the simluation results as a .csv file and the Q-table as a .txt file in /logs/ . - 'learning' - Set this to 'True' to tell the driving agent to use your Q-Learning implementation. - 'optimized' - Set this to 'True' to tell the driving agent you are performing an optimized version of the Q-Learning implementation. Additional flags that can be adjusted as part of optimizing the Q-Learning agent: - 'n_test' - Set this to some positive number (previously 10) to perform that many testing trials. - 'alpha' - Set this to a real number between 0 - 1 to adjust the learning rate of the Q-Learning algorithm. - 'epsilon' - Set this to a real number between 0 - 1 to adjust the starting exploration factor of the Q-Learning algorithm. - 'tolerance' - set this to some small value larger than 0 (default was 0.05) to set the epsilon threshold for testing. Furthermore, use a decaying function of your choice for $\\epsilon$ (the exploration factor). Note that whichever function you use, it must decay to 'tolerance' at a reasonable rate . The Q-Learning agent will not begin testing until this occurs. Some example decaying functions (for $t$, the number of trials): $$ \\epsilon = a^t, \\textrm{for } 0 < a < 1 \\hspace{50px}\\epsilon = \\frac{1}{t^2}\\hspace{50px}\\epsilon = e^{-at}, \\textrm{for } 0 < a < 1 \\hspace{50px} \\epsilon = \\cos(at), \\textrm{for } 0 < a < 1$$ You may also use a decaying function for $\\alpha$ (the learning rate) if you so choose, however this is typically less common. If you do so, be sure that it adheres to the inequality $0 \\leq \\alpha \\leq 1$. If you have difficulty getting your implementation to work, try setting the 'verbose' flag to True to help debug. Flags that have been set here should be returned to their default setting when debugging. It is important that you understand what each flag does and how it affects the simulation! Once you have successfully completed the improved Q-Learning simulation, run the code cell below to visualize the results. Note that log files are overwritten when identical simulations are run, so be careful with what log file is being loaded!","title":"Improved Q-Learning Simulation Results"},{"location":"rl/smartcab/smartcab/#visualizing-some-decay-functions","text":"import numpy as np import math as math import pandas as pd import matplotlib.pyplot as plt from matplotlib import cm class decay ( object ): def __init__ ( self ,t,a): self . t = t self . a = a def fun1 ( self ): return 1.0 - ( self . t * 0.01 ) def fun2 ( self ): return self . a ** ( self . t) def fun3 ( self ): return np . exp( - self . a * self . t) def fun4 ( self ): return np . cos( self . a * self . t) def fun5 ( self ): return self . a / np . sqrt( self . t) - 0.5 def fun6 ( self ): return 1.0 / (( self . a * self . t) ** 3 ) a1 = np . arange( 0.001 , 1.0 , 0.01 ) x = np . arange( 1 , 100.0 , 1.0 ) plt . figure(figsize = ( 12 , 10 )) for ia in a1: plt . subplot( 2 , 3 , 1 ) plt . plot(x,decay(x,ia) . fun1()) plt . subplot( 2 , 3 , 2 ) plt . plot(x,decay(x,ia) . fun2()) plt . subplot( 2 , 3 , 3 ) plt . plot(x,decay(x,ia) . fun3()) a2 = np . arange( 0.001 , 0.009 , 0.0005 ) for ia in a2: plt . subplot( 2 , 3 , 4 ) plt . plot(x,decay(x,ia) . fun4()) plt . subplot( 2 , 3 , 5 ) plt . plot(x,decay(x,ia) . fun5()) plt . subplot( 2 , 3 , 6 ) plt . plot(x,decay(x,ia) . fun6())","title":"Visualizing some decay functions"},{"location":"rl/smartcab/smartcab/#choice-of-different-decay-functions","text":"","title":"Choice of different decay functions"},{"location":"rl/smartcab/smartcab/#case-i","text":"model epsilon tolerance alpha a safety reliability trainings linear $$\\epsilon = \\epsilon -0.005\\hspace{25px}$$ 0.0001 0.06 - A+ A+ 200 # Load the 'sim_improved-learning' file from the improved Q-Learning simulation vs . plot_trials( 'sim_improved-learning1.csv' )","title":"Case-I:"},{"location":"rl/smartcab/smartcab/#case-ii","text":"model epsilon tolerance alpha a safety reliability trainings exponential $$\\epsilon = e^{-at}\\hspace{50px}$$ 0.0001 0.06 0.01 A+ B ~700 # Load the 'sim_improved-learning' file from the improved Q-Learning simulation # Linear function self.epsilon = math.exp(1)**(-a*self.trials) a =0.01 vs . plot_trials( 'sim_improved-learning2.csv' )","title":"Case-II:"},{"location":"rl/smartcab/smartcab/#caseiii","text":"model epsilon tolerance alpha a safety reliability trainings exponential $$\\epsilon = a^t\\hspace{25px}$$ 0.0001 0.06 0.99 A+ B ~700 # Load the 'sim_improved-learning' file from the improved Q-Learning simulation # Linear function self.epsilon = a**self.trials a =0.99 vs . plot_trials( 'sim_improved-learning3.csv' )","title":"Case:III:"},{"location":"rl/smartcab/smartcab/#case-iv","text":"| model |epsilon |tolerance |alpha | a | safety |reliability |trainings | |---|---|---|---|---|---|---|---| | | trigonometric | $$ \\epsilon = \\cos(at) +0.5\\hspace{50px}$$| 0.01 | 0.06 | 0.001 | A+ | A | ~2000 | # Load the 'sim_improved-learning' file from the improved Q-Learning simulation # Linear function self.epsilon = math.cos(a*self.trials)+0.5 a=0.001 tolerance=0.01 vs . plot_trials( 'sim_improved-learning5.csv' )","title":"Case-IV:"},{"location":"rl/smartcab/smartcab/#question-7","text":"Using the visualization above that was produced from your improved Q-Learning simulation, provide a final analysis and make observations about the improved driving agent like in Question 6 . Questions you should answer: - What decaying function was used for epsilon (the exploration factor)? - Approximately how many training trials were needed for your agent before begining testing? - What epsilon-tolerance and alpha (learning rate) did you use? Why did you use them? - How much improvement was made with this Q-Learner when compared to the default Q-Learner from the previous section? - Would you say that the Q-Learner results show that your driving agent successfully learned an appropriate policy? - Are you satisfied with the safety and reliability ratings of the Smartcab ? Answer: Following is the summary table for all models used: model epsilon tolerance alpha a safety reliability trainings linear $$\\epsilon = \\epsilon -0.005\\hspace{25px}$$ 0.0001 0.06 - A+ A+ 200 exponential $$\\epsilon = e^{-at}\\hspace{50px}$$ 0.0001 0.06 0.01 A+ B ~700 exponential $$\\epsilon = a^t\\hspace{25px}$$ 0.0001 0.06 0.99 A+ B ~700 trigonometric $$ \\epsilon = \\cos(at) +0.5\\hspace{50px}$$ 0.01 0.06 0.001 A+ A ~2000 What decaying function was used for epsilon (the exploration factor)? Different decay function used asr listed in the chart above. Approximately how many training trials were needed for your agent before begining testing? From above chart it ranges from 200 to 2000. How much improvement was made with this Q-Learner when compared to the default Q-Learner from the previous section? There is a tremendous increase in terms safety and reliability. The Smartcab managed to achieve A/A+, A+/B, A+/B A+/A grades respectively, comparing to F in case of the default Q-Learner. Would you say that the Q-Learner results show that your driving agent successfully learned an appropriate policy? Taking into account the rewards, the linear and exponential decay functions got into a positive reward sector after some training examples. Other functions slowly achive that success. In each of the case driving agent is learning successfully. Are you satisfied with the safety and reliability ratings of the Smartcab? Yes, I am satisfied with the learning rate and only partially with the safety and reliability ratings, there is a room for improvement here because I have got B+ some where. Obviously the Smartcab is not ready for usage in real world, it still causes road accidents.","title":"Question 7"},{"location":"rl/smartcab/smartcab/#define-an-optimal-policy","text":"Sometimes, the answer to the important question \"what am I trying to get my agent to learn?\" only has a theoretical answer and cannot be concretely described. Here, however, you can concretely define what it is the agent is trying to learn, and that is the U.S. right-of-way traffic laws. Since these laws are known information, you can further define, for each state the Smartcab is occupying, the optimal action for the driving agent based on these laws. In that case, we call the set of optimal state-action pairs an optimal policy . Hence, unlike some theoretical answers, it is clear whether the agent is acting \"incorrectly\" not only by the reward (penalty) it receives, but also by pure observation. If the agent drives through a red light, we both see it receive a negative reward but also know that it is not the correct behavior. This can be used to your advantage for verifying whether the policy your driving agent has learned is the correct one, or if it is a suboptimal policy .","title":"Define an Optimal Policy"},{"location":"rl/smartcab/smartcab/#question-8","text":"Provide a few examples (using the states you've defined) of what an optimal policy for this problem would look like. Afterwards, investigate the 'sim_improved-learning.txt' text file to see the results of your improved Q-Learning algorithm. For each state that has been recorded from the simulation, is the policy (the action with the highest value) correct for the given state? Are there any states where the policy is different than what would be expected from an optimal policy? Provide an example of a state and all state-action rewards recorded, and explain why it is the correct policy. Answer: In most cases the policy does make sense. The state dictionary is defined like this: state = (inputs['light'],\\ inputs['oncoming'],\\ inputs['right'],\\ inputs['left'],\\ waypoint) Example-1 : Consider following state-action-reward ('green', None, 'forward', 'right', 'left')---> left -- forward : 0.13 -- None : -0.78 -- right : 0.17 -- left : 0.67 Here traffic light is green , there is no upcomimg traffic and the waypoint is left . So , moving to left has highest reward and going left is optimal policy. Example-2 : Consider following state-action-reward ('green', 'right', None, None, 'right')---> right -- forward : 0.41 -- None : -3.46 -- right : 1.62 -- left : -12.22 Here traffic light is green , the waypoint is right . Moving to right has highest reward and going right is optimal policy. Example-3 : Consider following state-action-reward ('green', 'right', 'left', None, 'right')--> forward -- forward : 0.68 -- None : -1.34 -- right : 0.42 -- left : -2.34 Here traffic light is green , the waypoint is right . But policy shows going forward has maximum reward. While the correct action would be going to right . So , this is a sub-optimal policy. This will be corected while agent gets more steps to learn.","title":"Question 8"},{"location":"rl/smartcab/smartcab/#optional-future-rewards-discount-factor-gamma","text":"Curiously, as part of the Q-Learning algorithm, you were asked to not use the discount factor, 'gamma' in the implementation. Including future rewards in the algorithm is used to aid in propogating positive rewards backwards from a future state to the current state. Essentially, if the driving agent is given the option to make several actions to arrive at different states, including future rewards will bias the agent towards states that could provide even more rewards. An example of this would be the driving agent moving towards a goal: With all actions and rewards equal, moving towards the goal would theoretically yield better rewards if there is an additional reward for reaching the goal. However, even though in this project, the driving agent is trying to reach a destination in the allotted time, including future rewards will not benefit the agent. In fact, if the agent were given many trials to learn, it could negatively affect Q-values!","title":"Optional: Future Rewards - Discount Factor, 'gamma'"},{"location":"rl/smartcab/smartcab/#optional-question-9","text":"There are two characteristics about the project that invalidate the use of future rewards in the Q-Learning algorithm. One characteristic has to do with the Smartcab itself, and the other has to do with the environment. Can you figure out what they are and why future rewards won't work for this project? Answer: Note : Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to File -> Download as -> HTML (.html) . Include the finished document along with this notebook as your submission.","title":"Optional Question 9"}]}