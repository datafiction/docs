<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../../../img/favicon.ico">
  <title>Ensamble Methods - Machine Learning</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>
  <link href='https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Ensamble Methods";
    var mkdocs_page_input_path = "ml/Classifiers/Ensamble/ensamble.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../../../.." class="icon icon-home"> Machine Learning</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="tocbase current">
    
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../../../..">Home</a>
  </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">Machine Learning</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../LinearModels/Linear-models/">Linear Regression</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../Logistic/Logistic/">Logistic Regression</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../Tree/Tree/">Decision Tree</a>
  </li>
        
          


  
    
    <li class="navtree toctree-l2 page current">
      <a class="current" href="./">
        Ensamble Methods
          <span class="toctree-expand"></span>
      </a>
    </li>
    
      

  <li class="toctree-l2 current with-children">
    <a href="#ensamble">
      Ensamble
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l2 current">
    <ul class="subnav-l2 current">
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#1-single-estimator-versus-bagging-bias-variance-decomposition-source">1. Single estimator versus bagging: bias-variance decomposition (source)</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#partition">Partition</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#plot">Plot</a>
        </li>
    
      
          

  <li class="toctree-l3">
    <a href="#2-oob-errors-for-random-forests-source">
      2. OOB Errors for Random Forests (source)
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l3">
    <ul class="subnav-l3 toc-hidden">
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#data-set">Data Set</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#warm_start">warm_start</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#plot_1">Plot</a>
        </li>
    
    </ul>
  </li>

      
    
      
          

  <li class="toctree-l3">
    <a href="#3-feature-transformations-with-ensembles-of-trees-source">
      3. Feature transformations with ensembles of trees (source)
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l3">
    <ul class="subnav-l3 toc-hidden">
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#data">Data</a>
        </li>
    
      
        <li class="toctree-l4">
          <a class="toctree-l5" href="#model">Model</a>
        </li>
    
    </ul>
  </li>

      
    
      
          

  <li class="toctree-l3">
    <a href="#4-pixel-importances-with-a-parallel-forest-of-trees-source">
      4. Pixel importances with a parallel forest of trees (source)
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l3">
    <ul class="subnav-l3 toc-hidden">
    
      
          

  <li class="toctree-l4 current with-children">
    <a href="#data_1">
      Data
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l4 current">
    <ul class="subnav-l4 current">
    
      
        <li class="toctree-l5">
          <a class="toctree-l6" href="#plot_2">Plot</a>
        </li>
    
    </ul>
  </li>

      
    
    </ul>
  </li>

      
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#5-feature-importances-with-forests-of-trees-source">5. Feature importances with forests of trees (source)</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#6-isolationforest-example-source">6. IsolationForest example (source)</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#7-hashing-feature-transformation-using-totally-random-trees-source">7.  Hashing feature transformation using Totally Random Trees (source)</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#8-two-class-adaboost-source">8. Two-class AdaBoost (source)</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#9-multi-class-adaboosted-decision-trees-source">9. Multi-class AdaBoosted Decision Trees (source)</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#10-discrete-versus-real-adaboost-source">10 .Discrete versus Real AdaBoost (source)</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#11-decision-tree-regression-with-adaboost-source">11. Decision Tree Regression with AdaBoost (source)</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#12-gradient-boosting-out-of-bag-estimates-source">12. Gradient Boosting Out-of-Bag estimates (source)</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#13-prediction-intervals-for-gradient-boosting-regression-source">13. Prediction Intervals for Gradient Boosting Regression (source)</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#14-gradient-boosting-regression-source">14. Gradient Boosting regression (source)</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#15-gradient-boosting-regularization-source">15. Gradient Boosting regularization (source)</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#16-partial-dependence-plots-source">16. Partial Dependence Plots (source)</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#17-plot-the-decision-boundaries-of-a-votingclassifier-source">17. Plot the decision boundaries of a VotingClassifier (source)</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#18-plot-class-probabilities-calculated-by-the-votingclassifier-source">18. Plot class probabilities calculated by the VotingClassifier (source)</a>
        </li>
    
    </ul>
  </li>


  
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../SVM/svm/">Support Vector Machine</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../Clustering/Kmeans/Kmeans/">Kmeans Clustering</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../Clustering/Agglomerative/Agglomerative/">Agglomerative Clustering</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../Clustering/AffinityPropagation/Affinity-Propagation/">Affinity Propagation</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../Clustering/Spectral/Spectral/">Spectral Clustering</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../Clustering/DBSCAN/DBSCAN/">DBSCAN Clustering</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../Clustering/MeanShift/Mean-shift/">Mean Shift Clustering</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../Clustering/Comparison/Comparison/">Cluster Comparison</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">ML Projects</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../mlp/intro/">Introduction</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../mlp/boston_housing/boston_housing/">Boston Housing</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../mlp/Customer_segments/customer_segments/">Customer Clustering</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../mlp/finding_donors/finding_donors/">Finding Donors</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../mlp/vehicle-detection/CARND-Project-5/">Vehicle Detection</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../mlp/perceptron/dlnd-your-first-neural-network/">Perceptron</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">Deep Learning</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../dl/intro/">Introduction</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../dl/Vanila/1.Vanila-LSTM/">Vanila LSTM</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../dl/stacked/2.Stacked-LSTM/">Stacked LSTM</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../dl/bidirectional/5. BiDirectional-LSTM/">Bidirectional LSTM</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../dl/rnn/RNN_project/">Recurrent Neural Network</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">DL Projects</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../dl/NMIST/NMIST/">Digit Classifier</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../dl/CIFRT10/CIFR10/">Image Classifier</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../dl/traffic-sign/Traffic_Sign_Classifier/">Traffic Sign Detection</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../dl/translator/dlnd_language_translation/">Language Translator</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">Rinforcement Learning</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../rl/smartcab/smartcab/">SmartCab</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">GAN Project</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../../../dl/gan/dlnd_face_generation/">Face Generation</a>
  </li>
        
      </ul>
    </li>
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../../../../References/ref.md">References</a>
  </li>
    
  </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../../..">Machine Learning</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../..">Docs</a> &raquo;</li>
    
      
        
          <li>Machine Learning &raquo;</li>
        
      
    
    <li>Ensamble Methods</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="ensamble">Ensamble</h1>
<p>The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</p>
<p>Two families of ensemble methods are usually distinguished:</p>
<ul>
<li>
<p>In <strong><em>averaging methods</em></strong>, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.</p>
<p>Examples: 
 - Bagging methods, 
 - Forests of randomized trees, ...</p>
</li>
<li>
<p>By contrast, in <strong>boosting methods</strong>, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.</p>
<p>Examples: 
 - AdaBoost, 
 - Gradient Tree Boosting, ...</p>
</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #555555">%</span>matplotlib inline
</pre></div>


<hr />
<h3 id="1-single-estimator-versus-bagging-bias-variance-decomposition-source">1. Single estimator versus bagging: bias-variance decomposition (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>This example illustrates and compares the bias-variance decomposition of the
expected mean squared error of a single estimator against a bagging ensemble.</p>
<p>In regression, the expected mean squared error of an estimator can be
decomposed in terms of bias, variance and noise. On average over datasets of
the regression problem, the bias term measures the average amount by which the
predictions of the estimator differ from the predictions of the best possible
estimator for the problem (i.e., the Bayes model). The variance term measures
the variability of the predictions of the estimator when fit over different
instances LS of the problem. Finally, the noise measures the irreducible part
of the error which is due the variability in the data.</p>
<p>The upper left figure illustrates the predictions (in dark red) of a single
decision tree trained over a random dataset LS (the blue dots) of a toy 1d
regression problem. It also illustrates the predictions (in light red) of other
single decision trees trained over other (and different) randomly drawn
instances LS of the problem. Intuitively, the variance term here corresponds to
the width of the beam of predictions (in light red) of the individual
estimators. The larger the variance, the more sensitive are the predictions for
<code>x</code> to small changes in the training set. The bias term corresponds to the
difference between the average prediction of the estimator (in cyan) and the
best possible model (in dark blue). On this problem, we can thus observe that
the bias is quite low (both the cyan and the blue curves are close to each
other) while the variance is large (the red beam is rather wide).</p>
<p>The lower left figure plots the pointwise decomposition of the expected mean
squared error of a single decision tree. It confirms that the bias term (in
blue) is low while the variance is large (in green). It also illustrates the
noise part of the error which, as expected, appears to be constant and around
<code>0.01</code>.</p>
<p>The right figures correspond to the same plots but using instead a bagging
ensemble of decision trees. In both figures, we can observe that the bias term
is larger than in the previous case. In the upper right figure, the difference
between the average prediction (in cyan) and the best possible model is larger
(e.g., notice the offset around <code>x=2</code>). In the lower right figure, the bias
curve is also slightly higher than in the lower left figure. In terms of
variance however, the beam of predictions is narrower, which suggests that the
variance is lower. Indeed, as the lower right figure confirms, the variance
term (in green) is lower than for single decision trees. Overall, the bias-
variance decomposition is therefore no longer the same. The tradeoff is better
for bagging: averaging several decision trees fit on bootstrap copies of the
dataset slightly increases the bias term but allows for a larger reduction of
the variance, which results in a lower overall mean squared error (compare the
red curves int the lower figures). The script output also confirms this
intuition. The total error of the bagging ensemble is lower than the total
error of a single decision tree, and this difference indeed mainly stems from a
reduced variance.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> BaggingRegressor
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.tree</span> <span style="color: #006699; font-weight: bold">import</span> DecisionTreeRegressor
</pre></div>


<ul>
<li>Setting Parameters</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Settings</span>
n_repeat <span style="color: #555555">=</span> <span style="color: #FF6600">50</span>       <span style="color: #0099FF; font-style: italic"># Number of iterations for computing expectations</span>
n_train <span style="color: #555555">=</span> <span style="color: #FF6600">50</span>        <span style="color: #0099FF; font-style: italic"># Size of the training set</span>
n_test <span style="color: #555555">=</span> <span style="color: #FF6600">1000</span>       <span style="color: #0099FF; font-style: italic"># Size of the test set</span>
noise <span style="color: #555555">=</span> <span style="color: #FF6600">0.1</span>         <span style="color: #0099FF; font-style: italic"># Standard deviation of the noise</span>
np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>seed(<span style="color: #FF6600">0</span>)
</pre></div>


<p>Change this for exploring the bias-variance decomposition of other
estimators. This should work well for estimators with high variance (e.g.,
decision trees or KNN), but poorly for estimators with low variance (e.g.,
linear models).</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>estimators <span style="color: #555555">=</span> [(<span style="color: #CC3300">&quot;Tree&quot;</span>, DecisionTreeRegressor()),
              (<span style="color: #CC3300">&quot;Bagging(Tree)&quot;</span>, BaggingRegressor(DecisionTreeRegressor()))]

n_estimators <span style="color: #555555">=</span> <span style="color: #336666">len</span>(estimators)
</pre></div>


<p>### Data</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Generate data</span>
<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">f</span>(x):
    x <span style="color: #555555">=</span> x<span style="color: #555555">.</span>ravel()

    <span style="color: #006699; font-weight: bold">return</span> np<span style="color: #555555">.</span>exp(<span style="color: #555555">-</span>x <span style="color: #555555">**</span> <span style="color: #FF6600">2</span>) <span style="color: #555555">+</span> <span style="color: #FF6600">1.5</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>exp(<span style="color: #555555">-</span>(x <span style="color: #555555">-</span> <span style="color: #FF6600">2</span>) <span style="color: #555555">**</span> <span style="color: #FF6600">2</span>)

<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">generate</span>(n_samples, noise, n_repeat<span style="color: #555555">=</span><span style="color: #FF6600">1</span>):
    X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>rand(n_samples) <span style="color: #555555">*</span> <span style="color: #FF6600">10</span> <span style="color: #555555">-</span> <span style="color: #FF6600">5</span>
    X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>sort(X)

    <span style="color: #006699; font-weight: bold">if</span> n_repeat <span style="color: #555555">==</span> <span style="color: #FF6600">1</span>:
        y <span style="color: #555555">=</span> f(X) <span style="color: #555555">+</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>normal(<span style="color: #FF6600">0.0</span>, noise, n_samples)
    <span style="color: #006699; font-weight: bold">else</span>:
        y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros((n_samples, n_repeat))

        <span style="color: #006699; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">range</span>(n_repeat):
            y[:, i] <span style="color: #555555">=</span> f(X) <span style="color: #555555">+</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>normal(<span style="color: #FF6600">0.0</span>, noise, n_samples)

    X <span style="color: #555555">=</span> X<span style="color: #555555">.</span>reshape((n_samples, <span style="color: #FF6600">1</span>))

    <span style="color: #006699; font-weight: bold">return</span> X, y
</pre></div>


<h3 id="partition">Partition</h3>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>X_train <span style="color: #555555">=</span> []
y_train <span style="color: #555555">=</span> []

<span style="color: #006699; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">range</span>(n_repeat):
    X, y <span style="color: #555555">=</span> generate(n_samples<span style="color: #555555">=</span>n_train, noise<span style="color: #555555">=</span>noise)
    X_train<span style="color: #555555">.</span>append(X)
    y_train<span style="color: #555555">.</span>append(y)

X_test, y_test <span style="color: #555555">=</span> generate(n_samples<span style="color: #555555">=</span>n_test, noise<span style="color: #555555">=</span>noise, n_repeat<span style="color: #555555">=</span>n_repeat)
</pre></div>


<h3 id="plot">Plot</h3>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Loop over estimators to compare</span>
<span style="color: #006699; font-weight: bold">for</span> n, (name, estimator) <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">enumerate</span>(estimators):
    <span style="color: #0099FF; font-style: italic"># Compute predictions</span>
    y_predict <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros((n_test, n_repeat))

    <span style="color: #006699; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">range</span>(n_repeat):
        estimator<span style="color: #555555">.</span>fit(X_train[i], y_train[i])
        y_predict[:, i] <span style="color: #555555">=</span> estimator<span style="color: #555555">.</span>predict(X_test)

    <span style="color: #0099FF; font-style: italic"># Bias^2 + Variance + Noise decomposition of the mean squared error</span>
    y_error <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros(n_test)

    <span style="color: #006699; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">range</span>(n_repeat):
        <span style="color: #006699; font-weight: bold">for</span> j <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">range</span>(n_repeat):
            y_error <span style="color: #555555">+=</span> (y_test[:, j] <span style="color: #555555">-</span> y_predict[:, i]) <span style="color: #555555">**</span> <span style="color: #FF6600">2</span>

    y_error <span style="color: #555555">/=</span> (n_repeat <span style="color: #555555">*</span> n_repeat)

    y_noise <span style="color: #555555">=</span> np<span style="color: #555555">.</span>var(y_test, axis<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)
    y_bias <span style="color: #555555">=</span> (f(X_test) <span style="color: #555555">-</span> np<span style="color: #555555">.</span>mean(y_predict, axis<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)) <span style="color: #555555">**</span> <span style="color: #FF6600">2</span>
    y_var <span style="color: #555555">=</span> np<span style="color: #555555">.</span>var(y_predict, axis<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)

    <span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;</span><span style="color: #AA0000">{0}</span><span style="color: #CC3300">: </span><span style="color: #AA0000">{1:.4f}</span><span style="color: #CC3300"> (error) = </span><span style="color: #AA0000">{2:.4f}</span><span style="color: #CC3300"> (bias^2) &quot;</span>
          <span style="color: #CC3300">&quot; + </span><span style="color: #AA0000">{3:.4f}</span><span style="color: #CC3300"> (var) + </span><span style="color: #AA0000">{4:.4f}</span><span style="color: #CC3300"> (noise)&quot;</span><span style="color: #555555">.</span>format(name,
                                                      np<span style="color: #555555">.</span>mean(y_error),
                                                      np<span style="color: #555555">.</span>mean(y_bias),
                                                      np<span style="color: #555555">.</span>mean(y_var),
                                                      np<span style="color: #555555">.</span>mean(y_noise)))

    <span style="color: #0099FF; font-style: italic"># Plot figures</span>
    plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">2</span>, n_estimators, n <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>)
    plt<span style="color: #555555">.</span>plot(X_test, f(X_test), <span style="color: #CC3300">&quot;b&quot;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;$f(x)$&quot;</span>)
    plt<span style="color: #555555">.</span>plot(X_train[<span style="color: #FF6600">0</span>], y_train[<span style="color: #FF6600">0</span>], <span style="color: #CC3300">&quot;.b&quot;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;LS ~ $y = f(x)+noise$&quot;</span>)

    <span style="color: #006699; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">range</span>(n_repeat):
        <span style="color: #006699; font-weight: bold">if</span> i <span style="color: #555555">==</span> <span style="color: #FF6600">0</span>:
            plt<span style="color: #555555">.</span>plot(X_test, y_predict[:, i], <span style="color: #CC3300">&quot;r&quot;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;$\^y(x)$&quot;</span>)
        <span style="color: #006699; font-weight: bold">else</span>:
            plt<span style="color: #555555">.</span>plot(X_test, y_predict[:, i], <span style="color: #CC3300">&quot;r&quot;</span>, alpha<span style="color: #555555">=</span><span style="color: #FF6600">0.05</span>)

    plt<span style="color: #555555">.</span>plot(X_test, np<span style="color: #555555">.</span>mean(y_predict, axis<span style="color: #555555">=</span><span style="color: #FF6600">1</span>), <span style="color: #CC3300">&quot;c&quot;</span>,
             label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;$\mathbb</span><span style="color: #AA0000">{E}</span><span style="color: #CC3300">_</span><span style="color: #AA0000">{LS}</span><span style="color: #CC3300"> \^y(x)$&quot;</span>)

    plt<span style="color: #555555">.</span>xlim([<span style="color: #555555">-</span><span style="color: #FF6600">5</span>, <span style="color: #FF6600">5</span>])
    plt<span style="color: #555555">.</span>title(name)

    <span style="color: #006699; font-weight: bold">if</span> n <span style="color: #555555">==</span> <span style="color: #FF6600">0</span>:
        plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&quot;upper left&quot;</span>, prop<span style="color: #555555">=</span>{<span style="color: #CC3300">&quot;size&quot;</span>: <span style="color: #FF6600">11</span>})

    plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">2</span>, n_estimators, n_estimators <span style="color: #555555">+</span> n <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>)
    plt<span style="color: #555555">.</span>plot(X_test, y_error, <span style="color: #CC3300">&quot;r&quot;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;$error(x)$&quot;</span>)
    plt<span style="color: #555555">.</span>plot(X_test, y_bias, <span style="color: #CC3300">&quot;b&quot;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;$bias^2(x)$&quot;</span>),
    plt<span style="color: #555555">.</span>plot(X_test, y_var, <span style="color: #CC3300">&quot;g&quot;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;$variance(x)$&quot;</span>),
    plt<span style="color: #555555">.</span>plot(X_test, y_noise, <span style="color: #CC3300">&quot;c&quot;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;$noise(x)$&quot;</span>)

    plt<span style="color: #555555">.</span>xlim([<span style="color: #555555">-</span><span style="color: #FF6600">5</span>, <span style="color: #FF6600">5</span>])
    plt<span style="color: #555555">.</span>ylim([<span style="color: #FF6600">0</span>, <span style="color: #FF6600">0.1</span>])

    <span style="color: #006699; font-weight: bold">if</span> n <span style="color: #555555">==</span> <span style="color: #FF6600">0</span>:
        plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&quot;upper left&quot;</span>, prop<span style="color: #555555">=</span>{<span style="color: #CC3300">&quot;size&quot;</span>: <span style="color: #FF6600">11</span>})

plt<span style="color: #555555">.</span>show()
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>Tree: 0.0255 (error) = 0.0003 (bias^2)  + 0.0152 (var) + 0.0098 (noise)
Bagging(Tree): 0.0196 (error) = 0.0004 (bias^2)  + 0.0092 (var) + 0.0098 (noise)
</pre></div>


<p><img alt="png" src="../output_18_1.png" /></p>
<hr />
<h3 id="2-oob-errors-for-random-forests-source">2. OOB Errors for Random Forests (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>The <code>RandomForestClassifier</code> is trained using <em>bootstrap aggregation</em>, where each new tree is fit from a bootstrap sample of the training observations</p>
<p>$z_i = (x_i, y_i)$.</p>
<p>The <strong>out-of-bag</strong> (OOB) error is the average error for
each $z_i$ calculated using predictions from the trees that do not
contain $z_i$ in their respective bootstrap sample. This allows the
<code>RandomForestClassifier</code> to be fit and validated whilst being trained </p>
<p>The example below demonstrates how the OOB error can be measured at the
addition of each new tree during training. The resulting plot allows a
practitioner to approximate a suitable value of <code>n_estimators</code> at which the error stabilizes.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">collections</span> <span style="color: #006699; font-weight: bold">import</span> OrderedDict
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.datasets</span> <span style="color: #006699; font-weight: bold">import</span> make_classification
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> RandomForestClassifier, ExtraTreesClassifier
</pre></div>


<h4 id="data-set">Data Set</h4>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>RANDOM_STATE <span style="color: #555555">=</span> <span style="color: #FF6600">123</span>

<span style="color: #0099FF; font-style: italic"># Generate a binary classification dataset.</span>
X, y <span style="color: #555555">=</span> make_classification(n_samples<span style="color: #555555">=</span><span style="color: #FF6600">500</span>, n_features<span style="color: #555555">=</span><span style="color: #FF6600">25</span>,
                           n_clusters_per_class<span style="color: #555555">=</span><span style="color: #FF6600">1</span>, n_informative<span style="color: #555555">=</span><span style="color: #FF6600">15</span>,
                           random_state<span style="color: #555555">=</span>RANDOM_STATE)
</pre></div>


<h4 id="warm_start">warm_start</h4>
<ul>
<li>NOTE: Setting the <code>warm_start</code> construction parameter to <code>True</code> disables
support for paralellised ensembles but is necessary for tracking the OOB
error trajectory during training.</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>ensemble_clfs <span style="color: #555555">=</span> [
    (<span style="color: #CC3300">&quot;RandomForestClassifier, max_features=&#39;sqrt&#39;&quot;</span>,
        RandomForestClassifier(warm_start<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>, oob_score<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>,
                               max_features<span style="color: #555555">=</span><span style="color: #CC3300">&quot;sqrt&quot;</span>,
                               random_state<span style="color: #555555">=</span>RANDOM_STATE)),
    (<span style="color: #CC3300">&quot;RandomForestClassifier, max_features=&#39;log2&#39;&quot;</span>,
        RandomForestClassifier(warm_start<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>, max_features<span style="color: #555555">=</span><span style="color: #CC3300">&#39;log2&#39;</span>,
                               oob_score<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>,
                               random_state<span style="color: #555555">=</span>RANDOM_STATE)),
    (<span style="color: #CC3300">&quot;RandomForestClassifier, max_features=None&quot;</span>,
        RandomForestClassifier(warm_start<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>, max_features<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">None</span>,
                               oob_score<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>,
                               random_state<span style="color: #555555">=</span>RANDOM_STATE))
]
</pre></div>


<ul>
<li>Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>error_rate <span style="color: #555555">=</span> OrderedDict((label, []) <span style="color: #006699; font-weight: bold">for</span> label, _ <span style="color: #000000; font-weight: bold">in</span> ensemble_clfs)
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Range of `n_estimators` values to explore.</span>
min_estimators <span style="color: #555555">=</span> <span style="color: #FF6600">15</span>
max_estimators <span style="color: #555555">=</span> <span style="color: #FF6600">175</span>

<span style="color: #006699; font-weight: bold">for</span> label, clf <span style="color: #000000; font-weight: bold">in</span> ensemble_clfs:
    <span style="color: #006699; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">range</span>(min_estimators, max_estimators <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>):
        clf<span style="color: #555555">.</span>set_params(n_estimators<span style="color: #555555">=</span>i)
        clf<span style="color: #555555">.</span>fit(X, y)

        <span style="color: #0099FF; font-style: italic"># Record the OOB error for each `n_estimators=i` setting.</span>
        oob_error <span style="color: #555555">=</span> <span style="color: #FF6600">1</span> <span style="color: #555555">-</span> clf<span style="color: #555555">.</span>oob_score_
        error_rate[label]<span style="color: #555555">.</span>append((i, oob_error))
</pre></div>


<h4 id="plot_1">Plot</h4>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Generate the &quot;OOB error rate&quot; vs. &quot;n_estimators&quot; plot.</span>
<span style="color: #006699; font-weight: bold">for</span> label, clf_err <span style="color: #000000; font-weight: bold">in</span> error_rate<span style="color: #555555">.</span>items():
    xs, ys <span style="color: #555555">=</span> <span style="color: #336666">zip</span>(<span style="color: #555555">*</span>clf_err)
    plt<span style="color: #555555">.</span>plot(xs, ys, label<span style="color: #555555">=</span>label)
plt<span style="color: #555555">.</span>xlim(min_estimators, max_estimators)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;n_estimators&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;OOB error rate&quot;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&quot;upper right&quot;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_33_0.png" /></p>
<hr />
<h3 id="3-feature-transformations-with-ensembles-of-trees-source">3. Feature transformations with ensembles of trees (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>Transform your features into a higher dimensional, sparse space. Then
train a linear model on these features.</p>
<p>First fit an ensemble of trees (totally random trees, a random
forest, or gradient boosted trees) on the training set. Then each leaf
of each tree in the ensemble is assigned a fixed arbitrary feature
index in a new feature space. These leaf indices are then encoded in a
one-hot fashion.</p>
<p>Each sample goes through the decisions of each tree of the ensemble
and ends up in one leaf per tree. The sample is encoded by setting
feature values for these leaves to 1 and the other feature values to 0.</p>
<p>The resulting transformer has then learned a supervised, sparse,
high-dimensional categorical embedding of the data.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>seed(<span style="color: #FF6600">10</span>)
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #0099FF; font-style: italic">#======== models =====================================</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> LogisticRegression
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> (RandomTreesEmbedding, RandomForestClassifier,
                              GradientBoostingClassifier)
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #006699; font-weight: bold">import</span> OneHotEncoder
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.cross_validation</span> <span style="color: #006699; font-weight: bold">import</span> train_test_split
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.metrics</span> <span style="color: #006699; font-weight: bold">import</span> roc_curve
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.pipeline</span> <span style="color: #006699; font-weight: bold">import</span> make_pipeline
<span style="color: #0099FF; font-style: italic">#======= data ==================================</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.datasets</span> <span style="color: #006699; font-weight: bold">import</span> make_classification
</pre></div>


<h4 id="data">Data</h4>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>n_estimator <span style="color: #555555">=</span> <span style="color: #FF6600">10</span>
X, y <span style="color: #555555">=</span> make_classification(n_samples<span style="color: #555555">=</span><span style="color: #FF6600">80000</span>)
X_train, X_test, y_train, y_test <span style="color: #555555">=</span> train_test_split(X, y, test_size<span style="color: #555555">=</span><span style="color: #FF6600">0.5</span>)
</pre></div>


<p>It is important to train the ensemble of trees on a different subset
of the training data than the linear regression model to avoid
overfitting, in particular if the total number of leaves is
similar to the number of training samples</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>X_train, X_train_lr, y_train, y_train_lr <span style="color: #555555">=</span> train_test_split(X_train,
                                                            y_train,
                                                            test_size<span style="color: #555555">=</span><span style="color: #FF6600">0.5</span>)
</pre></div>


<ul>
<li>Unsupervised transformation</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Unsupervised transformation based on totally random trees</span>
rt <span style="color: #555555">=</span> RandomTreesEmbedding(max_depth<span style="color: #555555">=</span><span style="color: #FF6600">3</span>, n_estimators<span style="color: #555555">=</span>n_estimator,
    random_state<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)

rt_lm <span style="color: #555555">=</span> LogisticRegression()
pipeline <span style="color: #555555">=</span> make_pipeline(rt, rt_lm)
pipeline<span style="color: #555555">.</span>fit(X_train, y_train)
y_pred_rt <span style="color: #555555">=</span> pipeline<span style="color: #555555">.</span>predict_proba(X_test)[:, <span style="color: #FF6600">1</span>]
fpr_rt_lm, tpr_rt_lm, _ <span style="color: #555555">=</span> roc_curve(y_test, y_pred_rt)
</pre></div>


<h4 id="model">Model</h4>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Supervised transformation based on random forests</span>
rf <span style="color: #555555">=</span> RandomForestClassifier(max_depth<span style="color: #555555">=</span><span style="color: #FF6600">3</span>, n_estimators<span style="color: #555555">=</span>n_estimator)
rf_enc <span style="color: #555555">=</span> OneHotEncoder()
rf_lm <span style="color: #555555">=</span> LogisticRegression()
rf<span style="color: #555555">.</span>fit(X_train, y_train)
rf_enc<span style="color: #555555">.</span>fit(rf<span style="color: #555555">.</span>apply(X_train))
rf_lm<span style="color: #555555">.</span>fit(rf_enc<span style="color: #555555">.</span>transform(rf<span style="color: #555555">.</span>apply(X_train_lr)), y_train_lr)

y_pred_rf_lm <span style="color: #555555">=</span> rf_lm<span style="color: #555555">.</span>predict_proba(rf_enc<span style="color: #555555">.</span>transform(rf<span style="color: #555555">.</span>apply(X_test)))[:, <span style="color: #FF6600">1</span>]
fpr_rf_lm, tpr_rf_lm, _ <span style="color: #555555">=</span> roc_curve(y_test, y_pred_rf_lm)

grd <span style="color: #555555">=</span> GradientBoostingClassifier(n_estimators<span style="color: #555555">=</span>n_estimator)
grd_enc <span style="color: #555555">=</span> OneHotEncoder()
grd_lm <span style="color: #555555">=</span> LogisticRegression()
grd<span style="color: #555555">.</span>fit(X_train, y_train)
grd_enc<span style="color: #555555">.</span>fit(grd<span style="color: #555555">.</span>apply(X_train)[:, :, <span style="color: #FF6600">0</span>])
grd_lm<span style="color: #555555">.</span>fit(grd_enc<span style="color: #555555">.</span>transform(grd<span style="color: #555555">.</span>apply(X_train_lr)[:, :, <span style="color: #FF6600">0</span>]), y_train_lr)

y_pred_grd_lm <span style="color: #555555">=</span> grd_lm<span style="color: #555555">.</span>predict_proba(
    grd_enc<span style="color: #555555">.</span>transform(grd<span style="color: #555555">.</span>apply(X_test)[:, :, <span style="color: #FF6600">0</span>]))[:, <span style="color: #FF6600">1</span>]
fpr_grd_lm, tpr_grd_lm, _ <span style="color: #555555">=</span> roc_curve(y_test, y_pred_grd_lm)


<span style="color: #0099FF; font-style: italic"># The gradient boosted model by itself</span>
y_pred_grd <span style="color: #555555">=</span> grd<span style="color: #555555">.</span>predict_proba(X_test)[:, <span style="color: #FF6600">1</span>]
fpr_grd, tpr_grd, _ <span style="color: #555555">=</span> roc_curve(y_test, y_pred_grd)


<span style="color: #0099FF; font-style: italic"># The random forest model by itself</span>
y_pred_rf <span style="color: #555555">=</span> rf<span style="color: #555555">.</span>predict_proba(X_test)[:, <span style="color: #FF6600">1</span>]
fpr_rf, tpr_rf, _ <span style="color: #555555">=</span> roc_curve(y_test, y_pred_rf)
</pre></div>


<ul>
<li>Plot-1</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>plt<span style="color: #555555">.</span>figure(<span style="color: #FF6600">1</span>)
plt<span style="color: #555555">.</span>plot([<span style="color: #FF6600">0</span>, <span style="color: #FF6600">1</span>], [<span style="color: #FF6600">0</span>, <span style="color: #FF6600">1</span>], <span style="color: #CC3300">&#39;k--&#39;</span>)
plt<span style="color: #555555">.</span>plot(fpr_rt_lm, tpr_rt_lm, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;RT + LR&#39;</span>)
plt<span style="color: #555555">.</span>plot(fpr_rf, tpr_rf, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;RF&#39;</span>)
plt<span style="color: #555555">.</span>plot(fpr_rf_lm, tpr_rf_lm, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;RF + LR&#39;</span>)
plt<span style="color: #555555">.</span>plot(fpr_grd, tpr_grd, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;GBT&#39;</span>)
plt<span style="color: #555555">.</span>plot(fpr_grd_lm, tpr_grd_lm, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;GBT + LR&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;False positive rate&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;True positive rate&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;ROC curve&#39;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;best&#39;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_48_0.png" /></p>
<ul>
<li>Plot-2</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>plt<span style="color: #555555">.</span>figure(<span style="color: #FF6600">2</span>)
plt<span style="color: #555555">.</span>xlim(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">0.2</span>)
plt<span style="color: #555555">.</span>ylim(<span style="color: #FF6600">0.8</span>, <span style="color: #FF6600">1</span>)
plt<span style="color: #555555">.</span>plot([<span style="color: #FF6600">0</span>, <span style="color: #FF6600">1</span>], [<span style="color: #FF6600">0</span>, <span style="color: #FF6600">1</span>], <span style="color: #CC3300">&#39;k--&#39;</span>)
plt<span style="color: #555555">.</span>plot(fpr_rt_lm, tpr_rt_lm, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;RT + LR&#39;</span>)
plt<span style="color: #555555">.</span>plot(fpr_rf, tpr_rf, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;RF&#39;</span>)
plt<span style="color: #555555">.</span>plot(fpr_rf_lm, tpr_rf_lm, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;RF + LR&#39;</span>)
plt<span style="color: #555555">.</span>plot(fpr_grd, tpr_grd, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;GBT&#39;</span>)
plt<span style="color: #555555">.</span>plot(fpr_grd_lm, tpr_grd_lm, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;GBT + LR&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;False positive rate&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;True positive rate&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;ROC curve (zoomed in at top left)&#39;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;best&#39;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_50_0.png" /></p>
<hr />
<h3 id="4-pixel-importances-with-a-parallel-forest-of-trees-source">4. Pixel importances with a parallel forest of trees (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>This example shows the use of forests of trees to evaluate the importance
of the pixels in an image classification task (faces). The hotter the pixel,
the more important.</p>
<p>The code below also illustrates how the construction and the computation
of the predictions can be parallelized within multiple jobs.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">time</span> <span style="color: #006699; font-weight: bold">import</span> time
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.datasets</span> <span style="color: #006699; font-weight: bold">import</span> fetch_olivetti_faces
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> ExtraTreesClassifier
</pre></div>


<h4 id="data_1">Data</h4>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Number of cores to use to perform parallel fitting of the forest model</span>
n_jobs <span style="color: #555555">=</span> <span style="color: #FF6600">1</span>

<span style="color: #0099FF; font-style: italic"># Load the faces dataset</span>
data <span style="color: #555555">=</span> fetch_olivetti_faces()
X <span style="color: #555555">=</span> data<span style="color: #555555">.</span>images<span style="color: #555555">.</span>reshape((<span style="color: #336666">len</span>(data<span style="color: #555555">.</span>images), <span style="color: #555555">-</span><span style="color: #FF6600">1</span>))
y <span style="color: #555555">=</span> data<span style="color: #555555">.</span>target

mask <span style="color: #555555">=</span> y <span style="color: #555555">&lt;</span> <span style="color: #FF6600">5</span>  <span style="color: #0099FF; font-style: italic"># Limit to 5 classes</span>
X <span style="color: #555555">=</span> X[mask]
y <span style="color: #555555">=</span> y[mask]
</pre></div>


<ul>
<li>Perform parallel fitting of the forest model</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Build a forest and compute the pixel importances</span>
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Fitting ExtraTreesClassifier on faces data with </span><span style="color: #AA0000">%d</span><span style="color: #CC3300"> cores...&quot;</span> <span style="color: #555555">%</span> n_jobs)
t0 <span style="color: #555555">=</span> time()
forest <span style="color: #555555">=</span> ExtraTreesClassifier(n_estimators<span style="color: #555555">=</span><span style="color: #FF6600">1000</span>,
                              max_features<span style="color: #555555">=</span><span style="color: #FF6600">128</span>,
                              n_jobs<span style="color: #555555">=</span>n_jobs,
                              random_state<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)

forest<span style="color: #555555">.</span>fit(X, y)
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;done in </span><span style="color: #AA0000">%0.3f</span><span style="color: #CC3300">s&quot;</span> <span style="color: #555555">%</span> (time() <span style="color: #555555">-</span> t0))
importances <span style="color: #555555">=</span> forest<span style="color: #555555">.</span>feature_importances_
importances <span style="color: #555555">=</span> importances<span style="color: #555555">.</span>reshape(data<span style="color: #555555">.</span>images[<span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>shape)
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>Fitting ExtraTreesClassifier on faces data with 1 cores...
done in 2.155s
</pre></div>


<h5 id="plot_2">Plot</h5>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Plot pixel importances</span>
plt<span style="color: #555555">.</span>matshow(importances, cmap<span style="color: #555555">=</span>plt<span style="color: #555555">.</span>cm<span style="color: #555555">.</span>hot)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Pixel importances with forests of trees&quot;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_61_0.png" /></p>
<hr />
<h3 id="5-feature-importances-with-forests-of-trees-source">5. Feature importances with forests of trees (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>This examples shows the use of forests of trees to evaluate the importance of
features on an artificial classification task. The red bars are the feature
importances of the forest, along with their inter-trees variability.</p>
<p>As expected, the plot suggests that 3 features are informative, while the
remaining are not.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.datasets</span> <span style="color: #006699; font-weight: bold">import</span> make_classification
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> ExtraTreesClassifier
</pre></div>


<ul>
<li>Build a classification task</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Build a classification task using 3 informative features</span>
X, y <span style="color: #555555">=</span> make_classification(n_samples<span style="color: #555555">=</span><span style="color: #FF6600">1000</span>,
                           n_features<span style="color: #555555">=</span><span style="color: #FF6600">10</span>,
                           n_informative<span style="color: #555555">=</span><span style="color: #FF6600">3</span>,
                           n_redundant<span style="color: #555555">=</span><span style="color: #FF6600">0</span>,
                           n_repeated<span style="color: #555555">=</span><span style="color: #FF6600">0</span>,
                           n_classes<span style="color: #555555">=</span><span style="color: #FF6600">2</span>,
                           random_state<span style="color: #555555">=</span><span style="color: #FF6600">0</span>,
                           shuffle<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">False</span>)
</pre></div>


<ul>
<li>Data</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Build a forest and compute the feature importances</span>
forest <span style="color: #555555">=</span> ExtraTreesClassifier(n_estimators<span style="color: #555555">=</span><span style="color: #FF6600">250</span>,
                              random_state<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)

forest<span style="color: #555555">.</span>fit(X, y)
importances <span style="color: #555555">=</span> forest<span style="color: #555555">.</span>feature_importances_
std <span style="color: #555555">=</span> np<span style="color: #555555">.</span>std([tree<span style="color: #555555">.</span>feature_importances_ <span style="color: #006699; font-weight: bold">for</span> tree <span style="color: #000000; font-weight: bold">in</span> forest<span style="color: #555555">.</span>estimators_],
             axis<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)
indices <span style="color: #555555">=</span> np<span style="color: #555555">.</span>argsort(importances)[::<span style="color: #555555">-</span><span style="color: #FF6600">1</span>]
</pre></div>


<ul>
<li>Feature Ranking</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Print the feature ranking</span>
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Feature ranking:&quot;</span>)

<span style="color: #006699; font-weight: bold">for</span> f <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">range</span>(X<span style="color: #555555">.</span>shape[<span style="color: #FF6600">1</span>]):
    <span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;</span><span style="color: #AA0000">%d</span><span style="color: #CC3300">. feature </span><span style="color: #AA0000">%d</span><span style="color: #CC3300"> (</span><span style="color: #AA0000">%f</span><span style="color: #CC3300">)&quot;</span> <span style="color: #555555">%</span> (f <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>, indices[f], importances[indices[f]]))
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>Feature ranking:
1. feature 0 (0.250402)
2. feature 1 (0.231094)
3. feature 2 (0.148057)
4. feature 3 (0.055632)
5. feature 5 (0.054583)
6. feature 8 (0.054573)
7. feature 6 (0.052606)
8. feature 7 (0.051109)
9. feature 9 (0.051010)
10. feature 4 (0.050934)
</pre></div>


<ul>
<li>Plot the feature importances of the forest</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Plot the feature importances of the forest</span>
plt<span style="color: #555555">.</span>figure()
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Feature importances&quot;</span>)
plt<span style="color: #555555">.</span>bar(<span style="color: #336666">range</span>(X<span style="color: #555555">.</span>shape[<span style="color: #FF6600">1</span>]), importances[indices],
       color<span style="color: #555555">=</span><span style="color: #CC3300">&quot;r&quot;</span>, yerr<span style="color: #555555">=</span>std[indices], align<span style="color: #555555">=</span><span style="color: #CC3300">&quot;center&quot;</span>)
plt<span style="color: #555555">.</span>xticks(<span style="color: #336666">range</span>(X<span style="color: #555555">.</span>shape[<span style="color: #FF6600">1</span>]), indices)
plt<span style="color: #555555">.</span>xlim([<span style="color: #555555">-</span><span style="color: #FF6600">1</span>, X<span style="color: #555555">.</span>shape[<span style="color: #FF6600">1</span>]])
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_74_0.png" /></p>
<hr />
<h3 id="6-isolationforest-example-source">6. IsolationForest example (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>An example using IsolationForest for anomaly detection.</p>
<p>The IsolationForest 'isolates' observations by randomly selecting a feature
and then randomly selecting a split value between the maximum and minimum
values of the selected feature.</p>
<p>Since recursive partitioning can be represented by a tree structure, the
number of splittings required to isolate a sample is equivalent to the path
length from the root node to the terminating node.</p>
<p>This path length, averaged over a forest of such random trees, is a measure
of abnormality and our decision function.</p>
<p>Random partitioning produces noticeable shorter paths for anomalies.
Hence, when a forest of random trees collectively produce shorter path lengths
for particular samples, they are highly likely to be anomalies.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> IsolationForest
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>rng <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>RandomState(<span style="color: #FF6600">42</span>)

<span style="color: #0099FF; font-style: italic"># Generate train data</span>
X <span style="color: #555555">=</span> <span style="color: #FF6600">0.3</span> <span style="color: #555555">*</span> rng<span style="color: #555555">.</span>randn(<span style="color: #FF6600">100</span>, <span style="color: #FF6600">2</span>)
X_train <span style="color: #555555">=</span> np<span style="color: #555555">.</span>r_[X <span style="color: #555555">+</span> <span style="color: #FF6600">2</span>, X <span style="color: #555555">-</span> <span style="color: #FF6600">2</span>]
<span style="color: #0099FF; font-style: italic"># Generate some regular novel observations</span>
X <span style="color: #555555">=</span> <span style="color: #FF6600">0.3</span> <span style="color: #555555">*</span> rng<span style="color: #555555">.</span>randn(<span style="color: #FF6600">20</span>, <span style="color: #FF6600">2</span>)
X_test <span style="color: #555555">=</span> np<span style="color: #555555">.</span>r_[X <span style="color: #555555">+</span> <span style="color: #FF6600">2</span>, X <span style="color: #555555">-</span> <span style="color: #FF6600">2</span>]
<span style="color: #0099FF; font-style: italic"># Generate some abnormal novel observations</span>
X_outliers <span style="color: #555555">=</span> rng<span style="color: #555555">.</span>uniform(low<span style="color: #555555">=-</span><span style="color: #FF6600">4</span>, high<span style="color: #555555">=</span><span style="color: #FF6600">4</span>, size<span style="color: #555555">=</span>(<span style="color: #FF6600">20</span>, <span style="color: #FF6600">2</span>))
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># fit the model</span>
clf <span style="color: #555555">=</span> IsolationForest(max_samples<span style="color: #555555">=</span><span style="color: #FF6600">100</span>, random_state<span style="color: #555555">=</span>rng)
clf<span style="color: #555555">.</span>fit(X_train)
y_pred_train <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>predict(X_train)
y_pred_test <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>predict(X_test)
y_pred_outliers <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>predict(X_outliers)

<span style="color: #0099FF; font-style: italic"># plot the line, the samples, and the nearest vectors to the plane</span>
xx, yy <span style="color: #555555">=</span> np<span style="color: #555555">.</span>meshgrid(np<span style="color: #555555">.</span>linspace(<span style="color: #555555">-</span><span style="color: #FF6600">5</span>, <span style="color: #FF6600">5</span>, <span style="color: #FF6600">50</span>), np<span style="color: #555555">.</span>linspace(<span style="color: #555555">-</span><span style="color: #FF6600">5</span>, <span style="color: #FF6600">5</span>, <span style="color: #FF6600">50</span>))
Z <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>decision_function(np<span style="color: #555555">.</span>c_[xx<span style="color: #555555">.</span>ravel(), yy<span style="color: #555555">.</span>ravel()])
Z <span style="color: #555555">=</span> Z<span style="color: #555555">.</span>reshape(xx<span style="color: #555555">.</span>shape)

plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;IsolationForest&quot;</span>)
plt<span style="color: #555555">.</span>contourf(xx, yy, Z, cmap<span style="color: #555555">=</span>plt<span style="color: #555555">.</span>cm<span style="color: #555555">.</span>Blues_r)

b1 <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>scatter(X_train[:, <span style="color: #FF6600">0</span>], X_train[:, <span style="color: #FF6600">1</span>], c<span style="color: #555555">=</span><span style="color: #CC3300">&#39;white&#39;</span>)
b2 <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>scatter(X_test[:, <span style="color: #FF6600">0</span>], X_test[:, <span style="color: #FF6600">1</span>], c<span style="color: #555555">=</span><span style="color: #CC3300">&#39;green&#39;</span>)
c <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>scatter(X_outliers[:, <span style="color: #FF6600">0</span>], X_outliers[:, <span style="color: #FF6600">1</span>], c<span style="color: #555555">=</span><span style="color: #CC3300">&#39;red&#39;</span>)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&#39;tight&#39;</span>)
plt<span style="color: #555555">.</span>xlim((<span style="color: #555555">-</span><span style="color: #FF6600">5</span>, <span style="color: #FF6600">5</span>))
plt<span style="color: #555555">.</span>ylim((<span style="color: #555555">-</span><span style="color: #FF6600">5</span>, <span style="color: #FF6600">5</span>))
plt<span style="color: #555555">.</span>legend([b1, b2, c],
           [<span style="color: #CC3300">&quot;training observations&quot;</span>,
            <span style="color: #CC3300">&quot;new regular observations&quot;</span>, <span style="color: #CC3300">&quot;new abnormal observations&quot;</span>],
           loc<span style="color: #555555">=</span><span style="color: #CC3300">&quot;upper left&quot;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_81_0.png" /></p>
<hr />
<h3 id="7-hashing-feature-transformation-using-totally-random-trees-source">7.  Hashing feature transformation using Totally Random Trees (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>RandomTreesEmbedding provides a way to map data to a
very high-dimensional, sparse representation, which might
be beneficial for classification.
The mapping is completely unsupervised and very efficient.</p>
<p>This example visualizes the partitions given by several
trees and shows how the transformation can also be used for
non-linear dimensionality reduction or non-linear classification.</p>
<p>Points that are neighboring often share the same leaf of a tree and therefore
share large parts of their hashed representation. This allows to
separate two concentric circles simply based on the principal components of the
transformed data.</p>
<p>In high-dimensional spaces, linear classifiers often achieve
excellent accuracy. For sparse binary data, BernoulliNB
is particularly well-suited. The bottom row compares the
decision boundary obtained by BernoulliNB in the transformed
space with an ExtraTreesClassifier forests learned on the
original data.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.datasets</span> <span style="color: #006699; font-weight: bold">import</span> make_circles
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> RandomTreesEmbedding, ExtraTreesClassifier
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.decomposition</span> <span style="color: #006699; font-weight: bold">import</span> TruncatedSVD
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.naive_bayes</span> <span style="color: #006699; font-weight: bold">import</span> BernoulliNB
</pre></div>


<ul>
<li>Data</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># make a synthetic dataset</span>
X, y <span style="color: #555555">=</span> make_circles(factor<span style="color: #555555">=</span><span style="color: #FF6600">0.5</span>, random_state<span style="color: #555555">=</span><span style="color: #FF6600">0</span>, noise<span style="color: #555555">=</span><span style="color: #FF6600">0.05</span>)

<span style="color: #0099FF; font-style: italic"># use RandomTreesEmbedding to transform data</span>
hasher <span style="color: #555555">=</span> RandomTreesEmbedding(n_estimators<span style="color: #555555">=</span><span style="color: #FF6600">10</span>, random_state<span style="color: #555555">=</span><span style="color: #FF6600">0</span>, max_depth<span style="color: #555555">=</span><span style="color: #FF6600">3</span>)
X_transformed <span style="color: #555555">=</span> hasher<span style="color: #555555">.</span>fit_transform(X)
</pre></div>


<ul>
<li>PCA</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Visualize result using PCA</span>
pca <span style="color: #555555">=</span> TruncatedSVD(n_components<span style="color: #555555">=</span><span style="color: #FF6600">2</span>)
X_reduced <span style="color: #555555">=</span> pca<span style="color: #555555">.</span>fit_transform(X_transformed)
</pre></div>


<ul>
<li>Naive Bayes classifier</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Learn a Naive Bayes classifier on the transformed data</span>
nb <span style="color: #555555">=</span> BernoulliNB()
nb<span style="color: #555555">.</span>fit(X_transformed, y)
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)
</pre></div>


<ul>
<li>ExtraTreesClassifier</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Learn an ExtraTreesClassifier for comparison</span>
trees <span style="color: #555555">=</span> ExtraTreesClassifier(max_depth<span style="color: #555555">=</span><span style="color: #FF6600">3</span>, n_estimators<span style="color: #555555">=</span><span style="color: #FF6600">10</span>, random_state<span style="color: #555555">=</span><span style="color: #FF6600">0</span>)
trees<span style="color: #555555">.</span>fit(X, y)
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion=&#39;gini&#39;,
           max_depth=3, max_features=&#39;auto&#39;, max_leaf_nodes=None,
           min_samples_leaf=1, min_samples_split=2,
           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
           oob_score=False, random_state=0, verbose=0, warm_start=False)
</pre></div>


<ul>
<li>Plot</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># scatter plot of original and reduced data</span>
fig <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">9</span>, <span style="color: #FF6600">8</span>))

ax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">221</span>)
ax<span style="color: #555555">.</span>scatter(X[:, <span style="color: #FF6600">0</span>], X[:, <span style="color: #FF6600">1</span>], c<span style="color: #555555">=</span>y, s<span style="color: #555555">=</span><span style="color: #FF6600">50</span>)
ax<span style="color: #555555">.</span>set_title(<span style="color: #CC3300">&quot;Original Data (2d)&quot;</span>)
ax<span style="color: #555555">.</span>set_xticks(())
ax<span style="color: #555555">.</span>set_yticks(())

ax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">222</span>)
ax<span style="color: #555555">.</span>scatter(X_reduced[:, <span style="color: #FF6600">0</span>], X_reduced[:, <span style="color: #FF6600">1</span>], c<span style="color: #555555">=</span>y, s<span style="color: #555555">=</span><span style="color: #FF6600">50</span>)
ax<span style="color: #555555">.</span>set_title(<span style="color: #CC3300">&quot;PCA reduction (2d) of transformed data (</span><span style="color: #AA0000">%d</span><span style="color: #CC3300">d)&quot;</span> <span style="color: #555555">%</span>
             X_transformed<span style="color: #555555">.</span>shape[<span style="color: #FF6600">1</span>])
ax<span style="color: #555555">.</span>set_xticks(())
ax<span style="color: #555555">.</span>set_yticks(())

<span style="color: #0099FF; font-style: italic"># Plot the decision in original space. For that, we will assign a color to each</span>
<span style="color: #0099FF; font-style: italic"># point in the mesh [x_min, m_max] x [y_min, y_max].</span>
h <span style="color: #555555">=</span> <span style="color: #555555">.</span><span style="color: #FF6600">01</span>
x_min, x_max <span style="color: #555555">=</span> X[:, <span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>min() <span style="color: #555555">-</span> <span style="color: #555555">.</span><span style="color: #FF6600">5</span>, X[:, <span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>max() <span style="color: #555555">+</span> <span style="color: #555555">.</span><span style="color: #FF6600">5</span>
y_min, y_max <span style="color: #555555">=</span> X[:, <span style="color: #FF6600">1</span>]<span style="color: #555555">.</span>min() <span style="color: #555555">-</span> <span style="color: #555555">.</span><span style="color: #FF6600">5</span>, X[:, <span style="color: #FF6600">1</span>]<span style="color: #555555">.</span>max() <span style="color: #555555">+</span> <span style="color: #555555">.</span><span style="color: #FF6600">5</span>
xx, yy <span style="color: #555555">=</span> np<span style="color: #555555">.</span>meshgrid(np<span style="color: #555555">.</span>arange(x_min, x_max, h), np<span style="color: #555555">.</span>arange(y_min, y_max, h))

<span style="color: #0099FF; font-style: italic"># transform grid using RandomTreesEmbedding</span>
transformed_grid <span style="color: #555555">=</span> hasher<span style="color: #555555">.</span>transform(np<span style="color: #555555">.</span>c_[xx<span style="color: #555555">.</span>ravel(), yy<span style="color: #555555">.</span>ravel()])
y_grid_pred <span style="color: #555555">=</span> nb<span style="color: #555555">.</span>predict_proba(transformed_grid)[:, <span style="color: #FF6600">1</span>]

ax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">223</span>)
ax<span style="color: #555555">.</span>set_title(<span style="color: #CC3300">&quot;Naive Bayes on Transformed data&quot;</span>)
ax<span style="color: #555555">.</span>pcolormesh(xx, yy, y_grid_pred<span style="color: #555555">.</span>reshape(xx<span style="color: #555555">.</span>shape))
ax<span style="color: #555555">.</span>scatter(X[:, <span style="color: #FF6600">0</span>], X[:, <span style="color: #FF6600">1</span>], c<span style="color: #555555">=</span>y, s<span style="color: #555555">=</span><span style="color: #FF6600">50</span>)
ax<span style="color: #555555">.</span>set_ylim(<span style="color: #555555">-</span><span style="color: #FF6600">1.4</span>, <span style="color: #FF6600">1.4</span>)
ax<span style="color: #555555">.</span>set_xlim(<span style="color: #555555">-</span><span style="color: #FF6600">1.4</span>, <span style="color: #FF6600">1.4</span>)
ax<span style="color: #555555">.</span>set_xticks(())
ax<span style="color: #555555">.</span>set_yticks(())

<span style="color: #0099FF; font-style: italic"># transform grid using ExtraTreesClassifier</span>
y_grid_pred <span style="color: #555555">=</span> trees<span style="color: #555555">.</span>predict_proba(np<span style="color: #555555">.</span>c_[xx<span style="color: #555555">.</span>ravel(), yy<span style="color: #555555">.</span>ravel()])[:, <span style="color: #FF6600">1</span>]

ax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">224</span>)
ax<span style="color: #555555">.</span>set_title(<span style="color: #CC3300">&quot;ExtraTrees predictions&quot;</span>)
ax<span style="color: #555555">.</span>pcolormesh(xx, yy, y_grid_pred<span style="color: #555555">.</span>reshape(xx<span style="color: #555555">.</span>shape))
ax<span style="color: #555555">.</span>scatter(X[:, <span style="color: #FF6600">0</span>], X[:, <span style="color: #FF6600">1</span>], c<span style="color: #555555">=</span>y, s<span style="color: #555555">=</span><span style="color: #FF6600">50</span>)
ax<span style="color: #555555">.</span>set_ylim(<span style="color: #555555">-</span><span style="color: #FF6600">1.4</span>, <span style="color: #FF6600">1.4</span>)
ax<span style="color: #555555">.</span>set_xlim(<span style="color: #555555">-</span><span style="color: #FF6600">1.4</span>, <span style="color: #FF6600">1.4</span>)
ax<span style="color: #555555">.</span>set_xticks(())
ax<span style="color: #555555">.</span>set_yticks(())

plt<span style="color: #555555">.</span>tight_layout()
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_96_0.png" /></p>
<hr />
<h3 id="8-two-class-adaboost-source">8. Two-class AdaBoost (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>This example fits an AdaBoosted decision stump on a non-linearly separable
classification dataset composed of two "Gaussian quantiles" clusters
(see :func:<code>sklearn.datasets.make_gaussian_quantiles</code>) and plots the decision
boundary and decision scores. The distributions of decision scores are shown
separately for samples of class A and B. The predicted class label for each
sample is determined by the sign of the decision score. Samples with decision
scores greater than zero are classified as B, and are otherwise classified
as A. The magnitude of a decision score determines the degree of likeness with
the predicted class label. Additionally, a new dataset could be constructed
containing a desired purity of class B, for example, by only selecting samples
with a decision score above some value.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> AdaBoostClassifier
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.tree</span> <span style="color: #006699; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.datasets</span> <span style="color: #006699; font-weight: bold">import</span> make_gaussian_quantiles
</pre></div>


<ul>
<li>Data</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Construct dataset</span>
X1, y1 <span style="color: #555555">=</span> make_gaussian_quantiles(cov<span style="color: #555555">=</span><span style="color: #FF6600">2.</span>,
                                 n_samples<span style="color: #555555">=</span><span style="color: #FF6600">200</span>, n_features<span style="color: #555555">=</span><span style="color: #FF6600">2</span>,
                                 n_classes<span style="color: #555555">=</span><span style="color: #FF6600">2</span>, random_state<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)
X2, y2 <span style="color: #555555">=</span> make_gaussian_quantiles(mean<span style="color: #555555">=</span>(<span style="color: #FF6600">3</span>, <span style="color: #FF6600">3</span>), cov<span style="color: #555555">=</span><span style="color: #FF6600">1.5</span>,
                                 n_samples<span style="color: #555555">=</span><span style="color: #FF6600">300</span>, n_features<span style="color: #555555">=</span><span style="color: #FF6600">2</span>,
                                 n_classes<span style="color: #555555">=</span><span style="color: #FF6600">2</span>, random_state<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)
X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>concatenate((X1, X2))
y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>concatenate((y1, <span style="color: #555555">-</span> y2 <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>))
</pre></div>


<ul>
<li>Model</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Create and fit an AdaBoosted decision tree</span>
bdt <span style="color: #555555">=</span> AdaBoostClassifier(DecisionTreeClassifier(max_depth<span style="color: #555555">=</span><span style="color: #FF6600">1</span>),
                         algorithm<span style="color: #555555">=</span><span style="color: #CC3300">&quot;SAMME&quot;</span>,
                         n_estimators<span style="color: #555555">=</span><span style="color: #FF6600">200</span>)

bdt<span style="color: #555555">.</span>fit(X, y)
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>AdaBoostClassifier(algorithm=&#39;SAMME&#39;,
          base_estimator=DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=1,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter=&#39;best&#39;),
          learning_rate=1.0, n_estimators=200, random_state=None)
</pre></div>


<ul>
<li>Plot</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>plot_colors <span style="color: #555555">=</span> <span style="color: #CC3300">&quot;br&quot;</span>
plot_step <span style="color: #555555">=</span> <span style="color: #FF6600">0.02</span>
class_names <span style="color: #555555">=</span> <span style="color: #CC3300">&quot;AB&quot;</span>

plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">10</span>, <span style="color: #FF6600">5</span>))

<span style="color: #0099FF; font-style: italic"># Plot the decision boundaries</span>
plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">121</span>)
x_min, x_max <span style="color: #555555">=</span> X[:, <span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>min() <span style="color: #555555">-</span> <span style="color: #FF6600">1</span>, X[:, <span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>max() <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>
y_min, y_max <span style="color: #555555">=</span> X[:, <span style="color: #FF6600">1</span>]<span style="color: #555555">.</span>min() <span style="color: #555555">-</span> <span style="color: #FF6600">1</span>, X[:, <span style="color: #FF6600">1</span>]<span style="color: #555555">.</span>max() <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>
xx, yy <span style="color: #555555">=</span> np<span style="color: #555555">.</span>meshgrid(np<span style="color: #555555">.</span>arange(x_min, x_max, plot_step),
                     np<span style="color: #555555">.</span>arange(y_min, y_max, plot_step))

Z <span style="color: #555555">=</span> bdt<span style="color: #555555">.</span>predict(np<span style="color: #555555">.</span>c_[xx<span style="color: #555555">.</span>ravel(), yy<span style="color: #555555">.</span>ravel()])
Z <span style="color: #555555">=</span> Z<span style="color: #555555">.</span>reshape(xx<span style="color: #555555">.</span>shape)
cs <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>contourf(xx, yy, Z, cmap<span style="color: #555555">=</span>plt<span style="color: #555555">.</span>cm<span style="color: #555555">.</span>Paired)
plt<span style="color: #555555">.</span>axis(<span style="color: #CC3300">&quot;tight&quot;</span>)

<span style="color: #0099FF; font-style: italic"># Plot the training points</span>
<span style="color: #006699; font-weight: bold">for</span> i, n, c <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">zip</span>(<span style="color: #336666">range</span>(<span style="color: #FF6600">2</span>), class_names, plot_colors):
    idx <span style="color: #555555">=</span> np<span style="color: #555555">.</span>where(y <span style="color: #555555">==</span> i)
    plt<span style="color: #555555">.</span>scatter(X[idx, <span style="color: #FF6600">0</span>], X[idx, <span style="color: #FF6600">1</span>],
                c<span style="color: #555555">=</span>c, cmap<span style="color: #555555">=</span>plt<span style="color: #555555">.</span>cm<span style="color: #555555">.</span>Paired,
                label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;Class </span><span style="color: #AA0000">%s</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> n)
plt<span style="color: #555555">.</span>xlim(x_min, x_max)
plt<span style="color: #555555">.</span>ylim(y_min, y_max)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;upper right&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;x&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;y&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Decision Boundary&#39;</span>)

<span style="color: #0099FF; font-style: italic"># Plot the two-class decision scores</span>
twoclass_output <span style="color: #555555">=</span> bdt<span style="color: #555555">.</span>decision_function(X)
plot_range <span style="color: #555555">=</span> (twoclass_output<span style="color: #555555">.</span>min(), twoclass_output<span style="color: #555555">.</span>max())
plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">122</span>)
<span style="color: #006699; font-weight: bold">for</span> i, n, c <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">zip</span>(<span style="color: #336666">range</span>(<span style="color: #FF6600">2</span>), class_names, plot_colors):
    plt<span style="color: #555555">.</span>hist(twoclass_output[y <span style="color: #555555">==</span> i],
             bins<span style="color: #555555">=</span><span style="color: #FF6600">10</span>,
             <span style="color: #336666">range</span><span style="color: #555555">=</span>plot_range,
             facecolor<span style="color: #555555">=</span>c,
             label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Class </span><span style="color: #AA0000">%s</span><span style="color: #CC3300">&#39;</span> <span style="color: #555555">%</span> n,
             alpha<span style="color: #555555">=.</span><span style="color: #FF6600">5</span>)
x1, x2, y1, y2 <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>axis()
plt<span style="color: #555555">.</span>axis((x1, x2, y1, y2 <span style="color: #555555">*</span> <span style="color: #FF6600">1.2</span>))
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;upper right&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Samples&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;Score&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Decision Scores&#39;</span>)

plt<span style="color: #555555">.</span>tight_layout()
plt<span style="color: #555555">.</span>subplots_adjust(wspace<span style="color: #555555">=</span><span style="color: #FF6600">0.35</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_107_0.png" /></p>
<hr />
<h3 id="9-multi-class-adaboosted-decision-trees-source">9. Multi-class AdaBoosted Decision Trees (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>This example reproduces Figure 1 of Zhu et al [1] and shows how boosting can
improve prediction accuracy on a multi-class problem. The classification
dataset is constructed by taking a ten-dimensional standard normal distribution
and defining three classes separated by nested concentric ten-dimensional
spheres such that roughly equal numbers of samples are in each class (quantiles
of the :math:<code>\chi^2</code> distribution).</p>
<p>The performance of the SAMME and SAMME.R [1] algorithms are compared. SAMME.R
uses the probability estimates to update the additive model, while SAMME  uses
the classifications only. As the example illustrates, the SAMME.R algorithm
typically converges faster than SAMME, achieving a lower test error with fewer
boosting iterations. The error of each algorithm on the test set after each
boosting iteration is shown on the left, the classification error on the test
set of each tree is shown in the middle, and the boost weight of each tree is
shown on the right. All trees have a weight of one in the SAMME.R algorithm and
therefore are not shown.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.externals.six.moves</span> <span style="color: #006699; font-weight: bold">import</span> <span style="color: #336666">zip</span>

<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.datasets</span> <span style="color: #006699; font-weight: bold">import</span> make_gaussian_quantiles
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> AdaBoostClassifier
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.metrics</span> <span style="color: #006699; font-weight: bold">import</span> accuracy_score
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.tree</span> <span style="color: #006699; font-weight: bold">import</span> DecisionTreeClassifier
</pre></div>


<ul>
<li>Data</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>X, y <span style="color: #555555">=</span> make_gaussian_quantiles(n_samples<span style="color: #555555">=</span><span style="color: #FF6600">13000</span>, n_features<span style="color: #555555">=</span><span style="color: #FF6600">10</span>,
                               n_classes<span style="color: #555555">=</span><span style="color: #FF6600">3</span>, random_state<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)

n_split <span style="color: #555555">=</span> <span style="color: #FF6600">3000</span>

X_train, X_test <span style="color: #555555">=</span> X[:n_split], X[n_split:]
y_train, y_test <span style="color: #555555">=</span> y[:n_split], y[n_split:]
</pre></div>


<ul>
<li>Model</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>bdt_real <span style="color: #555555">=</span> AdaBoostClassifier(
    DecisionTreeClassifier(max_depth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>),
    n_estimators<span style="color: #555555">=</span><span style="color: #FF6600">600</span>,
    learning_rate<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)

bdt_discrete <span style="color: #555555">=</span> AdaBoostClassifier(
    DecisionTreeClassifier(max_depth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>),
    n_estimators<span style="color: #555555">=</span><span style="color: #FF6600">600</span>,
    learning_rate<span style="color: #555555">=</span><span style="color: #FF6600">1.5</span>,
    algorithm<span style="color: #555555">=</span><span style="color: #CC3300">&quot;SAMME&quot;</span>)

bdt_real<span style="color: #555555">.</span>fit(X_train, y_train)
bdt_discrete<span style="color: #555555">.</span>fit(X_train, y_train)

real_test_errors <span style="color: #555555">=</span> []
discrete_test_errors <span style="color: #555555">=</span> []

<span style="color: #006699; font-weight: bold">for</span> real_test_predict, discrete_train_predict <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">zip</span>(
        bdt_real<span style="color: #555555">.</span>staged_predict(X_test), bdt_discrete<span style="color: #555555">.</span>staged_predict(X_test)):
    real_test_errors<span style="color: #555555">.</span>append(
        <span style="color: #FF6600">1.</span> <span style="color: #555555">-</span> accuracy_score(real_test_predict, y_test))
    discrete_test_errors<span style="color: #555555">.</span>append(
        <span style="color: #FF6600">1.</span> <span style="color: #555555">-</span> accuracy_score(discrete_train_predict, y_test))

n_trees_discrete <span style="color: #555555">=</span> <span style="color: #336666">len</span>(bdt_discrete)
n_trees_real <span style="color: #555555">=</span> <span style="color: #336666">len</span>(bdt_real)

<span style="color: #0099FF; font-style: italic"># Boosting might terminate early, but the following arrays are always</span>
<span style="color: #0099FF; font-style: italic"># n_estimators long. We crop them to the actual number of trees here:</span>
discrete_estimator_errors <span style="color: #555555">=</span> bdt_discrete<span style="color: #555555">.</span>estimator_errors_[:n_trees_discrete]
real_estimator_errors <span style="color: #555555">=</span> bdt_real<span style="color: #555555">.</span>estimator_errors_[:n_trees_real]
discrete_estimator_weights <span style="color: #555555">=</span> bdt_discrete<span style="color: #555555">.</span>estimator_weights_[:n_trees_discrete]
</pre></div>


<ul>
<li>Plot</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">15</span>, <span style="color: #FF6600">5</span>))

plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">131</span>)
plt<span style="color: #555555">.</span>plot(<span style="color: #336666">range</span>(<span style="color: #FF6600">1</span>, n_trees_discrete <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>),
         discrete_test_errors, c<span style="color: #555555">=</span><span style="color: #CC3300">&#39;black&#39;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;SAMME&#39;</span>)
plt<span style="color: #555555">.</span>plot(<span style="color: #336666">range</span>(<span style="color: #FF6600">1</span>, n_trees_real <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>),
         real_test_errors, c<span style="color: #555555">=</span><span style="color: #CC3300">&#39;black&#39;</span>,
         linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;dashed&#39;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;SAMME.R&#39;</span>)
plt<span style="color: #555555">.</span>legend()
plt<span style="color: #555555">.</span>ylim(<span style="color: #FF6600">0.18</span>, <span style="color: #FF6600">0.62</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Test Error&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;Number of Trees&#39;</span>)

plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">132</span>)
plt<span style="color: #555555">.</span>plot(<span style="color: #336666">range</span>(<span style="color: #FF6600">1</span>, n_trees_discrete <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>), discrete_estimator_errors,
         <span style="color: #CC3300">&quot;b&quot;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;SAMME&#39;</span>, alpha<span style="color: #555555">=.</span><span style="color: #FF6600">5</span>)
plt<span style="color: #555555">.</span>plot(<span style="color: #336666">range</span>(<span style="color: #FF6600">1</span>, n_trees_real <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>), real_estimator_errors,
         <span style="color: #CC3300">&quot;r&quot;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;SAMME.R&#39;</span>, alpha<span style="color: #555555">=.</span><span style="color: #FF6600">5</span>)
plt<span style="color: #555555">.</span>legend()
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Error&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;Number of Trees&#39;</span>)
plt<span style="color: #555555">.</span>ylim((<span style="color: #555555">.</span><span style="color: #FF6600">2</span>,
         <span style="color: #336666">max</span>(real_estimator_errors<span style="color: #555555">.</span>max(),
             discrete_estimator_errors<span style="color: #555555">.</span>max()) <span style="color: #555555">*</span> <span style="color: #FF6600">1.2</span>))
plt<span style="color: #555555">.</span>xlim((<span style="color: #555555">-</span><span style="color: #FF6600">20</span>, <span style="color: #336666">len</span>(bdt_discrete) <span style="color: #555555">+</span> <span style="color: #FF6600">20</span>))

plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">133</span>)
plt<span style="color: #555555">.</span>plot(<span style="color: #336666">range</span>(<span style="color: #FF6600">1</span>, n_trees_discrete <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>), discrete_estimator_weights,
         <span style="color: #CC3300">&quot;b&quot;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;SAMME&#39;</span>)
plt<span style="color: #555555">.</span>legend()
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Weight&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;Number of Trees&#39;</span>)
plt<span style="color: #555555">.</span>ylim((<span style="color: #FF6600">0</span>, discrete_estimator_weights<span style="color: #555555">.</span>max() <span style="color: #555555">*</span> <span style="color: #FF6600">1.2</span>))
plt<span style="color: #555555">.</span>xlim((<span style="color: #555555">-</span><span style="color: #FF6600">20</span>, n_trees_discrete <span style="color: #555555">+</span> <span style="color: #FF6600">20</span>))

<span style="color: #0099FF; font-style: italic"># prevent overlapping y-axis labels</span>
plt<span style="color: #555555">.</span>subplots_adjust(wspace<span style="color: #555555">=</span><span style="color: #FF6600">0.25</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_118_0.png" /></p>
<hr />
<h3 id="10-discrete-versus-real-adaboost-source">10 .Discrete versus Real AdaBoost (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>This example is based on Figure 10.2 from Hastie et al 2009 [1] and illustrates
the difference in performance between the discrete SAMME [2] boosting
algorithm and real SAMME.R boosting algorithm. Both algorithms are evaluated
on a binary classification task where the target Y is a non-linear function
of 10 input features.</p>
<p>Discrete SAMME AdaBoost adapts based on errors in predicted class labels
whereas real SAMME.R uses the predicted class probabilities.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> datasets
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.tree</span> <span style="color: #006699; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.metrics</span> <span style="color: #006699; font-weight: bold">import</span> zero_one_loss
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> AdaBoostClassifier
</pre></div>


<ul>
<li>Data</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>n_estimators <span style="color: #555555">=</span> <span style="color: #FF6600">400</span>
<span style="color: #0099FF; font-style: italic"># A learning rate of 1. may not be optimal for both SAMME and SAMME.R</span>
learning_rate <span style="color: #555555">=</span> <span style="color: #FF6600">1.</span>

X, y <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>make_hastie_10_2(n_samples<span style="color: #555555">=</span><span style="color: #FF6600">12000</span>, random_state<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)

X_test, y_test <span style="color: #555555">=</span> X[<span style="color: #FF6600">2000</span>:], y[<span style="color: #FF6600">2000</span>:]
X_train, y_train <span style="color: #555555">=</span> X[:<span style="color: #FF6600">2000</span>], y[:<span style="color: #FF6600">2000</span>]
</pre></div>


<ul>
<li>Model</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>dt_stump <span style="color: #555555">=</span> DecisionTreeClassifier(max_depth<span style="color: #555555">=</span><span style="color: #FF6600">1</span>, min_samples_leaf<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)
dt_stump<span style="color: #555555">.</span>fit(X_train, y_train)
dt_stump_err <span style="color: #555555">=</span> <span style="color: #FF6600">1.0</span> <span style="color: #555555">-</span> dt_stump<span style="color: #555555">.</span>score(X_test, y_test)

dt <span style="color: #555555">=</span> DecisionTreeClassifier(max_depth<span style="color: #555555">=</span><span style="color: #FF6600">9</span>, min_samples_leaf<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)
dt<span style="color: #555555">.</span>fit(X_train, y_train)
dt_err <span style="color: #555555">=</span> <span style="color: #FF6600">1.0</span> <span style="color: #555555">-</span> dt<span style="color: #555555">.</span>score(X_test, y_test)

ada_discrete <span style="color: #555555">=</span> AdaBoostClassifier(
    base_estimator<span style="color: #555555">=</span>dt_stump,
    learning_rate<span style="color: #555555">=</span>learning_rate,
    n_estimators<span style="color: #555555">=</span>n_estimators,
    algorithm<span style="color: #555555">=</span><span style="color: #CC3300">&quot;SAMME&quot;</span>)
ada_discrete<span style="color: #555555">.</span>fit(X_train, y_train)

ada_real <span style="color: #555555">=</span> AdaBoostClassifier(
    base_estimator<span style="color: #555555">=</span>dt_stump,
    learning_rate<span style="color: #555555">=</span>learning_rate,
    n_estimators<span style="color: #555555">=</span>n_estimators,
    algorithm<span style="color: #555555">=</span><span style="color: #CC3300">&quot;SAMME.R&quot;</span>)
ada_real<span style="color: #555555">.</span>fit(X_train, y_train)
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>AdaBoostClassifier(algorithm=&#39;SAMME.R&#39;,
          base_estimator=DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=1,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter=&#39;best&#39;),
          learning_rate=1.0, n_estimators=400, random_state=None)
</pre></div>


<ul>
<li>Plot</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>fig <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>figure()
ax <span style="color: #555555">=</span> fig<span style="color: #555555">.</span>add_subplot(<span style="color: #FF6600">111</span>)

ax<span style="color: #555555">.</span>plot([<span style="color: #FF6600">1</span>, n_estimators], [dt_stump_err] <span style="color: #555555">*</span> <span style="color: #FF6600">2</span>, <span style="color: #CC3300">&#39;k-&#39;</span>,
        label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Decision Stump Error&#39;</span>)
ax<span style="color: #555555">.</span>plot([<span style="color: #FF6600">1</span>, n_estimators], [dt_err] <span style="color: #555555">*</span> <span style="color: #FF6600">2</span>, <span style="color: #CC3300">&#39;k--&#39;</span>,
        label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Decision Tree Error&#39;</span>)

ada_discrete_err <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros((n_estimators,))
<span style="color: #006699; font-weight: bold">for</span> i, y_pred <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">enumerate</span>(ada_discrete<span style="color: #555555">.</span>staged_predict(X_test)):
    ada_discrete_err[i] <span style="color: #555555">=</span> zero_one_loss(y_pred, y_test)

ada_discrete_err_train <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros((n_estimators,))
<span style="color: #006699; font-weight: bold">for</span> i, y_pred <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">enumerate</span>(ada_discrete<span style="color: #555555">.</span>staged_predict(X_train)):
    ada_discrete_err_train[i] <span style="color: #555555">=</span> zero_one_loss(y_pred, y_train)

ada_real_err <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros((n_estimators,))
<span style="color: #006699; font-weight: bold">for</span> i, y_pred <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">enumerate</span>(ada_real<span style="color: #555555">.</span>staged_predict(X_test)):
    ada_real_err[i] <span style="color: #555555">=</span> zero_one_loss(y_pred, y_test)

ada_real_err_train <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros((n_estimators,))
<span style="color: #006699; font-weight: bold">for</span> i, y_pred <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">enumerate</span>(ada_real<span style="color: #555555">.</span>staged_predict(X_train)):
    ada_real_err_train[i] <span style="color: #555555">=</span> zero_one_loss(y_pred, y_train)

ax<span style="color: #555555">.</span>plot(np<span style="color: #555555">.</span>arange(n_estimators) <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>, ada_discrete_err,
        label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Discrete AdaBoost Test Error&#39;</span>,
        color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;red&#39;</span>)
ax<span style="color: #555555">.</span>plot(np<span style="color: #555555">.</span>arange(n_estimators) <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>, ada_discrete_err_train,
        label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Discrete AdaBoost Train Error&#39;</span>,
        color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;blue&#39;</span>)
ax<span style="color: #555555">.</span>plot(np<span style="color: #555555">.</span>arange(n_estimators) <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>, ada_real_err,
        label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Real AdaBoost Test Error&#39;</span>,
        color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;orange&#39;</span>)
ax<span style="color: #555555">.</span>plot(np<span style="color: #555555">.</span>arange(n_estimators) <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>, ada_real_err_train,
        label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Real AdaBoost Train Error&#39;</span>,
        color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;green&#39;</span>)

ax<span style="color: #555555">.</span>set_ylim((<span style="color: #FF6600">0.0</span>, <span style="color: #FF6600">0.5</span>))
ax<span style="color: #555555">.</span>set_xlabel(<span style="color: #CC3300">&#39;n_estimators&#39;</span>)
ax<span style="color: #555555">.</span>set_ylabel(<span style="color: #CC3300">&#39;error rate&#39;</span>)

leg <span style="color: #555555">=</span> ax<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;upper right&#39;</span>, fancybox<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>)
leg<span style="color: #555555">.</span>get_frame()<span style="color: #555555">.</span>set_alpha(<span style="color: #FF6600">0.7</span>)

plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_129_0.png" /></p>
<hr />
<h3 id="11-decision-tree-regression-with-adaboost-source">11. Decision Tree Regression with AdaBoost (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>A decision tree is boosted using the AdaBoost.R2 [1] algorithm on a 1D
sinusoidal dataset with a small amount of Gaussian noise.
299 boosts (300 decision trees) is compared with a single decision tree
regressor. As the number of boosts is increased the regressor can fit more
detail.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># importing necessary libraries</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.tree</span> <span style="color: #006699; font-weight: bold">import</span> DecisionTreeRegressor
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> AdaBoostRegressor
</pre></div>


<ul>
<li>Data</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Create the dataset</span>
rng <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>RandomState(<span style="color: #FF6600">1</span>)
X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>linspace(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">6</span>, <span style="color: #FF6600">100</span>)[:, np<span style="color: #555555">.</span>newaxis]
y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>sin(X)<span style="color: #555555">.</span>ravel() <span style="color: #555555">+</span> np<span style="color: #555555">.</span>sin(<span style="color: #FF6600">6</span> <span style="color: #555555">*</span> X)<span style="color: #555555">.</span>ravel() <span style="color: #555555">+</span> rng<span style="color: #555555">.</span>normal(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">0.1</span>, X<span style="color: #555555">.</span>shape[<span style="color: #FF6600">0</span>])
</pre></div>


<ul>
<li>Model</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Fit regression model</span>
regr_1 <span style="color: #555555">=</span> DecisionTreeRegressor(max_depth<span style="color: #555555">=</span><span style="color: #FF6600">4</span>)

regr_2 <span style="color: #555555">=</span> AdaBoostRegressor(DecisionTreeRegressor(max_depth<span style="color: #555555">=</span><span style="color: #FF6600">4</span>),
                          n_estimators<span style="color: #555555">=</span><span style="color: #FF6600">300</span>, random_state<span style="color: #555555">=</span>rng)

regr_1<span style="color: #555555">.</span>fit(X, y)
regr_2<span style="color: #555555">.</span>fit(X, y)
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>AdaBoostRegressor(base_estimator=DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=4, max_features=None,
           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,
           min_weight_fraction_leaf=0.0, presort=False, random_state=None,
           splitter=&#39;best&#39;),
         learning_rate=1.0, loss=&#39;linear&#39;, n_estimators=300,
         random_state=&lt;mtrand.RandomState object at 0x114302d90&gt;)
</pre></div>


<ul>
<li>Predict</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Predict</span>
y_1 <span style="color: #555555">=</span> regr_1<span style="color: #555555">.</span>predict(X)
y_2 <span style="color: #555555">=</span> regr_2<span style="color: #555555">.</span>predict(X)
</pre></div>


<ul>
<li>Plot</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Plot the results</span>
plt<span style="color: #555555">.</span>figure()
plt<span style="color: #555555">.</span>scatter(X, y, c<span style="color: #555555">=</span><span style="color: #CC3300">&quot;k&quot;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;training samples&quot;</span>)
plt<span style="color: #555555">.</span>plot(X, y_1, c<span style="color: #555555">=</span><span style="color: #CC3300">&quot;g&quot;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;n_estimators=1&quot;</span>, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>)
plt<span style="color: #555555">.</span>plot(X, y_2, c<span style="color: #555555">=</span><span style="color: #CC3300">&quot;r&quot;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&quot;n_estimators=300&quot;</span>, linewidth<span style="color: #555555">=</span><span style="color: #FF6600">2</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&quot;data&quot;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&quot;target&quot;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&quot;Boosted Decision Tree Regression&quot;</span>)
plt<span style="color: #555555">.</span>legend()
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_142_0.png" /></p>
<hr />
<h3 id="12-gradient-boosting-out-of-bag-estimates-source">12. Gradient Boosting Out-of-Bag estimates (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>Out-of-bag (OOB) estimates can be a useful heuristic to estimate
the "optimal" number of boosting iterations.
OOB estimates are almost identical to cross-validation estimates but
they can be computed on-the-fly without the need for repeated model
fitting.
OOB estimates are only available for Stochastic Gradient Boosting
(i.e. <code>subsample &lt; 1.0</code>), the estimates are derived from the improvement
in loss based on the examples not included in the bootstrap sample
(the so-called out-of-bag examples).
The OOB estimator is a pessimistic estimator of the true
test loss, but remains a fairly good approximation for a small number of trees.</p>
<p>The figure shows the cumulative sum of the negative OOB improvements
as a function of the boosting iteration. As you can see, it tracks the test
loss for the first hundred iterations but then diverges in a
pessimistic way.
The figure also shows the performance of 3-fold cross validation which
usually gives a better estimate of the test loss
but is computationally more demanding.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> ensemble
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.cross_validation</span> <span style="color: #006699; font-weight: bold">import</span> KFold
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.cross_validation</span> <span style="color: #006699; font-weight: bold">import</span> train_test_split
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>/home/bitnami/anaconda3/envs/caseolap/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  &quot;This module will be removed in 0.20.&quot;, DeprecationWarning)
</pre></div>


<ul>
<li>Data</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Generate data (adapted from G. Ridgeway&#39;s gbm example)</span>
n_samples <span style="color: #555555">=</span> <span style="color: #FF6600">1000</span>
random_state <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>RandomState(<span style="color: #FF6600">13</span>)
x1 <span style="color: #555555">=</span> random_state<span style="color: #555555">.</span>uniform(size<span style="color: #555555">=</span>n_samples)
x2 <span style="color: #555555">=</span> random_state<span style="color: #555555">.</span>uniform(size<span style="color: #555555">=</span>n_samples)
x3 <span style="color: #555555">=</span> random_state<span style="color: #555555">.</span>randint(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">4</span>, size<span style="color: #555555">=</span>n_samples)

p <span style="color: #555555">=</span> <span style="color: #FF6600">1</span> <span style="color: #555555">/</span> (<span style="color: #FF6600">1.0</span> <span style="color: #555555">+</span> np<span style="color: #555555">.</span>exp(<span style="color: #555555">-</span>(np<span style="color: #555555">.</span>sin(<span style="color: #FF6600">3</span> <span style="color: #555555">*</span> x1) <span style="color: #555555">-</span> <span style="color: #FF6600">4</span> <span style="color: #555555">*</span> x2 <span style="color: #555555">+</span> x3)))
y <span style="color: #555555">=</span> random_state<span style="color: #555555">.</span>binomial(<span style="color: #FF6600">1</span>, p, size<span style="color: #555555">=</span>n_samples)

X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>c_[x1, x2, x3]

X <span style="color: #555555">=</span> X<span style="color: #555555">.</span>astype(np<span style="color: #555555">.</span>float32)
X_train, X_test, y_train, y_test <span style="color: #555555">=</span> train_test_split(X, y, test_size<span style="color: #555555">=</span><span style="color: #FF6600">0.5</span>,
                                                    random_state<span style="color: #555555">=</span><span style="color: #FF6600">9</span>)
</pre></div>


<ul>
<li>Model</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Fit classifier with out-of-bag estimates</span>
params <span style="color: #555555">=</span> {<span style="color: #CC3300">&#39;n_estimators&#39;</span>: <span style="color: #FF6600">1200</span>, <span style="color: #CC3300">&#39;max_depth&#39;</span>: <span style="color: #FF6600">3</span>, <span style="color: #CC3300">&#39;subsample&#39;</span>: <span style="color: #FF6600">0.5</span>,
          <span style="color: #CC3300">&#39;learning_rate&#39;</span>: <span style="color: #FF6600">0.01</span>, <span style="color: #CC3300">&#39;min_samples_leaf&#39;</span>: <span style="color: #FF6600">1</span>, <span style="color: #CC3300">&#39;random_state&#39;</span>: <span style="color: #FF6600">3</span>}
clf <span style="color: #555555">=</span> ensemble<span style="color: #555555">.</span>GradientBoostingClassifier(<span style="color: #555555">**</span>params)

clf<span style="color: #555555">.</span>fit(X_train, y_train)
acc <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>score(X_test, y_test)
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Accuracy: </span><span style="color: #AA0000">{:.4f}</span><span style="color: #CC3300">&quot;</span><span style="color: #555555">.</span>format(acc))

n_estimators <span style="color: #555555">=</span> params[<span style="color: #CC3300">&#39;n_estimators&#39;</span>]
x <span style="color: #555555">=</span> np<span style="color: #555555">.</span>arange(n_estimators) <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>


<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">heldout_score</span>(clf, X_test, y_test):
    <span style="color: #CC3300; font-style: italic">&quot;&quot;&quot;compute deviance scores on ``X_test`` and ``y_test``. &quot;&quot;&quot;</span>
    score <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros((n_estimators,), dtype<span style="color: #555555">=</span>np<span style="color: #555555">.</span>float64)
    <span style="color: #006699; font-weight: bold">for</span> i, y_pred <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">enumerate</span>(clf<span style="color: #555555">.</span>staged_decision_function(X_test)):
        score[i] <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>loss_(y_test, y_pred)
    <span style="color: #006699; font-weight: bold">return</span> score


<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">cv_estimate</span>(n_folds<span style="color: #555555">=</span><span style="color: #FF6600">3</span>):
    cv <span style="color: #555555">=</span> KFold(n_folds<span style="color: #555555">=</span>n_folds)
    cv_clf <span style="color: #555555">=</span> ensemble<span style="color: #555555">.</span>GradientBoostingClassifier(<span style="color: #555555">**</span>params)
    val_scores <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros((n_estimators,), dtype<span style="color: #555555">=</span>np<span style="color: #555555">.</span>float64)
    <span style="color: #006699; font-weight: bold">for</span> train, test <span style="color: #000000; font-weight: bold">in</span> cv<span style="color: #555555">.</span>split(X_train, y_train):
        cv_clf<span style="color: #555555">.</span>fit(X_train[train], y_train[train])
        val_scores <span style="color: #555555">+=</span> heldout_score(cv_clf, X_train[test], y_train[test])
    val_scores <span style="color: #555555">/=</span> n_folds
    <span style="color: #006699; font-weight: bold">return</span> val_scores


<span style="color: #0099FF; font-style: italic"># Estimate best n_estimator using cross-validation</span>
cv_score <span style="color: #555555">=</span> cv_estimate(<span style="color: #FF6600">3</span>)

<span style="color: #0099FF; font-style: italic"># Compute best n_estimator for test data</span>
test_score <span style="color: #555555">=</span> heldout_score(clf, X_test, y_test)

<span style="color: #0099FF; font-style: italic"># negative cumulative sum of oob improvements</span>
cumsum <span style="color: #555555">=</span> <span style="color: #555555">-</span>np<span style="color: #555555">.</span>cumsum(clf<span style="color: #555555">.</span>oob_improvement_)

<span style="color: #0099FF; font-style: italic"># min loss according to OOB</span>
oob_best_iter <span style="color: #555555">=</span> x[np<span style="color: #555555">.</span>argmin(cumsum)]

<span style="color: #0099FF; font-style: italic"># min loss according to test (normalize such that first loss is 0)</span>
test_score <span style="color: #555555">-=</span> test_score[<span style="color: #FF6600">0</span>]
test_best_iter <span style="color: #555555">=</span> x[np<span style="color: #555555">.</span>argmin(test_score)]

<span style="color: #0099FF; font-style: italic"># min loss according to cv (normalize such that first loss is 0)</span>
cv_score <span style="color: #555555">-=</span> cv_score[<span style="color: #FF6600">0</span>]
cv_best_iter <span style="color: #555555">=</span> x[np<span style="color: #555555">.</span>argmin(cv_score)]
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>Accuracy: 0.6840
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># color brew for the three curves</span>
oob_color <span style="color: #555555">=</span> <span style="color: #336666">list</span>(<span style="color: #336666">map</span>(<span style="color: #006699; font-weight: bold">lambda</span> x: x <span style="color: #555555">/</span> <span style="color: #FF6600">256.0</span>, (<span style="color: #FF6600">190</span>, <span style="color: #FF6600">174</span>, <span style="color: #FF6600">212</span>)))
test_color <span style="color: #555555">=</span> <span style="color: #336666">list</span>(<span style="color: #336666">map</span>(<span style="color: #006699; font-weight: bold">lambda</span> x: x <span style="color: #555555">/</span> <span style="color: #FF6600">256.0</span>, (<span style="color: #FF6600">127</span>, <span style="color: #FF6600">201</span>, <span style="color: #FF6600">127</span>)))
cv_color <span style="color: #555555">=</span> <span style="color: #336666">list</span>(<span style="color: #336666">map</span>(<span style="color: #006699; font-weight: bold">lambda</span> x: x <span style="color: #555555">/</span> <span style="color: #FF6600">256.0</span>, (<span style="color: #FF6600">253</span>, <span style="color: #FF6600">192</span>, <span style="color: #FF6600">134</span>)))

<span style="color: #0099FF; font-style: italic"># plot curves and vertical lines for best iterations</span>
plt<span style="color: #555555">.</span>plot(x, cumsum, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;OOB loss&#39;</span>, color<span style="color: #555555">=</span>oob_color)
plt<span style="color: #555555">.</span>plot(x, test_score, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Test loss&#39;</span>, color<span style="color: #555555">=</span>test_color)
plt<span style="color: #555555">.</span>plot(x, cv_score, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;CV loss&#39;</span>, color<span style="color: #555555">=</span>cv_color)
plt<span style="color: #555555">.</span>axvline(x<span style="color: #555555">=</span>oob_best_iter, color<span style="color: #555555">=</span>oob_color)
plt<span style="color: #555555">.</span>axvline(x<span style="color: #555555">=</span>test_best_iter, color<span style="color: #555555">=</span>test_color)
plt<span style="color: #555555">.</span>axvline(x<span style="color: #555555">=</span>cv_best_iter, color<span style="color: #555555">=</span>cv_color)

<span style="color: #0099FF; font-style: italic"># add three vertical lines to xticks</span>
xticks <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>xticks()
xticks_pos <span style="color: #555555">=</span> np<span style="color: #555555">.</span>array(xticks[<span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>tolist() <span style="color: #555555">+</span>
                      [oob_best_iter, cv_best_iter, test_best_iter])
xticks_label <span style="color: #555555">=</span> np<span style="color: #555555">.</span>array(<span style="color: #336666">list</span>(<span style="color: #336666">map</span>(<span style="color: #006699; font-weight: bold">lambda</span> t: <span style="color: #336666">int</span>(t), xticks[<span style="color: #FF6600">0</span>])) <span style="color: #555555">+</span>
                        [<span style="color: #CC3300">&#39;OOB&#39;</span>, <span style="color: #CC3300">&#39;CV&#39;</span>, <span style="color: #CC3300">&#39;Test&#39;</span>])
ind <span style="color: #555555">=</span> np<span style="color: #555555">.</span>argsort(xticks_pos)
xticks_pos <span style="color: #555555">=</span> xticks_pos[ind]
xticks_label <span style="color: #555555">=</span> xticks_label[ind]
plt<span style="color: #555555">.</span>xticks(xticks_pos, xticks_label)

plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;upper right&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;normalized loss&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;number of iterations&#39;</span>)

plt<span style="color: #555555">.</span>show()
</pre></div>


<hr />
<h3 id="13-prediction-intervals-for-gradient-boosting-regression-source">13. Prediction Intervals for Gradient Boosting Regression (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>This example shows how quantile regression can be used
to create prediction intervals.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> GradientBoostingRegressor
</pre></div>


<ul>
<li>Data</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>seed(<span style="color: #FF6600">1</span>)


<span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">f</span>(x):
    <span style="color: #CC3300; font-style: italic">&quot;&quot;&quot;The function to predict.&quot;&quot;&quot;</span>
    <span style="color: #006699; font-weight: bold">return</span> x <span style="color: #555555">*</span> np<span style="color: #555555">.</span>sin(x)

<span style="color: #0099FF; font-style: italic">#----------------------------------------------------------------------</span>
<span style="color: #0099FF; font-style: italic">#  First the noiseless case</span>
X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>atleast_2d(np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>uniform(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">10.0</span>, size<span style="color: #555555">=</span><span style="color: #FF6600">100</span>))<span style="color: #555555">.</span>T
X <span style="color: #555555">=</span> X<span style="color: #555555">.</span>astype(np<span style="color: #555555">.</span>float32)

<span style="color: #0099FF; font-style: italic"># Observations</span>
y <span style="color: #555555">=</span> f(X)<span style="color: #555555">.</span>ravel()

dy <span style="color: #555555">=</span> <span style="color: #FF6600">1.5</span> <span style="color: #555555">+</span> <span style="color: #FF6600">1.0</span> <span style="color: #555555">*</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>random(y<span style="color: #555555">.</span>shape)
noise <span style="color: #555555">=</span> np<span style="color: #555555">.</span>random<span style="color: #555555">.</span>normal(<span style="color: #FF6600">0</span>, dy)
y <span style="color: #555555">+=</span> noise
y <span style="color: #555555">=</span> y<span style="color: #555555">.</span>astype(np<span style="color: #555555">.</span>float32)

<span style="color: #0099FF; font-style: italic"># Mesh the input space for evaluations of the real function, the prediction and</span>
<span style="color: #0099FF; font-style: italic"># its MSE</span>
xx <span style="color: #555555">=</span> np<span style="color: #555555">.</span>atleast_2d(np<span style="color: #555555">.</span>linspace(<span style="color: #FF6600">0</span>, <span style="color: #FF6600">10</span>, <span style="color: #FF6600">1000</span>))<span style="color: #555555">.</span>T
xx <span style="color: #555555">=</span> xx<span style="color: #555555">.</span>astype(np<span style="color: #555555">.</span>float32)

alpha <span style="color: #555555">=</span> <span style="color: #FF6600">0.95</span>
</pre></div>


<ul>
<li>Model</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>clf <span style="color: #555555">=</span> GradientBoostingRegressor(loss<span style="color: #555555">=</span><span style="color: #CC3300">&#39;quantile&#39;</span>, alpha<span style="color: #555555">=</span>alpha,
                                n_estimators<span style="color: #555555">=</span><span style="color: #FF6600">250</span>, max_depth<span style="color: #555555">=</span><span style="color: #FF6600">3</span>,
                                learning_rate<span style="color: #555555">=.</span><span style="color: #FF6600">1</span>, min_samples_leaf<span style="color: #555555">=</span><span style="color: #FF6600">9</span>,
                                min_samples_split<span style="color: #555555">=</span><span style="color: #FF6600">9</span>)

clf<span style="color: #555555">.</span>fit(X, y)

<span style="color: #0099FF; font-style: italic"># Make the prediction on the meshed x-axis</span>
y_upper <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>predict(xx)

clf<span style="color: #555555">.</span>set_params(alpha<span style="color: #555555">=</span><span style="color: #FF6600">1.0</span> <span style="color: #555555">-</span> alpha)
clf<span style="color: #555555">.</span>fit(X, y)

<span style="color: #0099FF; font-style: italic"># Make the prediction on the meshed x-axis</span>
y_lower <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>predict(xx)

clf<span style="color: #555555">.</span>set_params(loss<span style="color: #555555">=</span><span style="color: #CC3300">&#39;ls&#39;</span>)
clf<span style="color: #555555">.</span>fit(X, y)

<span style="color: #0099FF; font-style: italic"># Make the prediction on the meshed x-axis</span>
y_pred <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>predict(xx)
</pre></div>


<ul>
<li>Plot</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Plot the function, the prediction and the 90% confidence interval based on</span>
<span style="color: #0099FF; font-style: italic"># the MSE</span>
fig <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>figure()
plt<span style="color: #555555">.</span>plot(xx, f(xx), <span style="color: #CC3300">&#39;g:&#39;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">u&#39;$f(x) = x\,\sin(x)$&#39;</span>)
plt<span style="color: #555555">.</span>plot(X, y, <span style="color: #CC3300">&#39;b.&#39;</span>, markersize<span style="color: #555555">=</span><span style="color: #FF6600">10</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">u&#39;Observations&#39;</span>)
plt<span style="color: #555555">.</span>plot(xx, y_pred, <span style="color: #CC3300">&#39;r-&#39;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">u&#39;Prediction&#39;</span>)
plt<span style="color: #555555">.</span>plot(xx, y_upper, <span style="color: #CC3300">&#39;k-&#39;</span>)
plt<span style="color: #555555">.</span>plot(xx, y_lower, <span style="color: #CC3300">&#39;k-&#39;</span>)
plt<span style="color: #555555">.</span>fill(np<span style="color: #555555">.</span>concatenate([xx, xx[::<span style="color: #555555">-</span><span style="color: #FF6600">1</span>]]),
         np<span style="color: #555555">.</span>concatenate([y_upper, y_lower[::<span style="color: #555555">-</span><span style="color: #FF6600">1</span>]]),
         alpha<span style="color: #555555">=.</span><span style="color: #FF6600">5</span>, fc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;b&#39;</span>, ec<span style="color: #555555">=</span><span style="color: #CC3300">&#39;None&#39;</span>, label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;90% prediction interval&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;$x$&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;$f(x)$&#39;</span>)
plt<span style="color: #555555">.</span>ylim(<span style="color: #555555">-</span><span style="color: #FF6600">10</span>, <span style="color: #FF6600">20</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;upper left&#39;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_164_0.png" /></p>
<hr />
<h3 id="14-gradient-boosting-regression-source">14. Gradient Boosting regression (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>Demonstrate Gradient Boosting on the Boston housing dataset.</p>
<p>This example fits a Gradient Boosting model with least squares loss and
500 regression trees of depth 4.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> ensemble
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> datasets
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.utils</span> <span style="color: #006699; font-weight: bold">import</span> shuffle
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.metrics</span> <span style="color: #006699; font-weight: bold">import</span> mean_squared_error
</pre></div>


<ul>
<li>Data</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Load data</span>
boston <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>load_boston()
X, y <span style="color: #555555">=</span> shuffle(boston<span style="color: #555555">.</span>data, boston<span style="color: #555555">.</span>target, random_state<span style="color: #555555">=</span><span style="color: #FF6600">13</span>)
X <span style="color: #555555">=</span> X<span style="color: #555555">.</span>astype(np<span style="color: #555555">.</span>float32)
offset <span style="color: #555555">=</span> <span style="color: #336666">int</span>(X<span style="color: #555555">.</span>shape[<span style="color: #FF6600">0</span>] <span style="color: #555555">*</span> <span style="color: #FF6600">0.9</span>)
X_train, y_train <span style="color: #555555">=</span> X[:offset], y[:offset]
X_test, y_test <span style="color: #555555">=</span> X[offset:], y[offset:]
</pre></div>


<ul>
<li>Model</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Fit regression model</span>
params <span style="color: #555555">=</span> {<span style="color: #CC3300">&#39;n_estimators&#39;</span>: <span style="color: #FF6600">500</span>, <span style="color: #CC3300">&#39;max_depth&#39;</span>: <span style="color: #FF6600">4</span>, <span style="color: #CC3300">&#39;min_samples_split&#39;</span>: <span style="color: #FF6600">2</span>,
          <span style="color: #CC3300">&#39;learning_rate&#39;</span>: <span style="color: #FF6600">0.01</span>, <span style="color: #CC3300">&#39;loss&#39;</span>: <span style="color: #CC3300">&#39;ls&#39;</span>}
clf <span style="color: #555555">=</span> ensemble<span style="color: #555555">.</span>GradientBoostingRegressor(<span style="color: #555555">**</span>params)

clf<span style="color: #555555">.</span>fit(X_train, y_train)
mse <span style="color: #555555">=</span> mean_squared_error(y_test, clf<span style="color: #555555">.</span>predict(X_test))
<span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;MSE: </span><span style="color: #AA0000">%.4f</span><span style="color: #CC3300">&quot;</span> <span style="color: #555555">%</span> mse)
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>MSE: 6.6213
</pre></div>


<ul>
<li>Plot training deviance</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Plot training deviance</span>

<span style="color: #0099FF; font-style: italic"># compute test set deviance</span>
test_score <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros((params[<span style="color: #CC3300">&#39;n_estimators&#39;</span>],), dtype<span style="color: #555555">=</span>np<span style="color: #555555">.</span>float64)

<span style="color: #006699; font-weight: bold">for</span> i, y_pred <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">enumerate</span>(clf<span style="color: #555555">.</span>staged_predict(X_test)):
    test_score[i] <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>loss_(y_test, y_pred)

plt<span style="color: #555555">.</span>figure(figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">12</span>, <span style="color: #FF6600">6</span>))
plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">1</span>, <span style="color: #FF6600">2</span>, <span style="color: #FF6600">1</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Deviance&#39;</span>)
plt<span style="color: #555555">.</span>plot(np<span style="color: #555555">.</span>arange(params[<span style="color: #CC3300">&#39;n_estimators&#39;</span>]) <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>, clf<span style="color: #555555">.</span>train_score_, <span style="color: #CC3300">&#39;b-&#39;</span>,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Training Set Deviance&#39;</span>)
plt<span style="color: #555555">.</span>plot(np<span style="color: #555555">.</span>arange(params[<span style="color: #CC3300">&#39;n_estimators&#39;</span>]) <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>, test_score, <span style="color: #CC3300">&#39;r-&#39;</span>,
         label<span style="color: #555555">=</span><span style="color: #CC3300">&#39;Test Set Deviance&#39;</span>)
plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;upper right&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;Boosting Iterations&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Deviance&#39;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_175_0.png" /></p>
<ul>
<li>Plot</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Plot feature importance</span>
feature_importance <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>feature_importances_
<span style="color: #0099FF; font-style: italic"># make importances relative to max importance</span>
feature_importance <span style="color: #555555">=</span> <span style="color: #FF6600">100.0</span> <span style="color: #555555">*</span> (feature_importance <span style="color: #555555">/</span> feature_importance<span style="color: #555555">.</span>max())
sorted_idx <span style="color: #555555">=</span> np<span style="color: #555555">.</span>argsort(feature_importance)
pos <span style="color: #555555">=</span> np<span style="color: #555555">.</span>arange(sorted_idx<span style="color: #555555">.</span>shape[<span style="color: #FF6600">0</span>]) <span style="color: #555555">+</span> <span style="color: #555555">.</span><span style="color: #FF6600">5</span>
plt<span style="color: #555555">.</span>subplot(<span style="color: #FF6600">1</span>, <span style="color: #FF6600">2</span>, <span style="color: #FF6600">2</span>)
plt<span style="color: #555555">.</span>barh(pos, feature_importance[sorted_idx], align<span style="color: #555555">=</span><span style="color: #CC3300">&#39;center&#39;</span>)
plt<span style="color: #555555">.</span>yticks(pos, boston<span style="color: #555555">.</span>feature_names[sorted_idx])
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;Relative Importance&#39;</span>)
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Variable Importance&#39;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_177_0.png" /></p>
<hr />
<h3 id="15-gradient-boosting-regularization-source">15. Gradient Boosting regularization (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>Illustration of the effect of different regularization strategies
for Gradient Boosting. The example is taken from Hastie et al 2009.</p>
<p>The loss function used is binomial deviance. Regularization via
shrinkage (<code>learning_rate &lt; 1.0</code>) improves performance considerably.
In combination with shrinkage, stochastic gradient boosting
(<code>subsample &lt; 1.0</code>) can produce more accurate models by reducing the
variance via bagging.
Subsampling without shrinkage usually does poorly.
Another strategy to reduce the variance is by subsampling the features
analogous to the random splits in Random Forests
(via the <code>max_features</code> parameter).</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> ensemble
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> datasets
</pre></div>


<ul>
<li>Data</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>X, y <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>make_hastie_10_2(n_samples<span style="color: #555555">=</span><span style="color: #FF6600">12000</span>, random_state<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)
X <span style="color: #555555">=</span> X<span style="color: #555555">.</span>astype(np<span style="color: #555555">.</span>float32)

<span style="color: #0099FF; font-style: italic"># map labels from {-1, 1} to {0, 1}</span>
labels, y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>unique(y, return_inverse<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>)

X_train, X_test <span style="color: #555555">=</span> X[:<span style="color: #FF6600">2000</span>], X[<span style="color: #FF6600">2000</span>:]
y_train, y_test <span style="color: #555555">=</span> y[:<span style="color: #FF6600">2000</span>], y[<span style="color: #FF6600">2000</span>:]

original_params <span style="color: #555555">=</span> {<span style="color: #CC3300">&#39;n_estimators&#39;</span>: <span style="color: #FF6600">1000</span>, <span style="color: #CC3300">&#39;max_leaf_nodes&#39;</span>: <span style="color: #FF6600">4</span>, <span style="color: #CC3300">&#39;max_depth&#39;</span>: <span style="color: #006699; font-weight: bold">None</span>, <span style="color: #CC3300">&#39;random_state&#39;</span>: <span style="color: #FF6600">2</span>,
                   <span style="color: #CC3300">&#39;min_samples_split&#39;</span>: <span style="color: #FF6600">5</span>}
</pre></div>


<ul>
<li>Plot n  Model</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>plt<span style="color: #555555">.</span>figure()

<span style="color: #006699; font-weight: bold">for</span> label, color, setting <span style="color: #000000; font-weight: bold">in</span> [(<span style="color: #CC3300">&#39;No shrinkage&#39;</span>, <span style="color: #CC3300">&#39;orange&#39;</span>,
                               {<span style="color: #CC3300">&#39;learning_rate&#39;</span>: <span style="color: #FF6600">1.0</span>, <span style="color: #CC3300">&#39;subsample&#39;</span>: <span style="color: #FF6600">1.0</span>}),
                              (<span style="color: #CC3300">&#39;learning_rate=0.1&#39;</span>, <span style="color: #CC3300">&#39;turquoise&#39;</span>,
                               {<span style="color: #CC3300">&#39;learning_rate&#39;</span>: <span style="color: #FF6600">0.1</span>, <span style="color: #CC3300">&#39;subsample&#39;</span>: <span style="color: #FF6600">1.0</span>}),
                              (<span style="color: #CC3300">&#39;subsample=0.5&#39;</span>, <span style="color: #CC3300">&#39;blue&#39;</span>,
                               {<span style="color: #CC3300">&#39;learning_rate&#39;</span>: <span style="color: #FF6600">1.0</span>, <span style="color: #CC3300">&#39;subsample&#39;</span>: <span style="color: #FF6600">0.5</span>}),
                              (<span style="color: #CC3300">&#39;learning_rate=0.1, subsample=0.5&#39;</span>, <span style="color: #CC3300">&#39;gray&#39;</span>,
                               {<span style="color: #CC3300">&#39;learning_rate&#39;</span>: <span style="color: #FF6600">0.1</span>, <span style="color: #CC3300">&#39;subsample&#39;</span>: <span style="color: #FF6600">0.5</span>}),
                              (<span style="color: #CC3300">&#39;learning_rate=0.1, max_features=2&#39;</span>, <span style="color: #CC3300">&#39;magenta&#39;</span>,
                               {<span style="color: #CC3300">&#39;learning_rate&#39;</span>: <span style="color: #FF6600">0.1</span>, <span style="color: #CC3300">&#39;max_features&#39;</span>: <span style="color: #FF6600">2</span>})]:
    params <span style="color: #555555">=</span> <span style="color: #336666">dict</span>(original_params)
    params<span style="color: #555555">.</span>update(setting)

    clf <span style="color: #555555">=</span> ensemble<span style="color: #555555">.</span>GradientBoostingClassifier(<span style="color: #555555">**</span>params)
    clf<span style="color: #555555">.</span>fit(X_train, y_train)

    <span style="color: #0099FF; font-style: italic"># compute test set deviance</span>
    test_deviance <span style="color: #555555">=</span> np<span style="color: #555555">.</span>zeros((params[<span style="color: #CC3300">&#39;n_estimators&#39;</span>],), dtype<span style="color: #555555">=</span>np<span style="color: #555555">.</span>float64)

    <span style="color: #006699; font-weight: bold">for</span> i, y_pred <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">enumerate</span>(clf<span style="color: #555555">.</span>staged_decision_function(X_test)):
        <span style="color: #0099FF; font-style: italic"># clf.loss_ assumes that y_test[i] in {0, 1}</span>
        test_deviance[i] <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>loss_(y_test, y_pred)

    plt<span style="color: #555555">.</span>plot((np<span style="color: #555555">.</span>arange(test_deviance<span style="color: #555555">.</span>shape[<span style="color: #FF6600">0</span>]) <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>)[::<span style="color: #FF6600">5</span>], test_deviance[::<span style="color: #FF6600">5</span>],
            <span style="color: #CC3300">&#39;-&#39;</span>, color<span style="color: #555555">=</span>color, label<span style="color: #555555">=</span>label)

plt<span style="color: #555555">.</span>legend(loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;upper left&#39;</span>)
plt<span style="color: #555555">.</span>xlabel(<span style="color: #CC3300">&#39;Boosting Iterations&#39;</span>)
plt<span style="color: #555555">.</span>ylabel(<span style="color: #CC3300">&#39;Test Set Deviance&#39;</span>)

plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_186_0.png" /></p>
<hr />
<h3 id="16-partial-dependence-plots-source">16. Partial Dependence Plots (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>Partial dependence plots show the dependence between the target function [2]_
and a set of 'target' features, marginalizing over the
values of all other features (the complement features). Due to the limits
of human perception the size of the target feature set must be small (usually,
one or two) thus the target features are usually chosen among the most
important features
(see :attr:<code>~sklearn.ensemble.GradientBoostingRegressor.feature_importances_</code>).</p>
<p>This example shows how to obtain partial dependence plots from a
:class:<code>~sklearn.ensemble.GradientBoostingRegressor</code> trained on the California
housing dataset. The example is taken from [1]_.</p>
<p>The plot shows four one-way and one two-way partial dependence plots.
The target variables for the one-way PDP are:
median income (<code>MedInc</code>), avg. occupants per household (<code>AvgOccup</code>),
median house age (<code>HouseAge</code>), and avg. rooms per household (<code>AveRooms</code>).</p>
<p>We can clearly see that the median house price shows a linear relationship
with the median income (top left) and that the house price drops when the
avg. occupants per household increases (top middle).
The top right plot shows that the house age in a district does not have
a strong influence on the (median) house price; so does the average rooms
per household.
The tick marks on the x-axis represent the deciles of the feature values
in the training data.</p>
<p>Partial dependence plots with two target features enable us to visualize
interactions among them. The two-way partial dependence plot shows the
dependence of median house price on joint values of house age and avg.
occupants per household. We can clearly see an interaction between the
two features:
For an avg. occupancy greater than two, the house price is nearly independent
of the house age, whereas for values less than two there is a strong dependence
on age.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">__future__</span> <span style="color: #006699; font-weight: bold">import</span> print_function


<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">mpl_toolkits.mplot3d</span> <span style="color: #006699; font-weight: bold">import</span> Axes3D

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.cross_validation</span> <span style="color: #006699; font-weight: bold">import</span> train_test_split
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> GradientBoostingRegressor
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble.partial_dependence</span> <span style="color: #006699; font-weight: bold">import</span> plot_partial_dependence
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble.partial_dependence</span> <span style="color: #006699; font-weight: bold">import</span> partial_dependence
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.datasets.california_housing</span> <span style="color: #006699; font-weight: bold">import</span> fetch_california_housing
</pre></div>


<ul>
<li>Alltogether</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">def</span> <span style="color: #CC00FF">main</span>():
    cal_housing <span style="color: #555555">=</span> fetch_california_housing()

    <span style="color: #0099FF; font-style: italic"># split 80/20 train-test</span>
    X_train, X_test, y_train, y_test <span style="color: #555555">=</span> train_test_split(cal_housing<span style="color: #555555">.</span>data,
                                                        cal_housing<span style="color: #555555">.</span>target,
                                                        test_size<span style="color: #555555">=</span><span style="color: #FF6600">0.2</span>,
                                                        random_state<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)
    names <span style="color: #555555">=</span> cal_housing<span style="color: #555555">.</span>feature_names

    <span style="color: #336666">print</span>(<span style="color: #CC3300">&quot;Training GBRT...&quot;</span>, flush<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>, end<span style="color: #555555">=</span><span style="color: #CC3300">&#39;&#39;</span>)
    clf <span style="color: #555555">=</span> GradientBoostingRegressor(n_estimators<span style="color: #555555">=</span><span style="color: #FF6600">100</span>, max_depth<span style="color: #555555">=</span><span style="color: #FF6600">4</span>,
                                    learning_rate<span style="color: #555555">=</span><span style="color: #FF6600">0.1</span>, loss<span style="color: #555555">=</span><span style="color: #CC3300">&#39;huber&#39;</span>,
                                    random_state<span style="color: #555555">=</span><span style="color: #FF6600">1</span>)
    clf<span style="color: #555555">.</span>fit(X_train, y_train)
    <span style="color: #336666">print</span>(<span style="color: #CC3300">&quot; done.&quot;</span>)

    <span style="color: #336666">print</span>(<span style="color: #CC3300">&#39;Convenience plot with ``partial_dependence_plots``&#39;</span>)

    features <span style="color: #555555">=</span> [<span style="color: #FF6600">0</span>, <span style="color: #FF6600">5</span>, <span style="color: #FF6600">1</span>, <span style="color: #FF6600">2</span>, (<span style="color: #FF6600">5</span>, <span style="color: #FF6600">1</span>)]
    fig, axs <span style="color: #555555">=</span> plot_partial_dependence(clf, X_train, features,
                                       feature_names<span style="color: #555555">=</span>names,
                                       n_jobs<span style="color: #555555">=</span><span style="color: #FF6600">3</span>, grid_resolution<span style="color: #555555">=</span><span style="color: #FF6600">50</span>)
    fig<span style="color: #555555">.</span>suptitle(<span style="color: #CC3300">&#39;Partial dependence of house value on nonlocation features</span><span style="color: #CC3300; font-weight: bold">\n</span><span style="color: #CC3300">&#39;</span>
                 <span style="color: #CC3300">&#39;for the California housing dataset&#39;</span>)
    plt<span style="color: #555555">.</span>subplots_adjust(top<span style="color: #555555">=</span><span style="color: #FF6600">0.9</span>)  <span style="color: #0099FF; font-style: italic"># tight_layout causes overlap with suptitle</span>

    <span style="color: #336666">print</span>(<span style="color: #CC3300">&#39;Custom 3d plot via ``partial_dependence``&#39;</span>)
    fig <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>figure()

    target_feature <span style="color: #555555">=</span> (<span style="color: #FF6600">1</span>, <span style="color: #FF6600">5</span>)
    pdp, axes <span style="color: #555555">=</span> partial_dependence(clf, target_feature,
                                   X<span style="color: #555555">=</span>X_train, grid_resolution<span style="color: #555555">=</span><span style="color: #FF6600">50</span>)
    XX, YY <span style="color: #555555">=</span> np<span style="color: #555555">.</span>meshgrid(axes[<span style="color: #FF6600">0</span>], axes[<span style="color: #FF6600">1</span>])
    Z <span style="color: #555555">=</span> pdp[<span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>reshape(<span style="color: #336666">list</span>(<span style="color: #336666">map</span>(np<span style="color: #555555">.</span>size, axes)))<span style="color: #555555">.</span>T
    ax <span style="color: #555555">=</span> Axes3D(fig)
    surf <span style="color: #555555">=</span> ax<span style="color: #555555">.</span>plot_surface(XX, YY, Z, rstride<span style="color: #555555">=</span><span style="color: #FF6600">1</span>, cstride<span style="color: #555555">=</span><span style="color: #FF6600">1</span>, cmap<span style="color: #555555">=</span>plt<span style="color: #555555">.</span>cm<span style="color: #555555">.</span>BuPu)
    ax<span style="color: #555555">.</span>set_xlabel(names[target_feature[<span style="color: #FF6600">0</span>]])
    ax<span style="color: #555555">.</span>set_ylabel(names[target_feature[<span style="color: #FF6600">1</span>]])
    ax<span style="color: #555555">.</span>set_zlabel(<span style="color: #CC3300">&#39;Partial dependence&#39;</span>)
    <span style="color: #0099FF; font-style: italic">#  pretty init view</span>
    ax<span style="color: #555555">.</span>view_init(elev<span style="color: #555555">=</span><span style="color: #FF6600">22</span>, azim<span style="color: #555555">=</span><span style="color: #FF6600">122</span>)
    plt<span style="color: #555555">.</span>colorbar(surf)
    plt<span style="color: #555555">.</span>suptitle(<span style="color: #CC3300">&#39;Partial dependence of house value on median age and &#39;</span>
                 <span style="color: #CC3300">&#39;average occupancy&#39;</span>)
    plt<span style="color: #555555">.</span>subplots_adjust(top<span style="color: #555555">=</span><span style="color: #FF6600">0.9</span>)

    plt<span style="color: #555555">.</span>show()


<span style="color: #0099FF; font-style: italic"># Needed on Windows because plot_partial_dependence uses multiprocessing</span>
<span style="color: #006699; font-weight: bold">if</span> <span style="color: #003333">__name__</span> <span style="color: #555555">==</span> <span style="color: #CC3300">&#39;__main__&#39;</span>:
    main()
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /home/bitnami/scikit_learn_data


Training GBRT... done.
Convenience plot with ``partial_dependence_plots``
Custom 3d plot via ``partial_dependence``
</pre></div>


<p><img alt="png" src="../output_193_2.png" /></p>
<p><img alt="png" src="../output_193_3.png" /></p>
<hr />
<h3 id="17-plot-the-decision-boundaries-of-a-votingclassifier-source">17. Plot the decision boundaries of a VotingClassifier (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>Plot the decision boundaries of a <code>VotingClassifier</code> for
two features of the Iris dataset.</p>
<p>Plot the class probabilities of the first sample in a toy dataset
predicted by three different classifiers and averaged by the
<code>VotingClassifier</code>.</p>
<p>First, three exemplary classifiers are initialized (<code>DecisionTreeClassifier</code>,
<code>KNeighborsClassifier</code>, and <code>SVC</code>) and used to initialize a
soft-voting <code>VotingClassifier</code> with weights <code>[2, 1, 2]</code>, which means that
the predicted probabilities of the <code>DecisionTreeClassifier</code> and <code>SVC</code>
count 5 times as much as the weights of the <code>KNeighborsClassifier</code> classifier
when the averaged probability is calculated.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">itertools</span> <span style="color: #006699; font-weight: bold">import</span> product

<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn</span> <span style="color: #006699; font-weight: bold">import</span> datasets
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.tree</span> <span style="color: #006699; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.neighbors</span> <span style="color: #006699; font-weight: bold">import</span> KNeighborsClassifier
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.svm</span> <span style="color: #006699; font-weight: bold">import</span> SVC
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> VotingClassifier
</pre></div>


<ul>
<li>Data</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Loading some example data</span>
iris <span style="color: #555555">=</span> datasets<span style="color: #555555">.</span>load_iris()
X <span style="color: #555555">=</span> iris<span style="color: #555555">.</span>data[:, [<span style="color: #FF6600">0</span>, <span style="color: #FF6600">2</span>]]
y <span style="color: #555555">=</span> iris<span style="color: #555555">.</span>target
</pre></div>


<ul>
<li>Model</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Training classifiers</span>
clf1 <span style="color: #555555">=</span> DecisionTreeClassifier(max_depth<span style="color: #555555">=</span><span style="color: #FF6600">4</span>)
clf2 <span style="color: #555555">=</span> KNeighborsClassifier(n_neighbors<span style="color: #555555">=</span><span style="color: #FF6600">7</span>)
clf3 <span style="color: #555555">=</span> SVC(kernel<span style="color: #555555">=</span><span style="color: #CC3300">&#39;rbf&#39;</span>, probability<span style="color: #555555">=</span><span style="color: #006699; font-weight: bold">True</span>)
eclf <span style="color: #555555">=</span> VotingClassifier(estimators<span style="color: #555555">=</span>[(<span style="color: #CC3300">&#39;dt&#39;</span>, clf1), (<span style="color: #CC3300">&#39;knn&#39;</span>, clf2),
                                    (<span style="color: #CC3300">&#39;svc&#39;</span>, clf3)],
                        voting<span style="color: #555555">=</span><span style="color: #CC3300">&#39;soft&#39;</span>, weights<span style="color: #555555">=</span>[<span style="color: #FF6600">2</span>, <span style="color: #FF6600">1</span>, <span style="color: #FF6600">2</span>])

clf1<span style="color: #555555">.</span>fit(X, y)
clf2<span style="color: #555555">.</span>fit(X, y)
clf3<span style="color: #555555">.</span>fit(X, y)
eclf<span style="color: #555555">.</span>fit(X, y)
</pre></div>


<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>VotingClassifier(estimators=[(&#39;dt&#39;, DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=4,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter=&#39;best&#39;)), (&#39;knn&#39;, ...&#39;,
  max_iter=-1, probability=True, random_state=None, shrinking=True,
  tol=0.001, verbose=False))],
         voting=&#39;soft&#39;, weights=[2, 1, 2])
</pre></div>


<ul>
<li>Plot</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># Plotting decision regions</span>
x_min, x_max <span style="color: #555555">=</span> X[:, <span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>min() <span style="color: #555555">-</span> <span style="color: #FF6600">1</span>, X[:, <span style="color: #FF6600">0</span>]<span style="color: #555555">.</span>max() <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>
y_min, y_max <span style="color: #555555">=</span> X[:, <span style="color: #FF6600">1</span>]<span style="color: #555555">.</span>min() <span style="color: #555555">-</span> <span style="color: #FF6600">1</span>, X[:, <span style="color: #FF6600">1</span>]<span style="color: #555555">.</span>max() <span style="color: #555555">+</span> <span style="color: #FF6600">1</span>
xx, yy <span style="color: #555555">=</span> np<span style="color: #555555">.</span>meshgrid(np<span style="color: #555555">.</span>arange(x_min, x_max, <span style="color: #FF6600">0.1</span>),
                     np<span style="color: #555555">.</span>arange(y_min, y_max, <span style="color: #FF6600">0.1</span>))

f, axarr <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>subplots(<span style="color: #FF6600">2</span>, <span style="color: #FF6600">2</span>, sharex<span style="color: #555555">=</span><span style="color: #CC3300">&#39;col&#39;</span>, sharey<span style="color: #555555">=</span><span style="color: #CC3300">&#39;row&#39;</span>, figsize<span style="color: #555555">=</span>(<span style="color: #FF6600">10</span>, <span style="color: #FF6600">8</span>))

<span style="color: #006699; font-weight: bold">for</span> idx, clf, tt <span style="color: #000000; font-weight: bold">in</span> <span style="color: #336666">zip</span>(product([<span style="color: #FF6600">0</span>, <span style="color: #FF6600">1</span>], [<span style="color: #FF6600">0</span>, <span style="color: #FF6600">1</span>]),
                        [clf1, clf2, clf3, eclf],
                        [<span style="color: #CC3300">&#39;Decision Tree (depth=4)&#39;</span>, <span style="color: #CC3300">&#39;KNN (k=7)&#39;</span>,
                         <span style="color: #CC3300">&#39;Kernel SVM&#39;</span>, <span style="color: #CC3300">&#39;Soft Voting&#39;</span>]):

    Z <span style="color: #555555">=</span> clf<span style="color: #555555">.</span>predict(np<span style="color: #555555">.</span>c_[xx<span style="color: #555555">.</span>ravel(), yy<span style="color: #555555">.</span>ravel()])
    Z <span style="color: #555555">=</span> Z<span style="color: #555555">.</span>reshape(xx<span style="color: #555555">.</span>shape)

    axarr[idx[<span style="color: #FF6600">0</span>], idx[<span style="color: #FF6600">1</span>]]<span style="color: #555555">.</span>contourf(xx, yy, Z, alpha<span style="color: #555555">=</span><span style="color: #FF6600">0.4</span>)
    axarr[idx[<span style="color: #FF6600">0</span>], idx[<span style="color: #FF6600">1</span>]]<span style="color: #555555">.</span>scatter(X[:, <span style="color: #FF6600">0</span>], X[:, <span style="color: #FF6600">1</span>], c<span style="color: #555555">=</span>y, alpha<span style="color: #555555">=</span><span style="color: #FF6600">0.8</span>)
    axarr[idx[<span style="color: #FF6600">0</span>], idx[<span style="color: #FF6600">1</span>]]<span style="color: #555555">.</span>set_title(tt)

plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_204_0.png" /></p>
<hr />
<h3 id="18-plot-class-probabilities-calculated-by-the-votingclassifier-source">18. Plot class probabilities calculated by the VotingClassifier (<a href="https://github.com/scikit-learn/scikit-learn/tree/master/examples/ensemble">source</a>)</h3>
<hr />
<p>Plot the class probabilities of the first sample in a toy dataset
predicted by three different classifiers and averaged by the
<code>VotingClassifier</code>.</p>
<p>First, three examplary classifiers are initialized (<code>LogisticRegression</code>,
<code>GaussianNB</code>, and <code>RandomForestClassifier</code>) and used to initialize a
soft-voting <code>VotingClassifier</code> with weights <code>[1, 1, 5]</code>, which means that
the predicted probabilities of the <code>RandomForestClassifier</code> count 5 times
as much as the weights of the other classifiers when the averaged probability
is calculated.</p>
<p>To visualize the probability weighting, we fit each classifier on the training
set and plot the predicted class probabilities for the first sample in this
example dataset.</p>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">numpy</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">np</span>
<span style="color: #006699; font-weight: bold">import</span> <span style="color: #00CCFF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #006699; font-weight: bold">as</span> <span style="color: #00CCFF; font-weight: bold">plt</span>

<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.linear_model</span> <span style="color: #006699; font-weight: bold">import</span> LogisticRegression
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.naive_bayes</span> <span style="color: #006699; font-weight: bold">import</span> GaussianNB
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> RandomForestClassifier
<span style="color: #006699; font-weight: bold">from</span> <span style="color: #00CCFF; font-weight: bold">sklearn.ensemble</span> <span style="color: #006699; font-weight: bold">import</span> VotingClassifier
</pre></div>


<ul>
<li>Model </li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span>clf1 <span style="color: #555555">=</span> LogisticRegression(random_state<span style="color: #555555">=</span><span style="color: #FF6600">123</span>)
clf2 <span style="color: #555555">=</span> RandomForestClassifier(random_state<span style="color: #555555">=</span><span style="color: #FF6600">123</span>)
clf3 <span style="color: #555555">=</span> GaussianNB()
X <span style="color: #555555">=</span> np<span style="color: #555555">.</span>array([[<span style="color: #555555">-</span><span style="color: #FF6600">1.0</span>, <span style="color: #555555">-</span><span style="color: #FF6600">1.0</span>], [<span style="color: #555555">-</span><span style="color: #FF6600">1.2</span>, <span style="color: #555555">-</span><span style="color: #FF6600">1.4</span>], [<span style="color: #555555">-</span><span style="color: #FF6600">3.4</span>, <span style="color: #555555">-</span><span style="color: #FF6600">2.2</span>], [<span style="color: #FF6600">1.1</span>, <span style="color: #FF6600">1.2</span>]])
y <span style="color: #555555">=</span> np<span style="color: #555555">.</span>array([<span style="color: #FF6600">1</span>, <span style="color: #FF6600">1</span>, <span style="color: #FF6600">2</span>, <span style="color: #FF6600">2</span>])

eclf <span style="color: #555555">=</span> VotingClassifier(estimators<span style="color: #555555">=</span>[(<span style="color: #CC3300">&#39;lr&#39;</span>, clf1), (<span style="color: #CC3300">&#39;rf&#39;</span>, clf2), (<span style="color: #CC3300">&#39;gnb&#39;</span>, clf3)],
                        voting<span style="color: #555555">=</span><span style="color: #CC3300">&#39;soft&#39;</span>,
                        weights<span style="color: #555555">=</span>[<span style="color: #FF6600">1</span>, <span style="color: #FF6600">1</span>, <span style="color: #FF6600">5</span>])
</pre></div>


<ul>
<li>Predict </li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># predict class probabilities for all classifiers</span>
probas <span style="color: #555555">=</span> [c<span style="color: #555555">.</span>fit(X, y)<span style="color: #555555">.</span>predict_proba(X) <span style="color: #006699; font-weight: bold">for</span> c <span style="color: #000000; font-weight: bold">in</span> (clf1, clf2, clf3, eclf)]

<span style="color: #0099FF; font-style: italic"># get class probabilities for the first sample in the dataset</span>
class1_1 <span style="color: #555555">=</span> [pr[<span style="color: #FF6600">0</span>, <span style="color: #FF6600">0</span>] <span style="color: #006699; font-weight: bold">for</span> pr <span style="color: #000000; font-weight: bold">in</span> probas]
class2_1 <span style="color: #555555">=</span> [pr[<span style="color: #FF6600">0</span>, <span style="color: #FF6600">1</span>] <span style="color: #006699; font-weight: bold">for</span> pr <span style="color: #000000; font-weight: bold">in</span> probas]
</pre></div>


<ul>
<li>Plot</li>
</ul>
<div class="codehilite" style="background: #f0f3f3"><pre style="line-height: 125%"><span></span><span style="color: #0099FF; font-style: italic"># plotting</span>

N <span style="color: #555555">=</span> <span style="color: #FF6600">4</span>  <span style="color: #0099FF; font-style: italic"># number of groups</span>
ind <span style="color: #555555">=</span> np<span style="color: #555555">.</span>arange(N)  <span style="color: #0099FF; font-style: italic"># group positions</span>
width <span style="color: #555555">=</span> <span style="color: #FF6600">0.35</span>  <span style="color: #0099FF; font-style: italic"># bar width</span>

fig, ax <span style="color: #555555">=</span> plt<span style="color: #555555">.</span>subplots()

<span style="color: #0099FF; font-style: italic"># bars for classifier 1-3</span>
p1 <span style="color: #555555">=</span> ax<span style="color: #555555">.</span>bar(ind, np<span style="color: #555555">.</span>hstack(([class1_1[:<span style="color: #555555">-</span><span style="color: #FF6600">1</span>], [<span style="color: #FF6600">0</span>]])), width, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;green&#39;</span>)
p2 <span style="color: #555555">=</span> ax<span style="color: #555555">.</span>bar(ind <span style="color: #555555">+</span> width, np<span style="color: #555555">.</span>hstack(([class2_1[:<span style="color: #555555">-</span><span style="color: #FF6600">1</span>], [<span style="color: #FF6600">0</span>]])), width, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;lightgreen&#39;</span>)

<span style="color: #0099FF; font-style: italic"># bars for VotingClassifier</span>
p3 <span style="color: #555555">=</span> ax<span style="color: #555555">.</span>bar(ind, [<span style="color: #FF6600">0</span>, <span style="color: #FF6600">0</span>, <span style="color: #FF6600">0</span>, class1_1[<span style="color: #555555">-</span><span style="color: #FF6600">1</span>]], width, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;blue&#39;</span>)
p4 <span style="color: #555555">=</span> ax<span style="color: #555555">.</span>bar(ind <span style="color: #555555">+</span> width, [<span style="color: #FF6600">0</span>, <span style="color: #FF6600">0</span>, <span style="color: #FF6600">0</span>, class2_1[<span style="color: #555555">-</span><span style="color: #FF6600">1</span>]], width, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;steelblue&#39;</span>)

<span style="color: #0099FF; font-style: italic"># plot annotations</span>
plt<span style="color: #555555">.</span>axvline(<span style="color: #FF6600">2.8</span>, color<span style="color: #555555">=</span><span style="color: #CC3300">&#39;k&#39;</span>, linestyle<span style="color: #555555">=</span><span style="color: #CC3300">&#39;dashed&#39;</span>)
ax<span style="color: #555555">.</span>set_xticks(ind <span style="color: #555555">+</span> width)
ax<span style="color: #555555">.</span>set_xticklabels([<span style="color: #CC3300">&#39;LogisticRegression</span><span style="color: #CC3300; font-weight: bold">\n</span><span style="color: #CC3300">weight 1&#39;</span>,
                    <span style="color: #CC3300">&#39;GaussianNB</span><span style="color: #CC3300; font-weight: bold">\n</span><span style="color: #CC3300">weight 1&#39;</span>,
                    <span style="color: #CC3300">&#39;RandomForestClassifier</span><span style="color: #CC3300; font-weight: bold">\n</span><span style="color: #CC3300">weight 5&#39;</span>,
                    <span style="color: #CC3300">&#39;VotingClassifier</span><span style="color: #CC3300; font-weight: bold">\n</span><span style="color: #CC3300">(average probabilities)&#39;</span>],
                   rotation<span style="color: #555555">=</span><span style="color: #FF6600">40</span>,
                   ha<span style="color: #555555">=</span><span style="color: #CC3300">&#39;right&#39;</span>)
plt<span style="color: #555555">.</span>ylim([<span style="color: #FF6600">0</span>, <span style="color: #FF6600">1</span>])
plt<span style="color: #555555">.</span>title(<span style="color: #CC3300">&#39;Class probabilities for sample 1 by different classifiers&#39;</span>)
plt<span style="color: #555555">.</span>legend([p1[<span style="color: #FF6600">0</span>], p2[<span style="color: #FF6600">0</span>]], [<span style="color: #CC3300">&#39;class 1&#39;</span>, <span style="color: #CC3300">&#39;class 2&#39;</span>], loc<span style="color: #555555">=</span><span style="color: #CC3300">&#39;upper left&#39;</span>)
plt<span style="color: #555555">.</span>show()
</pre></div>


<p><img alt="png" src="../output_215_0.png" /></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../SVM/svm/" class="btn btn-neutral float-right" title="Support Vector Machine">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../Tree/Tree/" class="btn btn-neutral" title="Decision Tree"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../Tree/Tree/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../SVM/svm/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../../../..';</script>
    <script src="../../../../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../../../../search/main.js" defer></script>

</body>
</html>
